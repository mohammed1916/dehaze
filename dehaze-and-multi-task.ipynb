{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc25307",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-19T07:17:42.112406Z",
     "iopub.status.busy": "2025-05-19T07:17:42.111800Z",
     "iopub.status.idle": "2025-05-19T07:17:56.705604Z",
     "shell.execute_reply": "2025-05-19T07:17:56.705039Z"
    },
    "papermill": {
     "duration": 14.626495,
     "end_time": "2025-05-19T07:17:56.706938",
     "exception": false,
     "start_time": "2025-05-19T07:17:42.080443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import fnmatch\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from math import log10\n",
    "from random import randrange\n",
    "\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.utils.data.sampler as sampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.init import trunc_normal_, _calculate_fan_in_and_fan_out\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from skimage import measure\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "from torchvision.models import vgg16\n",
    "import torchvision.utils as utils\n",
    "\n",
    "from timm.layers import to_2tuple, trunc_normal_\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9877fa42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:17:56.761299Z",
     "iopub.status.busy": "2025-05-19T07:17:56.760604Z",
     "iopub.status.idle": "2025-05-19T07:17:56.819125Z",
     "shell.execute_reply": "2025-05-19T07:17:56.818344Z"
    },
    "papermill": {
     "duration": 0.086719,
     "end_time": "2025-05-19T07:17:56.820473",
     "exception": false,
     "start_time": "2025-05-19T07:17:56.733754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40127466",
   "metadata": {
    "papermill": {
     "duration": 0.026151,
     "end_time": "2025-05-19T07:17:56.874254",
     "exception": false,
     "start_time": "2025-05-19T07:17:56.848103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CityScapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd41ff74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:17:56.973029Z",
     "iopub.status.busy": "2025-05-19T07:17:56.972493Z",
     "iopub.status.idle": "2025-05-19T07:17:56.978846Z",
     "shell.execute_reply": "2025-05-19T07:17:56.978128Z"
    },
    "papermill": {
     "duration": 0.07939,
     "end_time": "2025-05-19T07:17:56.979946",
     "exception": false,
     "start_time": "2025-05-19T07:17:56.900556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CityScapes(Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        self.train = train\n",
    "        self.root = os.path.expanduser(root)\n",
    "\n",
    "        # read the data file\n",
    "        if train:\n",
    "            self.data_path = os.path.join(root, 'train')\n",
    "        else:\n",
    "            self.data_path = os.path.join(root, 'val')\n",
    "\n",
    "        # calculate data length\n",
    "        self.data_len = len(fnmatch.filter(os.listdir(os.path.join(self.data_path, 'image')), '*.npy'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load data from the pre-processed npy files\n",
    "        image = torch.from_numpy(np.moveaxis(np.load(os.path.join(self.data_path, 'image', '{:d}.npy'.format(index))), -1, 0))\n",
    "        semantic = torch.from_numpy(np.load(os.path.join(self.data_path, 'label', '{:d}.npy'.format(index))))\n",
    "        depth = torch.from_numpy(np.moveaxis(np.load(os.path.join(self.data_path, 'depth', '{:d}.npy'.format(index))), -1, 0))\n",
    "        \n",
    "        return {'image': image.float(), 'semantic': semantic.float(), 'depth': depth.float()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33bde968",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:17:57.033377Z",
     "iopub.status.busy": "2025-05-19T07:17:57.032819Z",
     "iopub.status.idle": "2025-05-19T07:17:57.038807Z",
     "shell.execute_reply": "2025-05-19T07:17:57.038154Z"
    },
    "papermill": {
     "duration": 0.034015,
     "end_time": "2025-05-19T07:17:57.039963",
     "exception": false,
     "start_time": "2025-05-19T07:17:57.005948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CityScapesPrep(Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        self.train = train\n",
    "        self.root = os.path.expanduser(root)\n",
    "\n",
    "        # read the data file\n",
    "        if train:\n",
    "            self.data_path = os.path.join(root, 'train')\n",
    "        else:\n",
    "            self.data_path = os.path.join(root, 'val')\n",
    "\n",
    "        # calculate data length\n",
    "        self.data_len = len(fnmatch.filter(os.listdir(os.path.join(self.data_path, 'image')), '*.npy'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load data from the pre-processed npy files\n",
    "        image = torch.from_numpy(np.moveaxis(np.load(os.path.join(self.data_path, 'image', '{:d}.npy'.format(index))), -1, 0))\n",
    "        semantic = torch.from_numpy(np.load(os.path.join(self.data_path, 'label', '{:d}.npy'.format(index))))\n",
    "        depth = torch.from_numpy(np.moveaxis(np.load(os.path.join(self.data_path, 'depth', '{:d}.npy'.format(index))), -1, 0))\n",
    "        \n",
    "        return {'image': image.float(), 'semantic': semantic.float(), 'depth': depth.float()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef1fbe",
   "metadata": {
    "papermill": {
     "duration": 0.025676,
     "end_time": "2025-05-19T07:17:57.092017",
     "exception": false,
     "start_time": "2025-05-19T07:17:57.066341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1cd596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:17:57.146668Z",
     "iopub.status.busy": "2025-05-19T07:17:57.146392Z",
     "iopub.status.idle": "2025-05-19T07:17:57.373828Z",
     "shell.execute_reply": "2025-05-19T07:17:57.373226Z"
    },
    "papermill": {
     "duration": 0.256503,
     "end_time": "2025-05-19T07:17:57.374993",
     "exception": false,
     "start_time": "2025-05-19T07:17:57.118490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Definitions\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# a label and all meta information\n",
    "Label = namedtuple( 'Label' , [\n",
    "\n",
    "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                    # We use them to uniquely name a class\n",
    "\n",
    "    'id'          , # An integer ID that is associated with this label.\n",
    "                    # The IDs are used to represent the label in ground truth images\n",
    "                    # An ID of -1 means that this label does not have an ID and thus\n",
    "                    # is ignored when creating ground truth images (e.g. license plate).\n",
    "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
    "                    # evaluation server.\n",
    "\n",
    "    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
    "                    # ground truth images with train IDs, using the tools provided in the\n",
    "                    # 'preparation' folder. However, make sure to validate or submit results\n",
    "                    # to our evaluation server using the regular IDs above!\n",
    "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
    "                    # are mapped to the same class in the ground truth images. For the inverse\n",
    "                    # mapping, we use the label that is defined first in the list below.\n",
    "                    # For example, mapping all void-type classes to the same ID in training,\n",
    "                    # might make sense for some approaches.\n",
    "                    # Max value is 255!\n",
    "\n",
    "    'category'    , # The name of the category that this label belongs to\n",
    "\n",
    "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
    "                    # on category level.\n",
    "\n",
    "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
    "\n",
    "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
    "                    # during evaluations or not\n",
    "\n",
    "    'color'       , # The color of this label\n",
    "    ] )\n",
    "\n",
    "\n",
    "labels = [\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,      19 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,      19 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,      19 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,      19 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,      19 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,      19 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,      19 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,      19 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,      19 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,      19 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , 34 ,       19 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "]\n",
    "\n",
    "\n",
    "#  dictionaries for a fast lookup\n",
    "\n",
    "# name to label object\n",
    "name2label      = { label.name    : label for label in labels           }\n",
    "# id to label object\n",
    "id2label        = { label.id      : label for label in labels           }\n",
    "# trainId to label object\n",
    "trainId2label   = { label.trainId : label for label in reversed(labels) }\n",
    "# category to list of label objects\n",
    "category2labels = {}\n",
    "for label in labels:\n",
    "    category = label.category\n",
    "    if category in category2labels:\n",
    "        category2labels[category].append(label)\n",
    "    else:\n",
    "        category2labels[category] = [label]\n",
    "\n",
    "def assureSingleInstanceName( name ):\n",
    "    # if the name is known, it is not a group\n",
    "    if name in name2label:\n",
    "        return name\n",
    "    # test if the name actually denotes a group\n",
    "    if not name.endswith(\"group\"):\n",
    "        return None\n",
    "    # remove group\n",
    "    name = name[:-len(\"group\")]\n",
    "    # test if the new name exists\n",
    "    if not name in name2label:\n",
    "        return None\n",
    "    # test if the new name denotes a label that actually has instances\n",
    "    if not name2label[name].hasInstances:\n",
    "        return None\n",
    "    # all good then\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf75bf39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:17:57.428928Z",
     "iopub.status.busy": "2025-05-19T07:17:57.428687Z",
     "iopub.status.idle": "2025-05-19T07:17:57.437884Z",
     "shell.execute_reply": "2025-05-19T07:17:57.437335Z"
    },
    "papermill": {
     "duration": 0.036735,
     "end_time": "2025-05-19T07:17:57.438959",
     "exception": false,
     "start_time": "2025-05-19T07:17:57.402224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CityScapesDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, split='train', label_map='trainId', crop=True):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "        self.crop = crop\n",
    "\n",
    "        self.left_paths = sorted(glob.glob(os.path.join(root, 'leftImg8bit', split, '*.png')))\n",
    "        self.mask_paths = sorted(glob.glob(os.path.join(root, 'gtFine', split, '*.png')))\n",
    "        self.depth_paths = sorted(glob.glob(os.path.join('/kaggle/input/prepcityscapesdepth', 'depth', split, '*.png')))\n",
    "\n",
    "        print(\"Sample image path:\", self.left_paths[0])\n",
    "        print(\"Sample mask path:\", self.mask_paths[0])\n",
    "        print(\"Sample depth path:\", self.depth_paths[0])\n",
    "\n",
    "        # Create LUTs for remapping\n",
    "        self.id_to_trainid_lut = self._create_lut('trainId')\n",
    "        self.id_to_categoryid_lut = self._create_lut('categoryId')\n",
    "\n",
    "        # Human-readable mappings (optional)\n",
    "        self.id_2_name = {-1: 'unlabeled'}\n",
    "        self.trainid_2_name = {19: 'unlabeled'}\n",
    "\n",
    "        for lbl in labels:\n",
    "            if lbl.trainId != 19:\n",
    "                self.trainid_2_name[lbl.trainId] = lbl.name\n",
    "            if lbl.id > 0:\n",
    "                self.id_2_name[lbl.id] = lbl.name\n",
    "\n",
    "    def _create_lut(self, target='trainId'):\n",
    "        lut = np.full((256,), 19, dtype=np.int16)  # Supports -1 safely\n",
    "        for lbl in labels:\n",
    "            value = getattr(lbl, target)\n",
    "            lut[lbl.id] = value\n",
    "        return lut\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        left = cv2.cvtColor(cv2.imread(self.left_paths[idx]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_UNCHANGED).astype(np.uint8)\n",
    "        depth = cv2.imread(self.depth_paths[idx], cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "\n",
    "        if self.crop:\n",
    "            left = left[:800, :, :]\n",
    "            mask = mask[:800, :]\n",
    "            depth = depth[:800, :]\n",
    "\n",
    "        # Apply label remapping via LUT\n",
    "        if self.label_map == 'id':\n",
    "            pass  # keep original label ids\n",
    "        elif self.label_map == 'trainId':\n",
    "            # print(\"Shape Before:\", mask.shape)\n",
    "            mask = self.id_to_trainid_lut[mask]\n",
    "            # print(\"Shape After:\", mask.shape)\n",
    "        elif self.label_map == 'categoryId':\n",
    "            mask = self.id_to_categoryid_lut[mask]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown label_map: {self.label_map}\")\n",
    "\n",
    "        # Make sure depth is non-negative\n",
    "        depth[depth < 0] = 0\n",
    "\n",
    "        sample = {'left': left, 'mask': mask, 'depth': depth}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.left_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "795e82b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:17:57.491182Z",
     "iopub.status.busy": "2025-05-19T07:17:57.490941Z",
     "iopub.status.idle": "2025-05-19T07:17:57.495863Z",
     "shell.execute_reply": "2025-05-19T07:17:57.495193Z"
    },
    "papermill": {
     "duration": 0.032228,
     "end_time": "2025-05-19T07:17:57.496874",
     "exception": false,
     "start_time": "2025-05-19T07:17:57.464646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd8e967",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:17:57.549748Z",
     "iopub.status.busy": "2025-05-19T07:17:57.549502Z",
     "iopub.status.idle": "2025-05-19T07:17:57.605170Z",
     "shell.execute_reply": "2025-05-19T07:17:57.604559Z"
    },
    "papermill": {
     "duration": 0.083608,
     "end_time": "2025-05-19T07:17:57.606511",
     "exception": false,
     "start_time": "2025-05-19T07:17:57.522903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_path = 'dataset/data'\n",
    "# dataset_path = '../../kaggle/input/prepcityscapes'\n",
    "dataset_path = '/kaggle/input/cityscapes-depth-and-segmentation/data'\n",
    "train_set = CityScapes(root=dataset_path, train=True)\n",
    "test_set = CityScapes(root=dataset_path, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3bde09",
   "metadata": {
    "papermill": {
     "duration": 0.026737,
     "end_time": "2025-05-19T07:17:57.660274",
     "exception": false,
     "start_time": "2025-05-19T07:17:57.633537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### num of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f433dc71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:17:57.714784Z",
     "iopub.status.busy": "2025-05-19T07:17:57.714070Z",
     "iopub.status.idle": "2025-05-19T07:19:10.452940Z",
     "shell.execute_reply": "2025-05-19T07:19:10.452178Z"
    },
    "papermill": {
     "duration": 72.794537,
     "end_time": "2025-05-19T07:19:10.481247",
     "exception": false,
     "start_time": "2025-05-19T07:17:57.686710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique semantic class IDs: tensor([-1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n",
      "        13., 14., 15., 16., 17., 18.])\n",
      "Number of classes: 20\n"
     ]
    }
   ],
   "source": [
    "semantic_labels = []\n",
    "for i in range(len(train_set)):\n",
    "    sample = train_set[i]\n",
    "    semantic_labels.append(sample['semantic'].unique())\n",
    "\n",
    "# Flatten and get unique labels across all images\n",
    "all_labels = torch.unique(torch.cat(semantic_labels))\n",
    "print(\"Unique semantic class IDs:\", all_labels)\n",
    "print(\"Number of classes:\", len(all_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5458d2da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:10.535491Z",
     "iopub.status.busy": "2025-05-19T07:19:10.535215Z",
     "iopub.status.idle": "2025-05-19T07:19:22.813084Z",
     "shell.execute_reply": "2025-05-19T07:19:22.812266Z"
    },
    "papermill": {
     "duration": 12.306372,
     "end_time": "2025-05-19T07:19:22.814321",
     "exception": false,
     "start_time": "2025-05-19T07:19:10.507949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique semantic class IDs: tensor([-1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n",
      "        13., 14., 15., 16., 17., 18.])\n",
      "Number of classes: 20\n"
     ]
    }
   ],
   "source": [
    "semantic_labels = []\n",
    "for i in range(len(test_set)):\n",
    "    sample = test_set[i]\n",
    "    semantic_labels.append(sample['semantic'].unique())\n",
    "\n",
    "# Flatten and get unique labels across all images\n",
    "all_labels = torch.unique(torch.cat(semantic_labels))\n",
    "print(\"Unique semantic class IDs:\", all_labels)\n",
    "print(\"Number of classes:\", len(all_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a22460",
   "metadata": {
    "papermill": {
     "duration": 0.025709,
     "end_time": "2025-05-19T07:19:22.867832",
     "exception": false,
     "start_time": "2025-05-19T07:19:22.842123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "698b293c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:22.920113Z",
     "iopub.status.busy": "2025-05-19T07:19:22.919845Z",
     "iopub.status.idle": "2025-05-19T07:19:22.932612Z",
     "shell.execute_reply": "2025-05-19T07:19:22.931887Z"
    },
    "papermill": {
     "duration": 0.040597,
     "end_time": "2025-05-19T07:19:22.933660",
     "exception": false,
     "start_time": "2025-05-19T07:19:22.893063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "# a label and all meta information\n",
    "Label = namedtuple( 'Label' , [\n",
    "\n",
    "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                    # We use them to uniquely name a class\n",
    "\n",
    "    'id'          , # An integer ID that is associated with this label.\n",
    "                    # The IDs are used to represent the label in ground truth images\n",
    "                    # An ID of -1 means that this label does not have an ID and thus\n",
    "                    # is ignored when creating ground truth images (e.g. license plate).\n",
    "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
    "                    # evaluation server.\n",
    "\n",
    "    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
    "                    # ground truth images with train IDs, using the tools provided in the\n",
    "                    # 'preparation' folder. However, make sure to validate or submit results\n",
    "                    # to our evaluation server using the regular IDs above!\n",
    "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
    "                    # are mapped to the same class in the ground truth images. For the inverse\n",
    "                    # mapping, we use the label that is defined first in the list below.\n",
    "                    # For example, mapping all void-type classes to the same ID in training,\n",
    "                    # might make sense for some approaches.\n",
    "                    # Max value is 255!\n",
    "\n",
    "    'category'    , # The name of the category that this label belongs to\n",
    "\n",
    "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
    "                    # on category level.\n",
    "\n",
    "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
    "\n",
    "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
    "                    # during evaluations or not\n",
    "\n",
    "    'color'       , # The color of this label\n",
    "    ] )\n",
    "\n",
    "labels = [\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,      20 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      20 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      20 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      20 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      20 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,      20 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,      20 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,      20 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,      20 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,      20 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,      20 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,      20 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,      20 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,      20 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,      20 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       19 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372234a3",
   "metadata": {
    "papermill": {
     "duration": 0.025144,
     "end_time": "2025-05-19T07:19:22.984829",
     "exception": false,
     "start_time": "2025-05-19T07:19:22.959685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c365724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:23.037972Z",
     "iopub.status.busy": "2025-05-19T07:19:23.037554Z",
     "iopub.status.idle": "2025-05-19T07:19:23.389167Z",
     "shell.execute_reply": "2025-05-19T07:19:23.388434Z"
    },
    "papermill": {
     "duration": 0.378851,
     "end_time": "2025-05-19T07:19:23.390334",
     "exception": false,
     "start_time": "2025-05-19T07:19:23.011483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 108, 256]) torch.Size([103, 256]) torch.Size([1, 108, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x78be85e56090>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_dict = test_set.__getitem__(21)\n",
    "img = img_dict['image'][:, :-20]\n",
    "sem = img_dict['semantic'][:-25, :]\n",
    "depth = img_dict['depth'][:, :-20]\n",
    "print(img.shape, sem.shape, depth.shape)\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(sem)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(depth.permute(1, 2, 0), cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99b616",
   "metadata": {
    "papermill": {
     "duration": 0.026312,
     "end_time": "2025-05-19T07:19:23.444384",
     "exception": false,
     "start_time": "2025-05-19T07:19:23.418072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "947160cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:23.498641Z",
     "iopub.status.busy": "2025-05-19T07:19:23.498374Z",
     "iopub.status.idle": "2025-05-19T07:19:23.502172Z",
     "shell.execute_reply": "2025-05-19T07:19:23.501642Z"
    },
    "papermill": {
     "duration": 0.031936,
     "end_time": "2025-05-19T07:19:23.503307",
     "exception": false,
     "start_time": "2025-05-19T07:19:23.471371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch . utils . data import DataLoader\n",
    "batch_size = 2\n",
    "train_loader = DataLoader ( train_set , batch_size = batch_size , shuffle = True )\n",
    "test_loader = DataLoader ( test_set , batch_size = batch_size , shuffle = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461bcc4",
   "metadata": {
    "papermill": {
     "duration": 0.02665,
     "end_time": "2025-05-19T07:19:23.557019",
     "exception": false,
     "start_time": "2025-05-19T07:19:23.530369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5433057e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:23.612142Z",
     "iopub.status.busy": "2025-05-19T07:19:23.611904Z",
     "iopub.status.idle": "2025-05-19T07:19:23.634270Z",
     "shell.execute_reply": "2025-05-19T07:19:23.633498Z"
    },
    "papermill": {
     "duration": 0.05145,
     "end_time": "2025-05-19T07:19:23.635422",
     "exception": false,
     "start_time": "2025-05-19T07:19:23.583972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 128, 256]) torch.Size([2, 128, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, batch in tqdm(enumerate(train_loader)):\n",
    "    print(batch['depth'].shape,batch['semantic'].shape )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09789729",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:23.692552Z",
     "iopub.status.busy": "2025-05-19T07:19:23.691909Z",
     "iopub.status.idle": "2025-05-19T07:19:23.696524Z",
     "shell.execute_reply": "2025-05-19T07:19:23.695960Z"
    },
    "papermill": {
     "duration": 0.032705,
     "end_time": "2025-05-19T07:19:23.697544",
     "exception": false,
     "start_time": "2025-05-19T07:19:23.664839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2975, 500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset), len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07af5bf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:23.756455Z",
     "iopub.status.busy": "2025-05-19T07:19:23.755799Z",
     "iopub.status.idle": "2025-05-19T07:19:23.759288Z",
     "shell.execute_reply": "2025-05-19T07:19:23.758566Z"
    },
    "papermill": {
     "duration": 0.0357,
     "end_time": "2025-05-19T07:19:23.760499",
     "exception": false,
     "start_time": "2025-05-19T07:19:23.724799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "root = '/kaggle/input/prepcityscapes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f4bf3f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:23.815540Z",
     "iopub.status.busy": "2025-05-19T07:19:23.815303Z",
     "iopub.status.idle": "2025-05-19T07:19:24.322078Z",
     "shell.execute_reply": "2025-05-19T07:19:24.321224Z"
    },
    "papermill": {
     "duration": 0.535735,
     "end_time": "2025-05-19T07:19:24.323352",
     "exception": false,
     "start_time": "2025-05-19T07:19:23.787617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image path: /kaggle/input/prepcityscapes/leftImg8bit/train/aachen_000000_000019_leftImg8bit.png\n",
      "Sample mask path: /kaggle/input/prepcityscapes/gtFine/train/aachen_000000_000019_gtFine_labelIds.png\n",
      "Sample depth path: /kaggle/input/prepcityscapesdepth/depth/train/aachen_000000_000019_depth.png\n"
     ]
    }
   ],
   "source": [
    "dataset = CityScapesDataset(root, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32079ba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:24.379662Z",
     "iopub.status.busy": "2025-05-19T07:19:24.379406Z",
     "iopub.status.idle": "2025-05-19T07:19:24.398948Z",
     "shell.execute_reply": "2025-05-19T07:19:24.398236Z"
    },
    "papermill": {
     "duration": 0.04854,
     "end_time": "2025-05-19T07:19:24.399975",
     "exception": false,
     "start_time": "2025-05-19T07:19:24.351435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "# from albumentations.augmentations.transforms import RandomShadow\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\" Normalizes RGB image to  0-mean 1-std_dev \"\"\" \n",
    "    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], depth_norm=5, max_depth=250):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.depth_norm = depth_norm\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            \n",
    "        return {'left': TF.normalize(left, self.mean, self.std), \n",
    "                'mask': mask, \n",
    "                'depth' : torch.clip( # saftey clip :)\n",
    "                            torch.log(torch.clip(depth, 0, self.max_depth))/self.depth_norm, \n",
    "                            0, \n",
    "                            self.max_depth),\n",
    "               }\n",
    "\n",
    "\n",
    "class AddColorJitter(object):\n",
    "    \"\"\"Convert a color image to grayscale and normalize the color range to [0,1].\"\"\" \n",
    "    def __init__(self, brightness, contrast, saturation, hue):\n",
    "        ''' Applies brightness, constrast, saturation, and hue jitter to image ''' \n",
    "        self.color_jitter = transforms.ColorJitter(brightness, contrast, saturation, hue)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "\n",
    "        return {'left': self.color_jitter(left), \n",
    "                'mask': mask, \n",
    "                'depth' : depth}\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\" Rescales images with bilinear interpolation and masks with nearest interpolation \"\"\"\n",
    "\n",
    "    def __init__(self, h, w):\n",
    "        self.h, self.w = h, w\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "        \n",
    "        # Get original dimensions\n",
    "        orig_h, orig_w = left.shape[-2], left.shape[-1]\n",
    "\n",
    "        # Rescale left image\n",
    "        left = TF.resize(left, (self.h, self.w))\n",
    "        \n",
    "        # Rescale mask with nearest neighbor interpolation\n",
    "        mask = TF.resize(mask.unsqueeze(0), (self.h, self.w), transforms.InterpolationMode.NEAREST).squeeze(0)\n",
    "        \n",
    "        # Rescale depth\n",
    "        depth = TF.resize(depth.unsqueeze(0), (self.h, self.w)).squeeze(0)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'left': left,\n",
    "            'mask': mask,\n",
    "            'depth': depth,\n",
    "        }\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, h, w, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.scale = scale\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "        i, j, h, w = transforms.RandomResizedCrop.get_params(left, scale=self.scale, ratio=self.ratio)\n",
    "\n",
    "        return {'left': TF.resized_crop(left, i, j, h, w, (self.h, self.w)), \n",
    "                'mask': TF.resized_crop(mask.unsqueeze(0), i, j, h, w, (self.h, self.w), interpolation=TF.InterpolationMode.NEAREST),\n",
    "                'depth' : TF.resized_crop(depth.unsqueeze(0), i, j, h, w, (self.h, self.w))}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "         \n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "        return {'left': transforms.ToTensor()(left), \n",
    "                'mask': torch.as_tensor(mask, dtype=torch.int64),\n",
    "                'depth' : transforms.ToTensor()(depth).type(torch.float32),\n",
    "               }\n",
    "    \n",
    "\n",
    "class ElasticTransform(object):\n",
    "    def __init__(self, alpha=25.0, sigma=5.0, prob=0.5):\n",
    "        self.alpha = [1.0, alpha]\n",
    "        self.sigma = [1, sigma]\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        if torch.rand(1) < self.prob:\n",
    "\n",
    "            left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            _, H, W = mask.shape\n",
    "            displacement = transforms.ElasticTransform.get_params(self.alpha, self.sigma, [H, W])\n",
    "\n",
    "            # # TEMP\n",
    "            # print(TF.elastic_transform(left, displacement).shape)\n",
    "            # print(TF.elastic_transform(mask.unsqueeze(0), displacement, interpolation=TF.InterpolationMode.NEAREST).shape)\n",
    "            # print(torch.clip(TF.elastic_transform(depth, displacement), 0, depth.max()).shape)\n",
    "\n",
    "            return {'left': TF.elastic_transform(left, displacement), \n",
    "                    'mask': TF.elastic_transform(mask.unsqueeze(0), displacement, interpolation=TF.InterpolationMode.NEAREST), \n",
    "                    'depth' : torch.clip(TF.elastic_transform(depth, displacement), 0, depth.max())} \n",
    "        \n",
    "        else:\n",
    "            return sample\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "# new transform to rotate the images\n",
    "class RandomRotate(object):\n",
    "    def __init__(self, angle):\n",
    "        if not isinstance(angle, (list, tuple)):\n",
    "            self.angle = (-abs(angle), abs(angle))\n",
    "        else:\n",
    "            self.angle = angle\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "\n",
    "        angle = transforms.RandomRotation.get_params(self.angle)\n",
    "\n",
    "        return {'left': TF.rotate(left, angle), \n",
    "                'mask': TF.rotate(mask.unsqueeze(0), angle), \n",
    "                'depth' : TF.rotate(depth, angle)}\n",
    "    \n",
    "    \n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        if torch.rand(1) < self.prob:\n",
    "            left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            return {'left': TF.hflip(left), \n",
    "                    'mask': TF.hflip(mask), \n",
    "                    'depth' : TF.hflip(depth)}\n",
    "        else:\n",
    "            return sample\n",
    "        \n",
    "\n",
    "class RandomVerticalFlip(object):\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            left, mask, depth = sample['left'], sample['mask'], sample['depth']\n",
    "            return {'left': TF.vflip(left), \n",
    "                    'mask': TF.vflip(mask), \n",
    "                    'depth' : TF.vflip(depth)}\n",
    "        else:\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbea73b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:24.454903Z",
     "iopub.status.busy": "2025-05-19T07:19:24.454286Z",
     "iopub.status.idle": "2025-05-19T07:19:25.390821Z",
     "shell.execute_reply": "2025-05-19T07:19:25.390154Z"
    },
    "papermill": {
     "duration": 0.964982,
     "end_time": "2025-05-19T07:19:25.391947",
     "exception": false,
     "start_time": "2025-05-19T07:19:24.426965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image path: /kaggle/input/prepcityscapes/leftImg8bit/train/aachen_000000_000019_leftImg8bit.png\n",
      "Sample mask path: /kaggle/input/prepcityscapes/gtFine/train/aachen_000000_000019_gtFine_labelIds.png\n",
      "Sample depth path: /kaggle/input/prepcityscapesdepth/depth/train/aachen_000000_000019_depth.png\n",
      "Sample image path: /kaggle/input/prepcityscapes/leftImg8bit/val/frankfurt_000000_000294_leftImg8bit.png\n",
      "Sample mask path: /kaggle/input/prepcityscapes/gtFine/val/frankfurt_000000_000294_gtFine_labelIds.png\n",
      "Sample depth path: /kaggle/input/prepcityscapesdepth/depth/val/frankfurt_000000_000294_depth.png\n",
      "Sample image path: /kaggle/input/prepcityscapes/leftImg8bit/test/berlin_000000_000019_leftImg8bit.png\n",
      "Sample mask path: /kaggle/input/prepcityscapes/gtFine/test/berlin_000000_000019_gtFine_labelIds.png\n",
      "Sample depth path: /kaggle/input/prepcityscapesdepth/depth/test/berlin_000000_000019_depth.png\n"
     ]
    }
   ],
   "source": [
    "OG_W, OG_H = 2048, 800 # OG width and height after crop\n",
    "# W, H = int(OG_W//9.14), int(OG_H//3.57) # resize w,h for training\n",
    "W, H = int(OG_W//16), int(OG_H//6.25) # resize w,h for training\n",
    "BATCH_SIZE = 5\n",
    "# W, H = int(OG_W//32), int(OG_H//12.5) # resize w,h for training\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     ToTensor(),\n",
    "#     RandomCrop(H, W),\n",
    "#     # ElasticTransform(alpha=100.0, sigma=25.0, prob=0.5),\n",
    "#     AddColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "#     RandomHorizontalFlip(0.5),\n",
    "#     RandomVerticalFlip(0.2),\n",
    "#     # RandomRotate((-30, 30)),\n",
    "#     Normalize()\n",
    "# ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Rescale(H, W),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Rescale(H, W),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Rescale(H, W),\n",
    "    Normalize()\n",
    "])\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "     \"\"\"\n",
    "     Handles batches with variable-sized bounding boxes.\n",
    "     \"\"\"\n",
    "     images = torch.stack([item['left'] for item in batch])  # Stack images\n",
    "     masks = torch.stack([item['mask'] for item in batch])  # Stack masks\n",
    "     depths = torch.stack([item['depth'] for item in batch])  # Stack depths\n",
    "    \n",
    "    \n",
    "     return {'left': images, 'mask': masks, 'depth': depths}\n",
    "\n",
    "# def custom_collate_fn(batch):\n",
    "    # return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = CityScapesDataset(root, transform=transform, split='train', label_map='trainId') # 'trainId')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn, pin_memory=False, shuffle=False, drop_last=True)\n",
    "\n",
    "# temp_valid_dataset = CityScapesDataset(root, transform=valid_transform, split='val', label_map='trainId', crop=False)\n",
    "valid_dataset = CityScapesDataset(root, transform=valid_transform, split='val', label_map='trainId')\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn, pin_memory=False, shuffle=False, drop_last=True)\n",
    "\n",
    "test_dataset = CityScapesDataset(root, transform=test_transform, split='test', label_map='trainId')\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn, pin_memory=False, shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "544f29fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:25.448549Z",
     "iopub.status.busy": "2025-05-19T07:19:25.447873Z",
     "iopub.status.idle": "2025-05-19T07:19:25.452065Z",
     "shell.execute_reply": "2025-05-19T07:19:25.451350Z"
    },
    "papermill": {
     "duration": 0.033035,
     "end_time": "2025-05-19T07:19:25.453091",
     "exception": false,
     "start_time": "2025-05-19T07:19:25.420056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader_c = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn, pin_memory=False, shuffle=False)\n",
    "valid_loader_c = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn, pin_memory=False, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn, pin_memory=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93bca80",
   "metadata": {
    "papermill": {
     "duration": 0.027064,
     "end_time": "2025-05-19T07:19:25.508452",
     "exception": false,
     "start_time": "2025-05-19T07:19:25.481388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb634b77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:25.563816Z",
     "iopub.status.busy": "2025-05-19T07:19:25.563378Z",
     "iopub.status.idle": "2025-05-19T07:19:25.584491Z",
     "shell.execute_reply": "2025-05-19T07:19:25.583618Z"
    },
    "papermill": {
     "duration": 0.050473,
     "end_time": "2025-05-19T07:19:25.585910",
     "exception": false,
     "start_time": "2025-05-19T07:19:25.535437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4996)\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(x_pred, x_output, task_type):\n",
    "    device = x_pred.device\n",
    "\n",
    "    # binary mark to mask out undefined pixel space\n",
    "    binary_mask = (torch.sum(x_output, dim=1) != 0).float().unsqueeze(1).to(device)\n",
    "\n",
    "    if task_type == 'semantic':\n",
    "        # semantic loss : depth - wise cross entropy\n",
    "        loss = F.nll_loss(x_pred, x_output, ignore_index=-1)\n",
    "\n",
    "    if task_type == 'depth':\n",
    "        # depth loss : l1 norm\n",
    "        loss = torch.sum(torch.abs(x_pred - x_output) * binary_mask) / torch.nonzero(\n",
    "            binary_mask, as_tuple=False).size(0)\n",
    "\n",
    "    return loss\n",
    "xx = torch.rand(4,13,128,256)\n",
    "yy = torch.rand(4,128,256).long()\n",
    "print(compute_loss(xx,yy,'semantic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d8e31c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:25.645436Z",
     "iopub.status.busy": "2025-05-19T07:19:25.645096Z",
     "iopub.status.idle": "2025-05-19T07:19:25.649980Z",
     "shell.execute_reply": "2025-05-19T07:19:25.649280Z"
    },
    "papermill": {
     "duration": 0.034791,
     "end_time": "2025-05-19T07:19:25.651090",
     "exception": false,
     "start_time": "2025-05-19T07:19:25.616299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class GLPDepthWithSegmentation(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(GLPDepthWithSegmentation, self).__init__()\n",
    "#         # initialise network parameters\n",
    "\n",
    "#         filter = [64, 128, 256, 512, 512]\n",
    "\n",
    "#         self.class_nb = 21\n",
    "\n",
    "#         # define encoder decoder layers\n",
    "#         self.ViT_encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n",
    "#         self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n",
    "#         for i in range(4):\n",
    "#             self.ViT_encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n",
    "#             self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n",
    "\n",
    "#         # define convolution layer with\n",
    "#         self.tranformer_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n",
    "#         self.mlp = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n",
    "#         for i in range(4):\n",
    "#             if i == 0:\n",
    "#                 self.tranformer_block.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n",
    "#                 self.mlp.append(self.conv_layer([filter[i], filter[i]]))\n",
    "#             else:\n",
    "#                 self.tranformer_block.append(nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n",
    "#                                                           self.conv_layer([filter[i + 1], filter[i + 1]])))\n",
    "#                 self.mlp.append(nn.Sequential(self.conv_layer([filter[i], filter[i]]),\n",
    "#                                                           self.conv_layer([filter[i], filter[i]])))\n",
    "\n",
    "#         # define task specific layers\n",
    "#         self.SegFormerDecoder = nn.Sequential(nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n",
    "#                                          nn.Conv2d(in_channels=filter[0], out_channels=self.class_nb, kernel_size=1, padding=0))\n",
    "#         self.GLPDepthDecoder = nn.Sequential(nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n",
    "#                                          nn.Conv2d(in_channels=filter[0], out_channels=1, kernel_size=1, padding=0))\n",
    "\n",
    "#         # define pooling and unpooling functions\n",
    "#         self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "#         self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 nn.init.xavier_normal_(m.weight)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 nn.init.constant_(m.weight, 1)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 nn.init.xavier_normal_(m.weight)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#     # define convolutional block\n",
    "#     def conv_layer(self, channel):\n",
    "#         conv_block = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(num_features=channel[1]),\n",
    "#             nn.GELU()\n",
    "#         )\n",
    "#         return conv_block\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n",
    "#         for i in range(5):\n",
    "#             g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n",
    "\n",
    "#         # global shared encoder - decoder network\n",
    "#         for i in range(5):\n",
    "#             if i == 0:\n",
    "#                 g_encoder[i][0] = self.ViT_encoder_block[i](x)\n",
    "#                 g_encoder[i][1] = self.tranformer_block[i](g_encoder[i][0])\n",
    "#                 g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n",
    "#             else:\n",
    "#                 g_encoder[i][0] = self.ViT_encoder_block[i](g_maxpool[i - 1])\n",
    "#                 g_encoder[i][1] = self.tranformer_block[i](g_encoder[i][0])\n",
    "#                 g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n",
    "\n",
    "#         for i in range(5):\n",
    "#             if i == 0:\n",
    "#                 g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n",
    "#                 g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n",
    "#                 g_decoder[i][1] = self.mlp[-i - 1](g_decoder[i][0])\n",
    "#             else:\n",
    "#                 g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n",
    "#                 g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n",
    "#                 g_decoder[i][1] = self.mlp[-i - 1](g_decoder[i][0])\n",
    "\n",
    "#         # define task prediction layers\n",
    "#         pred_seg = F.log_softmax(self.SegFormerDecoder(g_decoder[i][1]), dim=1)\n",
    "#         pred_d = self.GLPDepthDecoder(g_decoder[i][1])\n",
    "\n",
    "#         return {'semantic': pred_seg, 'depth': pred_d}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aec93e",
   "metadata": {
    "papermill": {
     "duration": 0.027559,
     "end_time": "2025-05-19T07:19:25.706581",
     "exception": false,
     "start_time": "2025-05-19T07:19:25.679022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a166050",
   "metadata": {
    "papermill": {
     "duration": 0.026649,
     "end_time": "2025-05-19T07:19:25.760248",
     "exception": false,
     "start_time": "2025-05-19T07:19:25.733599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RevisedLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6466007",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:25.816590Z",
     "iopub.status.busy": "2025-05-19T07:19:25.815986Z",
     "iopub.status.idle": "2025-05-19T07:19:25.822533Z",
     "shell.execute_reply": "2025-05-19T07:19:25.822032Z"
    },
    "papermill": {
     "duration": 0.035846,
     "end_time": "2025-05-19T07:19:25.823567",
     "exception": false,
     "start_time": "2025-05-19T07:19:25.787721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RevisedLayerNorm(nn.Module):\n",
    "    \"\"\"Revised LayerNorm\"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n",
    "        super(RevisedLayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.detach_gradient = detach_gradient\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n",
    "        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n",
    "\n",
    "        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "\n",
    "        trunc_normal_(self.scale_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.scale_mlp.bias, 1)\n",
    "\n",
    "        trunc_normal_(self.shift_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.shift_mlp.bias, 0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n",
    "        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n",
    "\n",
    "        normalized_tensor = (input_tensor - mean_value) / std_value\n",
    "\n",
    "        if self.detach_gradient:\n",
    "            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n",
    "        else:\n",
    "            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n",
    "\n",
    "        output = normalized_tensor * self.scale + self.shift\n",
    "        return output, rescale, rebias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "511c6bbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:25.879209Z",
     "iopub.status.busy": "2025-05-19T07:19:25.879020Z",
     "iopub.status.idle": "2025-05-19T07:19:25.893673Z",
     "shell.execute_reply": "2025-05-19T07:19:25.893167Z"
    },
    "papermill": {
     "duration": 0.043405,
     "end_time": "2025-05-19T07:19:25.894633",
     "exception": false,
     "start_time": "2025-05-19T07:19:25.851228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n",
    "        super().__init__()\n",
    "        output_channels = output_channels or input_channels\n",
    "        hidden_channels = hidden_channels or input_channels\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            gain = (8 * self.depth) ** (-1 / 4)\n",
    "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "            trunc_normal_(layer.weight, std=std)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "\n",
    "def partition_into_windows(tensor, window_size):\n",
    "    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n",
    "    batch_size, height, width, channels = tensor.shape\n",
    "    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n",
    "\n",
    "    tensor = tensor.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n",
    "    )\n",
    "    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, height, width):\n",
    "    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n",
    "    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n",
    "    tensor = windows.view(\n",
    "        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc286ea",
   "metadata": {
    "papermill": {
     "duration": 0.027965,
     "end_time": "2025-05-19T07:19:25.950771",
     "exception": false,
     "start_time": "2025-05-19T07:19:25.922806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29c4b379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:26.008691Z",
     "iopub.status.busy": "2025-05-19T07:19:26.007980Z",
     "iopub.status.idle": "2025-05-19T07:19:26.079259Z",
     "shell.execute_reply": "2025-05-19T07:19:26.078525Z"
    },
    "papermill": {
     "duration": 0.101277,
     "end_time": "2025-05-19T07:19:26.080551",
     "exception": false,
     "start_time": "2025-05-19T07:19:25.979274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 16, 16]),\n",
       " torch.Size([32, 16, 64]),\n",
       " torch.Size([2, 16, 16, 64]),\n",
       " True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize the MultiLayerPerceptron with sample parameters\n",
    "depth = 4\n",
    "input_channels = 64\n",
    "hidden_channels = 128\n",
    "output_channels = 64\n",
    "\n",
    "mlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n",
    "\n",
    "# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\n",
    "input_tensor = torch.randn(2, 64, 16, 16)\n",
    "output_tensor = mlp(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "mlp_output_shape = output_tensor.shape\n",
    "\n",
    "# Test window partition and merging\n",
    "batch_size, height, width, channels = 2, 16, 16, 64\n",
    "window_size = 4\n",
    "\n",
    "# Create a random tensor for window functions (B, H, W, C) format\n",
    "input_window_tensor = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Apply partitioning and merging\n",
    "windows = partition_into_windows(input_window_tensor, window_size)\n",
    "reconstructed_tensor = merge_windows(windows, window_size, height, width)\n",
    "\n",
    "# Check shapes\n",
    "windows_shape = windows.shape\n",
    "reconstructed_shape = reconstructed_tensor.shape\n",
    "\n",
    "# Validate if the reconstruction matches the original input shape\n",
    "is_shape_correct = reconstructed_shape == input_window_tensor.shape\n",
    "\n",
    "# Output results\n",
    "mlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab1166",
   "metadata": {
    "papermill": {
     "duration": 0.027586,
     "end_time": "2025-05-19T07:19:26.138819",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.111233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### LocalWindowAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e876a78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:26.196268Z",
     "iopub.status.busy": "2025-05-19T07:19:26.195742Z",
     "iopub.status.idle": "2025-05-19T07:19:26.202565Z",
     "shell.execute_reply": "2025-05-19T07:19:26.201964Z"
    },
    "papermill": {
     "duration": 0.036534,
     "end_time": "2025-05-19T07:19:26.203624",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.167090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, window_size, num_heads):\n",
    "        \"\"\"Self-attention mechanism within local windows.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # (height, width)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n",
    "\n",
    "        # Compute and store relative positional encodings\n",
    "        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n",
    "        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n",
    "\n",
    "        # Learnable transformation of relative position embeddings\n",
    "        self.relative_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_heads, bias=True)\n",
    "        )\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n",
    "        batch_size, num_tokens, _ = qkv.shape\n",
    "\n",
    "        # Reshape qkv into separate query, key, and value tensors\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n",
    "\n",
    "        # Scale query for stable attention computation\n",
    "        q = q * self.scaling_factor\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Compute relative position bias\n",
    "        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n",
    "        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n",
    "        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n",
    "\n",
    "        # Apply softmax and compute weighted values\n",
    "        attention_weights = self.attention_softmax(attention_scores)\n",
    "        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879211b5",
   "metadata": {
    "papermill": {
     "duration": 0.026565,
     "end_time": "2025-05-19T07:19:26.257731",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.231166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### compute_log_relative_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cbcce27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:26.312683Z",
     "iopub.status.busy": "2025-05-19T07:19:26.312463Z",
     "iopub.status.idle": "2025-05-19T07:19:26.317096Z",
     "shell.execute_reply": "2025-05-19T07:19:26.316406Z"
    },
    "papermill": {
     "duration": 0.033457,
     "end_time": "2025-05-19T07:19:26.318155",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.284698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_log_relative_positions(window_size):\n",
    "    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n",
    "    coord_range = torch.arange(window_size)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n",
    "\n",
    "    # Compute relative positions\n",
    "    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n",
    "\n",
    "    # Format and apply log transformation\n",
    "    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n",
    "    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "    return log_relative_positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3387236c",
   "metadata": {
    "papermill": {
     "duration": 0.028189,
     "end_time": "2025-05-19T07:19:26.374286",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.346097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### AdaptiveAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55e0b34f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:26.431763Z",
     "iopub.status.busy": "2025-05-19T07:19:26.431501Z",
     "iopub.status.idle": "2025-05-19T07:19:26.444665Z",
     "shell.execute_reply": "2025-05-19T07:19:26.443934Z"
    },
    "papermill": {
     "duration": 0.043628,
     "end_time": "2025-05-19T07:19:26.445783",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.402155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n",
    "        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.network_depth = network_depth\n",
    "        self.enable_attention = enable_attention\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "        # Define convolutional processing based on mode\n",
    "        if self.conv_mode == 'Conv':\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "            )\n",
    "\n",
    "        if self.conv_mode == 'DWConv':\n",
    "            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n",
    "            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight_shape = module.weight.shape\n",
    "\n",
    "            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "            else:\n",
    "                gain = (8 * self.network_depth) ** (-1/4)\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def pad_for_window_processing(self, x, shift=False):\n",
    "        \"\"\"Pads the input tensor to fit window processing requirements with left and right shifts.\"\"\"\n",
    "        _, _, height, width = x.size()\n",
    "        pad_h = (self.window_size - height % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - width % self.window_size) % self.window_size\n",
    "\n",
    "        if shift:\n",
    "            # Apply left or right shift instead of cyclic shift\n",
    "            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n",
    "                          0, (self.window_size - pad_h) % self.window_size), mode='reflect')\n",
    "        else:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            v_proj = self.value_projection(x)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            qk_proj = self.query_key_projection(x)\n",
    "            qkv = torch.cat([qk_proj, v_proj], dim=1)\n",
    "\n",
    "            # Apply padding for shifted window processing\n",
    "            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n",
    "            padded_height, padded_width = padded_qkv.shape[2:]\n",
    "\n",
    "            # Partition into windows\n",
    "            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n",
    "            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n",
    "\n",
    "            # Apply window-based attention\n",
    "            attn_windows = self.window_attention(qkv_windows)\n",
    "\n",
    "            # Merge back to original spatial dimensions\n",
    "            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n",
    "\n",
    "            # Apply left or right shift (avoid cyclic)\n",
    "            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), 0:(self.shift_size + width), :]\n",
    "            attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.conv_mode in ['Conv', 'DWConv']:\n",
    "                conv_output = self.conv_layer(v_proj)\n",
    "                print(f\"conv_output shape: {conv_output.shape}\")\n",
    "                print(f\"attn_output shape: {attn_output.shape}\")\n",
    "                # print(f\"conv_output + attn_output shape: {conv_output + attn_output.shape}\")\n",
    "                print(f\"self.output_projection: {self.output_projection}\")\n",
    "                output = self.output_projection(conv_output + attn_output)\n",
    "            else:\n",
    "                output = self.output_projection(attn_output)\n",
    "\n",
    "        else:\n",
    "            if self.conv_mode == 'Conv':\n",
    "                output = self.conv_layer(x)  # No attention, using convolution only\n",
    "            elif self.conv_mode == 'DWConv':\n",
    "                output = self.output_projection(self.conv_layer(v_proj))\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105c62b",
   "metadata": {
    "papermill": {
     "duration": 0.027451,
     "end_time": "2025-05-19T07:19:26.501624",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.474173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### VisionTransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cb46542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:26.558552Z",
     "iopub.status.busy": "2025-05-19T07:19:26.558281Z",
     "iopub.status.idle": "2025-05-19T07:19:26.564929Z",
     "shell.execute_reply": "2025-05-19T07:19:26.564226Z"
    },
    "papermill": {
     "duration": 0.036774,
     "end_time": "2025-05-19T07:19:26.566138",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.529364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n",
    "                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n",
    "        \"\"\"\n",
    "        A transformer block that includes attention (optional) and MLP layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enable_attention = enable_attention\n",
    "        self.enable_mlp_norm = enable_mlp_norm\n",
    "\n",
    "        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n",
    "        self.attention_layer = AdaptiveAttention(\n",
    "            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n",
    "            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n",
    "        )\n",
    "\n",
    "        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n",
    "        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.enable_attention:\n",
    "            x, rescale, rebias = self.pre_norm(x)\n",
    "        x = self.attention_layer(x)\n",
    "        if self.enable_attention:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        residual = x\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x, rescale, rebias = self.post_norm(x)\n",
    "        x = self.mlp_layer(x)\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663aec14",
   "metadata": {
    "papermill": {
     "duration": 0.027164,
     "end_time": "2025-05-19T07:19:26.621060",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.593896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PatchEmbedding and PatchReconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c780734e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:26.677070Z",
     "iopub.status.busy": "2025-05-19T07:19:26.676619Z",
     "iopub.status.idle": "2025-05-19T07:19:26.682764Z",
     "shell.execute_reply": "2025-05-19T07:19:26.682098Z"
    },
    "papermill": {
     "duration": 0.035476,
     "end_time": "2025-05-19T07:19:26.683827",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.648351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch embedding module that projects input images into token embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to generate patch embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class PatchReconstruction(nn.Module):\n",
    "    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch reconstruction module that converts token embeddings back to image patches.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2, padding_mode='reflect'\n",
    "            ),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to reconstruct image from embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5181526",
   "metadata": {
    "papermill": {
     "duration": 0.027283,
     "end_time": "2025-05-19T07:19:26.739064",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.711781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SelectiveKernelFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81c8cddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:26.796281Z",
     "iopub.status.busy": "2025-05-19T07:19:26.795735Z",
     "iopub.status.idle": "2025-05-19T07:19:26.802292Z",
     "shell.execute_reply": "2025-05-19T07:19:26.801558Z"
    },
    "papermill": {
     "duration": 0.036636,
     "end_time": "2025-05-19T07:19:26.803420",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.766784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelectiveKernelFusion(nn.Module):\n",
    "    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n",
    "        \"\"\"\n",
    "        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n",
    "\n",
    "        Args:\n",
    "            channels (int): Number of input channels.\n",
    "            num_branches (int): Number of feature branches to fuse.\n",
    "            reduction_ratio (int): Reduction ratio for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_branches = num_branches\n",
    "        reduced_channels = max(int(channels / reduction_ratio), 4)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Forward pass for selective kernel fusion.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (list of tensors): A list of feature maps to be fused.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adaptively fused feature map.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = feature_maps[0].shape\n",
    "        \n",
    "        # Concatenate feature maps along a new dimension (num_branches)\n",
    "        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        aggregated_features = torch.sum(stacked_features, dim=1)\n",
    "        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n",
    "        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n",
    "\n",
    "        # Weighted sum of input feature maps\n",
    "        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd0444",
   "metadata": {
    "papermill": {
     "duration": 0.027032,
     "end_time": "2025-05-19T07:19:26.858646",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.831614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TransformerStage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c73347d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:26.914769Z",
     "iopub.status.busy": "2025-05-19T07:19:26.914188Z",
     "iopub.status.idle": "2025-05-19T07:19:26.921021Z",
     "shell.execute_reply": "2025-05-19T07:19:26.920310Z"
    },
    "papermill": {
     "duration": 0.036274,
     "end_time": "2025-05-19T07:19:26.922238",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.885964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, window_size=8,\n",
    "                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n",
    "        \"\"\"\n",
    "        A stage of transformer blocks with configurable attention placement.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        attention_layers = int(attention_ratio * num_layers)\n",
    "\n",
    "        if attention_placement == 'last':\n",
    "            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'first':\n",
    "            enable_attentions = [i < attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'middle':\n",
    "            enable_attentions = [\n",
    "                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(\n",
    "                network_depth=network_depth,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                enable_attention=enable_attentions[i],\n",
    "                conv_mode=conv_mode\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer stage.\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84360f0",
   "metadata": {
    "papermill": {
     "duration": 0.027776,
     "end_time": "2025-05-19T07:19:26.977935",
     "exception": false,
     "start_time": "2025-05-19T07:19:26.950159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DehazingTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec9c2a04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:27.035436Z",
     "iopub.status.busy": "2025-05-19T07:19:27.035165Z",
     "iopub.status.idle": "2025-05-19T07:19:27.050392Z",
     "shell.execute_reply": "2025-05-19T07:19:27.049686Z"
    },
    "papermill": {
     "duration": 0.046176,
     "end_time": "2025-05-19T07:19:27.051437",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.005261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DehazingTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "                 layer_depths=[16, 16, 16, 8, 8],\n",
    "                 num_heads=[2, 4, 6, 1, 1],\n",
    "                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding settings\n",
    "        self.patch_size = 4\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # Backbone layers\n",
    "        self.encoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[0],\n",
    "            num_layers=layer_depths[0],\n",
    "            num_heads=num_heads[0],\n",
    "            mlp_ratio=mlp_ratios[0],\n",
    "            norm_layer=norm_layers[0],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[0],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[0]\n",
    "        )\n",
    "        \n",
    "        self.downsample1 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "        \n",
    "        self.encoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[1],\n",
    "            num_layers=layer_depths[1],\n",
    "            num_heads=num_heads[1],\n",
    "            mlp_ratio=mlp_ratios[1],\n",
    "            norm_layer=norm_layers[1],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[1],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[1]\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "        \n",
    "        self.encoder_stage3 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[2],\n",
    "            num_layers=layer_depths[2],\n",
    "            num_heads=num_heads[2],\n",
    "            mlp_ratio=mlp_ratios[2],\n",
    "            norm_layer=norm_layers[2],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[2],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[2]\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n",
    "        \n",
    "        self.decoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[3],\n",
    "            num_layers=layer_depths[3],\n",
    "            num_heads=num_heads[3],\n",
    "            mlp_ratio=mlp_ratios[3],\n",
    "            norm_layer=norm_layers[3],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[3],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[3]\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n",
    "        \n",
    "        self.decoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[4],\n",
    "            num_layers=layer_depths[4],\n",
    "            num_heads=num_heads[4],\n",
    "            mlp_ratio=mlp_ratios[4],\n",
    "            norm_layer=norm_layers[4],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[4],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[4]\n",
    "        )\n",
    "\n",
    "        # Final patch reconstruction\n",
    "        self.patch_reconstruction = PatchReconstruction(\n",
    "            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def adjust_image_size(self, x):\n",
    "        # Ensures the input image size is compatible with the patch size\n",
    "        _, _, height, width = x.size()\n",
    "        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n",
    "        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder_stage1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.downsample1(x)\n",
    "        x = self.encoder_stage2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.downsample2(x)\n",
    "        x = self.encoder_stage3(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n",
    "        x = self.decoder_stage1(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n",
    "        x = self.decoder_stage2(x)\n",
    "        x = self.patch_reconstruction(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_height, original_width = x.shape[2:]\n",
    "        x = self.adjust_image_size(x)\n",
    "\n",
    "        features = self.extract_features(x)\n",
    "        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n",
    "\n",
    "        # Dehazing formula: I = J * t + A * (1 - t)\n",
    "        x = transmission_map * x - atmospheric_light + x\n",
    "        x = x[:, :, :original_height, :original_width]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198935b",
   "metadata": {
    "papermill": {
     "duration": 0.028141,
     "end_time": "2025-05-19T07:19:27.107586",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.079445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### build_dehazing_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc3bde26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:27.165304Z",
     "iopub.status.busy": "2025-05-19T07:19:27.164718Z",
     "iopub.status.idle": "2025-05-19T07:19:27.169281Z",
     "shell.execute_reply": "2025-05-19T07:19:27.168678Z"
    },
    "papermill": {
     "duration": 0.034548,
     "end_time": "2025-05-19T07:19:27.170351",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.135803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dehazing_transformer():\n",
    "    return DehazingTransformer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "        layer_depths=[12, 12, 12, 6, 6],\n",
    "        num_heads=[2, 4, 6, 1, 1],\n",
    "        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010397e8",
   "metadata": {
    "papermill": {
     "duration": 0.027977,
     "end_time": "2025-05-19T07:19:27.226445",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.198468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RevisedLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94621ba7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:27.282344Z",
     "iopub.status.busy": "2025-05-19T07:19:27.282090Z",
     "iopub.status.idle": "2025-05-19T07:19:27.289285Z",
     "shell.execute_reply": "2025-05-19T07:19:27.288559Z"
    },
    "papermill": {
     "duration": 0.036756,
     "end_time": "2025-05-19T07:19:27.290383",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.253627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RevisedLayerNorm(nn.Module):\n",
    "    \"\"\"Revised LayerNorm\"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n",
    "        super(RevisedLayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.detach_gradient = detach_gradient\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n",
    "        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n",
    "\n",
    "        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "\n",
    "        trunc_normal_(self.scale_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.scale_mlp.bias, 1)\n",
    "\n",
    "        trunc_normal_(self.shift_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.shift_mlp.bias, 0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n",
    "        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n",
    "\n",
    "        normalized_tensor = (input_tensor - mean_value) / std_value\n",
    "\n",
    "        if self.detach_gradient:\n",
    "            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n",
    "        else:\n",
    "            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n",
    "\n",
    "        output = normalized_tensor * self.scale + self.shift\n",
    "        return output, rescale, rebias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "baf4ade5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:27.347010Z",
     "iopub.status.busy": "2025-05-19T07:19:27.346400Z",
     "iopub.status.idle": "2025-05-19T07:19:27.354299Z",
     "shell.execute_reply": "2025-05-19T07:19:27.353775Z"
    },
    "papermill": {
     "duration": 0.037317,
     "end_time": "2025-05-19T07:19:27.355368",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.318051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n",
    "        super().__init__()\n",
    "        output_channels = output_channels or input_channels\n",
    "        hidden_channels = hidden_channels or input_channels\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            gain = (8 * self.depth) ** (-1 / 4)\n",
    "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "            trunc_normal_(layer.weight, std=std)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "\n",
    "def partition_into_windows(tensor, window_size):\n",
    "    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n",
    "    batch_size, height, width, channels = tensor.shape\n",
    "    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n",
    "\n",
    "    tensor = tensor.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n",
    "    )\n",
    "    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, height, width):\n",
    "    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n",
    "    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n",
    "    tensor = windows.view(\n",
    "        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a083335",
   "metadata": {
    "papermill": {
     "duration": 0.026732,
     "end_time": "2025-05-19T07:19:27.409624",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.382892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b31633f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:27.466013Z",
     "iopub.status.busy": "2025-05-19T07:19:27.465322Z",
     "iopub.status.idle": "2025-05-19T07:19:27.478090Z",
     "shell.execute_reply": "2025-05-19T07:19:27.477483Z"
    },
    "papermill": {
     "duration": 0.042689,
     "end_time": "2025-05-19T07:19:27.479267",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.436578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 16, 16]),\n",
       " torch.Size([32, 16, 64]),\n",
       " torch.Size([2, 16, 16, 64]),\n",
       " True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the MultiLayerPerceptron with sample parameters\n",
    "depth = 4\n",
    "input_channels = 64\n",
    "hidden_channels = 128\n",
    "output_channels = 64\n",
    "\n",
    "mlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n",
    "\n",
    "# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\n",
    "input_tensor = torch.randn(2, 64, 16, 16)\n",
    "output_tensor = mlp(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "mlp_output_shape = output_tensor.shape\n",
    "\n",
    "# Test window partition and merging\n",
    "batch_size, height, width, channels = 2, 16, 16, 64\n",
    "window_size = 4\n",
    "\n",
    "# Create a random tensor for window functions (B, H, W, C) format\n",
    "input_window_tensor = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Apply partitioning and merging\n",
    "windows = partition_into_windows(input_window_tensor, window_size)\n",
    "reconstructed_tensor = merge_windows(windows, window_size, height, width)\n",
    "\n",
    "# Check shapes\n",
    "windows_shape = windows.shape\n",
    "reconstructed_shape = reconstructed_tensor.shape\n",
    "\n",
    "# Validate if the reconstruction matches the original input shape\n",
    "is_shape_correct = reconstructed_shape == input_window_tensor.shape\n",
    "\n",
    "# Output results\n",
    "mlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bde5891f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:27.537752Z",
     "iopub.status.busy": "2025-05-19T07:19:27.537233Z",
     "iopub.status.idle": "2025-05-19T07:19:27.544368Z",
     "shell.execute_reply": "2025-05-19T07:19:27.543654Z"
    },
    "papermill": {
     "duration": 0.037144,
     "end_time": "2025-05-19T07:19:27.545490",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.508346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, window_size, num_heads):\n",
    "        \"\"\"Self-attention mechanism within local windows.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # (height, width)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n",
    "\n",
    "        # Compute and store relative positional encodings\n",
    "        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n",
    "        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n",
    "\n",
    "        # Learnable transformation of relative position embeddings\n",
    "        self.relative_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_heads, bias=True)\n",
    "        )\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n",
    "        batch_size, num_tokens, _ = qkv.shape\n",
    "\n",
    "        # Reshape qkv into separate query, key, and value tensors\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n",
    "\n",
    "        # Scale query for stable attention computation\n",
    "        q = q * self.scaling_factor\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Compute relative position bias\n",
    "        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n",
    "        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n",
    "        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n",
    "\n",
    "        # Apply softmax and compute weighted values\n",
    "        attention_weights = self.attention_softmax(attention_scores)\n",
    "        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "285352ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:27.604482Z",
     "iopub.status.busy": "2025-05-19T07:19:27.603858Z",
     "iopub.status.idle": "2025-05-19T07:19:27.608553Z",
     "shell.execute_reply": "2025-05-19T07:19:27.607979Z"
    },
    "papermill": {
     "duration": 0.035638,
     "end_time": "2025-05-19T07:19:27.609588",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.573950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_log_relative_positions(window_size):\n",
    "    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n",
    "    coord_range = torch.arange(window_size)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n",
    "\n",
    "    # Compute relative positions\n",
    "    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n",
    "\n",
    "    # Format and apply log transformation\n",
    "    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n",
    "    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "    return log_relative_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf668695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:27.666962Z",
     "iopub.status.busy": "2025-05-19T07:19:27.666719Z",
     "iopub.status.idle": "2025-05-19T07:19:27.679732Z",
     "shell.execute_reply": "2025-05-19T07:19:27.679027Z"
    },
    "papermill": {
     "duration": 0.042989,
     "end_time": "2025-05-19T07:19:27.680829",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.637840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n",
    "        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.network_depth = network_depth\n",
    "        self.enable_attention = enable_attention\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "        # Define convolutional processing based on mode\n",
    "        if self.conv_mode == 'Conv':\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "            )\n",
    "\n",
    "        if self.conv_mode == 'DWConv':\n",
    "            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n",
    "            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight_shape = module.weight.shape\n",
    "\n",
    "            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "            else:\n",
    "                gain = (8 * self.network_depth) ** (-1/4)\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def pad_for_window_processing(self, x, shift=False):\n",
    "        \"\"\"Pads the input tensor to fit window processing requirements.\"\"\"\n",
    "        _, _, height, width = x.size()\n",
    "        pad_h = (self.window_size - height % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - width % self.window_size) % self.window_size\n",
    "\n",
    "        if shift:\n",
    "            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n",
    "                          self.shift_size, (self.window_size - self.shift_size + pad_h) % self.window_size), mode='reflect')\n",
    "        else:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            v_proj = self.value_projection(x)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            qk_proj = self.query_key_projection(x)\n",
    "            qkv = torch.cat([qk_proj, v_proj], dim=1)\n",
    "\n",
    "            # Apply padding for shifted window processing\n",
    "            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n",
    "            padded_height, padded_width = padded_qkv.shape[2:]\n",
    "\n",
    "            # Partition into windows\n",
    "            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n",
    "            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n",
    "\n",
    "            # Apply window-based attention\n",
    "            attn_windows = self.window_attention(qkv_windows)\n",
    "\n",
    "            # Merge back to original spatial dimensions\n",
    "            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n",
    "\n",
    "            # Reverse the cyclic shift\n",
    "            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), self.shift_size:(self.shift_size + width), :]\n",
    "            attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.conv_mode in ['Conv', 'DWConv']:\n",
    "                conv_output = self.conv_layer(v_proj)\n",
    "                output = self.output_projection(conv_output + attn_output)\n",
    "            else:\n",
    "                output = self.output_projection(attn_output)\n",
    "\n",
    "        else:\n",
    "            if self.conv_mode == 'Conv':\n",
    "                output = self.conv_layer(x)  # No attention, using convolution only\n",
    "            elif self.conv_mode == 'DWConv':\n",
    "                output = self.output_projection(self.conv_layer(v_proj))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f6b00e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:27.738336Z",
     "iopub.status.busy": "2025-05-19T07:19:27.738078Z",
     "iopub.status.idle": "2025-05-19T07:19:27.748075Z",
     "shell.execute_reply": "2025-05-19T07:19:27.747445Z"
    },
    "papermill": {
     "duration": 0.040083,
     "end_time": "2025-05-19T07:19:27.749137",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.709054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n",
    "                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n",
    "        \"\"\"\n",
    "        A transformer block that includes attention (optional) and MLP layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enable_attention = enable_attention\n",
    "        self.enable_mlp_norm = enable_mlp_norm\n",
    "\n",
    "        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n",
    "        self.attention_layer = AdaptiveAttention(\n",
    "            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n",
    "            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n",
    "        )\n",
    "\n",
    "        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n",
    "        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.enable_attention:\n",
    "            x, rescale, rebias = self.pre_norm(x)\n",
    "        x = self.attention_layer(x)\n",
    "        if self.enable_attention:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        residual = x\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x, rescale, rebias = self.post_norm(x)\n",
    "        x = self.mlp_layer(x)\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, window_size=8,\n",
    "                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n",
    "        \"\"\"\n",
    "        A stage of transformer blocks with configurable attention placement.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        attention_layers = int(attention_ratio * num_layers)\n",
    "\n",
    "        if attention_placement == 'last':\n",
    "            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'first':\n",
    "            enable_attentions = [i < attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'middle':\n",
    "            enable_attentions = [\n",
    "                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(\n",
    "                network_depth=network_depth,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                enable_attention=enable_attentions[i],\n",
    "                conv_mode=conv_mode\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer stage.\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20fbe164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:27.806827Z",
     "iopub.status.busy": "2025-05-19T07:19:27.806282Z",
     "iopub.status.idle": "2025-05-19T07:19:27.812568Z",
     "shell.execute_reply": "2025-05-19T07:19:27.811967Z"
    },
    "papermill": {
     "duration": 0.036025,
     "end_time": "2025-05-19T07:19:27.813667",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.777642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch embedding module that projects input images into token embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to generate patch embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class PatchReconstruction(nn.Module):\n",
    "    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch reconstruction module that converts token embeddings back to image patches.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2, padding_mode='reflect'\n",
    "            ),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to reconstruct image from embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3173265d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:27.875522Z",
     "iopub.status.busy": "2025-05-19T07:19:27.874882Z",
     "iopub.status.idle": "2025-05-19T07:19:27.881569Z",
     "shell.execute_reply": "2025-05-19T07:19:27.881040Z"
    },
    "papermill": {
     "duration": 0.039995,
     "end_time": "2025-05-19T07:19:27.882736",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.842741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelectiveKernelFusion(nn.Module):\n",
    "    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n",
    "        \"\"\"\n",
    "        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n",
    "\n",
    "        Args:\n",
    "            channels (int): Number of input channels.\n",
    "            num_branches (int): Number of feature branches to fuse.\n",
    "            reduction_ratio (int): Reduction ratio for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_branches = num_branches\n",
    "        reduced_channels = max(int(channels / reduction_ratio), 4)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Forward pass for selective kernel fusion.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (list of tensors): A list of feature maps to be fused.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adaptively fused feature map.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = feature_maps[0].shape\n",
    "        \n",
    "        # Concatenate feature maps along a new dimension (num_branches)\n",
    "        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        aggregated_features = torch.sum(stacked_features, dim=1)\n",
    "        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n",
    "        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n",
    "\n",
    "        # Weighted sum of input feature maps\n",
    "        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c2ad69b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:27.942179Z",
     "iopub.status.busy": "2025-05-19T07:19:27.941904Z",
     "iopub.status.idle": "2025-05-19T07:19:27.956573Z",
     "shell.execute_reply": "2025-05-19T07:19:27.956033Z"
    },
    "papermill": {
     "duration": 0.045631,
     "end_time": "2025-05-19T07:19:27.957661",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.912030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DehazingTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "                 layer_depths=[16, 16, 16, 8, 8],\n",
    "                 num_heads=[2, 4, 6, 1, 1],\n",
    "                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding settings\n",
    "        self.patch_size = 4\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # Backbone layers\n",
    "        self.encoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[0],\n",
    "            num_layers=layer_depths[0],\n",
    "            num_heads=num_heads[0],\n",
    "            mlp_ratio=mlp_ratios[0],\n",
    "            norm_layer=norm_layers[0],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[0],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[0]\n",
    "        )\n",
    "        \n",
    "        self.downsample1 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "        \n",
    "        self.encoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[1],\n",
    "            num_layers=layer_depths[1],\n",
    "            num_heads=num_heads[1],\n",
    "            mlp_ratio=mlp_ratios[1],\n",
    "            norm_layer=norm_layers[1],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[1],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[1]\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "        \n",
    "        self.encoder_stage3 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[2],\n",
    "            num_layers=layer_depths[2],\n",
    "            num_heads=num_heads[2],\n",
    "            mlp_ratio=mlp_ratios[2],\n",
    "            norm_layer=norm_layers[2],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[2],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[2]\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n",
    "        \n",
    "        self.decoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[3],\n",
    "            num_layers=layer_depths[3],\n",
    "            num_heads=num_heads[3],\n",
    "            mlp_ratio=mlp_ratios[3],\n",
    "            norm_layer=norm_layers[3],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[3],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[3]\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n",
    "        \n",
    "        self.decoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[4],\n",
    "            num_layers=layer_depths[4],\n",
    "            num_heads=num_heads[4],\n",
    "            mlp_ratio=mlp_ratios[4],\n",
    "            norm_layer=norm_layers[4],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[4],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[4]\n",
    "        )\n",
    "\n",
    "        # Final patch reconstruction\n",
    "        self.patch_reconstruction = PatchReconstruction(\n",
    "            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def adjust_image_size(self, x):\n",
    "        # Ensures the input image size is compatible with the patch size\n",
    "        _, _, height, width = x.size()\n",
    "        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n",
    "        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder_stage1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.downsample1(x)\n",
    "        x = self.encoder_stage2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.downsample2(x)\n",
    "        x = self.encoder_stage3(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n",
    "        x = self.decoder_stage1(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n",
    "        x = self.decoder_stage2(x)\n",
    "        x = self.patch_reconstruction(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_height, original_width = x.shape[2:]\n",
    "        x = self.adjust_image_size(x)\n",
    "\n",
    "        features = self.extract_features(x)\n",
    "        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n",
    "\n",
    "        # Dehazing formula: I = J * t + A * (1 - t)\n",
    "        x = transmission_map * x - atmospheric_light + x\n",
    "        x = x[:, :, :original_height, :original_width]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "161a7b07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:28.015501Z",
     "iopub.status.busy": "2025-05-19T07:19:28.015166Z",
     "iopub.status.idle": "2025-05-19T07:19:28.020502Z",
     "shell.execute_reply": "2025-05-19T07:19:28.019917Z"
    },
    "papermill": {
     "duration": 0.035424,
     "end_time": "2025-05-19T07:19:28.021551",
     "exception": false,
     "start_time": "2025-05-19T07:19:27.986127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dehazing_transformer():\n",
    "    return DehazingTransformer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "        layer_depths=[12, 12, 12, 6, 6],\n",
    "        num_heads=[2, 4, 6, 1, 1],\n",
    "        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897dc82c",
   "metadata": {
    "papermill": {
     "duration": 0.027712,
     "end_time": "2025-05-19T07:19:28.077664",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.049952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ConvolutionalGuidedFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8d44ab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:28.135495Z",
     "iopub.status.busy": "2025-05-19T07:19:28.134883Z",
     "iopub.status.idle": "2025-05-19T07:19:28.142947Z",
     "shell.execute_reply": "2025-05-19T07:19:28.142021Z"
    },
    "papermill": {
     "duration": 0.038294,
     "end_time": "2025-05-19T07:19:28.144269",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.105975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvolutionalGuidedFilter(nn.Module):\n",
    "    def __init__(self, radius=1, norm_layer=nn.BatchNorm2d, conv_kernel_size: int = 1):\n",
    "        super(ConvolutionalGuidedFilter, self).__init__()\n",
    "\n",
    "        self.box_filter = nn.Conv2d(\n",
    "            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n",
    "        )\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                6,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                3,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.box_filter.weight.data[...] = 1.0\n",
    "\n",
    "    def forward(self, x_low_res, y_low_res, x_high_res):\n",
    "        _, _, h_lr, w_lr = x_low_res.size()\n",
    "        _, _, h_hr, w_hr = x_high_res.size()\n",
    "\n",
    "        N = self.box_filter(x_low_res.data.new().resize_((1, 3, h_lr, w_lr)).fill_(1.0))\n",
    "        ## mean_x\n",
    "        mean_x = self.box_filter(x_low_res) / N\n",
    "        ## mean_y\n",
    "        mean_y = self.box_filter(y_low_res) / N\n",
    "        ## cov_xy\n",
    "        cov_xy = self.box_filter(x_low_res * y_low_res) / N - mean_x * mean_y\n",
    "        ## var_x\n",
    "        var_x = self.box_filter(x_low_res * x_low_res) / N - mean_x * mean_x\n",
    "\n",
    "        ## A\n",
    "        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n",
    "        ## b\n",
    "        b = mean_y - A * mean_x\n",
    "\n",
    "        ## mean_A; mean_b\n",
    "        mean_A = F.interpolate(A, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "        mean_b = F.interpolate(b, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return mean_A * x_high_res + mean_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0e6c8",
   "metadata": {
    "papermill": {
     "duration": 0.02816,
     "end_time": "2025-05-19T07:19:28.201143",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.172983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PixelAttentionLayer and ChannelAttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92b67c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:28.260604Z",
     "iopub.status.busy": "2025-05-19T07:19:28.259928Z",
     "iopub.status.idle": "2025-05-19T07:19:28.266062Z",
     "shell.execute_reply": "2025-05-19T07:19:28.265525Z"
    },
    "papermill": {
     "duration": 0.036868,
     "end_time": "2025-05-19T07:19:28.267215",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.230347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(PixelAttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, 1, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_map = self.attention(x)\n",
    "        return x * attention_map\n",
    "\n",
    "class ChannelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelAttentionLayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, channels, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = self.avg_pool(x)\n",
    "        attention_map = self.attention(pooled)\n",
    "        return x * attention_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4504d810",
   "metadata": {
    "papermill": {
     "duration": 0.027047,
     "end_time": "2025-05-19T07:19:28.322216",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.295169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## HybridResidualDenseBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e5c7c64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:28.378742Z",
     "iopub.status.busy": "2025-05-19T07:19:28.378166Z",
     "iopub.status.idle": "2025-05-19T07:19:28.385293Z",
     "shell.execute_reply": "2025-05-19T07:19:28.384603Z"
    },
    "papermill": {
     "duration": 0.036227,
     "end_time": "2025-05-19T07:19:28.386431",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.350204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HybridResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_dense_layers=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.growth_rate = growth_rate\n",
    "        self.in_channels = in_channels\n",
    "        total_channels = in_channels\n",
    "\n",
    "        for i in range(num_dense_layers):\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(total_channels, growth_rate, kernel_size=3, padding=2**i,dilation=2**i)\n",
    "            )\n",
    "            total_channels += growth_rate\n",
    "\n",
    "        self.fusion = nn.Conv2d(total_channels, in_channels, kernel_size=1)\n",
    "        self.channel_attention = ChannelAttentionLayer(in_channels)\n",
    "        self.pixel_attention = PixelAttentionLayer(in_channels)\n",
    "\n",
    "        # Gated residual fusion\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"self.layers\", self.layers)\n",
    "        # print(\"HybridResidualDenseBlock: x.shape\", x.shape)\n",
    "        # x: (B, C, H, W)\n",
    "        features = [x]\n",
    "        # x: (B, C, H, W) -> (B, C, H, W) + (B, growth_rate, H, W) * num_dense_layers\n",
    "        # features: [(B, C, H, W), (B, growth_rate, H, W), ...] \n",
    "        for conv in self.layers:\n",
    "            # Apply convolution and ReLU activation\n",
    "            out = F.relu(conv(torch.cat(features, dim=1)))\n",
    "            # print(\"self.layers -> out.shape: \", out.shape)\n",
    "            features.append(out)\n",
    "        \n",
    "        # print(\"self.layers -> features: \", features)\n",
    "\n",
    "        dense_out = torch.cat(features, dim=1)\n",
    "        # print(\"self.layers -> dense_out.shape: \", dense_out.shape)\n",
    "        fused = self.fusion(dense_out)\n",
    "        # print(\"self.layers -> fused.shape: \", fused.shape)\n",
    "        # Apply channel attention and pixel attention\n",
    "        # fused: (B, C, H, W) -> (B, C, H, W) + (B, C, H, W) * 2\n",
    "        ca = self.channel_attention(fused)\n",
    "        # print(\"self.layers -> ca.shape: \", ca.shape)\n",
    "        pa = self.pixel_attention(ca)\n",
    "        # print(\"self.layers -> pa.shape: \", pa.shape)\n",
    "        # Gated residual fusion\n",
    "        # x: (B, C, H, W) + (B, C, H, W) * 2\n",
    "        gate_input = torch.cat([x, pa], dim=1)\n",
    "        # print(\"self.layers -> gate_input.shape: \", gate_input.shape)\n",
    "        gated_fusion = self.gate(gate_input)\n",
    "        # print(\"self.layers -> gated_fusion.shape: \", gated_fusion.shape)\n",
    "        # Apply gated fusion\n",
    "        return x * (1 - gated_fusion) + pa * gated_fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd8aa2",
   "metadata": {
    "papermill": {
     "duration": 0.027348,
     "end_time": "2025-05-19T07:19:28.441683",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.414335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## AdaptiveInstanceNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a815f95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:28.501082Z",
     "iopub.status.busy": "2025-05-19T07:19:28.500373Z",
     "iopub.status.idle": "2025-05-19T07:19:28.505162Z",
     "shell.execute_reply": "2025-05-19T07:19:28.504591Z"
    },
    "papermill": {
     "duration": 0.035757,
     "end_time": "2025-05-19T07:19:28.506324",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.470567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNormalization(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(AdaptiveInstanceNormalization, self).__init__()\n",
    "\n",
    "        # Learnable scaling factors\n",
    "        self.scale_x = nn.Parameter(torch.tensor(1.0))  # Identity scaling\n",
    "        self.scale_norm = nn.Parameter(torch.tensor(0.0))  # Initially no effect\n",
    "\n",
    "        # Instance normalization layer with affine transformation enabled\n",
    "        self.instance_norm = nn.InstanceNorm2d(num_channels, momentum=0.999, eps=0.001, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized_x = self.instance_norm(x)\n",
    "        return self.scale_x * x + self.scale_norm * normalized_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ccf302",
   "metadata": {
    "papermill": {
     "duration": 0.027849,
     "end_time": "2025-05-19T07:19:28.562809",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.534960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DEEPGUIDEDNETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd2c182c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:28.620656Z",
     "iopub.status.busy": "2025-05-19T07:19:28.620119Z",
     "iopub.status.idle": "2025-05-19T07:19:28.624250Z",
     "shell.execute_reply": "2025-05-19T07:19:28.623513Z"
    },
    "papermill": {
     "duration": 0.034223,
     "end_time": "2025-05-19T07:19:28.625407",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.591184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class DeepGuidedNetwork(nn.Module):\n",
    "#     def __init__(self, in_channels=3, radius=1):\n",
    "#         super().__init__()\n",
    "#         self.in_channels = in_channels\n",
    "#         norm = AdaptiveInstanceNormalization  # define this separately\n",
    "#         kernel_size = 3\n",
    "#         depth_rate = 64\n",
    "#         growth_rate = 16\n",
    "#         num_dense_layer = 4\n",
    "\n",
    "#         self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "#         self.conv_out = nn.Conv2d(depth_rate, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "#         self.rdb1 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "#         self.rdb2 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "#         self.rdb3 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "#         self.rdb4 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "\n",
    "#         self.tail = nn.Sequential(\n",
    "#             nn.Conv2d(depth_rate, 256, 3, padding=1),       # 64 → 256\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(256, 48, 3, padding=1), # 256 → 48 for 3 × 4²\n",
    "#             nn.PixelShuffle(4)                              # 48 → 3, upsample ×4\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, sr= False):\n",
    "#         # print(f\"Shape of x: {x.shape}\")\n",
    "#         x_in = self.conv_in(x)\n",
    "#         # print(f\"Shape of x_in: {x_in.shape}\")\n",
    "#         feat1 = self.rdb1(x_in)\n",
    "#         # print(f\"Shape of feat1: {feat1.shape}\")\n",
    "#         feat2 = self.rdb2(feat1)\n",
    "#         # print(f\"Shape of feat2: {feat2.shape}\")\n",
    "#         feat3 = self.rdb3(feat2)\n",
    "#         # print(f\"Shape of feat3: {feat3.shape}\")\n",
    "#         feat4 = self.rdb4(feat3)\n",
    "#         # print(f\"Shape of feat4: {feat4.shape}\")\n",
    "#         out_feat = self.conv_out(feat4)\n",
    "#         # print(\"CONV Tail final\", self.tail)\n",
    "\n",
    "#         sr = self.tail(out_feat)  # shape: (B, 3, H×4, W×4)\n",
    "#         return sr, sr, [feat1, feat2, feat3, feat4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57e2d0db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:28.682979Z",
     "iopub.status.busy": "2025-05-19T07:19:28.682714Z",
     "iopub.status.idle": "2025-05-19T07:19:28.690271Z",
     "shell.execute_reply": "2025-05-19T07:19:28.689679Z"
    },
    "papermill": {
     "duration": 0.037461,
     "end_time": "2025-05-19T07:19:28.691364",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.653903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class DeepGuidedNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=21, radius=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        kernel_size = 3\n",
    "        depth_rate = 64\n",
    "        growth_rate = 16\n",
    "        num_dense_layer = 4\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "        self.rdb1 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb2 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb3 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb4 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "\n",
    "        # Shared trunk\n",
    "        self.shared_trunk = nn.Sequential(\n",
    "            nn.Conv2d(depth_rate, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Segmentation Head\n",
    "        self.semantic_head = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, num_classes, 1)  # Output: [B, num_classes, H, W]\n",
    "        )\n",
    "\n",
    "        # Depth Head\n",
    "        self.depth_head = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 1, 1)  # Output: [B, 1, H, W]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_in(x)\n",
    "        feat1 = self.rdb1(x)\n",
    "        feat2 = self.rdb2(feat1)\n",
    "        feat3 = self.rdb3(feat2)\n",
    "        feat4 = self.rdb4(feat3)\n",
    "        x = self.conv_out(feat4)\n",
    "\n",
    "        shared = self.shared_trunk(x)\n",
    "\n",
    "        semantic_out = self.semantic_head(shared)\n",
    "        depth_out = self.depth_head(shared)\n",
    "\n",
    "        return semantic_out, depth_out, [feat1, feat2, feat3, feat4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9df5cf0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:28.748223Z",
     "iopub.status.busy": "2025-05-19T07:19:28.747963Z",
     "iopub.status.idle": "2025-05-19T07:19:28.755204Z",
     "shell.execute_reply": "2025-05-19T07:19:28.754677Z"
    },
    "papermill": {
     "duration": 0.036884,
     "end_time": "2025-05-19T07:19:28.756215",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.719331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepGuidedNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, radius=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        norm = AdaptiveInstanceNormalization  # define this separately\n",
    "        kernel_size = 3\n",
    "        depth_rate = 64\n",
    "        growth_rate = 16\n",
    "        num_dense_layer = 4\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "        self.rdb1 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb2 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb3 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb4 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(depth_rate, 256, 3, padding=1),       # 64 → 256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 48, 3, padding=1), # 256 → 48 for 3 × 4²\n",
    "            nn.PixelShuffle(4)                              # 48 → 3, upsample ×4\n",
    "        )\n",
    "        self.downsample = nn.Upsample(scale_factor=0.25, mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "        # Guided Filter & Dehazing Transformer\n",
    "        self.guided_filter = ConvolutionalGuidedFilter(radius, norm_layer=norm)\n",
    "        self.dehaze_network = build_dehazing_transformer()\n",
    "\n",
    "    def forward(self, x, sr= False):\n",
    "        # print(f\"Shape of x: {x.shape}\")\n",
    "        x_lr = self.downsample(x)\n",
    "        x_in = self.conv_in(x_lr)\n",
    "        # print(f\"Shape of x_in: {x_in.shape}\")\n",
    "        feat1 = self.rdb1(x_in)\n",
    "        # print(f\"Shape of feat1: {feat1.shape}\")\n",
    "        feat2 = self.rdb2(feat1)\n",
    "        # print(f\"Shape of feat2: {feat2.shape}\")\n",
    "        feat3 = self.rdb3(feat2)\n",
    "        # print(f\"Shape of feat3: {feat3.shape}\")\n",
    "        feat4 = self.rdb4(feat3)\n",
    "        # print(f\"Shape of feat4: {feat4.shape}\")\n",
    "        out_feat = self.conv_out(feat4)\n",
    "        # print(\"CONV Tail final\", self.tail)\n",
    "        \n",
    "        y_base = self.dehaze_network(x_lr)\n",
    "        # print(\"y_base\", y_base.shape)\n",
    "        # print(\"x_in\", x_in.shape)\n",
    "        # print(\"x\", x.shape)\n",
    "        refined_output = self.guided_filter(x_lr, y_base, x)\n",
    "        # print(\"refined_output\", refined_output.shape)\n",
    "        # print(\"out_feat\", out_feat.shape)\n",
    "\n",
    "        sr = self.tail(out_feat)  # shape: (B, 3, H×4, W×4)\n",
    "        # print(\"sr\", sr.shape)\n",
    "        y_out  = refined_output + sr\n",
    "        # print(\"y_out\", y_out.shape)\n",
    "        return sr, refined_output, [feat1, feat2, feat3, feat4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7771c41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:28.811843Z",
     "iopub.status.busy": "2025-05-19T07:19:28.811622Z",
     "iopub.status.idle": "2025-05-19T07:19:29.200330Z",
     "shell.execute_reply": "2025-05-19T07:19:29.199508Z"
    },
    "papermill": {
     "duration": 0.41769,
     "end_time": "2025-05-19T07:19:29.201540",
     "exception": false,
     "start_time": "2025-05-19T07:19:28.783850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test dehazing transformer\n",
    "d_transformer = build_dehazing_transformer()\n",
    "x = torch.randn(1, 3, 64, 64)  # Example input tensor\n",
    "t_out = d_transformer(x)\n",
    "t_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f82e4a1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:29.261576Z",
     "iopub.status.busy": "2025-05-19T07:19:29.261103Z",
     "iopub.status.idle": "2025-05-19T07:19:29.892166Z",
     "shell.execute_reply": "2025-05-19T07:19:29.891267Z"
    },
    "papermill": {
     "duration": 0.660482,
     "end_time": "2025-05-19T07:19:29.893411",
     "exception": false,
     "start_time": "2025-05-19T07:19:29.232929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sr shape: torch.Size([1, 3, 128, 128])\n",
      "refined_output shape: torch.Size([1, 3, 128, 128])\n",
      "feat1 shape: torch.Size([1, 64, 32, 32])\n",
      "feat2 shape: torch.Size([1, 64, 32, 32])\n",
      "feat3 shape: torch.Size([1, 64, 32, 32])\n",
      "feat4 shape: torch.Size([1, 64, 32, 32])\n",
      "t_out shape: torch.Size([1, 3, 64, 64])\n",
      "guided_filter output shape: torch.Size([1, 3, 128, 128])\n",
      "PixelAttentionLayer output shape: torch.Size([1, 64, 32, 32])\n",
      "ChannelAttentionLayer output shape: torch.Size([1, 64, 32, 32])\n",
      "HybridResidualDenseBlock output shape: torch.Size([1, 64, 32, 32])\n",
      "AdaptiveInstanceNormalization output shape: torch.Size([1, 64, 32, 32])\n",
      "PatchEmbedding output shape: torch.Size([1, 96, 32, 32])\n",
      "PatchReconstruction output shape: torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "test_net = DeepGuidedNet()\n",
    "x = torch.randn(1, 3, 128, 128)  # Example input tensor\n",
    "y_t = test_net(x)\n",
    "sr, refined_output, [feat1, feat2, feat3, feat4] = y_t\n",
    "print(\"sr shape:\", sr.shape)  # Output shape after the network\n",
    "print(\"refined_output shape:\", refined_output.shape)  # Output shape after the network\n",
    "print(\"feat1 shape:\", feat1.shape)  # Output shape after the network\n",
    "print(\"feat2 shape:\", feat2.shape)  # Output shape after the network\n",
    "print(\"feat3 shape:\", feat3.shape)  # Output shape after the network\n",
    "print(\"feat4 shape:\", feat4.shape)  # Output shape after the network\n",
    "# test the dehazing transformer\n",
    "x = torch.randn(1, 3, 64, 64)  # Example input tensor\n",
    "t_out = d_transformer(x)\n",
    "print(\"t_out shape:\", t_out.shape)  # Output shape after the network\n",
    "# test the convolutional guided filter\n",
    "x_low_res = torch.randn(1, 3, 32, 32)  # Example low-resolution input tensor\n",
    "y_low_res = torch.randn(1, 3, 32, 32)  # Example low-resolution input tensor\n",
    "x_high_res = torch.randn(1, 3, 128, 128)  # Example high-resolution input tensor\n",
    "guided_filter = ConvolutionalGuidedFilter(radius=1)\n",
    "output = guided_filter(x_low_res, y_low_res, x_high_res)\n",
    "print(\"guided_filter output shape:\", output.shape)  # Output shape after the network\n",
    "# test the pixel attention layer\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "pixel_attention_layer = PixelAttentionLayer(channels=64)\n",
    "output = pixel_attention_layer(x)\n",
    "print(\"PixelAttentionLayer output shape:\", output.shape)  # Output shape after the network\n",
    "# test the channel attention layer\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "channel_attention_layer = ChannelAttentionLayer(channels=64)\n",
    "output = channel_attention_layer(x)\n",
    "print(\"ChannelAttentionLayer output shape:\", output.shape)  # Output shape after the network\n",
    "# test the hybrid residual dense block\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "hybrid_residual_dense_block = HybridResidualDenseBlock(in_channels=64, growth_rate=16, num_dense_layers=4)\n",
    "output = hybrid_residual_dense_block(x)\n",
    "print(\"HybridResidualDenseBlock output shape:\", output.shape)  # Output shape after the network\n",
    "# test the adaptive instance normalization\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "adaptive_instance_norm = AdaptiveInstanceNormalization(num_channels=64)\n",
    "output = adaptive_instance_norm(x)\n",
    "print(\"AdaptiveInstanceNormalization output shape:\", output.shape)  # Output shape after the network\n",
    "# test the patch embedding\n",
    "x = torch.randn(1, 3, 128, 128)  # Example input tensor\n",
    "# (batch_size, channels, height, width)\n",
    "patch_embedding = PatchEmbedding(patch_size=4, input_channels=3, embedding_dim=96, kernel_size=3)\n",
    "output = patch_embedding(x)\n",
    "print(\"PatchEmbedding output shape:\", output.shape)  # Output shape after the network\n",
    "# test the patch reconstruction\n",
    "x = torch.randn(1, 96, 32, 32)  # Example input tensor\n",
    "# (batch_size, channels, height, width)\n",
    "patch_reconstruction = PatchReconstruction(patch_size=4, output_channels=3, embedding_dim=96, kernel_size=3)\n",
    "output = patch_reconstruction(x)\n",
    "print(\"PatchReconstruction output shape:\", output.shape)  # Output shape after the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818cd2a0",
   "metadata": {
    "papermill": {
     "duration": 0.027644,
     "end_time": "2025-05-19T07:19:29.952760",
     "exception": false,
     "start_time": "2025-05-19T07:19:29.925116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With pixle shuffle 2 and conv out 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ed9e3df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:30.011028Z",
     "iopub.status.busy": "2025-05-19T07:19:30.010337Z",
     "iopub.status.idle": "2025-05-19T07:19:30.273878Z",
     "shell.execute_reply": "2025-05-19T07:19:30.272972Z"
    },
    "papermill": {
     "duration": 0.293532,
     "end_time": "2025-05-19T07:19:30.275145",
     "exception": false,
     "start_time": "2025-05-19T07:19:29.981613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 21, 128, 128])\n",
      "Feature shapes: [torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128])]\n"
     ]
    }
   ],
   "source": [
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, 3, 128, 128)  # Batch size of 1, 3 channels, 256x256 image\n",
    "\n",
    "# Initialize the DeepGuidedNetwork\n",
    "model = DeepGuidedNetwork(radius=1)\n",
    "   \n",
    "# Forward pass through the network\n",
    "output_tensor, dummy_out, features = model(input_tensor)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output_tensor.shape)\n",
    "print(\"Feature shapes:\", [f.shape for f in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d1ec0c",
   "metadata": {
    "papermill": {
     "duration": 0.027951,
     "end_time": "2025-05-19T07:19:30.334423",
     "exception": false,
     "start_time": "2025-05-19T07:19:30.306472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With pixle shuffle 2 and conv out 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae4aa2a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:30.392036Z",
     "iopub.status.busy": "2025-05-19T07:19:30.391464Z",
     "iopub.status.idle": "2025-05-19T07:19:30.394915Z",
     "shell.execute_reply": "2025-05-19T07:19:30.394339Z"
    },
    "papermill": {
     "duration": 0.033223,
     "end_time": "2025-05-19T07:19:30.395873",
     "exception": false,
     "start_time": "2025-05-19T07:19:30.362650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create a random input tensor\n",
    "# input_tensor = torch.randn(1, 3, 128, 128)  # Batch size of 1, 3 channels, 256x256 image\n",
    "\n",
    "# # Initialize the DeepGuidedNetwork\n",
    "# model = DeepGuidedNetwork(radius=1)\n",
    "\n",
    "# # Forward pass through the network\n",
    "# output_tensor, dummy_out, features = model(input_tensor)\n",
    "\n",
    "# # Print the output shape\n",
    "# print(\"Output shape:\", output_tensor.shape)\n",
    "# print(\"Feature shapes:\", [f.shape for f in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "70638589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:30.453433Z",
     "iopub.status.busy": "2025-05-19T07:19:30.452781Z",
     "iopub.status.idle": "2025-05-19T07:19:30.456085Z",
     "shell.execute_reply": "2025-05-19T07:19:30.455519Z"
    },
    "papermill": {
     "duration": 0.032882,
     "end_time": "2025-05-19T07:19:30.457122",
     "exception": false,
     "start_time": "2025-05-19T07:19:30.424240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test to_psnr\n",
    "# to_psnr(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa174d3",
   "metadata": {
    "papermill": {
     "duration": 0.027285,
     "end_time": "2025-05-19T07:19:30.512331",
     "exception": false,
     "start_time": "2025-05-19T07:19:30.485046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c51812",
   "metadata": {
    "papermill": {
     "duration": 0.02702,
     "end_time": "2025-05-19T07:19:30.566709",
     "exception": false,
     "start_time": "2025-05-19T07:19:30.539689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb84e4d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:30.622845Z",
     "iopub.status.busy": "2025-05-19T07:19:30.622238Z",
     "iopub.status.idle": "2025-05-19T07:19:30.626554Z",
     "shell.execute_reply": "2025-05-19T07:19:30.626030Z"
    },
    "papermill": {
     "duration": 0.032863,
     "end_time": "2025-05-19T07:19:30.627534",
     "exception": false,
     "start_time": "2025-05-19T07:19:30.594671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_psnr(dehaze, gt):\n",
    "    \"\"\"\n",
    "    Compute PSNR (Peak Signal-to-Noise Ratio) between dehazed and ground truth images.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: PSNR values for each image in the batch.\n",
    "    \"\"\"\n",
    "    # print(\"Shapes: \", dehaze.shape, gt.shape)\n",
    "    mse = F.mse_loss(dehaze, gt, reduction='none').mean(dim=[1, 2, 3])  # Compute MSE per image\n",
    "    intensity_max = 1.0\n",
    "\n",
    "    # Compute PSNR safely, avoiding division by zero and extreme values\n",
    "    psnr_list = [10.0 * log10(intensity_max / max(mse_val.item(), 1e-6)) for mse_val in mse]\n",
    "\n",
    "    return psnr_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e8a137",
   "metadata": {
    "papermill": {
     "duration": 0.027526,
     "end_time": "2025-05-19T07:19:30.682552",
     "exception": false,
     "start_time": "2025-05-19T07:19:30.655026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cdd278b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:30.740382Z",
     "iopub.status.busy": "2025-05-19T07:19:30.739713Z",
     "iopub.status.idle": "2025-05-19T07:19:34.864618Z",
     "shell.execute_reply": "2025-05-19T07:19:34.864046Z"
    },
    "papermill": {
     "duration": 4.155157,
     "end_time": "2025-05-19T07:19:34.865878",
     "exception": false,
     "start_time": "2025-05-19T07:19:30.710721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "# Define SSIM metric\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0, reduction='none')\n",
    "\n",
    "def to_ssim(dehaze: torch.Tensor, gt: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute SSIM directly on the GPU using torchmetrics.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: SSIM values for each image in the batch.\n",
    "    \"\"\"\n",
    "    ssim_values = ssim_metric(dehaze, gt)  # Shape: [B]\n",
    "    # print(\"1\",ssim_values)\n",
    "    # print(\"2\",[ssim_values])\n",
    "    ssim_values = ssim_values.tolist() \n",
    "    # print(type(ssim_values))\n",
    "    if isinstance(ssim_values, float):  # Correct way to check for a float\n",
    "        return [ssim_values]  # Convert single float to a list\n",
    "    return ssim_values  # Otherwise, return as is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb83f548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:34.923642Z",
     "iopub.status.busy": "2025-05-19T07:19:34.923174Z",
     "iopub.status.idle": "2025-05-19T07:19:35.006686Z",
     "shell.execute_reply": "2025-05-19T07:19:35.005882Z"
    },
    "papermill": {
     "duration": 0.113238,
     "end_time": "2025-05-19T07:19:35.007967",
     "exception": false,
     "start_time": "2025-05-19T07:19:34.894729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004548392724245787]\n"
     ]
    }
   ],
   "source": [
    "# Test with a dummy tensor\n",
    "dehaze = torch.rand(1, 3, 360, 360)  # Random batch of images\n",
    "gt = torch.rand(1, 3, 360, 360)  # Random ground truth images\n",
    "\n",
    "ssim_scores = to_ssim(dehaze, gt)\n",
    "print(ssim_scores)  # Should print a list of 6 SSIM values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ecc49",
   "metadata": {
    "papermill": {
     "duration": 0.028326,
     "end_time": "2025-05-19T07:19:35.066031",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.037705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Validation Haze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fdd5e849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:35.124159Z",
     "iopub.status.busy": "2025-05-19T07:19:35.123513Z",
     "iopub.status.idle": "2025-05-19T07:19:35.129029Z",
     "shell.execute_reply": "2025-05-19T07:19:35.128411Z"
    },
    "papermill": {
     "duration": 0.035917,
     "end_time": "2025-05-19T07:19:35.130064",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.094147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validationB(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: Your deep learning model\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: GPU/CPU device\n",
    "    :param category: dataset type (indoor/outdoor)\n",
    "    :param save_tag: whether to save images\n",
    "    :return: average PSNR & SSIM values\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    \n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            haze, gt = val_data\n",
    "            haze, gt = haze.to(device), gt.to(device)\n",
    "            dehaze, _, _ = net(haze)\n",
    "\n",
    "        # --- Compute PSNR & SSIM --- #\n",
    "        batch_psnr = to_psnr(dehaze, gt)  # This returns a list\n",
    "        # print(batch_psnr)\n",
    "        batch_ssim = to_ssim(dehaze, gt)  # This returns a list\n",
    "        # print(batch_ssim)\n",
    "\n",
    "        psnr_list.extend(batch_psnr)  # Flatten the list\n",
    "        ssim_list.extend(batch_ssim)  # Flatten the list\n",
    "\n",
    "    # --- Ensure lists are not empty to avoid division by zero --- #\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710465c",
   "metadata": {
    "papermill": {
     "duration": 0.028622,
     "end_time": "2025-05-19T07:19:35.187431",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.158809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Validation SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2826b26e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:35.245021Z",
     "iopub.status.busy": "2025-05-19T07:19:35.244405Z",
     "iopub.status.idle": "2025-05-19T07:19:35.249810Z",
     "shell.execute_reply": "2025-05-19T07:19:35.249259Z"
    },
    "papermill": {
     "duration": 0.03485,
     "end_time": "2025-05-19T07:19:35.250788",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.215938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation_sr(net, sr_val_loader, device):\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    for lr, hr in sr_val_loader:\n",
    "        with torch.no_grad():\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr_out, _, _ = net(lr, sr = False)\n",
    "            hr = F.interpolate(hr, size=sr_out.shape[2:], mode='bilinear', align_corners=False)\n",
    "        # print(\"Shapes 1: \", sr_out.shape, hr.shape)\n",
    "        psnr_list.extend(to_psnr(sr_out, hr))\n",
    "        ssim_list.extend(to_ssim(sr_out, hr))\n",
    "\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212652c1",
   "metadata": {
    "papermill": {
     "duration": 0.027098,
     "end_time": "2025-05-19T07:19:35.305902",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.278804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f664a42d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:35.361948Z",
     "iopub.status.busy": "2025-05-19T07:19:35.361666Z",
     "iopub.status.idle": "2025-05-19T07:19:35.364949Z",
     "shell.execute_reply": "2025-05-19T07:19:35.364434Z"
    },
    "papermill": {
     "duration": 0.032457,
     "end_time": "2025-05-19T07:19:35.366037",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.333580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5595b958",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:35.422370Z",
     "iopub.status.busy": "2025-05-19T07:19:35.422098Z",
     "iopub.status.idle": "2025-05-19T07:19:35.425545Z",
     "shell.execute_reply": "2025-05-19T07:19:35.424827Z"
    },
    "papermill": {
     "duration": 0.032914,
     "end_time": "2025-05-19T07:19:35.426700",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.393786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# psnr, ssim = validationB(net, val_data_loader, device, category)\n",
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # psnr, ssim = validationB(model, val_loader, device, \"indoor\", save_tag=True)\n",
    "# print(f\"Validation PSNR: {psnr:.2f}, SSIM: {ssim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe953e",
   "metadata": {
    "papermill": {
     "duration": 0.028636,
     "end_time": "2025-05-19T07:19:35.484169",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.455533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b997c9",
   "metadata": {
    "papermill": {
     "duration": 0.028506,
     "end_time": "2025-05-19T07:19:35.540984",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.512478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exec Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1869df97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:35.598425Z",
     "iopub.status.busy": "2025-05-19T07:19:35.598132Z",
     "iopub.status.idle": "2025-05-19T07:19:35.607276Z",
     "shell.execute_reply": "2025-05-19T07:19:35.606712Z"
    },
    "papermill": {
     "duration": 0.039306,
     "end_time": "2025-05-19T07:19:35.608434",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.569128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fbadd8f3604440a8649d44723a1bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Execution Env:', index=1, options=('local', 'kaggle'), value='kaggle')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execution_env_widget = widgets.Dropdown(options=['local', 'kaggle'], value='kaggle', description='Execution Env:')\n",
    "display(execution_env_widget)\n",
    "\n",
    "# check if not in windows and not in kaggle\n",
    "if os.path.exists('/kaggle') and os.path.exists('/mnt'):\n",
    "    execution_env_widget.value = 'kaggle' \n",
    "else:\n",
    "    execution_env_widget.value = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a957afc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:35.666899Z",
     "iopub.status.busy": "2025-05-19T07:19:35.666623Z",
     "iopub.status.idle": "2025-05-19T07:19:35.697164Z",
     "shell.execute_reply": "2025-05-19T07:19:35.696527Z"
    },
    "papermill": {
     "duration": 0.061352,
     "end_time": "2025-05-19T07:19:35.698821",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.637469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81919ff853a4a7faab34fa2c9deb5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Learning Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b550fbd96ea406e9a8719c07cd1aa68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='128,128', description='Crop Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d78706df444077a44af44eaedfaf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Train Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5687db1f682743ebace84d6c668ff4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=0, description='Version:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a42a488b2404081b251f0ebcb787645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='Growth Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f639df3b62e4b6c876f45d2c32314e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.04, description='Lambda Loss:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97d9e6729d345e1b0c315d788b2fe1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Val Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c92c2a97ae042fba960a9d022d1f4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', index=2, options=('indoor', 'outdoor', 'reside', 'nh'), value='reside')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters set:\n",
      "learning_rate: 0.0001\n",
      "crop_size: [128, 128]\n",
      "train_batch_size: 2\n",
      "version: 0\n",
      "growth_rate: 16\n",
      "lambda_loss: 0.04\n",
      "val_batch_size: 2\n",
      "category: reside\n",
      "execution_env: kaggle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Create widgets for each hyper-parameter ---\n",
    "learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n",
    "crop_size_widget = widgets.Text(value='128,128', description='Crop Size:')\n",
    "train_batch_size_widget = widgets.IntText(value=2, description='Train Batch Size:')\n",
    "version_widget = widgets.IntText(value=0, description='Version:')\n",
    "growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n",
    "lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n",
    "val_batch_size_widget = widgets.IntText(value=2, description='Val Batch Size:')\n",
    "category_widget = widgets.Dropdown(options=['indoor', 'outdoor', 'reside', 'nh'], value='reside', description='Category:')\n",
    "\n",
    "# --- Display the widgets ---\n",
    "display(\n",
    "    learning_rate_widget, crop_size_widget, train_batch_size_widget, version_widget,\n",
    "    growth_rate_widget, lambda_loss_widget, \n",
    "    val_batch_size_widget, category_widget\n",
    ")\n",
    "\n",
    "# --- Function to parse crop size ---\n",
    "def parse_crop_size(crop_size_str):\n",
    "    return [int(x) for x in crop_size_str.split(',')]\n",
    "\n",
    "# --- Assign the widget values to variables ---\n",
    "learning_rate = learning_rate_widget.value\n",
    "crop_size = parse_crop_size(crop_size_widget.value)\n",
    "train_batch_size = train_batch_size_widget.value\n",
    "version = version_widget.value\n",
    "growth_rate = growth_rate_widget.value\n",
    "lambda_loss = lambda_loss_widget.value\n",
    "val_batch_size = val_batch_size_widget.value\n",
    "category = category_widget.value\n",
    "\n",
    "execution_env = execution_env_widget.value  # Local or Kaggle\n",
    "\n",
    "\n",
    "print('\\nHyper-parameters set:')\n",
    "print(f'learning_rate: {learning_rate}')\n",
    "print(f'crop_size: {crop_size}')\n",
    "print(f'train_batch_size: {train_batch_size}')\n",
    "print(f'version: {version}')\n",
    "print(f'growth_rate: {growth_rate}')\n",
    "print(f'lambda_loss: {lambda_loss}')\n",
    "print(f'val_batch_size: {val_batch_size}')\n",
    "print(f'category: {category}')\n",
    "print(f'execution_env: {execution_env}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0b47a",
   "metadata": {
    "papermill": {
     "duration": 0.027834,
     "end_time": "2025-05-19T07:19:35.755473",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.727639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Paths Dehaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b907dde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:35.812784Z",
     "iopub.status.busy": "2025-05-19T07:19:35.812276Z",
     "iopub.status.idle": "2025-05-19T07:19:35.815828Z",
     "shell.execute_reply": "2025-05-19T07:19:35.815126Z"
    },
    "papermill": {
     "duration": 0.03339,
     "end_time": "2025-05-19T07:19:35.816898",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.783508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0ff4750f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:35.875395Z",
     "iopub.status.busy": "2025-05-19T07:19:35.874739Z",
     "iopub.status.idle": "2025-05-19T07:19:35.878200Z",
     "shell.execute_reply": "2025-05-19T07:19:35.877643Z"
    },
    "papermill": {
     "duration": 0.03427,
     "end_time": "2025-05-19T07:19:35.879372",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.845102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # hazeeffected_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "# # hazefree_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "# # hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "# # hazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/GT'\n",
    "\n",
    "# hazeeffected_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN'\n",
    "# hazefree_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT'\n",
    "# hazeeffected_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN'\n",
    "# hazefree_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/GT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eaabffe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:35.939175Z",
     "iopub.status.busy": "2025-05-19T07:19:35.938618Z",
     "iopub.status.idle": "2025-05-19T07:19:35.942335Z",
     "shell.execute_reply": "2025-05-19T07:19:35.941793Z"
    },
    "papermill": {
     "duration": 0.034562,
     "end_time": "2025-05-19T07:19:35.943475",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.908913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Define paths\n",
    "# input_dir = 'dataset/SR_flickr/kaggle/working/filtered_HR'\n",
    "# output_dir = 'dataset/SR_flickr/kaggle/working/filtered_LR_2'\n",
    "\n",
    "# # Create output directory if it doesn't exist\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Function to resize images to half their resolution\n",
    "# def resize_image(input_path, output_path, target_size=(128, 128)):\n",
    "#     img = Image.open(input_path)\n",
    "#     img_resized = img.resize(target_size, Image.LANCZOS)\n",
    "#     img_resized.save(output_path)\n",
    "\n",
    "# # Get list of images in input directory\n",
    "# try:\n",
    "#     images = [f for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "# except FileNotFoundError as e:\n",
    "#     images = []\n",
    "#     error_message = str(e)\n",
    "\n",
    "# # Resize each image and save to output directory using tqdm\n",
    "# if images:\n",
    "#     for image_name in tqdm(images, desc=\"Resizing images\"):\n",
    "#         input_path = os.path.join(input_dir, image_name)\n",
    "#         output_path = os.path.join(output_dir, image_name)\n",
    "#         resize_image(input_path, output_path, target_size=(128, 128))\n",
    "#     result = \"Resizing completed\"\n",
    "# else:\n",
    "#     result = f\"Error: {error_message}\"\n",
    "\n",
    "# result = \"Resizing completed\" if images else f\"Error: {error_message}\"\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d398008a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:36.003378Z",
     "iopub.status.busy": "2025-05-19T07:19:36.003116Z",
     "iopub.status.idle": "2025-05-19T07:19:36.007740Z",
     "shell.execute_reply": "2025-05-19T07:19:36.007208Z"
    },
    "papermill": {
     "duration": 0.035514,
     "end_time": "2025-05-19T07:19:36.008851",
     "exception": false,
     "start_time": "2025-05-19T07:19:35.973337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_log(epoch, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, category):\n",
    "    log_dir = \"./training_log\"\n",
    "    os.makedirs(log_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    log_path = os.path.join(log_dir, f\"{category}_log.txt\")\n",
    "\n",
    "    print('({0:.0f}s) Epoch [{1}/{2}], Train_PSNR:{3:.2f}, Val_PSNR:{4:.2f}, Val_SSIM:{5:.4f}'\n",
    "          .format(one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim))\n",
    "\n",
    "    # --- Write the training log --- #\n",
    "    with open(log_path, 'a') as f:\n",
    "        print('Date: {0}, Time_Cost: {1:.0f}s, Epoch: [{2}/{3}], Train_PSNR: {4:.2f}, Val_PSNR: {5:.2f}, Val_SSIM: {6:.4f}'\n",
    "              .format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "                      one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c0f2fe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:36.069814Z",
     "iopub.status.busy": "2025-05-19T07:19:36.069139Z",
     "iopub.status.idle": "2025-05-19T07:19:36.072584Z",
     "shell.execute_reply": "2025-05-19T07:19:36.072057Z"
    },
    "papermill": {
     "duration": 0.034541,
     "end_time": "2025-05-19T07:19:36.073708",
     "exception": false,
     "start_time": "2025-05-19T07:19:36.039167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def adjust_learning_rate(optimizer, epoch, category, lr_decay=0.90):\n",
    "#     \"\"\"\n",
    "#     Adjusts the learning rate based on the epoch and dataset category.\n",
    "\n",
    "#     :param optimizer: The optimizer (e.g., Adam, SGD).\n",
    "#     :param epoch: Current epoch number.\n",
    "#     :param category: Dataset category ('indoor', 'outdoor', or 'NH').\n",
    "#     :param lr_decay: Multiplicative factor for learning rate decay.\n",
    "#     \"\"\"\n",
    "#     # Define learning rate decay steps based on category\n",
    "#     step_dict = {'indoor': 18, 'outdoor': 3, 'NH': 20}\n",
    "#     step = step_dict.get(category, 3)  # Default step size if category is unknown\n",
    "\n",
    "#     # Decay learning rate at the specified step\n",
    "#     if epoch > 0 and epoch % step == 0:\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] *= lr_decay\n",
    "#             print(f\"Epoch {epoch}: Learning rate adjusted to {param_group['lr']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6aa84b",
   "metadata": {
    "papermill": {
     "duration": 0.028016,
     "end_time": "2025-05-19T07:19:36.131363",
     "exception": false,
     "start_time": "2025-05-19T07:19:36.103347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dff039cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:36.189450Z",
     "iopub.status.busy": "2025-05-19T07:19:36.189210Z",
     "iopub.status.idle": "2025-05-19T07:19:36.198770Z",
     "shell.execute_reply": "2025-05-19T07:19:36.198227Z"
    },
    "papermill": {
     "duration": 0.039963,
     "end_time": "2025-05-19T07:19:36.199811",
     "exception": false,
     "start_time": "2025-05-19T07:19:36.159848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Perceptual Feature Loss Network --- #\n",
    "class PerceptualLossNet(nn.Module):\n",
    "    def __init__(self, vgg_model, use_style_loss=False, style_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = vgg_model\n",
    "        self.feature_layers = {'3': \"low_level\", '8': \"mid_level\", '15': \"high_level\"}\n",
    "        self.use_style_loss = use_style_loss\n",
    "        self.style_weight = style_weight\n",
    "\n",
    "        # Freeze VGG\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def get_feature_maps(self, x):\n",
    "        feature_maps = []\n",
    "        for layer_id, layer in self.feature_extractor.named_children():\n",
    "            x = layer(x)\n",
    "            if layer_id in self.feature_layers:\n",
    "                feature_maps.append(x)\n",
    "        return feature_maps\n",
    "\n",
    "    def compute_gram(self, feature):\n",
    "        b, c, h, w = feature.size()\n",
    "        feature = feature.view(b, c, -1)\n",
    "        gram = torch.bmm(feature, feature.transpose(1, 2)) / (c * h * w)\n",
    "        return gram\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        pred_feats = self.get_feature_maps(predicted)\n",
    "        target_feats = self.get_feature_maps(target)\n",
    "\n",
    "        # Perceptual loss (normalized MSE)\n",
    "        perceptual_loss = torch.stack([\n",
    "            F.mse_loss(F.normalize(p, dim=1), F.normalize(t, dim=1))\n",
    "            for p, t in zip(pred_feats, target_feats)\n",
    "        ]).mean()\n",
    "\n",
    "        if self.use_style_loss:\n",
    "            style_loss = torch.stack([\n",
    "                F.mse_loss(self.compute_gram(p), self.compute_gram(t))\n",
    "                for p, t in zip(pred_feats, target_feats)\n",
    "            ]).mean()\n",
    "            return perceptual_loss + self.style_weight * style_loss\n",
    "\n",
    "        return perceptual_loss\n",
    "\n",
    "\n",
    "class SSFM(nn.Module):\n",
    "    def __init__(self, loss_type='l1'):\n",
    "        super(SSFM, self).__init__()\n",
    "        assert loss_type in ['l1', 'l2'], \"loss_type must be 'l1' or 'l2'\"\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def forward(self, student_feats, teacher_feats):\n",
    "        \"\"\"\n",
    "        student_feats: List of feature maps from student RDBs [rdb1, rdb2, rdb3, rdb4]\n",
    "        teacher_feats: List of corresponding feature maps from teacher\n",
    "        \"\"\"\n",
    "        assert len(student_feats) == len(teacher_feats), \"Feature lists must match\"\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for s_feat, t_feat in zip(student_feats, teacher_feats):\n",
    "            # Match resolution\n",
    "            if s_feat.shape != t_feat.shape:\n",
    "                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            if self.loss_type == 'l1':\n",
    "                loss = F.l1_loss(s_feat, t_feat)\n",
    "            else:\n",
    "                loss = F.mse_loss(s_feat, t_feat)\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss / len(student_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "674b9cad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:36.258589Z",
     "iopub.status.busy": "2025-05-19T07:19:36.258387Z",
     "iopub.status.idle": "2025-05-19T07:19:40.953134Z",
     "shell.execute_reply": "2025-05-19T07:19:40.952285Z"
    },
    "papermill": {
     "duration": 4.725464,
     "end_time": "2025-05-19T07:19:40.954540",
     "exception": false,
     "start_time": "2025-05-19T07:19:36.229076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:02<00:00, 215MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No pretrained weights found at /kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\n",
      "📊 Total Trainable Parameters: 5,210,051\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "\n",
    "# --- Initialize Model --- #\n",
    "net = DeepGuidedNet().to(device)\n",
    "\n",
    "\n",
    "# --- Optimizer --- #\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Load Pretrained VGG16 for Perceptual Loss --- #\n",
    "vgg_features = vgg16(pretrained=True).features[:16].to(device)\n",
    "for param in vgg_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "loss_network = PerceptualLossNet(vgg_features)\n",
    "loss_network.eval()\n",
    "\n",
    "# --- Load Model Weights (if available) --- #\n",
    "checkpoint_path = \"/kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\" \n",
    "\n",
    "try:\n",
    "    net.load_state_dict(torch.load(checkpoint_path, weights_only=False, map_location=torch.device('cpu')))\n",
    "    print(f\"✅ Model weights loaded from {checkpoint_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️ No pretrained weights found at {checkpoint_path}\")\n",
    "\n",
    "# --- Compute Total Trainable Parameters --- #\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"📊 Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5210b153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:41.019479Z",
     "iopub.status.busy": "2025-05-19T07:19:41.018754Z",
     "iopub.status.idle": "2025-05-19T07:19:41.024834Z",
     "shell.execute_reply": "2025-05-19T07:19:41.024296Z"
    },
    "papermill": {
     "duration": 0.039154,
     "end_time": "2025-05-19T07:19:41.025911",
     "exception": false,
     "start_time": "2025-05-19T07:19:40.986757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeatureAffinityModule(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(FeatureAffinityModule, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def forward(self, student_features, teacher_features):\n",
    "        # Debug: Print input shapes\n",
    "        # print(\"Input student_features shape:\", student_features.shape)\n",
    "        # print(\"Input teacher_features shape:\", teacher_features.shape)\n",
    "\n",
    "        # # Normalize features\n",
    "        # print(\"\\nBefore normalization:\")\n",
    "        # print(\"Student features:\", student_features.shape)  # Example for the first feature of the first batch\n",
    "        # print(\"Teacher features:\", teacher_features.shape)  # Example for the first feature of the first batch\n",
    "\n",
    "        student_norm = F.normalize(student_features.view(student_features.size(0), self.channels, -1), dim=2)\n",
    "        # print(\"Student normalized features:\", student_norm.shape)  # Checking first normalized value\n",
    "\n",
    "        # print(\"\\nother:\")\n",
    "        # print(\"student_features.size(0)\",student_features.size(0))\n",
    "        # print(\"teacher_features.size(0)\",teacher_features.size(0))\n",
    "        # print(\"self.channels\",self.channels)\n",
    "        # print(\"student_features.view(student_features.size(0), self.channels, -1)\",student_features.view(student_features.size(0), self.channels, -1).shape)\n",
    "        # print(\"teacher_features.view(teacher_features.size(0), self.channels, -1)\",teacher_features.view(teacher_features.size(0), self.channels, -1).shape)\n",
    "        \n",
    "        teacher_norm = F.normalize(teacher_features.view(teacher_features.size(0), self.channels, -1), dim=2)\n",
    "        # print(\"Teacher normalized features:\", teacher_norm[0, 0, :2])  # Checking first normalized value\n",
    "\n",
    "        # Compute affinity matrices\n",
    "        student_affinity = torch.bmm(student_norm, student_norm.transpose(1, 2))\n",
    "        teacher_affinity = torch.bmm(teacher_norm, teacher_norm.transpose(1, 2))\n",
    "\n",
    "        # Debug: Print affinity matrix shapes\n",
    "        # print(\"\\nStudent affinity matrix shape:\", student_affinity.shape)\n",
    "        # print(\"Teacher affinity matrix shape:\", teacher_affinity.shape)\n",
    "\n",
    "        # Compute KL divergence\n",
    "        loss = F.kl_div(F.log_softmax(student_affinity, dim=-1),\n",
    "                        F.softmax(teacher_affinity, dim=-1),\n",
    "                        reduction='batchmean')\n",
    "\n",
    "        # Debug: Print computed loss\n",
    "        # print(\"\\nComputed loss:\", loss.item())\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dbb004c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:41.090591Z",
     "iopub.status.busy": "2025-05-19T07:19:41.089888Z",
     "iopub.status.idle": "2025-05-19T07:19:41.093342Z",
     "shell.execute_reply": "2025-05-19T07:19:41.092734Z"
    },
    "papermill": {
     "duration": 0.03647,
     "end_time": "2025-05-19T07:19:41.094610",
     "exception": false,
     "start_time": "2025-05-19T07:19:41.058140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f275f434",
   "metadata": {
    "papermill": {
     "duration": 0.030783,
     "end_time": "2025-05-19T07:19:41.156955",
     "exception": false,
     "start_time": "2025-05-19T07:19:41.126172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "345ced68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:41.220415Z",
     "iopub.status.busy": "2025-05-19T07:19:41.219834Z",
     "iopub.status.idle": "2025-05-19T07:19:41.231590Z",
     "shell.execute_reply": "2025-05-19T07:19:41.231061Z"
    },
    "papermill": {
     "duration": 0.044792,
     "end_time": "2025-05-19T07:19:41.232624",
     "exception": false,
     "start_time": "2025-05-19T07:19:41.187832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(train_loader, model, device, optimizer):\n",
    "    # iteration for all batches\n",
    "    model.train()\n",
    "    losses = {'semantic': [], 'depth': [], 'total': []}\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, batch in pbar:\n",
    "        images = batch['image']\n",
    "        # downsample image by factor of 4\n",
    "        # images = F.interpolate(images, scale_factor=0.25, mode='bilinear', align_corners=False)\n",
    "        images = images.to(device)\n",
    "        # print(\"Shape of images: \", images.shape)\n",
    "\n",
    "        semantic = batch['semantic'].long().to(device)\n",
    "        # print(torch.unique(semantic))  # should be in [0, C-1] or include -1 for ignore_index\n",
    "        depth = batch['depth'].to(device)\n",
    "        output = model(images)\n",
    "        # print(torch.unique(output[0]))\n",
    "        \n",
    "        # print(\"Shape of output and semantic: \", output[0].shape, semantic.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = {\n",
    "            'semantic': compute_loss(output[0], semantic, 'semantic'),\n",
    "            'depth': compute_loss(output[1], depth, 'depth')\n",
    "        }\n",
    "\n",
    "        loss = train_loss['semantic'] + train_loss['depth']\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses['semantic'].append(train_loss['semantic'].item())\n",
    "        losses['depth'].append(train_loss['depth'].item())\n",
    "        losses['total'].append(loss.item())\n",
    "\n",
    "        pbar.set_description(f\"Train | Sem: {losses['semantic'][-1]:.4f} | Depth: {losses['depth'][-1]:.4f} | Total: {losses['total'][-1]:.4f}\")\n",
    "\n",
    "    avg_losses = {task: sum(task_loss) / len(task_loss) for task, task_loss in losses.items()}\n",
    "    # print(1)\n",
    "    return avg_losses\n",
    "\n",
    "def evaluation_epoch(val_loader, model, device):\n",
    "    # iteration for all batches\n",
    "    model.eval()\n",
    "    losses = {'semantic': [], 'depth': [], 'total': []}\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "        for i, batch in pbar:\n",
    "            images = batch['image'].to(device)\n",
    "            # images = F.interpolate(images, scale_factor=0.25, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            semantic = batch['semantic'].long().to(device)\n",
    "            depth = batch['depth'].to(device)\n",
    "\n",
    "            output = model(images)\n",
    "\n",
    "            # Process semantic output\n",
    "            # semantic_output = torch.argmax(output['semantic'], dim=1).squeeze()\n",
    "\n",
    "            train_loss = {\n",
    "                'semantic': compute_loss(output[0], semantic, 'semantic'),\n",
    "                'depth': compute_loss(output[1], depth, 'depth')\n",
    "            }\n",
    "\n",
    "            loss = train_loss['semantic'] + train_loss['depth']\n",
    "\n",
    "            losses['semantic'].append(train_loss['semantic'].item())\n",
    "            losses['depth'].append(train_loss['depth'].item())\n",
    "            losses['total'].append(loss.item())\n",
    "\n",
    "            pbar.set_description(f\"Eval  | Sem: {losses['semantic'][-1]:.4f} | Depth: {losses['depth'][-1]:.4f} | Total: {losses['total'][-1]:.4f}\")\n",
    "\n",
    "    avg_losses = {task: sum(task_loss) / len(task_loss) for task, task_loss in losses.items()}\n",
    "    return avg_losses\n",
    "\n",
    "def train(train_loader, val_loader, model, device, optimizer, scheduler, epochs):\n",
    "    best_loss = 100.\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(train_loader, model, device, optimizer)\n",
    "        val_loss = evaluation_epoch(val_loader, model, device)  \n",
    "        scheduler.step()\n",
    "        print(train_loss, val_loss)\n",
    "            \n",
    "        print('Epoch: {:04d} | Train: Semantic Loss {:.4f} - Depth Loss {:.4f} - Total Loss {:.4f} || '\n",
    "              'Eval: Semantic Loss {:.4f} - Depth Loss {:.4f} - Total Loss {:.4f}'\n",
    "              .format(epoch + 1, train_loss['semantic'], train_loss['depth'], train_loss['total'],\n",
    "                      val_loss['semantic'], val_loss['depth'], val_loss['total']))\n",
    "        if val_loss['total'] < best_loss:\n",
    "            best_loss = val_loss['total']\n",
    "            torch.save(model, '/kaggle/working/GLPWIthSegFormer_best')\n",
    "            print(f\"Model saved: /kaggle/working/GLPWIthSegFormer.pth\")\n",
    "        if epoch %5 == 0:\n",
    "            torch.save(model, '/kaggle/working/GLPWIthSegFormer_l')\n",
    "            print(f\"Model saved: /kaggle/working/GLPWIthSegFormer_l.pth\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff6d1b84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:41.317828Z",
     "iopub.status.busy": "2025-05-19T07:19:41.316974Z",
     "iopub.status.idle": "2025-05-19T07:19:41.350471Z",
     "shell.execute_reply": "2025-05-19T07:19:41.349659Z"
    },
    "papermill": {
     "duration": 0.087863,
     "end_time": "2025-05-19T07:19:41.351608",
     "exception": false,
     "start_time": "2025-05-19T07:19:41.263745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: ./GLPWIthSegFormer.pth\n",
      "Model saved: ./GLPWIthSegFormer.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), './GLPWIthSegFormer.pth')\n",
    "print(f\"Model saved: ./GLPWIthSegFormer.pth\")\n",
    "torch.save(model, './GLPWIthSegFormer')\n",
    "print(f\"Model saved: ./GLPWIthSegFormer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b2ee30fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:41.415189Z",
     "iopub.status.busy": "2025-05-19T07:19:41.414840Z",
     "iopub.status.idle": "2025-05-19T07:19:41.421572Z",
     "shell.execute_reply": "2025-05-19T07:19:41.420825Z"
    },
    "papermill": {
     "duration": 0.039487,
     "end_time": "2025-05-19T07:19:41.422690",
     "exception": false,
     "start_time": "2025-05-19T07:19:41.383203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Example: using Adam optimizer for 'net' with the learning_rate variable\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Example: using StepLR scheduler (decays LR by gamma every step_size epochs)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b456975",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:41.486126Z",
     "iopub.status.busy": "2025-05-19T07:19:41.485307Z",
     "iopub.status.idle": "2025-05-19T07:19:41.490388Z",
     "shell.execute_reply": "2025-05-19T07:19:41.489817Z"
    },
    "papermill": {
     "duration": 0.03802,
     "end_time": "2025-05-19T07:19:41.491507",
     "exception": false,
     "start_time": "2025-05-19T07:19:41.453487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5fcfbc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:41.555766Z",
     "iopub.status.busy": "2025-05-19T07:19:41.555229Z",
     "iopub.status.idle": "2025-05-19T07:19:41.856265Z",
     "shell.execute_reply": "2025-05-19T07:19:41.855472Z"
    },
    "papermill": {
     "duration": 0.33435,
     "end_time": "2025-05-19T07:19:41.857509",
     "exception": false,
     "start_time": "2025-05-19T07:19:41.523159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 128, 128])\n",
      "SR output shape: torch.Size([1, 3, 128, 128])\n",
      "Refined output shape: torch.Size([1, 3, 128, 128])\n",
      "Feature shapes: [torch.Size([1, 64, 32, 32]), torch.Size([1, 64, 32, 32]), torch.Size([1, 64, 32, 32]), torch.Size([1, 64, 32, 32])]\n"
     ]
    }
   ],
   "source": [
    "# Test the DeepGuidedNet model input and output shapes\n",
    "\n",
    "# Create a random input tensor simulating an RGB image batch\n",
    "test_input = torch.randn(1, 3, 128, 128)  # (batch_size, channels, height, width)\n",
    "\n",
    "# Pass through the DeepGuidedNet model\n",
    "test_net = DeepGuidedNet()\n",
    "sr, refined_output, features = test_net(test_input)\n",
    "\n",
    "print(\"Input shape:\", test_input.shape)\n",
    "print(\"SR output shape:\", sr.shape)\n",
    "print(\"Refined output shape:\", refined_output.shape)\n",
    "print(\"Feature shapes:\", [f.shape for f in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1cda98",
   "metadata": {
    "papermill": {
     "duration": 0.030192,
     "end_time": "2025-05-19T07:19:41.921608",
     "exception": false,
     "start_time": "2025-05-19T07:19:41.891416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bd4554b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:41.984405Z",
     "iopub.status.busy": "2025-05-19T07:19:41.983860Z",
     "iopub.status.idle": "2025-05-19T07:19:41.987294Z",
     "shell.execute_reply": "2025-05-19T07:19:41.986577Z"
    },
    "papermill": {
     "duration": 0.036437,
     "end_time": "2025-05-19T07:19:41.988536",
     "exception": false,
     "start_time": "2025-05-19T07:19:41.952099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train(train_loader_c, valid_loader_c, model, device, optimizer, scheduler, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dc6cef1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:42.051629Z",
     "iopub.status.busy": "2025-05-19T07:19:42.051137Z",
     "iopub.status.idle": "2025-05-19T07:19:42.056884Z",
     "shell.execute_reply": "2025-05-19T07:19:42.056196Z"
    },
    "papermill": {
     "duration": 0.038454,
     "end_time": "2025-05-19T07:19:42.058107",
     "exception": false,
     "start_time": "2025-05-19T07:19:42.019653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remap_targets(targets):\n",
    "    \"\"\"\n",
    "    Remap target values to a contiguous range for one-hot encoding.\n",
    "    Args:\n",
    "        targets (torch.Tensor): Target tensor with arbitrary class indices.\n",
    "    Returns:\n",
    "        torch.Tensor: Remapped target tensor.\n",
    "    \"\"\"\n",
    "    # Ensure the tensor is on CPU for apply_\n",
    "    original_device = targets.device\n",
    "    targets = targets.cpu()\n",
    "\n",
    "    # Extract unique classes and create a mapping\n",
    "    unique_classes = torch.unique(targets)\n",
    "    class_mapping = {old: new for new, old in enumerate(unique_classes.tolist())}\n",
    "\n",
    "    # Apply the mapping to remap target values\n",
    "    remapped_targets = targets.clone()\n",
    "    remapped_targets.apply_(lambda x: class_mapping[x])\n",
    "\n",
    "    # Move the remapped tensor back to the original device\n",
    "    return remapped_targets.to(original_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9c59bc67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:42.121935Z",
     "iopub.status.busy": "2025-05-19T07:19:42.121370Z",
     "iopub.status.idle": "2025-05-19T07:19:42.128210Z",
     "shell.execute_reply": "2025-05-19T07:19:42.127488Z"
    },
    "papermill": {
     "duration": 0.039872,
     "end_time": "2025-05-19T07:19:42.129479",
     "exception": false,
     "start_time": "2025-05-19T07:19:42.089607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MulticlassDiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss for Multiclass Segmentation.\"\"\"\n",
    "    def __init__(self, num_classes, softmax_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.softmax_dim = softmax_dim\n",
    "\n",
    "    def forward(self, logits, targets, reduction='mean', smooth=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: Predicted probabilities/logits of shape [B, C, N].\n",
    "            targets: Ground truth labels of shape [B, N].\n",
    "            reduction: Specifies the reduction to apply to the output.\n",
    "            smooth: Smoothing factor to avoid division by zero.\n",
    "\n",
    "        Returns:\n",
    "            Dice loss.\n",
    "        \"\"\"\n",
    "        probabilities = logits\n",
    "        if self.softmax_dim is not None:\n",
    "            probabilities = nn.Softmax(dim=self.softmax_dim)(logits)\n",
    "\n",
    "        # Remap and one-hot encode targets\n",
    "        targets = remap_targets(targets)\n",
    "        targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=self.num_classes)\n",
    "\n",
    "        # Match the shape [B, N, C] to [B, C, N]\n",
    "        targets_one_hot = targets_one_hot.permute(0, 2, 1)\n",
    "        print(probabilities.shape, targets_one_hot.shape)\n",
    "\n",
    "        # Compute intersection and union\n",
    "        intersection = (targets_one_hot * probabilities).sum(dim=-1)\n",
    "        denominator = (targets_one_hot.sum(dim=-1) + probabilities.sum(dim=-1))\n",
    "\n",
    "        # Compute Dice coefficient\n",
    "        dice_coefficient = (2. * intersection + smooth) / (denominator + smooth)\n",
    "\n",
    "        # Dice loss is 1 - Dice coefficient\n",
    "        dice_loss = 1 - dice_coefficient\n",
    "\n",
    "        # Apply reduction\n",
    "        if reduction == 'mean':\n",
    "            return dice_loss.mean()\n",
    "        elif reduction == 'sum':\n",
    "            return dice_loss.sum()\n",
    "        return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f89d5c2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:42.192723Z",
     "iopub.status.busy": "2025-05-19T07:19:42.192042Z",
     "iopub.status.idle": "2025-05-19T07:19:42.197828Z",
     "shell.execute_reply": "2025-05-19T07:19:42.197278Z"
    },
    "papermill": {
     "duration": 0.038228,
     "end_time": "2025-05-19T07:19:42.198861",
     "exception": false,
     "start_time": "2025-05-19T07:19:42.160633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MulticlassCrossEntropyLoss(nn.Module):\n",
    "    \"\"\"Categorical Cross-Entropy Loss for Multiclass Segmentation.\"\"\"\n",
    "    def __init__(self, num_classes, softmax_dim=1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.softmax_dim = softmax_dim\n",
    "\n",
    "    def forward(self, logits, targets, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: Predicted logits of shape [B, C, N].\n",
    "            targets: Ground truth labels of shape [B, N].\n",
    "            reduction: Specifies the reduction to apply to the output.\n",
    "\n",
    "        Returns:\n",
    "            Categorical cross-entropy loss.\n",
    "        \"\"\"\n",
    "        probabilities = logits\n",
    "        if self.softmax_dim is not None:\n",
    "            probabilities = nn.Softmax(dim=self.softmax_dim)(logits)\n",
    "\n",
    "        # Remap targets to ensure values are in range [0, num_classes - 1]\n",
    "        targets = remap_targets(targets)\n",
    "\n",
    "        # Compute log probabilities for numerical stability\n",
    "        log_probabilities = torch.log(probabilities + 1e-10)\n",
    "\n",
    "        # Gather the probabilities of the correct classes\n",
    "        targets = targets.unsqueeze(1)  # Shape: [B, 1, N]\n",
    "        gathered_probabilities = torch.gather(log_probabilities, dim=1, index=targets)  # Shape: [B, 1, N]\n",
    "        gathered_probabilities = gathered_probabilities.squeeze(1)  # Shape: [B, N]\n",
    "\n",
    "        # Compute the negative log-likelihood\n",
    "        negative_log_likelihood = -gathered_probabilities\n",
    "\n",
    "        # Apply reduction\n",
    "        if reduction == 'mean':\n",
    "            return negative_log_likelihood.mean()\n",
    "        elif reduction == 'sum':\n",
    "            return negative_log_likelihood.sum()\n",
    "        return negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5958dcfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:42.263026Z",
     "iopub.status.busy": "2025-05-19T07:19:42.262346Z",
     "iopub.status.idle": "2025-05-19T07:19:42.267700Z",
     "shell.execute_reply": "2025-05-19T07:19:42.266985Z"
    },
    "papermill": {
     "duration": 0.038086,
     "end_time": "2025-05-19T07:19:42.268791",
     "exception": false,
     "start_time": "2025-05-19T07:19:42.230705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def threshold_accuracy(predicted, ground_truth, thresholds=[1.25, 1.25**2, 1.25**3]):\n",
    "    \"\"\"\n",
    "    Calculate threshold accuracy (1.25, 1.25², 1.25³) based on ratio between predicted and actual values.\n",
    "    \n",
    "    Args:\n",
    "        predicted (torch.Tensor): Predicted depth map (B, H, W).\n",
    "        ground_truth (torch.Tensor): Ground truth depth map (B, H, W).\n",
    "        thresholds (list): List of thresholds (e.g., [1.25, 1.25**2, 1.25**3]).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing threshold accuracy for each threshold.\n",
    "    \"\"\"\n",
    "    # Flatten tensors to compare at the pixel level\n",
    "    predicted = predicted.flatten()\n",
    "    ground_truth = ground_truth.flatten()\n",
    "\n",
    "    # Avoid division by zero by adding a small value to ground truth\n",
    "    ground_truth = torch.where(ground_truth == 0, torch.tensor(1e-6), ground_truth)\n",
    "    \n",
    "    # Calculate the ratio between predicted and ground truth\n",
    "    ratio = predicted / ground_truth\n",
    "\n",
    "    #print(\"Predicted:\", predicted[:10])  # Debug print for the first 10 predicted values\n",
    "    #print(\"Ground Truth:\", ground_truth[:10])  # Debug print for the first 10 ground truth values\n",
    "    #print(\"Ratios:\", ratio[:10])  # Debug print for the first 10 ratio values\n",
    "    \n",
    "    # Initialize dictionary to store threshold accuracies\n",
    "    accuracies = {}\n",
    "\n",
    "    for idx, t in enumerate(thresholds, start=1):\n",
    "        # Check how many pixels have a ratio <= threshold\n",
    "        accuracy = (ratio <= t).float().mean().item()\n",
    "        accuracies[f\"accuracy_{idx}\"] = accuracy\n",
    "        #print(f\"Threshold {t}: {accuracy:.4f}\")  # Debug print for each threshol#d\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc287e16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:42.332520Z",
     "iopub.status.busy": "2025-05-19T07:19:42.331816Z",
     "iopub.status.idle": "2025-05-19T07:19:47.435164Z",
     "shell.execute_reply": "2025-05-19T07:19:47.434074Z"
    },
    "papermill": {
     "duration": 5.136804,
     "end_time": "2025-05-19T07:19:47.437077",
     "exception": false,
     "start_time": "2025-05-19T07:19:42.300273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\r\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (25.0)\r\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (3.20.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->tensorboardX) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->tensorboardX) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->tensorboardX) (2024.2.0)\r\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tensorboardX\r\n",
      "Successfully installed tensorboardX-2.6.2.2\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7c7c9f39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:47.558603Z",
     "iopub.status.busy": "2025-05-19T07:19:47.557941Z",
     "iopub.status.idle": "2025-05-19T07:19:47.562017Z",
     "shell.execute_reply": "2025-05-19T07:19:47.561533Z"
    },
    "papermill": {
     "duration": 0.059417,
     "end_time": "2025-05-19T07:19:47.563371",
     "exception": false,
     "start_time": "2025-05-19T07:19:47.503954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1e-4\n",
    "log_dir = './log24'\n",
    "val_freq = 1\n",
    "save_model = True\n",
    "save_result = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "682fed3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:19:47.627654Z",
     "iopub.status.busy": "2025-05-19T07:19:47.627384Z",
     "iopub.status.idle": "2025-05-19T07:20:04.393661Z",
     "shell.execute_reply": "2025-05-19T07:20:04.392899Z"
    },
    "papermill": {
     "duration": 16.799791,
     "end_time": "2025-05-19T07:20:04.395100",
     "exception": false,
     "start_time": "2025-05-19T07:19:47.595309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "class SiLogLoss(nn.Module):\n",
    "    def __init__(self, lambd=0.85):\n",
    "        super(SiLogLoss, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Ensure pred and target are 4D: (batch_size, channels, height, width)\n",
    "        if pred.dim() == 3:\n",
    "            pred = pred.unsqueeze(1)\n",
    "        if target.dim() == 3:\n",
    "            target = target.unsqueeze(1)\n",
    "\n",
    "        # Create a mask for valid depth values\n",
    "        valid_mask = (target > 0).detach()\n",
    "        \n",
    "        # Apply mask and compute log difference\n",
    "        diff_log = torch.log(target[valid_mask]) - torch.log(pred[valid_mask])\n",
    "        \n",
    "        # SiLog Loss calculation\n",
    "        loss = torch.sqrt(torch.pow(diff_log, 2).mean() -\n",
    "                          self.lambd * torch.pow(diff_log.mean(), 2))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "# Metric names\n",
    "metric_name = ['d1', 'd2', 'd3', 'abs_rel', 'sq_rel', 'rmse', 'rmse_log', 'log10', 'silog']\n",
    "\n",
    "\"\"\"\n",
    "# Define CityScapes dataset class\n",
    "class CityScapes(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        self.train = train\n",
    "        self.root = os.path.expanduser(root)\n",
    "\n",
    "        if train:\n",
    "            self.data_path = os.path.join(root, 'train')\n",
    "        else:\n",
    "            self.data_path = os.path.join(root, 'val')\n",
    "\n",
    "        self.data_len = len(fnmatch.filter(os.listdir(os.path.join(self.data_path, 'image')), '*.npy'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = torch.from_numpy(np.moveaxis(np.load(os.path.join(self.data_path, 'image', f'{index}.npy')), -1, 0))\n",
    "        semantic = torch.from_numpy(np.load(os.path.join(self.data_path, 'label', f'{index}.npy')))\n",
    "        depth = torch.from_numpy(np.moveaxis(np.load(os.path.join(self.data_path, 'depth', f'{index}.npy')), -1, 0))\n",
    "        \n",
    "        return {'image': image.float(), 'semantic': semantic.float(), 'depth': depth.float()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\"\"\"\n",
    "\n",
    "cudnn.benchmark = True\n",
    "#model = torch.nn.DataParallel(model).to(DEVICE)\n",
    "\n",
    "# Training settings\n",
    "criterion_d = SiLogLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "021535f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:20:04.459184Z",
     "iopub.status.busy": "2025-05-19T07:20:04.458654Z",
     "iopub.status.idle": "2025-05-19T07:20:04.474664Z",
     "shell.execute_reply": "2025-05-19T07:20:04.473933Z"
    },
    "papermill": {
     "duration": 0.048825,
     "end_time": "2025-05-19T07:20:04.475771",
     "exception": false,
     "start_time": "2025-05-19T07:20:04.426946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "# _, term_width = os.popen('stty size', 'r').read().split()\n",
    "term_width = 2\n",
    "\n",
    "TOTAL_BAR_LENGTH = 30.\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "\n",
    "\n",
    "def progress_bar(current, total, epochs, cur_epoch, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH * current / total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "    remain_time = step_time * (total - current) + \\\n",
    "        (epochs - cur_epoch) * step_time * total\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    L.append(' | Rem: %s' % format_time(remain_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(term_width - int(TOTAL_BAR_LENGTH) - len(msg) - 3):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(term_width - int(TOTAL_BAR_LENGTH / 2) + 2):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current + 1, total))\n",
    "\n",
    "    if current < total - 1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "class AverageMeter():\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600 / 24)\n",
    "    seconds = seconds - days * 3600 * 24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours * 3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes * 60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds * 1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes).zfill(2) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf).zfill(2) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis).zfill(3) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f\n",
    "\n",
    "\n",
    "def display_result(result_dict):\n",
    "    line = \"\\n\"\n",
    "    line += \"=\" * 100 + '\\n'\n",
    "    for metric, value in result_dict.items():\n",
    "        line += \"{:>10} \".format(metric)\n",
    "    line += \"\\n\"\n",
    "    for metric, value in result_dict.items():\n",
    "        line += \"{:10.4f} \".format(value)\n",
    "    line += \"\\n\"\n",
    "    line += \"=\" * 100 + '\\n'\n",
    "\n",
    "    return line\n",
    "    \n",
    "\n",
    "def save_images(pred, save_path):\n",
    "    if len(pred.shape) > 3:\n",
    "        pred = pred.squeeze()\n",
    "\n",
    "    if isinstance(pred, torch.Tensor):\n",
    "        pred = pred.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "    if pred.shape[0] < 4:\n",
    "        pred = np.transpose(pred, (1, 2, 0))\n",
    "    cv2.imwrite(save_path, pred, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n",
    "\n",
    "\n",
    "def check_and_make_dirs(paths):\n",
    "    if not isinstance(paths, list):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "def log_args_to_txt(log_txt, args):\n",
    "    if not os.path.exists(log_txt):\n",
    "        with open(log_txt, 'w') as txtfile:\n",
    "            args_ = vars(args)\n",
    "            args_str = ''\n",
    "            for k, v in args_.items():\n",
    "                args_str = args_str + str(k) + ':' + str(v) + ',\\t\\n'\n",
    "            txtfile.write(args_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "29cd4c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:20:04.538434Z",
     "iopub.status.busy": "2025-05-19T07:20:04.538195Z",
     "iopub.status.idle": "2025-05-19T07:20:04.543684Z",
     "shell.execute_reply": "2025-05-19T07:20:04.542984Z"
    },
    "papermill": {
     "duration": 0.038133,
     "end_time": "2025-05-19T07:20:04.544837",
     "exception": false,
     "start_time": "2025-05-19T07:20:04.506704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_name = f\"{datetime.now().strftime('%m%d')}_GLPDepthWithSegmentation\"\n",
    "log_dir = os.path.join(log_dir, 'CityScapes', exp_name)\n",
    "check_and_make_dirs(log_dir)\n",
    "writer = SummaryWriter(logdir=log_dir)\n",
    "model_root = log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7494db15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:20:04.606957Z",
     "iopub.status.busy": "2025-05-19T07:20:04.606530Z",
     "iopub.status.idle": "2025-05-19T07:20:04.611335Z",
     "shell.execute_reply": "2025-05-19T07:20:04.610664Z"
    },
    "papermill": {
     "duration": 0.037072,
     "end_time": "2025-05-19T07:20:04.612477",
     "exception": false,
     "start_time": "2025-05-19T07:20:04.575405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scheduler\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.0e-3)\n",
    "\n",
    "seg_criterion = nn.CrossEntropyLoss()\n",
    "depth_criterion = nn.MSELoss()\n",
    "#rmse_loss = torch.sqrt(depth_criterion(x, y))\n",
    "scheduler = OneCycleLR(optimizer, \n",
    "                       max_lr=5.0e-4,                     # Upper learning rate boundaries in the cycle for each parameter group\n",
    "                       steps_per_epoch=len(train_loader), # The number of steps per epoch to train for.\n",
    "                       epochs=epochs,                     # The number of epochs to train for.\n",
    "                       anneal_strategy='cos')             # Specifies the annealing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "60c5396c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:20:04.673992Z",
     "iopub.status.busy": "2025-05-19T07:20:04.673489Z",
     "iopub.status.idle": "2025-05-19T07:20:04.678777Z",
     "shell.execute_reply": "2025-05-19T07:20:04.678203Z"
    },
    "papermill": {
     "duration": 0.037571,
     "end_time": "2025-05-19T07:20:04.679893",
     "exception": false,
     "start_time": "2025-05-19T07:20:04.642322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "# compute IOU or Jaccard Index\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, window_size=None):\n",
    "        self.length = 0\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def reset(self):\n",
    "        self.length = 0\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if self.window_size and (self.count >= self.window_size):\n",
    "            self.reset()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = np.round(self.sum / self.count, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5ace9a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:20:04.743153Z",
     "iopub.status.busy": "2025-05-19T07:20:04.742862Z",
     "iopub.status.idle": "2025-05-19T07:21:16.619489Z",
     "shell.execute_reply": "2025-05-19T07:21:16.618432Z"
    },
    "papermill": {
     "duration": 71.910496,
     "end_time": "2025-05-19T07:21:16.621400",
     "exception": false,
     "start_time": "2025-05-19T07:20:04.710904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation-models-pytorch\r\n",
      "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.31.1)\r\n",
      "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.26.4)\r\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.1.0)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\r\n",
      "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\r\n",
      "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2.4.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->segmentation-models-pytorch)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->segmentation-models-pytorch)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->segmentation-models-pytorch)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->segmentation-models-pytorch)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->segmentation-models-pytorch)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->segmentation-models-pytorch)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->segmentation-models-pytorch) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation-models-pytorch) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\r\n",
      "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, segmentation-models-pytorch\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 segmentation-models-pytorch-0.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fcf05197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:21:16.768549Z",
     "iopub.status.busy": "2025-05-19T07:21:16.767794Z",
     "iopub.status.idle": "2025-05-19T07:21:16.776279Z",
     "shell.execute_reply": "2025-05-19T07:21:16.775523Z"
    },
    "papermill": {
     "duration": 0.085185,
     "end_time": "2025-05-19T07:21:16.777599",
     "exception": false,
     "start_time": "2025-05-19T07:21:16.692414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8f3a98b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:21:16.923974Z",
     "iopub.status.busy": "2025-05-19T07:21:16.922969Z",
     "iopub.status.idle": "2025-05-19T07:21:16.927838Z",
     "shell.execute_reply": "2025-05-19T07:21:16.926980Z"
    },
    "papermill": {
     "duration": 0.074632,
     "end_time": "2025-05-19T07:21:16.929473",
     "exception": false,
     "start_time": "2025-05-19T07:21:16.854841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c28d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T07:21:17.091060Z",
     "iopub.status.busy": "2025-05-19T07:21:17.090339Z",
     "iopub.status.idle": "2025-05-19T13:46:08.598927Z",
     "shell.execute_reply": "2025-05-19T13:46:08.598172Z"
    },
    "papermill": {
     "duration": 23091.688784,
     "end_time": "2025-05-19T13:46:08.698314",
     "exception": false,
     "start_time": "2025-05-19T07:21:17.009530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 07:21:19.404107: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747639279.604259      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747639279.659954      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Iteration: 5 - LR: [2.0009303117243793e-05] dice_loss: 0.7796 depth_loss: 0.6821 loss: 8.4778 rmse: 0.8257\n",
      "Train Epoch: 1 Iteration: 10 - LR: [2.0037211747742155e-05] dice_loss: 0.7791 depth_loss: 0.6715 loss: 8.4625 rmse: 0.8190\n",
      "Train Epoch: 1 Iteration: 15 - LR: [2.008372372785109e-05] dice_loss: 0.7787 depth_loss: 0.6684 loss: 8.4558 rmse: 0.8171\n",
      "Train Epoch: 1 Iteration: 20 - LR: [2.014883545168385e-05] dice_loss: 0.7702 depth_loss: 0.6572 loss: 8.3595 rmse: 0.8099\n",
      "Train Epoch: 1 Iteration: 25 - LR: [2.023254187139055e-05] dice_loss: 0.7616 depth_loss: 0.6468 loss: 8.2628 rmse: 0.8033\n",
      "Train Epoch: 1 Iteration: 30 - LR: [2.0334836497549124e-05] dice_loss: 0.7638 depth_loss: 0.6394 loss: 8.2776 rmse: 0.7985\n",
      "Train Epoch: 1 Iteration: 35 - LR: [2.0455711399668835e-05] dice_loss: 0.7605 depth_loss: 0.6315 loss: 8.2362 rmse: 0.7934\n",
      "Train Epoch: 1 Iteration: 40 - LR: [2.0595157206804745e-05] dice_loss: 0.7591 depth_loss: 0.6222 loss: 8.2129 rmse: 0.7874\n",
      "Train Epoch: 1 Iteration: 45 - LR: [2.075316310828435e-05] dice_loss: 0.7523 depth_loss: 0.6128 loss: 8.1361 rmse: 0.7813\n",
      "Train Epoch: 1 Iteration: 50 - LR: [2.0929716854545873e-05] dice_loss: 0.7444 depth_loss: 0.6008 loss: 8.0453 rmse: 0.7734\n",
      "Train Epoch: 1 Iteration: 55 - LR: [2.1124804758087612e-05] dice_loss: 0.7419 depth_loss: 0.5905 loss: 8.0100 rmse: 0.7666\n",
      "Train Epoch: 1 Iteration: 60 - LR: [2.1338411694529194e-05] dice_loss: 0.7441 depth_loss: 0.5793 loss: 8.0203 rmse: 0.7589\n",
      "Train Epoch: 1 Iteration: 65 - LR: [2.15705211037842e-05] dice_loss: 0.7435 depth_loss: 0.5637 loss: 7.9990 rmse: 0.7480\n",
      "Train Epoch: 1 Iteration: 70 - LR: [2.1821114991343806e-05] dice_loss: 0.7475 depth_loss: 0.5504 loss: 8.0251 rmse: 0.7387\n",
      "Train Epoch: 1 Iteration: 75 - LR: [2.2090173929672206e-05] dice_loss: 0.7525 depth_loss: 0.5398 loss: 8.0653 rmse: 0.7315\n",
      "Train Epoch: 1 Iteration: 80 - LR: [2.2377677059712243e-05] dice_loss: 0.7538 depth_loss: 0.5234 loss: 8.0609 rmse: 0.7195\n",
      "Train Epoch: 1 Iteration: 85 - LR: [2.268360209250282e-05] dice_loss: 0.7601 depth_loss: 0.5096 loss: 8.1110 rmse: 0.7096\n",
      "Train Epoch: 1 Iteration: 90 - LR: [2.3007925310906908e-05] dice_loss: 0.7650 depth_loss: 0.4888 loss: 8.1390 rmse: 0.6932\n",
      "Train Epoch: 1 Iteration: 95 - LR: [2.3350621571450124e-05] dice_loss: 0.7649 depth_loss: 0.4663 loss: 8.1151 rmse: 0.6750\n",
      "Train Epoch: 1 Iteration: 100 - LR: [2.371166430627014e-05] dice_loss: 0.7685 depth_loss: 0.4394 loss: 8.1240 rmse: 0.6515\n",
      "Train Epoch: 1 Iteration: 105 - LR: [2.4091025525176005e-05] dice_loss: 0.7688 depth_loss: 0.4152 loss: 8.1029 rmse: 0.6306\n",
      "Train Epoch: 1 Iteration: 110 - LR: [2.4488675817818726e-05] dice_loss: 0.7691 depth_loss: 0.3923 loss: 8.0829 rmse: 0.6103\n",
      "Train Epoch: 1 Iteration: 115 - LR: [2.4904584355970748e-05] dice_loss: 0.7675 depth_loss: 0.3687 loss: 8.0436 rmse: 0.5884\n",
      "Train Epoch: 1 Iteration: 120 - LR: [2.533871889591634e-05] dice_loss: 0.7685 depth_loss: 0.3498 loss: 8.0350 rmse: 0.5717\n",
      "Train Epoch: 1 Iteration: 125 - LR: [2.579104578095113e-05] dice_loss: 0.7686 depth_loss: 0.3318 loss: 8.0182 rmse: 0.5554\n",
      "Train Epoch: 1 Iteration: 130 - LR: [2.626152994399144e-05] dice_loss: 0.7677 depth_loss: 0.3165 loss: 7.9931 rmse: 0.5420\n",
      "Train Epoch: 1 Iteration: 135 - LR: [2.6750134910292816e-05] dice_loss: 0.7633 depth_loss: 0.2982 loss: 7.9313 rmse: 0.5240\n",
      "Train Epoch: 1 Iteration: 140 - LR: [2.725682280027786e-05] dice_loss: 0.7594 depth_loss: 0.2823 loss: 7.8766 rmse: 0.5086\n",
      "Train Epoch: 1 Iteration: 145 - LR: [2.778155433247288e-05] dice_loss: 0.7630 depth_loss: 0.2680 loss: 7.8978 rmse: 0.4947\n",
      "Train Epoch: 1 Iteration: 150 - LR: [2.8324288826553205e-05] dice_loss: 0.7644 depth_loss: 0.2516 loss: 7.8954 rmse: 0.4773\n",
      "Train Epoch: 1 Iteration: 155 - LR: [2.888498420649681e-05] dice_loss: 0.7674 depth_loss: 0.2392 loss: 7.9134 rmse: 0.4650\n",
      "Train Epoch: 1 Iteration: 160 - LR: [2.9463597003846558e-05] dice_loss: 0.7719 depth_loss: 0.2266 loss: 7.9455 rmse: 0.4518\n",
      "Train Epoch: 1 Iteration: 165 - LR: [3.006008236108008e-05] dice_loss: 0.7733 depth_loss: 0.2156 loss: 7.9487 rmse: 0.4406\n",
      "Train Epoch: 1 Iteration: 170 - LR: [3.067439403508723e-05] dice_loss: 0.7681 depth_loss: 0.2027 loss: 7.8841 rmse: 0.4256\n",
      "Train Epoch: 1 Iteration: 175 - LR: [3.130648440075528e-05] dice_loss: 0.7661 depth_loss: 0.1905 loss: 7.8512 rmse: 0.4111\n",
      "Train Epoch: 1 Iteration: 180 - LR: [3.195630445466106e-05] dice_loss: 0.7614 depth_loss: 0.1799 loss: 7.7940 rmse: 0.3988\n",
      "Train Epoch: 1 Iteration: 185 - LR: [3.262380381886999e-05] dice_loss: 0.7568 depth_loss: 0.1706 loss: 7.7390 rmse: 0.3881\n",
      "Train Epoch: 1 Iteration: 190 - LR: [3.3308930744841727e-05] dice_loss: 0.7525 depth_loss: 0.1642 loss: 7.6895 rmse: 0.3813\n",
      "Train Epoch: 1 Iteration: 195 - LR: [3.4011632117441964e-05] dice_loss: 0.7490 depth_loss: 0.1586 loss: 7.6491 rmse: 0.3755\n",
      "Train Epoch: 1 Iteration: 200 - LR: [3.473185345906013e-05] dice_loss: 0.7452 depth_loss: 0.1513 loss: 7.6030 rmse: 0.3668\n",
      "Train Epoch: 1 Iteration: 205 - LR: [3.54695389338331e-05] dice_loss: 0.7448 depth_loss: 0.1445 loss: 7.5924 rmse: 0.3589\n",
      "Train Epoch: 1 Iteration: 210 - LR: [3.6224631351973696e-05] dice_loss: 0.7373 depth_loss: 0.1435 loss: 7.5165 rmse: 0.3591\n",
      "Train Epoch: 1 Iteration: 215 - LR: [3.699707217420442e-05] dice_loss: 0.7314 depth_loss: 0.1416 loss: 7.4553 rmse: 0.3582\n",
      "Train Epoch: 1 Iteration: 220 - LR: [3.778680151629587e-05] dice_loss: 0.7266 depth_loss: 0.1367 loss: 7.4022 rmse: 0.3526\n",
      "Train Epoch: 1 Iteration: 225 - LR: [3.859375815370921e-05] dice_loss: 0.7208 depth_loss: 0.1325 loss: 7.3407 rmse: 0.3477\n",
      "Train Epoch: 1 Iteration: 230 - LR: [3.941787952634274e-05] dice_loss: 0.7164 depth_loss: 0.1276 loss: 7.2916 rmse: 0.3415\n",
      "Train Epoch: 1 Iteration: 235 - LR: [4.025910174338174e-05] dice_loss: 0.7217 depth_loss: 0.1316 loss: 7.3484 rmse: 0.3476\n",
      "Train Epoch: 1 Iteration: 240 - LR: [4.1117359588251935e-05] dice_loss: 0.7271 depth_loss: 0.1322 loss: 7.4029 rmse: 0.3496\n",
      "Train Epoch: 1 Iteration: 245 - LR: [4.199258652367547e-05] dice_loss: 0.7293 depth_loss: 0.1335 loss: 7.4260 rmse: 0.3526\n",
      "Train Epoch: 1 Iteration: 250 - LR: [4.288471469682904e-05] dice_loss: 0.7257 depth_loss: 0.1326 loss: 7.3900 rmse: 0.3525\n",
      "Train Epoch: 1 Iteration: 255 - LR: [4.379367494460431e-05] dice_loss: 0.7346 depth_loss: 0.1322 loss: 7.4781 rmse: 0.3529\n",
      "Train Epoch: 1 Iteration: 260 - LR: [4.471939679896996e-05] dice_loss: 0.7374 depth_loss: 0.1317 loss: 7.5053 rmse: 0.3532\n",
      "Train Epoch: 1 Iteration: 265 - LR: [4.566180849243485e-05] dice_loss: 0.7438 depth_loss: 0.1297 loss: 7.5675 rmse: 0.3511\n",
      "Train Epoch: 1 Iteration: 270 - LR: [4.6620836963611736e-05] dice_loss: 0.7456 depth_loss: 0.1272 loss: 7.5831 rmse: 0.3483\n",
      "Train Epoch: 1 Iteration: 275 - LR: [4.7596407862881174e-05] dice_loss: 0.7438 depth_loss: 0.1233 loss: 7.5610 rmse: 0.3431\n",
      "Train Epoch: 1 Iteration: 280 - LR: [4.858844555815625e-05] dice_loss: 0.7431 depth_loss: 0.1198 loss: 7.5508 rmse: 0.3384\n",
      "Train Epoch: 1 Iteration: 285 - LR: [4.959687314074531e-05] dice_loss: 0.7455 depth_loss: 0.1169 loss: 7.5714 rmse: 0.3345\n",
      "Train Epoch: 1 Iteration: 290 - LR: [5.06216124313148e-05] dice_loss: 0.7460 depth_loss: 0.1136 loss: 7.5738 rmse: 0.3299\n",
      "Train Epoch: 1 Iteration: 295 - LR: [5.1662583985949775e-05] dice_loss: 0.7433 depth_loss: 0.1101 loss: 7.5428 rmse: 0.3249\n",
      "Train Epoch: 1 Iteration: 300 - LR: [5.2719707102313776e-05] dice_loss: 0.7389 depth_loss: 0.1059 loss: 7.4951 rmse: 0.3184\n",
      "Train Epoch: 1 Iteration: 305 - LR: [5.3792899825904044e-05] dice_loss: 0.7378 depth_loss: 0.1014 loss: 7.4794 rmse: 0.3111\n",
      "Train Epoch: 1 Iteration: 310 - LR: [5.488207895640627e-05] dice_loss: 0.7334 depth_loss: 0.0969 loss: 7.4307 rmse: 0.3036\n",
      "Train Epoch: 1 Iteration: 315 - LR: [5.598716005414427e-05] dice_loss: 0.7301 depth_loss: 0.0930 loss: 7.3937 rmse: 0.2971\n",
      "Train Epoch: 1 Iteration: 320 - LR: [5.710805744662612e-05] dice_loss: 0.7323 depth_loss: 0.0884 loss: 7.4114 rmse: 0.2891\n",
      "Train Epoch: 1 Iteration: 325 - LR: [5.824468423518635e-05] dice_loss: 0.7246 depth_loss: 0.0842 loss: 7.3305 rmse: 0.2815\n",
      "Train Epoch: 1 Iteration: 330 - LR: [5.939695230172264e-05] dice_loss: 0.7257 depth_loss: 0.0807 loss: 7.3380 rmse: 0.2753\n",
      "Train Epoch: 1 Iteration: 335 - LR: [6.056477231552738e-05] dice_loss: 0.7173 depth_loss: 0.0772 loss: 7.2501 rmse: 0.2690\n",
      "Train Epoch: 1 Iteration: 340 - LR: [6.174805374021305e-05] dice_loss: 0.7135 depth_loss: 0.0733 loss: 7.2087 rmse: 0.2615\n",
      "Train Epoch: 1 Iteration: 345 - LR: [6.294670484073095e-05] dice_loss: 0.7085 depth_loss: 0.0701 loss: 7.1554 rmse: 0.2554\n",
      "Train Epoch: 1 Iteration: 350 - LR: [6.41606326904834e-05] dice_loss: 0.7096 depth_loss: 0.0670 loss: 7.1635 rmse: 0.2497\n",
      "Train Epoch: 1 Iteration: 355 - LR: [6.538974317852795e-05] dice_loss: 0.7057 depth_loss: 0.0641 loss: 7.1212 rmse: 0.2438\n",
      "Train Epoch: 1 Iteration: 360 - LR: [6.663394101687301e-05] dice_loss: 0.6997 depth_loss: 0.0614 loss: 7.0583 rmse: 0.2388\n",
      "Train Epoch: 1 Iteration: 365 - LR: [6.789312974786569e-05] dice_loss: 0.7030 depth_loss: 0.0596 loss: 7.0893 rmse: 0.2357\n",
      "Train Epoch: 1 Iteration: 370 - LR: [6.916721175166907e-05] dice_loss: 0.7038 depth_loss: 0.0573 loss: 7.0958 rmse: 0.2312\n",
      "Train Epoch: 1 Iteration: 375 - LR: [7.045608825383126e-05] dice_loss: 0.6986 depth_loss: 0.0556 loss: 7.0416 rmse: 0.2280\n",
      "Train Epoch: 1 Iteration: 380 - LR: [7.17596593329421e-05] dice_loss: 0.7008 depth_loss: 0.0538 loss: 7.0617 rmse: 0.2244\n",
      "Train Epoch: 1 Iteration: 385 - LR: [7.307782392838001e-05] dice_loss: 0.6983 depth_loss: 0.0521 loss: 7.0351 rmse: 0.2211\n",
      "Train Epoch: 1 Iteration: 390 - LR: [7.441047984814669e-05] dice_loss: 0.6999 depth_loss: 0.0509 loss: 7.0494 rmse: 0.2190\n",
      "Train Epoch: 1 Iteration: 395 - LR: [7.57575237767902e-05] dice_loss: 0.6973 depth_loss: 0.0495 loss: 7.0225 rmse: 0.2162\n",
      "Train Epoch: 1 Iteration: 400 - LR: [7.711885128341365e-05] dice_loss: 0.6954 depth_loss: 0.0490 loss: 7.0027 rmse: 0.2156\n",
      "Train Epoch: 1 Iteration: 405 - LR: [7.849435682977214e-05] dice_loss: 0.6913 depth_loss: 0.0479 loss: 6.9609 rmse: 0.2132\n",
      "Train Epoch: 1 Iteration: 410 - LR: [7.988393377845424e-05] dice_loss: 0.6869 depth_loss: 0.0472 loss: 6.9165 rmse: 0.2122\n",
      "Train Epoch: 1 Iteration: 415 - LR: [8.128747440114931e-05] dice_loss: 0.6776 depth_loss: 0.0467 loss: 6.8231 rmse: 0.2115\n",
      "Train Epoch: 1 Iteration: 420 - LR: [8.270486988699936e-05] dice_loss: 0.6760 depth_loss: 0.0455 loss: 6.8056 rmse: 0.2088\n",
      "Train Epoch: 1 Iteration: 425 - LR: [8.413601035103438e-05] dice_loss: 0.6781 depth_loss: 0.0439 loss: 6.8254 rmse: 0.2052\n",
      "Train Epoch: 1 Iteration: 430 - LR: [8.558078484269179e-05] dice_loss: 0.6792 depth_loss: 0.0427 loss: 6.8344 rmse: 0.2025\n",
      "Train Epoch: 1 Iteration: 435 - LR: [8.703908135441736e-05] dice_loss: 0.6792 depth_loss: 0.0423 loss: 6.8341 rmse: 0.2015\n",
      "Train Epoch: 1 Iteration: 440 - LR: [8.851078683034933e-05] dice_loss: 0.6779 depth_loss: 0.0409 loss: 6.8198 rmse: 0.1981\n",
      "Train Epoch: 1 Iteration: 445 - LR: [8.999578717508276e-05] dice_loss: 0.6814 depth_loss: 0.0401 loss: 6.8543 rmse: 0.1964\n",
      "Train Epoch: 1 Iteration: 450 - LR: [9.149396726251498e-05] dice_loss: 0.6796 depth_loss: 0.0393 loss: 6.8354 rmse: 0.1949\n",
      "Train Epoch: 1 Iteration: 455 - LR: [9.300521094477094e-05] dice_loss: 0.6817 depth_loss: 0.0391 loss: 6.8559 rmse: 0.1945\n",
      "Train Epoch: 1 Iteration: 460 - LR: [9.452940106120778e-05] dice_loss: 0.6841 depth_loss: 0.0396 loss: 6.8809 rmse: 0.1958\n",
      "Train Epoch: 1 Iteration: 465 - LR: [9.606641944749736e-05] dice_loss: 0.6851 depth_loss: 0.0413 loss: 6.8920 rmse: 0.1997\n",
      "Train Epoch: 1 Iteration: 470 - LR: [9.761614694478763e-05] dice_loss: 0.6901 depth_loss: 0.0411 loss: 6.9420 rmse: 0.1995\n",
      "Train Epoch: 1 Iteration: 475 - LR: [9.91784634089403e-05] dice_loss: 0.6859 depth_loss: 0.0411 loss: 6.8998 rmse: 0.1997\n",
      "Train Epoch: 1 Iteration: 480 - LR: [0.00010075324771984494] dice_loss: 0.6806 depth_loss: 0.0416 loss: 6.8474 rmse: 0.2010\n",
      "Train Epoch: 1 Iteration: 485 - LR: [0.0001023403777908093] dice_loss: 0.6759 depth_loss: 0.0414 loss: 6.8006 rmse: 0.2006\n",
      "Train Epoch: 1 Iteration: 490 - LR: [0.00010393973057802402] dice_loss: 0.6736 depth_loss: 0.0427 loss: 6.7785 rmse: 0.2035\n",
      "Train Epoch: 1 Iteration: 495 - LR: [0.00010555118209010141] dice_loss: 0.6757 depth_loss: 0.0437 loss: 6.8007 rmse: 0.2057\n",
      "Train Epoch: 1 Iteration: 500 - LR: [0.00010717460739768881] dice_loss: 0.6742 depth_loss: 0.0447 loss: 6.7865 rmse: 0.2082\n",
      "Train Epoch: 1 Iteration: 505 - LR: [0.0001088098806431531] dice_loss: 0.6742 depth_loss: 0.0448 loss: 6.7872 rmse: 0.2085\n",
      "Train Epoch: 1 Iteration: 510 - LR: [0.000110456875050338] dice_loss: 0.6744 depth_loss: 0.0443 loss: 6.7883 rmse: 0.2075\n",
      "Train Epoch: 1 Iteration: 515 - LR: [0.00011211546293439311] dice_loss: 0.6651 depth_loss: 0.0432 loss: 6.6937 rmse: 0.2048\n",
      "Train Epoch: 1 Iteration: 520 - LR: [0.00011378551571167213] dice_loss: 0.6616 depth_loss: 0.0440 loss: 6.6600 rmse: 0.2068\n",
      "Train Epoch: 1 Iteration: 525 - LR: [0.00011546690390970186] dice_loss: 0.6635 depth_loss: 0.0443 loss: 6.6789 rmse: 0.2077\n",
      "Train Epoch: 1 Iteration: 530 - LR: [0.00011715949717721946] dice_loss: 0.6653 depth_loss: 0.0455 loss: 6.6985 rmse: 0.2105\n",
      "Train Epoch: 1 Iteration: 535 - LR: [0.00011886316429427819] dice_loss: 0.6624 depth_loss: 0.0456 loss: 6.6700 rmse: 0.2108\n",
      "Train Epoch: 1 Iteration: 540 - LR: [0.00012057777318242018] dice_loss: 0.6646 depth_loss: 0.0445 loss: 6.6909 rmse: 0.2084\n",
      "Train Epoch: 1 Iteration: 545 - LR: [0.00012230319091491618] dice_loss: 0.6611 depth_loss: 0.0448 loss: 6.6561 rmse: 0.2092\n",
      "Train Epoch: 1 Iteration: 550 - LR: [0.00012403928372707075] dice_loss: 0.6511 depth_loss: 0.0443 loss: 6.5556 rmse: 0.2079\n",
      "Train Epoch: 1 Iteration: 555 - LR: [0.0001257859170265923] dice_loss: 0.6488 depth_loss: 0.0430 loss: 6.5311 rmse: 0.2046\n",
      "Train Epoch: 1 Iteration: 560 - LR: [0.000127542955404028] dice_loss: 0.6435 depth_loss: 0.0417 loss: 6.4769 rmse: 0.2016\n",
      "Train Epoch: 1 Iteration: 565 - LR: [0.0001293102626432606] dice_loss: 0.6465 depth_loss: 0.0410 loss: 6.5065 rmse: 0.2000\n",
      "Train Epoch: 1 Iteration: 570 - LR: [0.00013108770173206986] dice_loss: 0.6468 depth_loss: 0.0404 loss: 6.5080 rmse: 0.1987\n",
      "Train Epoch: 1 Iteration: 575 - LR: [0.00013287513487275388] dice_loss: 0.6519 depth_loss: 0.0407 loss: 6.5593 rmse: 0.1996\n",
      "Train Epoch: 1 Iteration: 580 - LR: [0.0001346724234928119] dice_loss: 0.6572 depth_loss: 0.0403 loss: 6.6123 rmse: 0.1986\n",
      "Train Epoch: 1 Iteration: 585 - LR: [0.0001364794282556877] dice_loss: 0.6593 depth_loss: 0.0396 loss: 6.6322 rmse: 0.1969\n",
      "Train Epoch: 1 Iteration: 590 - LR: [0.00013829600907157156] dice_loss: 0.6614 depth_loss: 0.0383 loss: 6.6519 rmse: 0.1936\n",
      "Train Epoch: 1 Iteration: 595 - LR: [0.00014012202510826086] dice_loss: 0.6630 depth_loss: 0.0377 loss: 6.6676 rmse: 0.1922\n",
      "Validation Results - Epoch: 1  dice_loss: 0.6750 depth_loss: 0.0422 loss: 6.7918 rmse: 0.2046\n",
      "Model saved with loss 6.7918 at epoch 1\n",
      "Train Epoch: 2 Iteration: 600 - LR: [0.00014195733480207832] dice_loss: 0.6617 depth_loss: 0.0377 loss: 6.6545 rmse: 0.1924\n",
      "Train Epoch: 2 Iteration: 605 - LR: [0.00014380179586884667] dice_loss: 0.6628 depth_loss: 0.0369 loss: 6.6647 rmse: 0.1904\n",
      "Train Epoch: 2 Iteration: 610 - LR: [0.00014565526531491963] dice_loss: 0.6620 depth_loss: 0.0363 loss: 6.6560 rmse: 0.1886\n",
      "Train Epoch: 2 Iteration: 615 - LR: [0.0001475175994482676] dice_loss: 0.6543 depth_loss: 0.0358 loss: 6.5785 rmse: 0.1874\n",
      "Train Epoch: 2 Iteration: 620 - LR: [0.00014938865388961718] dice_loss: 0.6457 depth_loss: 0.0349 loss: 6.4923 rmse: 0.1851\n",
      "Train Epoch: 2 Iteration: 625 - LR: [0.00015126828358364482] dice_loss: 0.6471 depth_loss: 0.0349 loss: 6.5055 rmse: 0.1851\n",
      "Train Epoch: 2 Iteration: 630 - LR: [0.0001531563428102221] dice_loss: 0.6433 depth_loss: 0.0351 loss: 6.4681 rmse: 0.1856\n",
      "Train Epoch: 2 Iteration: 635 - LR: [0.00015505268519571254] dice_loss: 0.6431 depth_loss: 0.0353 loss: 6.4660 rmse: 0.1861\n",
      "Train Epoch: 2 Iteration: 640 - LR: [0.00015695716372432023] dice_loss: 0.6373 depth_loss: 0.0358 loss: 6.4092 rmse: 0.1875\n",
      "Train Epoch: 2 Iteration: 645 - LR: [0.00015886963074948624] dice_loss: 0.6320 depth_loss: 0.0361 loss: 6.3557 rmse: 0.1881\n",
      "Train Epoch: 2 Iteration: 650 - LR: [0.00016078993800533616] dice_loss: 0.6310 depth_loss: 0.0383 loss: 6.3487 rmse: 0.1932\n",
      "Train Epoch: 2 Iteration: 655 - LR: [0.00016271793661817348] dice_loss: 0.6351 depth_loss: 0.0383 loss: 6.3889 rmse: 0.1934\n",
      "Train Epoch: 2 Iteration: 660 - LR: [0.0001646534771180223] dice_loss: 0.6355 depth_loss: 0.0379 loss: 6.3932 rmse: 0.1925\n",
      "Train Epoch: 2 Iteration: 665 - LR: [0.00016659640945021405] dice_loss: 0.6418 depth_loss: 0.0380 loss: 6.4561 rmse: 0.1930\n",
      "Train Epoch: 2 Iteration: 670 - LR: [0.00016854658298702145] dice_loss: 0.6467 depth_loss: 0.0385 loss: 6.5052 rmse: 0.1943\n",
      "Train Epoch: 2 Iteration: 675 - LR: [0.00017050384653933583] dice_loss: 0.6460 depth_loss: 0.0383 loss: 6.4978 rmse: 0.1937\n",
      "Train Epoch: 2 Iteration: 680 - LR: [0.00017246804836838775] dice_loss: 0.6516 depth_loss: 0.0375 loss: 6.5537 rmse: 0.1917\n",
      "Train Epoch: 2 Iteration: 685 - LR: [0.0001744390361975115] dice_loss: 0.6554 depth_loss: 0.0382 loss: 6.5924 rmse: 0.1934\n",
      "Train Epoch: 2 Iteration: 690 - LR: [0.0001764166572239499] dice_loss: 0.6543 depth_loss: 0.0382 loss: 6.5813 rmse: 0.1935\n",
      "Train Epoch: 2 Iteration: 695 - LR: [0.00017840075813070042] dice_loss: 0.6572 depth_loss: 0.0377 loss: 6.6095 rmse: 0.1923\n",
      "Train Epoch: 2 Iteration: 700 - LR: [0.00018039118509840217] dice_loss: 0.6555 depth_loss: 0.0364 loss: 6.5916 rmse: 0.1886\n",
      "Train Epoch: 2 Iteration: 705 - LR: [0.00018238778381725936] dice_loss: 0.6556 depth_loss: 0.0359 loss: 6.5921 rmse: 0.1874\n",
      "Train Epoch: 2 Iteration: 710 - LR: [0.0001843903994990058] dice_loss: 0.6520 depth_loss: 0.0353 loss: 6.5551 rmse: 0.1861\n",
      "Train Epoch: 2 Iteration: 715 - LR: [0.00018639887688890415] dice_loss: 0.6530 depth_loss: 0.0356 loss: 6.5658 rmse: 0.1869\n",
      "Train Epoch: 2 Iteration: 720 - LR: [0.00018841306027778224] dice_loss: 0.6553 depth_loss: 0.0355 loss: 6.5884 rmse: 0.1866\n",
      "Train Epoch: 2 Iteration: 725 - LR: [0.0001904327935141051] dice_loss: 0.6547 depth_loss: 0.0360 loss: 6.5827 rmse: 0.1880\n",
      "Train Epoch: 2 Iteration: 730 - LR: [0.00019245792001608018] dice_loss: 0.6495 depth_loss: 0.0352 loss: 6.5302 rmse: 0.1857\n",
      "Train Epoch: 2 Iteration: 735 - LR: [0.0001944882827837968] dice_loss: 0.6450 depth_loss: 0.0345 loss: 6.4841 rmse: 0.1839\n",
      "Train Epoch: 2 Iteration: 740 - LR: [0.00019652372441139756] dice_loss: 0.6484 depth_loss: 0.0343 loss: 6.5187 rmse: 0.1836\n",
      "Train Epoch: 2 Iteration: 745 - LR: [0.0001985640870992818] dice_loss: 0.6481 depth_loss: 0.0332 loss: 6.5143 rmse: 0.1802\n",
      "Train Epoch: 2 Iteration: 750 - LR: [0.00020060921266633845] dice_loss: 0.6505 depth_loss: 0.0326 loss: 6.5374 rmse: 0.1787\n",
      "Train Epoch: 2 Iteration: 755 - LR: [0.00020265894256220985] dice_loss: 0.6545 depth_loss: 0.0317 loss: 6.5762 rmse: 0.1760\n",
      "Train Epoch: 2 Iteration: 760 - LR: [0.0002047131178795832] dice_loss: 0.6571 depth_loss: 0.0315 loss: 6.6024 rmse: 0.1756\n",
      "Train Epoch: 2 Iteration: 765 - LR: [0.00020677157936651002] dice_loss: 0.6512 depth_loss: 0.0315 loss: 6.5438 rmse: 0.1756\n",
      "Train Epoch: 2 Iteration: 770 - LR: [0.0002088341674387523] dice_loss: 0.6489 depth_loss: 0.0315 loss: 6.5207 rmse: 0.1757\n",
      "Train Epoch: 2 Iteration: 775 - LR: [0.00021090072219215453] dice_loss: 0.6452 depth_loss: 0.0316 loss: 6.4834 rmse: 0.1763\n",
      "Train Epoch: 2 Iteration: 780 - LR: [0.0002129710834150404] dice_loss: 0.6413 depth_loss: 0.0321 loss: 6.4448 rmse: 0.1777\n",
      "Train Epoch: 2 Iteration: 785 - LR: [0.0002150450906006333] dice_loss: 0.6380 depth_loss: 0.0324 loss: 6.4128 rmse: 0.1786\n",
      "Train Epoch: 2 Iteration: 790 - LR: [0.00021712258295949972] dice_loss: 0.6346 depth_loss: 0.0322 loss: 6.3787 rmse: 0.1781\n",
      "Train Epoch: 2 Iteration: 795 - LR: [0.00021920339943201482] dice_loss: 0.6301 depth_loss: 0.0317 loss: 6.3326 rmse: 0.1766\n",
      "Train Epoch: 2 Iteration: 800 - LR: [0.00022128737870084865] dice_loss: 0.6290 depth_loss: 0.0317 loss: 6.3219 rmse: 0.1767\n",
      "Train Epoch: 2 Iteration: 805 - LR: [0.00022337435920347207] dice_loss: 0.6215 depth_loss: 0.0312 loss: 6.2462 rmse: 0.1753\n",
      "Train Epoch: 2 Iteration: 810 - LR: [0.00022546417914468263] dice_loss: 0.6145 depth_loss: 0.0304 loss: 6.1756 rmse: 0.1731\n",
      "Train Epoch: 2 Iteration: 815 - LR: [0.0002275566765091477] dice_loss: 0.6100 depth_loss: 0.0306 loss: 6.1306 rmse: 0.1737\n",
      "Train Epoch: 2 Iteration: 820 - LR: [0.00022965168907396454] dice_loss: 0.6058 depth_loss: 0.0308 loss: 6.0890 rmse: 0.1744\n",
      "Train Epoch: 2 Iteration: 825 - LR: [0.00023174905442123722] dice_loss: 0.6020 depth_loss: 0.0316 loss: 6.0514 rmse: 0.1766\n",
      "Train Epoch: 2 Iteration: 830 - LR: [0.00023384860995066792] dice_loss: 0.6116 depth_loss: 0.0333 loss: 6.1496 rmse: 0.1807\n",
      "Train Epoch: 2 Iteration: 835 - LR: [0.00023595019289216298] dice_loss: 0.6209 depth_loss: 0.0348 loss: 6.2442 rmse: 0.1844\n",
      "Train Epoch: 2 Iteration: 840 - LR: [0.00023805364031845148] dice_loss: 0.6275 depth_loss: 0.0363 loss: 6.3111 rmse: 0.1881\n",
      "Train Epoch: 2 Iteration: 845 - LR: [0.00024015878915771663] dice_loss: 0.6246 depth_loss: 0.0382 loss: 6.2846 rmse: 0.1927\n",
      "Train Epoch: 2 Iteration: 850 - LR: [0.00024226547620623777] dice_loss: 0.6340 depth_loss: 0.0409 loss: 6.3804 rmse: 0.1986\n",
      "Train Epoch: 2 Iteration: 855 - LR: [0.00024437353814104327] dice_loss: 0.6378 depth_loss: 0.0424 loss: 6.4205 rmse: 0.2022\n",
      "Train Epoch: 2 Iteration: 860 - LR: [0.0002464828115325719] dice_loss: 0.6470 depth_loss: 0.0433 loss: 6.5130 rmse: 0.2045\n",
      "Train Epoch: 2 Iteration: 865 - LR: [0.00024859313285734315] dice_loss: 0.6490 depth_loss: 0.0436 loss: 6.5335 rmse: 0.2052\n",
      "Train Epoch: 2 Iteration: 870 - LR: [0.00025070433851063465] dice_loss: 0.6469 depth_loss: 0.0432 loss: 6.5120 rmse: 0.2046\n",
      "Train Epoch: 2 Iteration: 875 - LR: [0.00025281626481916513] dice_loss: 0.6486 depth_loss: 0.0443 loss: 6.5308 rmse: 0.2074\n",
      "Train Epoch: 2 Iteration: 880 - LR: [0.0002549287480537843] dice_loss: 0.6506 depth_loss: 0.0431 loss: 6.5487 rmse: 0.2044\n",
      "Train Epoch: 2 Iteration: 885 - LR: [0.00025704162444216546] dice_loss: 0.6525 depth_loss: 0.0420 loss: 6.5675 rmse: 0.2021\n",
      "Train Epoch: 2 Iteration: 890 - LR: [0.000259154730181502] dice_loss: 0.6489 depth_loss: 0.0414 loss: 6.5302 rmse: 0.2004\n",
      "Train Epoch: 2 Iteration: 895 - LR: [0.00026126790145120717] dice_loss: 0.6447 depth_loss: 0.0406 loss: 6.4877 rmse: 0.1986\n",
      "Train Epoch: 2 Iteration: 900 - LR: [0.00026338097442561356] dice_loss: 0.6429 depth_loss: 0.0398 loss: 6.4688 rmse: 0.1969\n",
      "Train Epoch: 2 Iteration: 905 - LR: [0.00026549378528667434] dice_loss: 0.6374 depth_loss: 0.0385 loss: 6.4127 rmse: 0.1935\n",
      "Train Epoch: 2 Iteration: 910 - LR: [0.000267606170236663] dice_loss: 0.6341 depth_loss: 0.0378 loss: 6.3789 rmse: 0.1918\n",
      "Train Epoch: 2 Iteration: 915 - LR: [0.00026971796551087274] dice_loss: 0.6368 depth_loss: 0.0368 loss: 6.4051 rmse: 0.1892\n",
      "Train Epoch: 2 Iteration: 920 - LR: [0.00027182900739031147] dice_loss: 0.6279 depth_loss: 0.0362 loss: 6.3152 rmse: 0.1878\n",
      "Train Epoch: 2 Iteration: 925 - LR: [0.0002739391322143949] dice_loss: 0.6287 depth_loss: 0.0355 loss: 6.3222 rmse: 0.1862\n",
      "Train Epoch: 2 Iteration: 930 - LR: [0.0002760481763936344] dice_loss: 0.6201 depth_loss: 0.0346 loss: 6.2358 rmse: 0.1836\n",
      "Train Epoch: 2 Iteration: 935 - LR: [0.0002781559764223195] dice_loss: 0.6180 depth_loss: 0.0337 loss: 6.2132 rmse: 0.1813\n",
      "Train Epoch: 2 Iteration: 940 - LR: [0.00028026236889119334] dice_loss: 0.6123 depth_loss: 0.0331 loss: 6.1565 rmse: 0.1796\n",
      "Train Epoch: 2 Iteration: 945 - LR: [0.000282367190500122] dice_loss: 0.6130 depth_loss: 0.0331 loss: 6.1633 rmse: 0.1796\n",
      "Train Epoch: 2 Iteration: 950 - LR: [0.00028447027807075375] dice_loss: 0.6091 depth_loss: 0.0325 loss: 6.1234 rmse: 0.1783\n",
      "Train Epoch: 2 Iteration: 955 - LR: [0.00028657146855916966] dice_loss: 0.6022 depth_loss: 0.0322 loss: 6.0539 rmse: 0.1775\n",
      "Train Epoch: 2 Iteration: 960 - LR: [0.0002886705990685245] dice_loss: 0.6058 depth_loss: 0.0314 loss: 6.0896 rmse: 0.1753\n",
      "Train Epoch: 2 Iteration: 965 - LR: [0.0002907675068616744] dice_loss: 0.6070 depth_loss: 0.0310 loss: 6.1008 rmse: 0.1744\n",
      "Train Epoch: 2 Iteration: 970 - LR: [0.00029286202937379394] dice_loss: 0.6040 depth_loss: 0.0307 loss: 6.0705 rmse: 0.1737\n",
      "Train Epoch: 2 Iteration: 975 - LR: [0.00029495400422497903] dice_loss: 0.6062 depth_loss: 0.0302 loss: 6.0919 rmse: 0.1721\n",
      "Train Epoch: 2 Iteration: 980 - LR: [0.0002970432692328355] dice_loss: 0.6040 depth_loss: 0.0313 loss: 6.0717 rmse: 0.1750\n",
      "Train Epoch: 2 Iteration: 985 - LR: [0.00029912966242505207] dice_loss: 0.6067 depth_loss: 0.0315 loss: 6.0987 rmse: 0.1758\n",
      "Train Epoch: 2 Iteration: 990 - LR: [0.0003012130220519583] dice_loss: 0.6047 depth_loss: 0.0326 loss: 6.0792 rmse: 0.1787\n",
      "Train Epoch: 2 Iteration: 995 - LR: [0.0003032931865990635] dice_loss: 0.6027 depth_loss: 0.0331 loss: 6.0603 rmse: 0.1801\n",
      "Train Epoch: 2 Iteration: 1000 - LR: [0.0003053699947995788] dice_loss: 0.5995 depth_loss: 0.0327 loss: 6.0282 rmse: 0.1790\n",
      "Train Epoch: 2 Iteration: 1005 - LR: [0.0003074432856469193] dice_loss: 0.5967 depth_loss: 0.0325 loss: 6.0000 rmse: 0.1787\n",
      "Train Epoch: 2 Iteration: 1010 - LR: [0.00030951289840718644] dice_loss: 0.5890 depth_loss: 0.0326 loss: 5.9228 rmse: 0.1789\n",
      "Train Epoch: 2 Iteration: 1015 - LR: [0.0003115786726316288] dice_loss: 0.5859 depth_loss: 0.0323 loss: 5.8910 rmse: 0.1782\n",
      "Train Epoch: 2 Iteration: 1020 - LR: [0.00031364044816908145] dice_loss: 0.5869 depth_loss: 0.0323 loss: 5.9013 rmse: 0.1781\n",
      "Train Epoch: 2 Iteration: 1025 - LR: [0.00031569806517838135] dice_loss: 0.5880 depth_loss: 0.0326 loss: 5.9125 rmse: 0.1791\n",
      "Train Epoch: 2 Iteration: 1030 - LR: [0.00031775136414075944] dice_loss: 0.5889 depth_loss: 0.0336 loss: 5.9224 rmse: 0.1818\n",
      "Train Epoch: 2 Iteration: 1035 - LR: [0.00031980018587220757] dice_loss: 0.5875 depth_loss: 0.0341 loss: 5.9089 rmse: 0.1829\n",
      "Train Epoch: 2 Iteration: 1040 - LR: [0.00032184437153581937] dice_loss: 0.5901 depth_loss: 0.0341 loss: 5.9354 rmse: 0.1826\n",
      "Train Epoch: 2 Iteration: 1045 - LR: [0.0003238837626541038] dice_loss: 0.5895 depth_loss: 0.0338 loss: 5.9288 rmse: 0.1818\n",
      "Train Epoch: 2 Iteration: 1050 - LR: [0.00032591820112127205] dice_loss: 0.5915 depth_loss: 0.0331 loss: 5.9480 rmse: 0.1800\n",
      "Train Epoch: 2 Iteration: 1055 - LR: [0.0003279475292154942] dice_loss: 0.5961 depth_loss: 0.0328 loss: 5.9940 rmse: 0.1794\n",
      "Train Epoch: 2 Iteration: 1060 - LR: [0.00032997158961112693] dice_loss: 0.5984 depth_loss: 0.0334 loss: 6.0177 rmse: 0.1809\n",
      "Train Epoch: 2 Iteration: 1065 - LR: [0.0003319902253909105] dice_loss: 0.6052 depth_loss: 0.0330 loss: 6.0850 rmse: 0.1799\n",
      "Train Epoch: 2 Iteration: 1070 - LR: [0.00033400328005813405] dice_loss: 0.6019 depth_loss: 0.0325 loss: 6.0511 rmse: 0.1787\n",
      "Train Epoch: 2 Iteration: 1075 - LR: [0.0003360105975487675] dice_loss: 0.5968 depth_loss: 0.0319 loss: 5.9996 rmse: 0.1768\n",
      "Train Epoch: 2 Iteration: 1080 - LR: [0.00033801202224356105] dice_loss: 0.5933 depth_loss: 0.0314 loss: 5.9643 rmse: 0.1757\n",
      "Train Epoch: 2 Iteration: 1085 - LR: [0.00034000739898011004] dice_loss: 0.5926 depth_loss: 0.0333 loss: 5.9592 rmse: 0.1800\n",
      "Train Epoch: 2 Iteration: 1090 - LR: [0.0003419965730648832] dice_loss: 0.5979 depth_loss: 0.0350 loss: 6.0137 rmse: 0.1841\n",
      "Train Epoch: 2 Iteration: 1095 - LR: [0.00034397939028521626] dice_loss: 0.5985 depth_loss: 0.0378 loss: 6.0229 rmse: 0.1905\n",
      "Train Epoch: 2 Iteration: 1100 - LR: [0.00034595569692126696] dice_loss: 0.5984 depth_loss: 0.0405 loss: 6.0242 rmse: 0.1964\n",
      "Train Epoch: 2 Iteration: 1105 - LR: [0.00034792533975793236] dice_loss: 0.5980 depth_loss: 0.0436 loss: 6.0238 rmse: 0.2032\n",
      "Train Epoch: 2 Iteration: 1110 - LR: [0.000349888166096727] dice_loss: 0.5904 depth_loss: 0.0428 loss: 5.9465 rmse: 0.2017\n",
      "Train Epoch: 2 Iteration: 1115 - LR: [0.0003518440237676216] dice_loss: 0.5896 depth_loss: 0.0427 loss: 5.9383 rmse: 0.2018\n",
      "Train Epoch: 2 Iteration: 1120 - LR: [0.0003537927611408389] dice_loss: 0.5925 depth_loss: 0.0421 loss: 5.9666 rmse: 0.2006\n",
      "Train Epoch: 2 Iteration: 1125 - LR: [0.0003557342271386102] dice_loss: 0.5954 depth_loss: 0.0422 loss: 5.9964 rmse: 0.2013\n",
      "Train Epoch: 2 Iteration: 1130 - LR: [0.00035766827124688726] dice_loss: 0.5951 depth_loss: 0.0413 loss: 5.9921 rmse: 0.1993\n",
      "Train Epoch: 2 Iteration: 1135 - LR: [0.0003595947435270109] dice_loss: 0.5988 depth_loss: 0.0408 loss: 6.0290 rmse: 0.1984\n",
      "Train Epoch: 2 Iteration: 1140 - LR: [0.0003615134946273351] dice_loss: 0.5957 depth_loss: 0.0397 loss: 5.9967 rmse: 0.1956\n",
      "Train Epoch: 2 Iteration: 1145 - LR: [0.0003634243757948066] dice_loss: 0.5850 depth_loss: 0.0383 loss: 5.8880 rmse: 0.1919\n",
      "Train Epoch: 2 Iteration: 1150 - LR: [0.0003653272388864957] dice_loss: 0.5837 depth_loss: 0.0367 loss: 5.8740 rmse: 0.1874\n",
      "Train Epoch: 2 Iteration: 1155 - LR: [0.00036722193638108187] dice_loss: 0.5774 depth_loss: 0.0356 loss: 5.8100 rmse: 0.1846\n",
      "Train Epoch: 2 Iteration: 1160 - LR: [0.00036910832139029094] dice_loss: 0.5803 depth_loss: 0.0343 loss: 5.8370 rmse: 0.1811\n",
      "Train Epoch: 2 Iteration: 1165 - LR: [0.0003709862476702819] dice_loss: 0.5810 depth_loss: 0.0331 loss: 5.8429 rmse: 0.1778\n",
      "Train Epoch: 2 Iteration: 1170 - LR: [0.00037285556963298505] dice_loss: 0.5867 depth_loss: 0.0325 loss: 5.8998 rmse: 0.1765\n",
      "Train Epoch: 2 Iteration: 1175 - LR: [0.0003747161423573889] dice_loss: 0.5939 depth_loss: 0.0319 loss: 5.9710 rmse: 0.1750\n",
      "Train Epoch: 2 Iteration: 1180 - LR: [0.00037656782160077517] dice_loss: 0.5961 depth_loss: 0.0315 loss: 5.9930 rmse: 0.1742\n",
      "Train Epoch: 2 Iteration: 1185 - LR: [0.0003784104638099012] dice_loss: 0.5993 depth_loss: 0.0310 loss: 6.0235 rmse: 0.1728\n",
      "Train Epoch: 2 Iteration: 1190 - LR: [0.00038024392613212975] dice_loss: 0.6014 depth_loss: 0.0301 loss: 6.0443 rmse: 0.1703\n",
      "Validation Results - Epoch: 2  dice_loss: 0.6027 depth_loss: 0.0244 loss: 6.0514 rmse: 0.1553\n",
      "Model saved with loss 6.0514 at epoch 2\n",
      "Train Epoch: 3 Iteration: 1195 - LR: [0.00038206806642650265] dice_loss: 0.5987 depth_loss: 0.0294 loss: 6.0167 rmse: 0.1683\n",
      "Train Epoch: 3 Iteration: 1200 - LR: [0.00038388274327476145] dice_loss: 0.5998 depth_loss: 0.0288 loss: 6.0265 rmse: 0.1667\n",
      "Train Epoch: 3 Iteration: 1205 - LR: [0.0003856878159923103] dice_loss: 0.5989 depth_loss: 0.0280 loss: 6.0173 rmse: 0.1645\n",
      "Train Epoch: 3 Iteration: 1210 - LR: [0.0003874831446391235] dice_loss: 0.5911 depth_loss: 0.0278 loss: 5.9385 rmse: 0.1641\n",
      "Train Epoch: 3 Iteration: 1215 - LR: [0.00038926859003059335] dice_loss: 0.5833 depth_loss: 0.0277 loss: 5.8605 rmse: 0.1641\n",
      "Train Epoch: 3 Iteration: 1220 - LR: [0.0003910440137483215] dice_loss: 0.5851 depth_loss: 0.0277 loss: 5.8792 rmse: 0.1643\n",
      "Train Epoch: 3 Iteration: 1225 - LR: [0.0003928092781508496] dice_loss: 0.5828 depth_loss: 0.0282 loss: 5.8562 rmse: 0.1656\n",
      "Train Epoch: 3 Iteration: 1230 - LR: [0.00039456424638433023] dice_loss: 0.5834 depth_loss: 0.0283 loss: 5.8625 rmse: 0.1661\n",
      "Train Epoch: 3 Iteration: 1235 - LR: [0.00039630878239313623] dice_loss: 0.5788 depth_loss: 0.0286 loss: 5.8171 rmse: 0.1672\n",
      "Train Epoch: 3 Iteration: 1240 - LR: [0.00039804275093040914] dice_loss: 0.5739 depth_loss: 0.0294 loss: 5.7683 rmse: 0.1695\n",
      "Train Epoch: 3 Iteration: 1245 - LR: [0.0003997660175685441] dice_loss: 0.5743 depth_loss: 0.0297 loss: 5.7723 rmse: 0.1704\n",
      "Train Epoch: 3 Iteration: 1250 - LR: [0.000401478448709611] dice_loss: 0.5779 depth_loss: 0.0297 loss: 5.8090 rmse: 0.1705\n",
      "Train Epoch: 3 Iteration: 1255 - LR: [0.0004031799115957128] dice_loss: 0.5796 depth_loss: 0.0294 loss: 5.8250 rmse: 0.1698\n",
      "Train Epoch: 3 Iteration: 1260 - LR: [0.0004048702743192768] dice_loss: 0.5872 depth_loss: 0.0290 loss: 5.9008 rmse: 0.1685\n",
      "Train Epoch: 3 Iteration: 1265 - LR: [0.0004065494058332812] dice_loss: 0.5928 depth_loss: 0.0294 loss: 5.9573 rmse: 0.1698\n",
      "Train Epoch: 3 Iteration: 1270 - LR: [0.0004082171759614149] dice_loss: 0.5920 depth_loss: 0.0298 loss: 5.9502 rmse: 0.1711\n",
      "Train Epoch: 3 Iteration: 1275 - LR: [0.0004098734554081694] dice_loss: 0.5977 depth_loss: 0.0297 loss: 6.0070 rmse: 0.1708\n",
      "Train Epoch: 3 Iteration: 1280 - LR: [0.00041151811576886215] dice_loss: 0.6022 depth_loss: 0.0293 loss: 6.0514 rmse: 0.1695\n",
      "Train Epoch: 3 Iteration: 1285 - LR: [0.00041315102953959175] dice_loss: 0.6010 depth_loss: 0.0286 loss: 6.0388 rmse: 0.1674\n",
      "Train Epoch: 3 Iteration: 1290 - LR: [0.0004147720701271229] dice_loss: 0.6025 depth_loss: 0.0282 loss: 6.0534 rmse: 0.1665\n",
      "Train Epoch: 3 Iteration: 1295 - LR: [0.0004163811118587002] dice_loss: 0.6011 depth_loss: 0.0276 loss: 6.0383 rmse: 0.1646\n",
      "Train Epoch: 3 Iteration: 1300 - LR: [0.0004179780299917913] dice_loss: 0.6008 depth_loss: 0.0274 loss: 6.0356 rmse: 0.1641\n",
      "Train Epoch: 3 Iteration: 1305 - LR: [0.0004195627007237575] dice_loss: 0.5986 depth_loss: 0.0267 loss: 6.0125 rmse: 0.1619\n",
      "Train Epoch: 3 Iteration: 1310 - LR: [0.0004211350012014524] dice_loss: 0.6008 depth_loss: 0.0270 loss: 6.0351 rmse: 0.1629\n",
      "Train Epoch: 3 Iteration: 1315 - LR: [0.00042269480953074505] dice_loss: 0.6035 depth_loss: 0.0275 loss: 6.0621 rmse: 0.1644\n",
      "Train Epoch: 3 Iteration: 1320 - LR: [0.0004242420047859708] dice_loss: 0.6029 depth_loss: 0.0279 loss: 6.0570 rmse: 0.1658\n",
      "Train Epoch: 3 Iteration: 1325 - LR: [0.0004257764670193057] dice_loss: 0.5975 depth_loss: 0.0281 loss: 6.0029 rmse: 0.1665\n",
      "Train Epoch: 3 Iteration: 1330 - LR: [0.0004272980772700661] dice_loss: 0.5932 depth_loss: 0.0284 loss: 5.9602 rmse: 0.1673\n",
      "Train Epoch: 3 Iteration: 1335 - LR: [0.00042880671757393056] dice_loss: 0.5969 depth_loss: 0.0291 loss: 5.9983 rmse: 0.1691\n",
      "Train Epoch: 3 Iteration: 1340 - LR: [0.0004303022709720854] dice_loss: 0.5978 depth_loss: 0.0290 loss: 6.0075 rmse: 0.1690\n",
      "Train Epoch: 3 Iteration: 1345 - LR: [0.0004317846215202925] dice_loss: 0.6025 depth_loss: 0.0293 loss: 6.0541 rmse: 0.1699\n",
      "Train Epoch: 3 Iteration: 1350 - LR: [0.00043325365429787703] dice_loss: 0.6079 depth_loss: 0.0298 loss: 6.1088 rmse: 0.1709\n",
      "Train Epoch: 3 Iteration: 1355 - LR: [0.00043470925541663785] dice_loss: 0.6123 depth_loss: 0.0317 loss: 6.1543 rmse: 0.1758\n",
      "Train Epoch: 3 Iteration: 1360 - LR: [0.0004361513120296758] dice_loss: 0.6071 depth_loss: 0.0324 loss: 6.1032 rmse: 0.1779\n",
      "Train Epoch: 3 Iteration: 1365 - LR: [0.00043757971234014326] dice_loss: 0.6058 depth_loss: 0.0323 loss: 6.0905 rmse: 0.1776\n",
      "Train Epoch: 3 Iteration: 1370 - LR: [0.0004389943456099105] dice_loss: 0.6021 depth_loss: 0.0318 loss: 6.0524 rmse: 0.1763\n",
      "Train Epoch: 3 Iteration: 1375 - LR: [0.000440395102168151] dice_loss: 0.5993 depth_loss: 0.0322 loss: 6.0248 rmse: 0.1774\n",
      "Train Epoch: 3 Iteration: 1380 - LR: [0.0004417818734198442] dice_loss: 0.5967 depth_loss: 0.0326 loss: 6.0000 rmse: 0.1784\n",
      "Train Epoch: 3 Iteration: 1385 - LR: [0.0004431545518541938] dice_loss: 0.5935 depth_loss: 0.0324 loss: 5.9670 rmse: 0.1778\n",
      "Train Epoch: 3 Iteration: 1390 - LR: [0.00044451303105296294] dice_loss: 0.5888 depth_loss: 0.0313 loss: 5.9196 rmse: 0.1748\n",
      "Train Epoch: 3 Iteration: 1395 - LR: [0.0004458572056987247] dice_loss: 0.5867 depth_loss: 0.0305 loss: 5.8975 rmse: 0.1725\n",
      "Train Epoch: 3 Iteration: 1400 - LR: [0.0004471869715830261] dice_loss: 0.5800 depth_loss: 0.0296 loss: 5.8293 rmse: 0.1699\n",
      "Train Epoch: 3 Iteration: 1405 - LR: [0.0004485022256144678] dice_loss: 0.5743 depth_loss: 0.0287 loss: 5.7714 rmse: 0.1671\n",
      "Train Epoch: 3 Iteration: 1410 - LR: [0.00044980286582669616] dice_loss: 0.5699 depth_loss: 0.0283 loss: 5.7274 rmse: 0.1662\n",
      "Train Epoch: 3 Iteration: 1415 - LR: [0.0004510887913863077] dice_loss: 0.5665 depth_loss: 0.0284 loss: 5.6937 rmse: 0.1666\n",
      "Train Epoch: 3 Iteration: 1420 - LR: [0.00045235990260066716] dice_loss: 0.5629 depth_loss: 0.0283 loss: 5.6577 rmse: 0.1663\n",
      "Train Epoch: 3 Iteration: 1425 - LR: [0.00045361610092563574] dice_loss: 0.5716 depth_loss: 0.0299 loss: 5.7461 rmse: 0.1707\n",
      "Train Epoch: 3 Iteration: 1430 - LR: [0.00045485728897321095] dice_loss: 0.5798 depth_loss: 0.0308 loss: 5.8291 rmse: 0.1729\n",
      "Train Epoch: 3 Iteration: 1435 - LR: [0.0004560833705190766] dice_loss: 0.5859 depth_loss: 0.0322 loss: 5.8911 rmse: 0.1769\n",
      "Train Epoch: 3 Iteration: 1440 - LR: [0.00045729425051006307] dice_loss: 0.5844 depth_loss: 0.0331 loss: 5.8769 rmse: 0.1792\n",
      "Train Epoch: 3 Iteration: 1445 - LR: [0.0004584898350715159] dice_loss: 0.5943 depth_loss: 0.0332 loss: 5.9758 rmse: 0.1796\n",
      "Train Epoch: 3 Iteration: 1450 - LR: [0.00045967003151457385] dice_loss: 0.5985 depth_loss: 0.0332 loss: 6.0182 rmse: 0.1799\n",
      "Train Epoch: 3 Iteration: 1455 - LR: [0.00046083474834335465] dice_loss: 0.6086 depth_loss: 0.0334 loss: 6.1196 rmse: 0.1804\n",
      "Train Epoch: 3 Iteration: 1460 - LR: [0.00046198389526204815] dice_loss: 0.6104 depth_loss: 0.0332 loss: 6.1372 rmse: 0.1802\n",
      "Train Epoch: 3 Iteration: 1465 - LR: [0.0004631173831819166] dice_loss: 0.6089 depth_loss: 0.0326 loss: 6.1220 rmse: 0.1786\n",
      "Train Epoch: 3 Iteration: 1470 - LR: [0.00046423512422820185] dice_loss: 0.6109 depth_loss: 0.0327 loss: 6.1414 rmse: 0.1790\n",
      "Train Epoch: 3 Iteration: 1475 - LR: [0.0004653370317469369] dice_loss: 0.6117 depth_loss: 0.0320 loss: 6.1492 rmse: 0.1771\n",
      "Train Epoch: 3 Iteration: 1480 - LR: [0.00046642302031166494] dice_loss: 0.6133 depth_loss: 0.0316 loss: 6.1643 rmse: 0.1760\n",
      "Train Epoch: 3 Iteration: 1485 - LR: [0.0004674930057300614] dice_loss: 0.6097 depth_loss: 0.0313 loss: 6.1281 rmse: 0.1753\n",
      "Train Epoch: 3 Iteration: 1490 - LR: [0.0004685469050504614] dice_loss: 0.6064 depth_loss: 0.0310 loss: 6.0950 rmse: 0.1746\n",
      "Train Epoch: 3 Iteration: 1495 - LR: [0.0004695846365682902] dice_loss: 0.6059 depth_loss: 0.0306 loss: 6.0893 rmse: 0.1735\n",
      "Train Epoch: 3 Iteration: 1500 - LR: [0.0004706061198323982] dice_loss: 0.6018 depth_loss: 0.0302 loss: 6.0486 rmse: 0.1725\n",
      "Train Epoch: 3 Iteration: 1505 - LR: [0.0004716112756512973] dice_loss: 0.5990 depth_loss: 0.0299 loss: 6.0195 rmse: 0.1715\n",
      "Train Epoch: 3 Iteration: 1510 - LR: [0.00047260002609930045] dice_loss: 0.6018 depth_loss: 0.0297 loss: 6.0482 rmse: 0.1712\n",
      "Train Epoch: 3 Iteration: 1515 - LR: [0.00047357229452256325] dice_loss: 0.5949 depth_loss: 0.0291 loss: 5.9777 rmse: 0.1693\n",
      "Train Epoch: 3 Iteration: 1520 - LR: [0.0004745280055450262] dice_loss: 0.5964 depth_loss: 0.0284 loss: 5.9924 rmse: 0.1673\n",
      "Train Epoch: 3 Iteration: 1525 - LR: [0.0004754670850742585] dice_loss: 0.5883 depth_loss: 0.0279 loss: 5.9104 rmse: 0.1656\n",
      "Train Epoch: 3 Iteration: 1530 - LR: [0.00047638946030720215] dice_loss: 0.5872 depth_loss: 0.0275 loss: 5.8991 rmse: 0.1646\n",
      "Train Epoch: 3 Iteration: 1535 - LR: [0.00047729505973581604] dice_loss: 0.5816 depth_loss: 0.0272 loss: 5.8435 rmse: 0.1637\n",
      "Train Epoch: 3 Iteration: 1540 - LR: [0.00047818381315261975] dice_loss: 0.5822 depth_loss: 0.0271 loss: 5.8489 rmse: 0.1636\n",
      "Train Epoch: 3 Iteration: 1545 - LR: [0.0004790556516561361] dice_loss: 0.5792 depth_loss: 0.0271 loss: 5.8193 rmse: 0.1636\n",
      "Train Epoch: 3 Iteration: 1550 - LR: [0.00047991050765623346] dice_loss: 0.5741 depth_loss: 0.0275 loss: 5.7685 rmse: 0.1649\n",
      "Train Epoch: 3 Iteration: 1555 - LR: [0.0004807483148793651] dice_loss: 0.5779 depth_loss: 0.0280 loss: 5.8065 rmse: 0.1662\n",
      "Train Epoch: 3 Iteration: 1560 - LR: [0.0004815690083737076] dice_loss: 0.5785 depth_loss: 0.0279 loss: 5.8128 rmse: 0.1662\n",
      "Train Epoch: 3 Iteration: 1565 - LR: [0.00048237252451419587] dice_loss: 0.5729 depth_loss: 0.0279 loss: 5.7567 rmse: 0.1661\n",
      "Train Epoch: 3 Iteration: 1570 - LR: [0.00048315880100745614] dice_loss: 0.5749 depth_loss: 0.0275 loss: 5.7768 rmse: 0.1650\n",
      "Train Epoch: 3 Iteration: 1575 - LR: [0.0004839277768966349] dice_loss: 0.5730 depth_loss: 0.0275 loss: 5.7579 rmse: 0.1650\n",
      "Train Epoch: 3 Iteration: 1580 - LR: [0.00048467939256612513] dice_loss: 0.5762 depth_loss: 0.0275 loss: 5.7894 rmse: 0.1652\n",
      "Train Epoch: 3 Iteration: 1585 - LR: [0.0004854135897461877] dice_loss: 0.5743 depth_loss: 0.0275 loss: 5.7704 rmse: 0.1650\n",
      "Train Epoch: 3 Iteration: 1590 - LR: [0.0004861303115174689] dice_loss: 0.5724 depth_loss: 0.0275 loss: 5.7511 rmse: 0.1650\n",
      "Train Epoch: 3 Iteration: 1595 - LR: [0.0004868295023154132] dice_loss: 0.5694 depth_loss: 0.0277 loss: 5.7218 rmse: 0.1653\n",
      "Train Epoch: 3 Iteration: 1600 - LR: [0.0004875111079345708] dice_loss: 0.5663 depth_loss: 0.0284 loss: 5.6917 rmse: 0.1672\n",
      "Train Epoch: 3 Iteration: 1605 - LR: [0.0004881750755328] dice_loss: 0.5597 depth_loss: 0.0292 loss: 5.6259 rmse: 0.1695\n",
      "Train Epoch: 3 Iteration: 1610 - LR: [0.0004888213536353642] dice_loss: 0.5561 depth_loss: 0.0293 loss: 5.5901 rmse: 0.1700\n",
      "Train Epoch: 3 Iteration: 1615 - LR: [0.0004894498921389221] dice_loss: 0.5580 depth_loss: 0.0291 loss: 5.6093 rmse: 0.1694\n",
      "Train Epoch: 3 Iteration: 1620 - LR: [0.0004900606423154119] dice_loss: 0.5591 depth_loss: 0.0296 loss: 5.6201 rmse: 0.1707\n",
      "Train Epoch: 3 Iteration: 1625 - LR: [0.0004906535568158296] dice_loss: 0.5587 depth_loss: 0.0303 loss: 5.6168 rmse: 0.1726\n",
      "Train Epoch: 3 Iteration: 1630 - LR: [0.0004912285896738992] dice_loss: 0.5581 depth_loss: 0.0298 loss: 5.6111 rmse: 0.1712\n",
      "Train Epoch: 3 Iteration: 1635 - LR: [0.0004917856963096367] dice_loss: 0.5613 depth_loss: 0.0292 loss: 5.6420 rmse: 0.1696\n",
      "Train Epoch: 3 Iteration: 1640 - LR: [0.0004923248335328056] dice_loss: 0.5613 depth_loss: 0.0296 loss: 5.6427 rmse: 0.1708\n",
      "Train Epoch: 3 Iteration: 1645 - LR: [0.0004928459595462661] dice_loss: 0.5639 depth_loss: 0.0294 loss: 5.6687 rmse: 0.1701\n",
      "Train Epoch: 3 Iteration: 1650 - LR: [0.0004933490339492149] dice_loss: 0.5696 depth_loss: 0.0295 loss: 5.7252 rmse: 0.1705\n",
      "Train Epoch: 3 Iteration: 1655 - LR: [0.0004938340177403171] dice_loss: 0.5724 depth_loss: 0.0300 loss: 5.7544 rmse: 0.1721\n",
      "Train Epoch: 3 Iteration: 1660 - LR: [0.0004943008733207308] dice_loss: 0.5792 depth_loss: 0.0301 loss: 5.8216 rmse: 0.1723\n",
      "Train Epoch: 3 Iteration: 1665 - LR: [0.0004947495644970208] dice_loss: 0.5753 depth_loss: 0.0297 loss: 5.7829 rmse: 0.1712\n",
      "Train Epoch: 3 Iteration: 1670 - LR: [0.0004951800564839653] dice_loss: 0.5707 depth_loss: 0.0290 loss: 5.7356 rmse: 0.1692\n",
      "Train Epoch: 3 Iteration: 1675 - LR: [0.0004955923159072524] dice_loss: 0.5665 depth_loss: 0.0286 loss: 5.6940 rmse: 0.1681\n",
      "Train Epoch: 3 Iteration: 1680 - LR: [0.0004959863108060675] dice_loss: 0.5659 depth_loss: 0.0294 loss: 5.6881 rmse: 0.1704\n",
      "Train Epoch: 3 Iteration: 1685 - LR: [0.0004963620106355712] dice_loss: 0.5716 depth_loss: 0.0298 loss: 5.7460 rmse: 0.1714\n",
      "Train Epoch: 3 Iteration: 1690 - LR: [0.0004967193862692672] dice_loss: 0.5723 depth_loss: 0.0299 loss: 5.7529 rmse: 0.1717\n",
      "Train Epoch: 3 Iteration: 1695 - LR: [0.0004970584100012601] dice_loss: 0.5721 depth_loss: 0.0293 loss: 5.7503 rmse: 0.1700\n",
      "Train Epoch: 3 Iteration: 1700 - LR: [0.000497379055548404] dice_loss: 0.5713 depth_loss: 0.0292 loss: 5.7425 rmse: 0.1699\n",
      "Train Epoch: 3 Iteration: 1705 - LR: [0.0004976812980523396] dice_loss: 0.5644 depth_loss: 0.0288 loss: 5.6725 rmse: 0.1688\n",
      "Train Epoch: 3 Iteration: 1710 - LR: [0.0004979651140814215] dice_loss: 0.5634 depth_loss: 0.0288 loss: 5.6632 rmse: 0.1687\n",
      "Train Epoch: 3 Iteration: 1715 - LR: [0.0004982304816325345] dice_loss: 0.5656 depth_loss: 0.0283 loss: 5.6845 rmse: 0.1674\n",
      "Train Epoch: 3 Iteration: 1720 - LR: [0.0004984773801328002] dice_loss: 0.5692 depth_loss: 0.0288 loss: 5.7211 rmse: 0.1688\n",
      "Train Epoch: 3 Iteration: 1725 - LR: [0.0004987057904411712] dice_loss: 0.5698 depth_loss: 0.0285 loss: 5.7263 rmse: 0.1679\n",
      "Train Epoch: 3 Iteration: 1730 - LR: [0.0004989156948499148] dice_loss: 0.5741 depth_loss: 0.0286 loss: 5.7700 rmse: 0.1682\n",
      "Train Epoch: 3 Iteration: 1735 - LR: [0.0004991070770859869] dice_loss: 0.5707 depth_loss: 0.0284 loss: 5.7359 rmse: 0.1677\n",
      "Train Epoch: 3 Iteration: 1740 - LR: [0.0004992799223122928] dice_loss: 0.5595 depth_loss: 0.0281 loss: 5.6232 rmse: 0.1669\n",
      "Train Epoch: 3 Iteration: 1745 - LR: [0.0004994342171288371] dice_loss: 0.5564 depth_loss: 0.0274 loss: 5.5910 rmse: 0.1647\n",
      "Train Epoch: 3 Iteration: 1750 - LR: [0.0004995699495737636] dice_loss: 0.5503 depth_loss: 0.0273 loss: 5.5304 rmse: 0.1641\n",
      "Train Epoch: 3 Iteration: 1755 - LR: [0.000499687109124282] dice_loss: 0.5528 depth_loss: 0.0267 loss: 5.5548 rmse: 0.1625\n",
      "Train Epoch: 3 Iteration: 1760 - LR: [0.0004997856866974834] dice_loss: 0.5534 depth_loss: 0.0264 loss: 5.5601 rmse: 0.1614\n",
      "Train Epoch: 3 Iteration: 1765 - LR: [0.0004998656746510453] dice_loss: 0.5599 depth_loss: 0.0267 loss: 5.6259 rmse: 0.1624\n",
      "Train Epoch: 3 Iteration: 1770 - LR: [0.0004999270667838234] dice_loss: 0.5675 depth_loss: 0.0265 loss: 5.7012 rmse: 0.1617\n",
      "Train Epoch: 3 Iteration: 1775 - LR: [0.0004999698583363326] dice_loss: 0.5706 depth_loss: 0.0260 loss: 5.7322 rmse: 0.1603\n",
      "Train Epoch: 3 Iteration: 1780 - LR: [0.0004999940459911161] dice_loss: 0.5742 depth_loss: 0.0255 loss: 5.7673 rmse: 0.1587\n",
      "Train Epoch: 3 Iteration: 1785 - LR: [0.0004999999288822531] dice_loss: 0.5760 depth_loss: 0.0249 loss: 5.7848 rmse: 0.1567\n",
      "Validation Results - Epoch: 3  dice_loss: 0.5896 depth_loss: 0.0255 loss: 5.9214 rmse: 0.1591\n",
      "Model saved with loss 5.9214 at epoch 3\n",
      "Train Epoch: 4 Iteration: 1790 - LR: [0.0004999974397653604] dice_loss: 0.5750 depth_loss: 0.0251 loss: 5.7747 rmse: 0.1571\n",
      "Train Epoch: 4 Iteration: 1795 - LR: [0.000499991394801585] dice_loss: 0.5748 depth_loss: 0.0246 loss: 5.7724 rmse: 0.1556\n",
      "Train Epoch: 4 Iteration: 1800 - LR: [0.0004999817940769079] dice_loss: 0.5739 depth_loss: 0.0243 loss: 5.7633 rmse: 0.1545\n",
      "Train Epoch: 4 Iteration: 1805 - LR: [0.000499968637727886] dice_loss: 0.5664 depth_loss: 0.0242 loss: 5.6879 rmse: 0.1543\n",
      "Train Epoch: 4 Iteration: 1810 - LR: [0.0004999519259416496] dice_loss: 0.5581 depth_loss: 0.0240 loss: 5.6054 rmse: 0.1534\n",
      "Train Epoch: 4 Iteration: 1815 - LR: [0.0004999316589559007] dice_loss: 0.5595 depth_loss: 0.0237 loss: 5.6185 rmse: 0.1528\n",
      "Train Epoch: 4 Iteration: 1820 - LR: [0.0004999078370589084] dice_loss: 0.5574 depth_loss: 0.0240 loss: 5.5980 rmse: 0.1538\n",
      "Train Epoch: 4 Iteration: 1825 - LR: [0.0004998804605895054] dice_loss: 0.5588 depth_loss: 0.0242 loss: 5.6122 rmse: 0.1544\n",
      "Train Epoch: 4 Iteration: 1830 - LR: [0.0004998495299370835] dice_loss: 0.5546 depth_loss: 0.0245 loss: 5.5709 rmse: 0.1552\n",
      "Train Epoch: 4 Iteration: 1835 - LR: [0.0004998150455415877] dice_loss: 0.5493 depth_loss: 0.0250 loss: 5.5175 rmse: 0.1568\n",
      "Train Epoch: 4 Iteration: 1840 - LR: [0.0004997770078935099] dice_loss: 0.5497 depth_loss: 0.0255 loss: 5.5228 rmse: 0.1585\n",
      "Train Epoch: 4 Iteration: 1845 - LR: [0.000499735417533882] dice_loss: 0.5534 depth_loss: 0.0266 loss: 5.5602 rmse: 0.1615\n",
      "Train Epoch: 4 Iteration: 1850 - LR: [0.0004996902750542681] dice_loss: 0.5539 depth_loss: 0.0267 loss: 5.5655 rmse: 0.1619\n",
      "Train Epoch: 4 Iteration: 1855 - LR: [0.0004996415810967565] dice_loss: 0.5617 depth_loss: 0.0264 loss: 5.6432 rmse: 0.1611\n",
      "Train Epoch: 4 Iteration: 1860 - LR: [0.00049958933635395] dice_loss: 0.5671 depth_loss: 0.0270 loss: 5.6984 rmse: 0.1626\n",
      "Train Epoch: 4 Iteration: 1865 - LR: [0.0004995335415689565] dice_loss: 0.5669 depth_loss: 0.0274 loss: 5.6961 rmse: 0.1639\n",
      "Train Epoch: 4 Iteration: 1870 - LR: [0.0004994741975353781] dice_loss: 0.5721 depth_loss: 0.0269 loss: 5.7478 rmse: 0.1627\n",
      "Train Epoch: 4 Iteration: 1875 - LR: [0.0004994113050972999] dice_loss: 0.5763 depth_loss: 0.0266 loss: 5.7897 rmse: 0.1617\n",
      "Train Epoch: 4 Iteration: 1880 - LR: [0.0004993448651492783] dice_loss: 0.5753 depth_loss: 0.0261 loss: 5.7788 rmse: 0.1602\n",
      "Train Epoch: 4 Iteration: 1885 - LR: [0.0004992748786363278] dice_loss: 0.5767 depth_loss: 0.0256 loss: 5.7922 rmse: 0.1587\n",
      "Train Epoch: 4 Iteration: 1890 - LR: [0.000499201346553908] dice_loss: 0.5747 depth_loss: 0.0250 loss: 5.7722 rmse: 0.1566\n",
      "Train Epoch: 4 Iteration: 1895 - LR: [0.0004991242699479087] dice_loss: 0.5743 depth_loss: 0.0247 loss: 5.7677 rmse: 0.1558\n",
      "Train Epoch: 4 Iteration: 1900 - LR: [0.0004990436499146366] dice_loss: 0.5710 depth_loss: 0.0240 loss: 5.7337 rmse: 0.1535\n",
      "Train Epoch: 4 Iteration: 1905 - LR: [0.0004989594876007974] dice_loss: 0.5729 depth_loss: 0.0240 loss: 5.7531 rmse: 0.1535\n",
      "Train Epoch: 4 Iteration: 1910 - LR: [0.0004988717842034818] dice_loss: 0.5767 depth_loss: 0.0239 loss: 5.7912 rmse: 0.1532\n",
      "Train Epoch: 4 Iteration: 1915 - LR: [0.0004987805409701467] dice_loss: 0.5776 depth_loss: 0.0243 loss: 5.8003 rmse: 0.1545\n",
      "Train Epoch: 4 Iteration: 1920 - LR: [0.0004986857591985986] dice_loss: 0.5730 depth_loss: 0.0240 loss: 5.7544 rmse: 0.1537\n",
      "Train Epoch: 4 Iteration: 1925 - LR: [0.0004985874402369744] dice_loss: 0.5697 depth_loss: 0.0243 loss: 5.7212 rmse: 0.1547\n",
      "Train Epoch: 4 Iteration: 1930 - LR: [0.0004984855854837228] dice_loss: 0.5744 depth_loss: 0.0246 loss: 5.7682 rmse: 0.1558\n",
      "Train Epoch: 4 Iteration: 1935 - LR: [0.0004983801963875842] dice_loss: 0.5764 depth_loss: 0.0242 loss: 5.7881 rmse: 0.1543\n",
      "Train Epoch: 4 Iteration: 1940 - LR: [0.0004982712744475697] dice_loss: 0.5822 depth_loss: 0.0240 loss: 5.8461 rmse: 0.1537\n",
      "Train Epoch: 4 Iteration: 1945 - LR: [0.0004981588212129404] dice_loss: 0.5875 depth_loss: 0.0238 loss: 5.8987 rmse: 0.1529\n",
      "Train Epoch: 4 Iteration: 1950 - LR: [0.000498042838283185] dice_loss: 0.5919 depth_loss: 0.0242 loss: 5.9436 rmse: 0.1543\n",
      "Train Epoch: 4 Iteration: 1955 - LR: [0.0004979233273079972] dice_loss: 0.5879 depth_loss: 0.0242 loss: 5.9032 rmse: 0.1543\n",
      "Train Epoch: 4 Iteration: 1960 - LR: [0.0004978002899872519] dice_loss: 0.5866 depth_loss: 0.0242 loss: 5.8905 rmse: 0.1546\n",
      "Train Epoch: 4 Iteration: 1965 - LR: [0.0004976737280709819] dice_loss: 0.5832 depth_loss: 0.0247 loss: 5.8571 rmse: 0.1562\n",
      "Train Epoch: 4 Iteration: 1970 - LR: [0.0004975436433593517] dice_loss: 0.5799 depth_loss: 0.0253 loss: 5.8239 rmse: 0.1578\n",
      "Train Epoch: 4 Iteration: 1975 - LR: [0.0004974100377026331] dice_loss: 0.5762 depth_loss: 0.0260 loss: 5.7879 rmse: 0.1601\n",
      "Train Epoch: 4 Iteration: 1980 - LR: [0.0004972729130011781] dice_loss: 0.5729 depth_loss: 0.0260 loss: 5.7548 rmse: 0.1601\n",
      "Train Epoch: 4 Iteration: 1985 - LR: [0.0004971322712053923] dice_loss: 0.5684 depth_loss: 0.0253 loss: 5.7093 rmse: 0.1576\n",
      "Train Epoch: 4 Iteration: 1990 - LR: [0.0004969881143157069] dice_loss: 0.5659 depth_loss: 0.0251 loss: 5.6837 rmse: 0.1572\n",
      "Train Epoch: 4 Iteration: 1995 - LR: [0.0004968404443825504] dice_loss: 0.5591 depth_loss: 0.0246 loss: 5.6160 rmse: 0.1556\n",
      "Train Epoch: 4 Iteration: 2000 - LR: [0.0004966892635063194] dice_loss: 0.5524 depth_loss: 0.0241 loss: 5.5479 rmse: 0.1539\n",
      "Train Epoch: 4 Iteration: 2005 - LR: [0.0004965345738373488] dice_loss: 0.5484 depth_loss: 0.0238 loss: 5.5083 rmse: 0.1532\n",
      "Train Epoch: 4 Iteration: 2010 - LR: [0.000496376377575881] dice_loss: 0.5450 depth_loss: 0.0238 loss: 5.4740 rmse: 0.1530\n",
      "Train Epoch: 4 Iteration: 2015 - LR: [0.0004962146769720348] dice_loss: 0.5418 depth_loss: 0.0236 loss: 5.4418 rmse: 0.1524\n",
      "Train Epoch: 4 Iteration: 2020 - LR: [0.0004960494743257731] dice_loss: 0.5504 depth_loss: 0.0239 loss: 5.5278 rmse: 0.1537\n",
      "Train Epoch: 4 Iteration: 2025 - LR: [0.0004958807719868709] dice_loss: 0.5587 depth_loss: 0.0245 loss: 5.6114 rmse: 0.1554\n",
      "Train Epoch: 4 Iteration: 2030 - LR: [0.000495708572354881] dice_loss: 0.5643 depth_loss: 0.0256 loss: 5.6690 rmse: 0.1584\n",
      "Train Epoch: 4 Iteration: 2035 - LR: [0.0004955328778791004] dice_loss: 0.5625 depth_loss: 0.0264 loss: 5.6510 rmse: 0.1609\n",
      "Train Epoch: 4 Iteration: 2040 - LR: [0.0004953536910585352] dice_loss: 0.5714 depth_loss: 0.0272 loss: 5.7413 rmse: 0.1631\n",
      "Train Epoch: 4 Iteration: 2045 - LR: [0.0004951710144418654] dice_loss: 0.5758 depth_loss: 0.0275 loss: 5.7859 rmse: 0.1641\n",
      "Train Epoch: 4 Iteration: 2050 - LR: [0.0004949848506274083] dice_loss: 0.5861 depth_loss: 0.0275 loss: 5.8886 rmse: 0.1644\n",
      "Train Epoch: 4 Iteration: 2055 - LR: [0.0004947952022630816] dice_loss: 0.5875 depth_loss: 0.0275 loss: 5.9029 rmse: 0.1642\n",
      "Train Epoch: 4 Iteration: 2060 - LR: [0.0004946020720463661] dice_loss: 0.5867 depth_loss: 0.0271 loss: 5.8940 rmse: 0.1631\n",
      "Train Epoch: 4 Iteration: 2065 - LR: [0.0004944054627242666] dice_loss: 0.5895 depth_loss: 0.0270 loss: 5.9220 rmse: 0.1631\n",
      "Train Epoch: 4 Iteration: 2070 - LR: [0.0004942053770932737] dice_loss: 0.5901 depth_loss: 0.0266 loss: 5.9278 rmse: 0.1619\n",
      "Train Epoch: 4 Iteration: 2075 - LR: [0.0004940018179993232] dice_loss: 0.5915 depth_loss: 0.0266 loss: 5.9415 rmse: 0.1619\n",
      "Train Epoch: 4 Iteration: 2080 - LR: [0.0004937947883377564] dice_loss: 0.5882 depth_loss: 0.0263 loss: 5.9087 rmse: 0.1610\n",
      "Train Epoch: 4 Iteration: 2085 - LR: [0.0004935842910532782] dice_loss: 0.5867 depth_loss: 0.0262 loss: 5.8937 rmse: 0.1609\n",
      "Train Epoch: 4 Iteration: 2090 - LR: [0.0004933703291399158] dice_loss: 0.5875 depth_loss: 0.0263 loss: 5.9017 rmse: 0.1613\n",
      "Train Epoch: 4 Iteration: 2095 - LR: [0.0004931529056409756] dice_loss: 0.5834 depth_loss: 0.0261 loss: 5.8598 rmse: 0.1606\n",
      "Train Epoch: 4 Iteration: 2100 - LR: [0.0004929320236490005] dice_loss: 0.5807 depth_loss: 0.0259 loss: 5.8328 rmse: 0.1601\n",
      "Train Epoch: 4 Iteration: 2105 - LR: [0.0004927076863057252] dice_loss: 0.5832 depth_loss: 0.0258 loss: 5.8574 rmse: 0.1599\n",
      "Train Epoch: 4 Iteration: 2110 - LR: [0.0004924798968020324] dice_loss: 0.5755 depth_loss: 0.0255 loss: 5.7806 rmse: 0.1590\n",
      "Train Epoch: 4 Iteration: 2115 - LR: [0.0004922486583779064] dice_loss: 0.5772 depth_loss: 0.0250 loss: 5.7966 rmse: 0.1573\n",
      "Train Epoch: 4 Iteration: 2120 - LR: [0.0004920139743223878] dice_loss: 0.5688 depth_loss: 0.0248 loss: 5.7128 rmse: 0.1565\n",
      "Train Epoch: 4 Iteration: 2125 - LR: [0.0004917758479735265] dice_loss: 0.5677 depth_loss: 0.0249 loss: 5.7024 rmse: 0.1570\n",
      "Train Epoch: 4 Iteration: 2130 - LR: [0.0004915342827183341] dice_loss: 0.5635 depth_loss: 0.0249 loss: 5.6599 rmse: 0.1569\n",
      "Train Epoch: 4 Iteration: 2135 - LR: [0.0004912892819927355] dice_loss: 0.5640 depth_loss: 0.0248 loss: 5.6645 rmse: 0.1567\n",
      "Train Epoch: 4 Iteration: 2140 - LR: [0.0004910408492815209] dice_loss: 0.5617 depth_loss: 0.0247 loss: 5.6420 rmse: 0.1564\n",
      "Train Epoch: 4 Iteration: 2145 - LR: [0.0004907889881182951] dice_loss: 0.5547 depth_loss: 0.0247 loss: 5.5717 rmse: 0.1564\n",
      "Train Epoch: 4 Iteration: 2150 - LR: [0.0004905337020854282] dice_loss: 0.5581 depth_loss: 0.0247 loss: 5.6060 rmse: 0.1567\n",
      "Train Epoch: 4 Iteration: 2155 - LR: [0.0004902749948140041] dice_loss: 0.5595 depth_loss: 0.0246 loss: 5.6193 rmse: 0.1563\n",
      "Train Epoch: 4 Iteration: 2160 - LR: [0.0004900128699837689] dice_loss: 0.5553 depth_loss: 0.0245 loss: 5.5770 rmse: 0.1560\n",
      "Train Epoch: 4 Iteration: 2165 - LR: [0.0004897473313230788] dice_loss: 0.5570 depth_loss: 0.0240 loss: 5.5940 rmse: 0.1544\n",
      "Train Epoch: 4 Iteration: 2170 - LR: [0.0004894783826088468] dice_loss: 0.5556 depth_loss: 0.0241 loss: 5.5797 rmse: 0.1546\n",
      "Train Epoch: 4 Iteration: 2175 - LR: [0.0004892060276664893] dice_loss: 0.5594 depth_loss: 0.0245 loss: 5.6190 rmse: 0.1559\n",
      "Train Epoch: 4 Iteration: 2180 - LR: [0.0004889302703698712] dice_loss: 0.5591 depth_loss: 0.0242 loss: 5.6149 rmse: 0.1547\n",
      "Train Epoch: 4 Iteration: 2185 - LR: [0.0004886511146412511] dice_loss: 0.5578 depth_loss: 0.0246 loss: 5.6023 rmse: 0.1560\n",
      "Train Epoch: 4 Iteration: 2190 - LR: [0.0004883685644512262] dice_loss: 0.5542 depth_loss: 0.0253 loss: 5.5672 rmse: 0.1578\n",
      "Train Epoch: 4 Iteration: 2195 - LR: [0.000488082623818674] dice_loss: 0.5512 depth_loss: 0.0262 loss: 5.5380 rmse: 0.1607\n",
      "Train Epoch: 4 Iteration: 2200 - LR: [0.0004877932968106972] dice_loss: 0.5445 depth_loss: 0.0270 loss: 5.4721 rmse: 0.1629\n",
      "Train Epoch: 4 Iteration: 2205 - LR: [0.00048750058754256444] dice_loss: 0.5413 depth_loss: 0.0271 loss: 5.4398 rmse: 0.1635\n",
      "Train Epoch: 4 Iteration: 2210 - LR: [0.0004872045001776523] dice_loss: 0.5434 depth_loss: 0.0273 loss: 5.4610 rmse: 0.1639\n",
      "Train Epoch: 4 Iteration: 2215 - LR: [0.00048690503892738626] dice_loss: 0.5444 depth_loss: 0.0272 loss: 5.4715 rmse: 0.1638\n",
      "Train Epoch: 4 Iteration: 2220 - LR: [0.00048660220805118033] dice_loss: 0.5450 depth_loss: 0.0274 loss: 5.4777 rmse: 0.1643\n",
      "Train Epoch: 4 Iteration: 2225 - LR: [0.0004862960118563767] dice_loss: 0.5445 depth_loss: 0.0267 loss: 5.4716 rmse: 0.1621\n",
      "Train Epoch: 4 Iteration: 2230 - LR: [0.00048598645469818453] dice_loss: 0.5476 depth_loss: 0.0262 loss: 5.5025 rmse: 0.1606\n",
      "Train Epoch: 4 Iteration: 2235 - LR: [0.000485673540979618] dice_loss: 0.5477 depth_loss: 0.0265 loss: 5.5039 rmse: 0.1617\n",
      "Train Epoch: 4 Iteration: 2240 - LR: [0.00048535727515143355] dice_loss: 0.5498 depth_loss: 0.0262 loss: 5.5242 rmse: 0.1607\n",
      "Train Epoch: 4 Iteration: 2245 - LR: [0.00048503766171206666] dice_loss: 0.5552 depth_loss: 0.0261 loss: 5.5778 rmse: 0.1604\n",
      "Train Epoch: 4 Iteration: 2250 - LR: [0.00048471470520756793] dice_loss: 0.5585 depth_loss: 0.0265 loss: 5.6111 rmse: 0.1617\n",
      "Train Epoch: 4 Iteration: 2255 - LR: [0.0004843884102315383] dice_loss: 0.5653 depth_loss: 0.0263 loss: 5.6795 rmse: 0.1612\n",
      "Train Epoch: 4 Iteration: 2260 - LR: [0.0004840587814250636] dice_loss: 0.5613 depth_loss: 0.0259 loss: 5.6386 rmse: 0.1599\n",
      "Train Epoch: 4 Iteration: 2265 - LR: [0.00048372582347664913] dice_loss: 0.5560 depth_loss: 0.0254 loss: 5.5856 rmse: 0.1583\n",
      "Train Epoch: 4 Iteration: 2270 - LR: [0.000483389541122152] dice_loss: 0.5529 depth_loss: 0.0251 loss: 5.5545 rmse: 0.1574\n",
      "Train Epoch: 4 Iteration: 2275 - LR: [0.00048304993914471473] dice_loss: 0.5527 depth_loss: 0.0259 loss: 5.5528 rmse: 0.1596\n",
      "Train Epoch: 4 Iteration: 2280 - LR: [0.0004827070223746966] dice_loss: 0.5595 depth_loss: 0.0264 loss: 5.6210 rmse: 0.1613\n",
      "Train Epoch: 4 Iteration: 2285 - LR: [0.00048236079568960517] dice_loss: 0.5592 depth_loss: 0.0269 loss: 5.6190 rmse: 0.1625\n",
      "Train Epoch: 4 Iteration: 2290 - LR: [0.0004820112640140269] dice_loss: 0.5592 depth_loss: 0.0264 loss: 5.6181 rmse: 0.1611\n",
      "Train Epoch: 4 Iteration: 2295 - LR: [0.00048165843231955704] dice_loss: 0.5593 depth_loss: 0.0265 loss: 5.6196 rmse: 0.1614\n",
      "Train Epoch: 4 Iteration: 2300 - LR: [0.00048130230562472905] dice_loss: 0.5518 depth_loss: 0.0260 loss: 5.5439 rmse: 0.1599\n",
      "Train Epoch: 4 Iteration: 2305 - LR: [0.0004809428889949429] dice_loss: 0.5507 depth_loss: 0.0257 loss: 5.5328 rmse: 0.1592\n",
      "Train Epoch: 4 Iteration: 2310 - LR: [0.0004805801875423936] dice_loss: 0.5534 depth_loss: 0.0254 loss: 5.5593 rmse: 0.1583\n",
      "Train Epoch: 4 Iteration: 2315 - LR: [0.00048021420642599784] dice_loss: 0.5572 depth_loss: 0.0256 loss: 5.5979 rmse: 0.1591\n",
      "Train Epoch: 4 Iteration: 2320 - LR: [0.00047984495085132107] dice_loss: 0.5571 depth_loss: 0.0251 loss: 5.5964 rmse: 0.1575\n",
      "Train Epoch: 4 Iteration: 2325 - LR: [0.00047947242607050304] dice_loss: 0.5614 depth_loss: 0.0249 loss: 5.6391 rmse: 0.1567\n",
      "Train Epoch: 4 Iteration: 2330 - LR: [0.00047909663738218374] dice_loss: 0.5586 depth_loss: 0.0247 loss: 5.6108 rmse: 0.1562\n",
      "Train Epoch: 4 Iteration: 2335 - LR: [0.00047871759013142726] dice_loss: 0.5475 depth_loss: 0.0244 loss: 5.4989 rmse: 0.1554\n",
      "Train Epoch: 4 Iteration: 2340 - LR: [0.0004783352897096465] dice_loss: 0.5435 depth_loss: 0.0238 loss: 5.4593 rmse: 0.1534\n",
      "Train Epoch: 4 Iteration: 2345 - LR: [0.00047794974155452577] dice_loss: 0.5369 depth_loss: 0.0235 loss: 5.3923 rmse: 0.1522\n",
      "Train Epoch: 4 Iteration: 2350 - LR: [0.0004775609511499441] dice_loss: 0.5401 depth_loss: 0.0229 loss: 5.4243 rmse: 0.1501\n",
      "Train Epoch: 4 Iteration: 2355 - LR: [0.00047716892402589685] dice_loss: 0.5403 depth_loss: 0.0224 loss: 5.4254 rmse: 0.1484\n",
      "Train Epoch: 4 Iteration: 2360 - LR: [0.00047677366575841703] dice_loss: 0.5472 depth_loss: 0.0225 loss: 5.4944 rmse: 0.1489\n",
      "Train Epoch: 4 Iteration: 2365 - LR: [0.0004763751819694963] dice_loss: 0.5551 depth_loss: 0.0226 loss: 5.5734 rmse: 0.1491\n",
      "Train Epoch: 4 Iteration: 2370 - LR: [0.0004759734783270046] dice_loss: 0.5586 depth_loss: 0.0228 loss: 5.6091 rmse: 0.1498\n",
      "Train Epoch: 4 Iteration: 2375 - LR: [0.00047556856054461013] dice_loss: 0.5624 depth_loss: 0.0226 loss: 5.6468 rmse: 0.1492\n",
      "Train Epoch: 4 Iteration: 2380 - LR: [0.0004751604343816972] dice_loss: 0.5647 depth_loss: 0.0222 loss: 5.6691 rmse: 0.1479\n",
      "Validation Results - Epoch: 4  dice_loss: 0.5738 depth_loss: 0.0243 loss: 5.7619 rmse: 0.1551\n",
      "Model saved with loss 5.7619 at epoch 4\n",
      "Train Epoch: 5 Iteration: 2385 - LR: [0.0004747491056432852] dice_loss: 0.5636 depth_loss: 0.0222 loss: 5.6579 rmse: 0.1480\n",
      "Train Epoch: 5 Iteration: 2390 - LR: [0.0004743345801799455] dice_loss: 0.5642 depth_loss: 0.0221 loss: 5.6642 rmse: 0.1478\n",
      "Train Epoch: 5 Iteration: 2395 - LR: [0.00047391686388771826] dice_loss: 0.5637 depth_loss: 0.0220 loss: 5.6593 rmse: 0.1473\n",
      "Train Epoch: 5 Iteration: 2400 - LR: [0.00047349596270802885] dice_loss: 0.5561 depth_loss: 0.0220 loss: 5.5831 rmse: 0.1475\n",
      "Train Epoch: 5 Iteration: 2405 - LR: [0.00047307188262760315] dice_loss: 0.5479 depth_loss: 0.0219 loss: 5.5006 rmse: 0.1473\n",
      "Train Epoch: 5 Iteration: 2410 - LR: [0.00047264462967838234] dice_loss: 0.5484 depth_loss: 0.0218 loss: 5.5063 rmse: 0.1470\n",
      "Train Epoch: 5 Iteration: 2415 - LR: [0.00047221420993743727] dice_loss: 0.5458 depth_loss: 0.0223 loss: 5.4806 rmse: 0.1484\n",
      "Train Epoch: 5 Iteration: 2420 - LR: [0.0004717806295268819] dice_loss: 0.5463 depth_loss: 0.0223 loss: 5.4853 rmse: 0.1484\n",
      "Train Epoch: 5 Iteration: 2425 - LR: [0.00047134389461378625] dice_loss: 0.5421 depth_loss: 0.0224 loss: 5.4431 rmse: 0.1489\n",
      "Train Epoch: 5 Iteration: 2430 - LR: [0.00047090401141008866] dice_loss: 0.5370 depth_loss: 0.0225 loss: 5.3924 rmse: 0.1492\n",
      "Train Epoch: 5 Iteration: 2435 - LR: [0.0004704609861725076] dice_loss: 0.5368 depth_loss: 0.0225 loss: 5.3903 rmse: 0.1492\n",
      "Train Epoch: 5 Iteration: 2440 - LR: [0.0004700148252024523] dice_loss: 0.5394 depth_loss: 0.0228 loss: 5.4169 rmse: 0.1503\n",
      "Train Epoch: 5 Iteration: 2445 - LR: [0.0004695655348459337] dice_loss: 0.5403 depth_loss: 0.0226 loss: 5.4260 rmse: 0.1497\n",
      "Train Epoch: 5 Iteration: 2450 - LR: [0.00046911312149347355] dice_loss: 0.5479 depth_loss: 0.0224 loss: 5.5010 rmse: 0.1490\n",
      "Train Epoch: 5 Iteration: 2455 - LR: [0.0004686575915800139] dice_loss: 0.5535 depth_loss: 0.0227 loss: 5.5575 rmse: 0.1498\n",
      "Train Epoch: 5 Iteration: 2460 - LR: [0.0004681989515848256] dice_loss: 0.5537 depth_loss: 0.0228 loss: 5.5599 rmse: 0.1502\n",
      "Train Epoch: 5 Iteration: 2465 - LR: [0.00046773720803141585] dice_loss: 0.5588 depth_loss: 0.0225 loss: 5.6104 rmse: 0.1491\n",
      "Train Epoch: 5 Iteration: 2470 - LR: [0.0004672723674874357] dice_loss: 0.5634 depth_loss: 0.0221 loss: 5.6563 rmse: 0.1477\n",
      "Train Epoch: 5 Iteration: 2475 - LR: [0.0004668044365645866] dice_loss: 0.5625 depth_loss: 0.0219 loss: 5.6472 rmse: 0.1473\n",
      "Train Epoch: 5 Iteration: 2480 - LR: [0.0004663334219185261] dice_loss: 0.5644 depth_loss: 0.0218 loss: 5.6659 rmse: 0.1468\n",
      "Train Epoch: 5 Iteration: 2485 - LR: [0.00046585933024877337] dice_loss: 0.5626 depth_loss: 0.0213 loss: 5.6474 rmse: 0.1452\n",
      "Train Epoch: 5 Iteration: 2490 - LR: [0.0004653821682986142] dice_loss: 0.5626 depth_loss: 0.0213 loss: 5.6472 rmse: 0.1452\n",
      "Train Epoch: 5 Iteration: 2495 - LR: [0.0004649019428550044] dice_loss: 0.5595 depth_loss: 0.0208 loss: 5.6160 rmse: 0.1435\n",
      "Train Epoch: 5 Iteration: 2500 - LR: [0.00046441866074847386] dice_loss: 0.5612 depth_loss: 0.0209 loss: 5.6333 rmse: 0.1436\n",
      "Train Epoch: 5 Iteration: 2505 - LR: [0.0004639323288530292] dice_loss: 0.5642 depth_loss: 0.0209 loss: 5.6626 rmse: 0.1437\n",
      "Train Epoch: 5 Iteration: 2510 - LR: [0.0004634429540860559] dice_loss: 0.5652 depth_loss: 0.0212 loss: 5.6732 rmse: 0.1448\n",
      "Train Epoch: 5 Iteration: 2515 - LR: [0.00046295054340822017] dice_loss: 0.5610 depth_loss: 0.0213 loss: 5.6310 rmse: 0.1450\n",
      "Train Epoch: 5 Iteration: 2520 - LR: [0.0004624551038233695] dice_loss: 0.5580 depth_loss: 0.0215 loss: 5.6013 rmse: 0.1459\n",
      "Train Epoch: 5 Iteration: 2525 - LR: [0.00046195664237843345] dice_loss: 0.5628 depth_loss: 0.0219 loss: 5.6496 rmse: 0.1471\n",
      "Train Epoch: 5 Iteration: 2530 - LR: [0.00046145516616332344] dice_loss: 0.5651 depth_loss: 0.0215 loss: 5.6728 rmse: 0.1458\n",
      "Train Epoch: 5 Iteration: 2535 - LR: [0.0004609506823108315] dice_loss: 0.5712 depth_loss: 0.0217 loss: 5.7337 rmse: 0.1464\n",
      "Train Epoch: 5 Iteration: 2540 - LR: [0.00046044319799652905] dice_loss: 0.5770 depth_loss: 0.0216 loss: 5.7911 rmse: 0.1459\n",
      "Train Epoch: 5 Iteration: 2545 - LR: [0.0004599327204386652] dice_loss: 0.5813 depth_loss: 0.0218 loss: 5.8347 rmse: 0.1468\n",
      "Train Epoch: 5 Iteration: 2550 - LR: [0.0004594192568980632] dice_loss: 0.5770 depth_loss: 0.0219 loss: 5.7918 rmse: 0.1471\n",
      "Train Epoch: 5 Iteration: 2555 - LR: [0.0004589028146780182] dice_loss: 0.5756 depth_loss: 0.0218 loss: 5.7777 rmse: 0.1470\n",
      "Train Epoch: 5 Iteration: 2560 - LR: [0.00045838340112419254] dice_loss: 0.5719 depth_loss: 0.0220 loss: 5.7411 rmse: 0.1477\n",
      "Train Epoch: 5 Iteration: 2565 - LR: [0.0004578610236245117] dice_loss: 0.5687 depth_loss: 0.0220 loss: 5.7094 rmse: 0.1478\n",
      "Train Epoch: 5 Iteration: 2570 - LR: [0.00045733568960905916] dice_loss: 0.5651 depth_loss: 0.0222 loss: 5.6729 rmse: 0.1482\n",
      "Train Epoch: 5 Iteration: 2575 - LR: [0.0004568074065499705] dice_loss: 0.5623 depth_loss: 0.0221 loss: 5.6455 rmse: 0.1480\n",
      "Train Epoch: 5 Iteration: 2580 - LR: [0.0004562761819613275] dice_loss: 0.5582 depth_loss: 0.0218 loss: 5.6035 rmse: 0.1468\n",
      "Train Epoch: 5 Iteration: 2585 - LR: [0.000455742023399051] dice_loss: 0.5558 depth_loss: 0.0220 loss: 5.5801 rmse: 0.1476\n",
      "Train Epoch: 5 Iteration: 2590 - LR: [0.0004552049384607933] dice_loss: 0.5493 depth_loss: 0.0219 loss: 5.5146 rmse: 0.1472\n",
      "Train Epoch: 5 Iteration: 2595 - LR: [0.00045466493478583047] dice_loss: 0.5429 depth_loss: 0.0215 loss: 5.4500 rmse: 0.1459\n",
      "Train Epoch: 5 Iteration: 2600 - LR: [0.00045412202005495347] dice_loss: 0.5391 depth_loss: 0.0213 loss: 5.4125 rmse: 0.1453\n",
      "Train Epoch: 5 Iteration: 2605 - LR: [0.0004535762019903588] dice_loss: 0.5348 depth_loss: 0.0212 loss: 5.3690 rmse: 0.1451\n",
      "Train Epoch: 5 Iteration: 2610 - LR: [0.00045302748835553895] dice_loss: 0.5322 depth_loss: 0.0210 loss: 5.3428 rmse: 0.1444\n",
      "Train Epoch: 5 Iteration: 2615 - LR: [0.0004524758869551717] dice_loss: 0.5405 depth_loss: 0.0217 loss: 5.4272 rmse: 0.1465\n",
      "Train Epoch: 5 Iteration: 2620 - LR: [0.00045192140563500924] dice_loss: 0.5494 depth_loss: 0.0222 loss: 5.5165 rmse: 0.1482\n",
      "Train Epoch: 5 Iteration: 2625 - LR: [0.0004513640522817666] dice_loss: 0.5555 depth_loss: 0.0237 loss: 5.5784 rmse: 0.1525\n",
      "Train Epoch: 5 Iteration: 2630 - LR: [0.00045080383482300944] dice_loss: 0.5541 depth_loss: 0.0245 loss: 5.5660 rmse: 0.1549\n",
      "Train Epoch: 5 Iteration: 2635 - LR: [0.00045024076122704117] dice_loss: 0.5634 depth_loss: 0.0253 loss: 5.6596 rmse: 0.1573\n",
      "Train Epoch: 5 Iteration: 2640 - LR: [0.00044967483950278993] dice_loss: 0.5683 depth_loss: 0.0254 loss: 5.7085 rmse: 0.1577\n",
      "Train Epoch: 5 Iteration: 2645 - LR: [0.0004491060776996943] dice_loss: 0.5796 depth_loss: 0.0256 loss: 5.8214 rmse: 0.1584\n",
      "Train Epoch: 5 Iteration: 2650 - LR: [0.0004485344839075889] dice_loss: 0.5815 depth_loss: 0.0257 loss: 5.8410 rmse: 0.1589\n",
      "Train Epoch: 5 Iteration: 2655 - LR: [0.00044796006625658975] dice_loss: 0.5809 depth_loss: 0.0254 loss: 5.8344 rmse: 0.1581\n",
      "Train Epoch: 5 Iteration: 2660 - LR: [0.0004473828329169779] dice_loss: 0.5840 depth_loss: 0.0258 loss: 5.8653 rmse: 0.1592\n",
      "Train Epoch: 5 Iteration: 2665 - LR: [0.00044680279209908375] dice_loss: 0.5845 depth_loss: 0.0254 loss: 5.8706 rmse: 0.1582\n",
      "Train Epoch: 5 Iteration: 2670 - LR: [0.00044621995205317027] dice_loss: 0.5855 depth_loss: 0.0256 loss: 5.8803 rmse: 0.1588\n",
      "Train Epoch: 5 Iteration: 2675 - LR: [0.00044563432106931516] dice_loss: 0.5826 depth_loss: 0.0256 loss: 5.8514 rmse: 0.1587\n",
      "Train Epoch: 5 Iteration: 2680 - LR: [0.0004450459074772937] dice_loss: 0.5815 depth_loss: 0.0255 loss: 5.8409 rmse: 0.1586\n",
      "Train Epoch: 5 Iteration: 2685 - LR: [0.0004444547196464594] dice_loss: 0.5821 depth_loss: 0.0256 loss: 5.8471 rmse: 0.1590\n",
      "Train Epoch: 5 Iteration: 2690 - LR: [0.00044386076598562586] dice_loss: 0.5782 depth_loss: 0.0254 loss: 5.8074 rmse: 0.1585\n",
      "Train Epoch: 5 Iteration: 2695 - LR: [0.0004432640549429463] dice_loss: 0.5753 depth_loss: 0.0256 loss: 5.7786 rmse: 0.1589\n",
      "Train Epoch: 5 Iteration: 2700 - LR: [0.00044266459500579404] dice_loss: 0.5780 depth_loss: 0.0255 loss: 5.8051 rmse: 0.1589\n",
      "Train Epoch: 5 Iteration: 2705 - LR: [0.00044206239470064154] dice_loss: 0.5708 depth_loss: 0.0251 loss: 5.7332 rmse: 0.1575\n",
      "Train Epoch: 5 Iteration: 2710 - LR: [0.0004414574625929391] dice_loss: 0.5723 depth_loss: 0.0245 loss: 5.7475 rmse: 0.1555\n",
      "Train Epoch: 5 Iteration: 2715 - LR: [0.00044084980728699294] dice_loss: 0.5637 depth_loss: 0.0241 loss: 5.6607 rmse: 0.1544\n",
      "Train Epoch: 5 Iteration: 2720 - LR: [0.0004402394374258432] dice_loss: 0.5622 depth_loss: 0.0238 loss: 5.6454 rmse: 0.1534\n",
      "Train Epoch: 5 Iteration: 2725 - LR: [0.0004396263616911405] dice_loss: 0.5578 depth_loss: 0.0235 loss: 5.6013 rmse: 0.1525\n",
      "Train Epoch: 5 Iteration: 2730 - LR: [0.00043901058880302276] dice_loss: 0.5581 depth_loss: 0.0233 loss: 5.6042 rmse: 0.1518\n",
      "Train Epoch: 5 Iteration: 2735 - LR: [0.0004383921275199912] dice_loss: 0.5564 depth_loss: 0.0233 loss: 5.5878 rmse: 0.1520\n",
      "Train Epoch: 5 Iteration: 2740 - LR: [0.00043777098663878546] dice_loss: 0.5498 depth_loss: 0.0234 loss: 5.5212 rmse: 0.1523\n",
      "Train Epoch: 5 Iteration: 2745 - LR: [0.000437147174994259] dice_loss: 0.5528 depth_loss: 0.0231 loss: 5.5513 rmse: 0.1513\n",
      "Train Epoch: 5 Iteration: 2750 - LR: [0.0004365207014592529] dice_loss: 0.5526 depth_loss: 0.0228 loss: 5.5491 rmse: 0.1504\n",
      "Train Epoch: 5 Iteration: 2755 - LR: [0.00043589157494446985] dice_loss: 0.5477 depth_loss: 0.0225 loss: 5.4992 rmse: 0.1495\n",
      "Train Epoch: 5 Iteration: 2760 - LR: [0.00043525980439834753] dice_loss: 0.5497 depth_loss: 0.0224 loss: 5.5189 rmse: 0.1488\n",
      "Train Epoch: 5 Iteration: 2765 - LR: [0.0004346253988069313] dice_loss: 0.5484 depth_loss: 0.0222 loss: 5.5062 rmse: 0.1484\n",
      "Train Epoch: 5 Iteration: 2770 - LR: [0.0004339883671937461] dice_loss: 0.5518 depth_loss: 0.0226 loss: 5.5408 rmse: 0.1495\n",
      "Train Epoch: 5 Iteration: 2775 - LR: [0.00043334871861966856] dice_loss: 0.5510 depth_loss: 0.0224 loss: 5.5320 rmse: 0.1489\n",
      "Train Epoch: 5 Iteration: 2780 - LR: [0.00043270646218279777] dice_loss: 0.5492 depth_loss: 0.0227 loss: 5.5143 rmse: 0.1500\n",
      "Train Epoch: 5 Iteration: 2785 - LR: [0.0004320616070183261] dice_loss: 0.5461 depth_loss: 0.0229 loss: 5.4838 rmse: 0.1506\n",
      "Train Epoch: 5 Iteration: 2790 - LR: [0.000431414162298409] dice_loss: 0.5435 depth_loss: 0.0238 loss: 5.4586 rmse: 0.1532\n",
      "Train Epoch: 5 Iteration: 2795 - LR: [0.00043076413723203475] dice_loss: 0.5374 depth_loss: 0.0245 loss: 5.3989 rmse: 0.1554\n",
      "Train Epoch: 5 Iteration: 2800 - LR: [0.00043011154106489347] dice_loss: 0.5344 depth_loss: 0.0246 loss: 5.3684 rmse: 0.1559\n",
      "Train Epoch: 5 Iteration: 2805 - LR: [0.0004294563830792455] dice_loss: 0.5364 depth_loss: 0.0244 loss: 5.3889 rmse: 0.1552\n",
      "Train Epoch: 5 Iteration: 2810 - LR: [0.0004287986725937896] dice_loss: 0.5367 depth_loss: 0.0248 loss: 5.3915 rmse: 0.1561\n",
      "Train Epoch: 5 Iteration: 2815 - LR: [0.00042813841896352994] dice_loss: 0.5364 depth_loss: 0.0247 loss: 5.3884 rmse: 0.1559\n",
      "Train Epoch: 5 Iteration: 2820 - LR: [0.0004274756315796436] dice_loss: 0.5358 depth_loss: 0.0241 loss: 5.3817 rmse: 0.1540\n",
      "Train Epoch: 5 Iteration: 2825 - LR: [0.0004268103198693467] dice_loss: 0.5395 depth_loss: 0.0236 loss: 5.4186 rmse: 0.1524\n",
      "Train Epoch: 5 Iteration: 2830 - LR: [0.00042614249329576016] dice_loss: 0.5397 depth_loss: 0.0235 loss: 5.4206 rmse: 0.1522\n",
      "Train Epoch: 5 Iteration: 2835 - LR: [0.00042547216135777543] dice_loss: 0.5426 depth_loss: 0.0230 loss: 5.4491 rmse: 0.1504\n",
      "Train Epoch: 5 Iteration: 2840 - LR: [0.0004247993335899193] dice_loss: 0.5486 depth_loss: 0.0230 loss: 5.5086 rmse: 0.1504\n",
      "Train Epoch: 5 Iteration: 2845 - LR: [0.00042412401956221817] dice_loss: 0.5523 depth_loss: 0.0235 loss: 5.5463 rmse: 0.1523\n",
      "Train Epoch: 5 Iteration: 2850 - LR: [0.0004234462288800618] dice_loss: 0.5589 depth_loss: 0.0233 loss: 5.6125 rmse: 0.1515\n",
      "Train Epoch: 5 Iteration: 2855 - LR: [0.00042276597118406716] dice_loss: 0.5545 depth_loss: 0.0229 loss: 5.5681 rmse: 0.1504\n",
      "Train Epoch: 5 Iteration: 2860 - LR: [0.0004220832561499408] dice_loss: 0.5496 depth_loss: 0.0224 loss: 5.5187 rmse: 0.1487\n",
      "Train Epoch: 5 Iteration: 2865 - LR: [0.0004213980934883415] dice_loss: 0.5459 depth_loss: 0.0222 loss: 5.4812 rmse: 0.1480\n",
      "Train Epoch: 5 Iteration: 2870 - LR: [0.0004207104929447422] dice_loss: 0.5456 depth_loss: 0.0228 loss: 5.4792 rmse: 0.1499\n",
      "Train Epoch: 5 Iteration: 2875 - LR: [0.000420020464299291] dice_loss: 0.5524 depth_loss: 0.0233 loss: 5.5471 rmse: 0.1514\n",
      "Train Epoch: 5 Iteration: 2880 - LR: [0.0004193280173666726] dice_loss: 0.5518 depth_loss: 0.0237 loss: 5.5414 rmse: 0.1526\n",
      "Train Epoch: 5 Iteration: 2885 - LR: [0.0004186331619959684] dice_loss: 0.5519 depth_loss: 0.0234 loss: 5.5422 rmse: 0.1518\n",
      "Train Epoch: 5 Iteration: 2890 - LR: [0.00041793590807051627] dice_loss: 0.5521 depth_loss: 0.0236 loss: 5.5444 rmse: 0.1524\n",
      "Train Epoch: 5 Iteration: 2895 - LR: [0.0004172362655077703] dice_loss: 0.5444 depth_loss: 0.0233 loss: 5.4677 rmse: 0.1514\n",
      "Train Epoch: 5 Iteration: 2900 - LR: [0.00041653424425915943] dice_loss: 0.5432 depth_loss: 0.0230 loss: 5.4547 rmse: 0.1505\n",
      "Train Epoch: 5 Iteration: 2905 - LR: [0.00041582985430994624] dice_loss: 0.5456 depth_loss: 0.0227 loss: 5.4788 rmse: 0.1495\n",
      "Train Epoch: 5 Iteration: 2910 - LR: [0.0004151231056790845] dice_loss: 0.5495 depth_loss: 0.0231 loss: 5.5177 rmse: 0.1510\n",
      "Train Epoch: 5 Iteration: 2915 - LR: [0.00041441400841907706] dice_loss: 0.5493 depth_loss: 0.0230 loss: 5.5156 rmse: 0.1506\n",
      "Train Epoch: 5 Iteration: 2920 - LR: [0.0004137025726158328] dice_loss: 0.5533 depth_loss: 0.0227 loss: 5.5554 rmse: 0.1497\n",
      "Train Epoch: 5 Iteration: 2925 - LR: [0.0004129888083885228] dice_loss: 0.5501 depth_loss: 0.0224 loss: 5.5238 rmse: 0.1488\n",
      "Train Epoch: 5 Iteration: 2930 - LR: [0.00041227272588943703] dice_loss: 0.5390 depth_loss: 0.0222 loss: 5.4117 rmse: 0.1480\n",
      "Train Epoch: 5 Iteration: 2935 - LR: [0.00041155433530383923] dice_loss: 0.5353 depth_loss: 0.0217 loss: 5.3746 rmse: 0.1464\n",
      "Train Epoch: 5 Iteration: 2940 - LR: [0.00041083364684982277] dice_loss: 0.5286 depth_loss: 0.0215 loss: 5.3073 rmse: 0.1456\n",
      "Train Epoch: 5 Iteration: 2945 - LR: [0.0004101106707781646] dice_loss: 0.5316 depth_loss: 0.0210 loss: 5.3372 rmse: 0.1441\n",
      "Train Epoch: 5 Iteration: 2950 - LR: [0.0004093854173721803] dice_loss: 0.5321 depth_loss: 0.0207 loss: 5.3420 rmse: 0.1430\n",
      "Train Epoch: 5 Iteration: 2955 - LR: [0.0004086578969475768] dice_loss: 0.5390 depth_loss: 0.0211 loss: 5.4112 rmse: 0.1442\n",
      "Train Epoch: 5 Iteration: 2960 - LR: [0.00040792811985230647] dice_loss: 0.5470 depth_loss: 0.0212 loss: 5.4910 rmse: 0.1445\n",
      "Train Epoch: 5 Iteration: 2965 - LR: [0.0004071960964664195] dice_loss: 0.5506 depth_loss: 0.0213 loss: 5.5271 rmse: 0.1451\n",
      "Train Epoch: 5 Iteration: 2970 - LR: [0.0004064618372019166] dice_loss: 0.5545 depth_loss: 0.0210 loss: 5.5664 rmse: 0.1440\n",
      "Train Epoch: 5 Iteration: 2975 - LR: [0.0004057253525026005] dice_loss: 0.5568 depth_loss: 0.0205 loss: 5.5889 rmse: 0.1421\n",
      "Validation Results - Epoch: 5  dice_loss: 0.5671 depth_loss: 0.0224 loss: 5.6933 rmse: 0.1488\n",
      "Model saved with loss 5.6933 at epoch 5\n",
      "Train Epoch: 6 Iteration: 2980 - LR: [0.0004049866528439277] dice_loss: 0.5557 depth_loss: 0.0206 loss: 5.5773 rmse: 0.1424\n",
      "Train Epoch: 6 Iteration: 2985 - LR: [0.0004042457487328592] dice_loss: 0.5562 depth_loss: 0.0204 loss: 5.5827 rmse: 0.1419\n",
      "Train Epoch: 6 Iteration: 2990 - LR: [0.0004035026507077116] dice_loss: 0.5557 depth_loss: 0.0206 loss: 5.5777 rmse: 0.1426\n",
      "Train Epoch: 6 Iteration: 2995 - LR: [0.00040275736933800637] dice_loss: 0.5483 depth_loss: 0.0208 loss: 5.5042 rmse: 0.1432\n",
      "Train Epoch: 6 Iteration: 3000 - LR: [0.0004020099152243204] dice_loss: 0.5406 depth_loss: 0.0207 loss: 5.4263 rmse: 0.1428\n",
      "Train Epoch: 6 Iteration: 3005 - LR: [0.0004012602989981345] dice_loss: 0.5413 depth_loss: 0.0208 loss: 5.4338 rmse: 0.1433\n",
      "Train Epoch: 6 Iteration: 3010 - LR: [0.0004005085313216827] dice_loss: 0.5387 depth_loss: 0.0213 loss: 5.4086 rmse: 0.1449\n",
      "Train Epoch: 6 Iteration: 3015 - LR: [0.00039975462288780014] dice_loss: 0.5393 depth_loss: 0.0212 loss: 5.4139 rmse: 0.1447\n",
      "Train Epoch: 6 Iteration: 3020 - LR: [0.0003989985844197715] dice_loss: 0.5351 depth_loss: 0.0215 loss: 5.3724 rmse: 0.1457\n",
      "Train Epoch: 6 Iteration: 3025 - LR: [0.0003982404266711779] dice_loss: 0.5299 depth_loss: 0.0214 loss: 5.3200 rmse: 0.1453\n",
      "Train Epoch: 6 Iteration: 3030 - LR: [0.0003974801604257444] dice_loss: 0.5293 depth_loss: 0.0213 loss: 5.3139 rmse: 0.1451\n",
      "Train Epoch: 6 Iteration: 3035 - LR: [0.0003967177964971865] dice_loss: 0.5322 depth_loss: 0.0215 loss: 5.3432 rmse: 0.1457\n",
      "Train Epoch: 6 Iteration: 3040 - LR: [0.00039595334572905616] dice_loss: 0.5324 depth_loss: 0.0213 loss: 5.3454 rmse: 0.1451\n",
      "Train Epoch: 6 Iteration: 3045 - LR: [0.0003951868189945878] dice_loss: 0.5398 depth_loss: 0.0212 loss: 5.4193 rmse: 0.1447\n",
      "Train Epoch: 6 Iteration: 3050 - LR: [0.0003944182271965435] dice_loss: 0.5460 depth_loss: 0.0212 loss: 5.4811 rmse: 0.1450\n",
      "Train Epoch: 6 Iteration: 3055 - LR: [0.0003936475812670578] dice_loss: 0.5464 depth_loss: 0.0212 loss: 5.4851 rmse: 0.1447\n",
      "Train Epoch: 6 Iteration: 3060 - LR: [0.0003928748921674827] dice_loss: 0.5519 depth_loss: 0.0208 loss: 5.5394 rmse: 0.1435\n",
      "Train Epoch: 6 Iteration: 3065 - LR: [0.000392100170888231] dice_loss: 0.5568 depth_loss: 0.0205 loss: 5.5882 rmse: 0.1423\n",
      "Train Epoch: 6 Iteration: 3070 - LR: [0.00039132342844862083] dice_loss: 0.5561 depth_loss: 0.0203 loss: 5.5814 rmse: 0.1418\n",
      "Train Epoch: 6 Iteration: 3075 - LR: [0.0003905446758967181] dice_loss: 0.5583 depth_loss: 0.0203 loss: 5.6030 rmse: 0.1418\n",
      "Train Epoch: 6 Iteration: 3080 - LR: [0.00038976392430918] dice_loss: 0.5569 depth_loss: 0.0199 loss: 5.5887 rmse: 0.1404\n",
      "Train Epoch: 6 Iteration: 3085 - LR: [0.00038898118479109686] dice_loss: 0.5573 depth_loss: 0.0200 loss: 5.5933 rmse: 0.1407\n",
      "Train Epoch: 6 Iteration: 3090 - LR: [0.00038819646847583505] dice_loss: 0.5543 depth_loss: 0.0195 loss: 5.5622 rmse: 0.1390\n",
      "Train Epoch: 6 Iteration: 3095 - LR: [0.00038740978652487747] dice_loss: 0.5560 depth_loss: 0.0194 loss: 5.5793 rmse: 0.1387\n",
      "Train Epoch: 6 Iteration: 3100 - LR: [0.00038662115012766597] dice_loss: 0.5591 depth_loss: 0.0194 loss: 5.6105 rmse: 0.1388\n",
      "Train Epoch: 6 Iteration: 3105 - LR: [0.00038583057050144127] dice_loss: 0.5602 depth_loss: 0.0199 loss: 5.6219 rmse: 0.1403\n",
      "Train Epoch: 6 Iteration: 3110 - LR: [0.00038503805889108383] dice_loss: 0.5561 depth_loss: 0.0198 loss: 5.5805 rmse: 0.1402\n",
      "Train Epoch: 6 Iteration: 3115 - LR: [0.00038424362656895404] dice_loss: 0.5530 depth_loss: 0.0200 loss: 5.5501 rmse: 0.1408\n",
      "Train Epoch: 6 Iteration: 3120 - LR: [0.0003834472848347317] dice_loss: 0.5573 depth_loss: 0.0204 loss: 5.5931 rmse: 0.1420\n",
      "Train Epoch: 6 Iteration: 3125 - LR: [0.0003826490450152549] dice_loss: 0.5590 depth_loss: 0.0200 loss: 5.6104 rmse: 0.1406\n",
      "Train Epoch: 6 Iteration: 3130 - LR: [0.00038184891846435997] dice_loss: 0.5654 depth_loss: 0.0202 loss: 5.6741 rmse: 0.1411\n",
      "Train Epoch: 6 Iteration: 3135 - LR: [0.00038104691656271875] dice_loss: 0.5709 depth_loss: 0.0200 loss: 5.7289 rmse: 0.1406\n",
      "Train Epoch: 6 Iteration: 3140 - LR: [0.0003802430507176777] dice_loss: 0.5756 depth_loss: 0.0203 loss: 5.7763 rmse: 0.1414\n",
      "Train Epoch: 6 Iteration: 3145 - LR: [0.0003794373323630949] dice_loss: 0.5719 depth_loss: 0.0204 loss: 5.7399 rmse: 0.1421\n",
      "Train Epoch: 6 Iteration: 3150 - LR: [0.00037862977295917806] dice_loss: 0.5706 depth_loss: 0.0205 loss: 5.7266 rmse: 0.1424\n",
      "Train Epoch: 6 Iteration: 3155 - LR: [0.00037782038399232124] dice_loss: 0.5662 depth_loss: 0.0206 loss: 5.6829 rmse: 0.1427\n",
      "Train Epoch: 6 Iteration: 3160 - LR: [0.0003770091769749411] dice_loss: 0.5620 depth_loss: 0.0206 loss: 5.6409 rmse: 0.1428\n",
      "Train Epoch: 6 Iteration: 3165 - LR: [0.00037619616344531384] dice_loss: 0.5581 depth_loss: 0.0207 loss: 5.6021 rmse: 0.1431\n",
      "Train Epoch: 6 Iteration: 3170 - LR: [0.00037538135496741065] dice_loss: 0.5552 depth_loss: 0.0205 loss: 5.5726 rmse: 0.1426\n",
      "Train Epoch: 6 Iteration: 3175 - LR: [0.0003745647631307333] dice_loss: 0.5507 depth_loss: 0.0201 loss: 5.5275 rmse: 0.1410\n",
      "Train Epoch: 6 Iteration: 3180 - LR: [0.00037374639955014936] dice_loss: 0.5487 depth_loss: 0.0198 loss: 5.5066 rmse: 0.1402\n",
      "Train Epoch: 6 Iteration: 3185 - LR: [0.00037292627586572684] dice_loss: 0.5422 depth_loss: 0.0196 loss: 5.4416 rmse: 0.1394\n",
      "Train Epoch: 6 Iteration: 3190 - LR: [0.00037210440374256905] dice_loss: 0.5361 depth_loss: 0.0194 loss: 5.3801 rmse: 0.1386\n",
      "Train Epoch: 6 Iteration: 3195 - LR: [0.00037128079487064796] dice_loss: 0.5326 depth_loss: 0.0193 loss: 5.3455 rmse: 0.1384\n",
      "Train Epoch: 6 Iteration: 3200 - LR: [0.00037045546096463875] dice_loss: 0.5284 depth_loss: 0.0195 loss: 5.3036 rmse: 0.1389\n",
      "Train Epoch: 6 Iteration: 3205 - LR: [0.0003696284137637525] dice_loss: 0.5257 depth_loss: 0.0194 loss: 5.2767 rmse: 0.1387\n",
      "Train Epoch: 6 Iteration: 3210 - LR: [0.0003687996650315696] dice_loss: 0.5337 depth_loss: 0.0199 loss: 5.3571 rmse: 0.1403\n",
      "Train Epoch: 6 Iteration: 3215 - LR: [0.00036796922655587215] dice_loss: 0.5422 depth_loss: 0.0203 loss: 5.4419 rmse: 0.1418\n",
      "Train Epoch: 6 Iteration: 3220 - LR: [0.00036713711014847684] dice_loss: 0.5478 depth_loss: 0.0214 loss: 5.4993 rmse: 0.1451\n",
      "Train Epoch: 6 Iteration: 3225 - LR: [0.0003663033276450662] dice_loss: 0.5467 depth_loss: 0.0220 loss: 5.4886 rmse: 0.1471\n",
      "Train Epoch: 6 Iteration: 3230 - LR: [0.00036546789090502094] dice_loss: 0.5560 depth_loss: 0.0227 loss: 5.5832 rmse: 0.1494\n",
      "Train Epoch: 6 Iteration: 3235 - LR: [0.0003646308118112506] dice_loss: 0.5613 depth_loss: 0.0228 loss: 5.6363 rmse: 0.1497\n",
      "Train Epoch: 6 Iteration: 3240 - LR: [0.00036379210227002523] dice_loss: 0.5729 depth_loss: 0.0229 loss: 5.7515 rmse: 0.1500\n",
      "Train Epoch: 6 Iteration: 3245 - LR: [0.0003629517742108055] dice_loss: 0.5749 depth_loss: 0.0229 loss: 5.7724 rmse: 0.1503\n",
      "Train Epoch: 6 Iteration: 3250 - LR: [0.00036210983958607324] dice_loss: 0.5750 depth_loss: 0.0229 loss: 5.7730 rmse: 0.1502\n",
      "Train Epoch: 6 Iteration: 3255 - LR: [0.0003612663103711615] dice_loss: 0.5793 depth_loss: 0.0233 loss: 5.8167 rmse: 0.1517\n",
      "Train Epoch: 6 Iteration: 3260 - LR: [0.000360421198564084] dice_loss: 0.5803 depth_loss: 0.0230 loss: 5.8256 rmse: 0.1505\n",
      "Train Epoch: 6 Iteration: 3265 - LR: [0.0003595745161853646] dice_loss: 0.5816 depth_loss: 0.0231 loss: 5.8393 rmse: 0.1510\n",
      "Train Epoch: 6 Iteration: 3270 - LR: [0.00035872627527786655] dice_loss: 0.5786 depth_loss: 0.0229 loss: 5.8088 rmse: 0.1503\n",
      "Train Epoch: 6 Iteration: 3275 - LR: [0.00035787648790662073] dice_loss: 0.5777 depth_loss: 0.0228 loss: 5.7996 rmse: 0.1501\n",
      "Train Epoch: 6 Iteration: 3280 - LR: [0.00035702516615865435] dice_loss: 0.5786 depth_loss: 0.0229 loss: 5.8094 rmse: 0.1506\n",
      "Train Epoch: 6 Iteration: 3285 - LR: [0.00035617232214281896] dice_loss: 0.5744 depth_loss: 0.0229 loss: 5.7672 rmse: 0.1506\n",
      "Train Epoch: 6 Iteration: 3290 - LR: [0.0003553179679896184] dice_loss: 0.5714 depth_loss: 0.0231 loss: 5.7370 rmse: 0.1513\n",
      "Train Epoch: 6 Iteration: 3295 - LR: [0.00035446211585103574] dice_loss: 0.5741 depth_loss: 0.0232 loss: 5.7642 rmse: 0.1517\n",
      "Train Epoch: 6 Iteration: 3300 - LR: [0.0003536047779003611] dice_loss: 0.5669 depth_loss: 0.0230 loss: 5.6920 rmse: 0.1510\n",
      "Train Epoch: 6 Iteration: 3305 - LR: [0.000352745966332018] dice_loss: 0.5684 depth_loss: 0.0226 loss: 5.7070 rmse: 0.1496\n",
      "Train Epoch: 6 Iteration: 3310 - LR: [0.00035188569336139024] dice_loss: 0.5599 depth_loss: 0.0228 loss: 5.6214 rmse: 0.1501\n",
      "Train Epoch: 6 Iteration: 3315 - LR: [0.0003510239712246478] dice_loss: 0.5586 depth_loss: 0.0226 loss: 5.6082 rmse: 0.1494\n",
      "Train Epoch: 6 Iteration: 3320 - LR: [0.00035016081217857317] dice_loss: 0.5540 depth_loss: 0.0222 loss: 5.5621 rmse: 0.1481\n",
      "Train Epoch: 6 Iteration: 3325 - LR: [0.0003492962285003869] dice_loss: 0.5550 depth_loss: 0.0219 loss: 5.5721 rmse: 0.1473\n",
      "Train Epoch: 6 Iteration: 3330 - LR: [0.0003484302324875727] dice_loss: 0.5527 depth_loss: 0.0219 loss: 5.5489 rmse: 0.1473\n",
      "Train Epoch: 6 Iteration: 3335 - LR: [0.0003475628364577028] dice_loss: 0.5456 depth_loss: 0.0218 loss: 5.4780 rmse: 0.1471\n",
      "Train Epoch: 6 Iteration: 3340 - LR: [0.000346694052748263] dice_loss: 0.5483 depth_loss: 0.0217 loss: 5.5044 rmse: 0.1467\n",
      "Train Epoch: 6 Iteration: 3345 - LR: [0.00034582389371647646] dice_loss: 0.5480 depth_loss: 0.0215 loss: 5.5015 rmse: 0.1459\n",
      "Train Epoch: 6 Iteration: 3350 - LR: [0.0003449523717391287] dice_loss: 0.5431 depth_loss: 0.0212 loss: 5.4522 rmse: 0.1449\n",
      "Train Epoch: 6 Iteration: 3355 - LR: [0.00034407949921239105] dice_loss: 0.5449 depth_loss: 0.0210 loss: 5.4699 rmse: 0.1443\n",
      "Train Epoch: 6 Iteration: 3360 - LR: [0.0003432052885516444] dice_loss: 0.5435 depth_loss: 0.0208 loss: 5.4561 rmse: 0.1436\n",
      "Train Epoch: 6 Iteration: 3365 - LR: [0.00034232975219130305] dice_loss: 0.5468 depth_loss: 0.0211 loss: 5.4890 rmse: 0.1444\n",
      "Train Epoch: 6 Iteration: 3370 - LR: [0.0003414529025846371] dice_loss: 0.5455 depth_loss: 0.0209 loss: 5.4761 rmse: 0.1438\n",
      "Train Epoch: 6 Iteration: 3375 - LR: [0.0003405747522035961] dice_loss: 0.5436 depth_loss: 0.0209 loss: 5.4570 rmse: 0.1438\n",
      "Train Epoch: 6 Iteration: 3380 - LR: [0.00033969531353863087] dice_loss: 0.5399 depth_loss: 0.0210 loss: 5.4202 rmse: 0.1441\n",
      "Train Epoch: 6 Iteration: 3385 - LR: [0.0003388145990985167] dice_loss: 0.5369 depth_loss: 0.0218 loss: 5.3909 rmse: 0.1467\n",
      "Train Epoch: 6 Iteration: 3390 - LR: [0.0003379326214101747] dice_loss: 0.5310 depth_loss: 0.0225 loss: 5.3326 rmse: 0.1489\n",
      "Train Epoch: 6 Iteration: 3395 - LR: [0.00033704939301849407] dice_loss: 0.5281 depth_loss: 0.0226 loss: 5.3034 rmse: 0.1494\n",
      "Train Epoch: 6 Iteration: 3400 - LR: [0.0003361649264861532] dice_loss: 0.5302 depth_loss: 0.0224 loss: 5.3246 rmse: 0.1486\n",
      "Train Epoch: 6 Iteration: 3405 - LR: [0.0003352792343934417] dice_loss: 0.5313 depth_loss: 0.0226 loss: 5.3355 rmse: 0.1491\n",
      "Train Epoch: 6 Iteration: 3410 - LR: [0.00033439232933808066] dice_loss: 0.5312 depth_loss: 0.0225 loss: 5.3350 rmse: 0.1489\n",
      "Train Epoch: 6 Iteration: 3415 - LR: [0.00033350422393504416] dice_loss: 0.5311 depth_loss: 0.0220 loss: 5.3332 rmse: 0.1472\n",
      "Train Epoch: 6 Iteration: 3420 - LR: [0.0003326149308163794] dice_loss: 0.5356 depth_loss: 0.0216 loss: 5.3776 rmse: 0.1460\n",
      "Train Epoch: 6 Iteration: 3425 - LR: [0.00033172446263102717] dice_loss: 0.5364 depth_loss: 0.0216 loss: 5.3860 rmse: 0.1459\n",
      "Train Epoch: 6 Iteration: 3430 - LR: [0.00033083283204464177] dice_loss: 0.5398 depth_loss: 0.0211 loss: 5.4190 rmse: 0.1442\n",
      "Train Epoch: 6 Iteration: 3435 - LR: [0.00032994005173941145] dice_loss: 0.5451 depth_loss: 0.0212 loss: 5.4723 rmse: 0.1446\n",
      "Train Epoch: 6 Iteration: 3440 - LR: [0.000329046134413877] dice_loss: 0.5483 depth_loss: 0.0217 loss: 5.5050 rmse: 0.1464\n",
      "Train Epoch: 6 Iteration: 3445 - LR: [0.00032815109278275234] dice_loss: 0.5548 depth_loss: 0.0217 loss: 5.5694 rmse: 0.1464\n",
      "Train Epoch: 6 Iteration: 3450 - LR: [0.00032725493957674244] dice_loss: 0.5504 depth_loss: 0.0214 loss: 5.5249 rmse: 0.1452\n",
      "Train Epoch: 6 Iteration: 3455 - LR: [0.0003263576875423635] dice_loss: 0.5455 depth_loss: 0.0209 loss: 5.4760 rmse: 0.1436\n",
      "Train Epoch: 6 Iteration: 3460 - LR: [0.00032545934944176025] dice_loss: 0.5415 depth_loss: 0.0209 loss: 5.4360 rmse: 0.1435\n",
      "Train Epoch: 6 Iteration: 3465 - LR: [0.0003245599380525259] dice_loss: 0.5411 depth_loss: 0.0216 loss: 5.4329 rmse: 0.1457\n",
      "Train Epoch: 6 Iteration: 3470 - LR: [0.00032365946616751937] dice_loss: 0.5472 depth_loss: 0.0220 loss: 5.4943 rmse: 0.1470\n",
      "Train Epoch: 6 Iteration: 3475 - LR: [0.0003227579465946834] dice_loss: 0.5468 depth_loss: 0.0221 loss: 5.4898 rmse: 0.1474\n",
      "Train Epoch: 6 Iteration: 3480 - LR: [0.0003218553921568631] dice_loss: 0.5468 depth_loss: 0.0219 loss: 5.4901 rmse: 0.1467\n",
      "Train Epoch: 6 Iteration: 3485 - LR: [0.0003209518156916228] dice_loss: 0.5471 depth_loss: 0.0222 loss: 5.4935 rmse: 0.1480\n",
      "Train Epoch: 6 Iteration: 3490 - LR: [0.00032004723005106355] dice_loss: 0.5390 depth_loss: 0.0221 loss: 5.4119 rmse: 0.1477\n",
      "Train Epoch: 6 Iteration: 3495 - LR: [0.00031914164810164093] dice_loss: 0.5374 depth_loss: 0.0219 loss: 5.3958 rmse: 0.1469\n",
      "Train Epoch: 6 Iteration: 3500 - LR: [0.0003182350827239812] dice_loss: 0.5403 depth_loss: 0.0216 loss: 5.4246 rmse: 0.1460\n",
      "Train Epoch: 6 Iteration: 3505 - LR: [0.00031732754681269886] dice_loss: 0.5440 depth_loss: 0.0219 loss: 5.4622 rmse: 0.1471\n",
      "Train Epoch: 6 Iteration: 3510 - LR: [0.00031641905327621256] dice_loss: 0.5436 depth_loss: 0.0216 loss: 5.4577 rmse: 0.1461\n",
      "Train Epoch: 6 Iteration: 3515 - LR: [0.00031550961503656205] dice_loss: 0.5474 depth_loss: 0.0214 loss: 5.4953 rmse: 0.1453\n",
      "Train Epoch: 6 Iteration: 3520 - LR: [0.00031459924502922395] dice_loss: 0.5445 depth_loss: 0.0213 loss: 5.4662 rmse: 0.1452\n",
      "Train Epoch: 6 Iteration: 3525 - LR: [0.00031368795620292825] dice_loss: 0.5337 depth_loss: 0.0212 loss: 5.3579 rmse: 0.1447\n",
      "Train Epoch: 6 Iteration: 3530 - LR: [0.00031277576151947346] dice_loss: 0.5303 depth_loss: 0.0208 loss: 5.3235 rmse: 0.1432\n",
      "Train Epoch: 6 Iteration: 3535 - LR: [0.000311862673953543] dice_loss: 0.5238 depth_loss: 0.0205 loss: 5.2590 rmse: 0.1423\n",
      "Train Epoch: 6 Iteration: 3540 - LR: [0.00031094870649252] dice_loss: 0.5264 depth_loss: 0.0200 loss: 5.2845 rmse: 0.1406\n",
      "Train Epoch: 6 Iteration: 3545 - LR: [0.0003100338721363031] dice_loss: 0.5269 depth_loss: 0.0196 loss: 5.2884 rmse: 0.1391\n",
      "Train Epoch: 6 Iteration: 3550 - LR: [0.00030911818389712105] dice_loss: 0.5338 depth_loss: 0.0198 loss: 5.3582 rmse: 0.1398\n",
      "Train Epoch: 6 Iteration: 3555 - LR: [0.0003082016547993482] dice_loss: 0.5420 depth_loss: 0.0200 loss: 5.4402 rmse: 0.1402\n",
      "Train Epoch: 6 Iteration: 3560 - LR: [0.0003072842978793185] dice_loss: 0.5455 depth_loss: 0.0201 loss: 5.4753 rmse: 0.1408\n",
      "Train Epoch: 6 Iteration: 3565 - LR: [0.0003063661261851409] dice_loss: 0.5495 depth_loss: 0.0198 loss: 5.5149 rmse: 0.1396\n",
      "Train Epoch: 6 Iteration: 3570 - LR: [0.0003054471527765132] dice_loss: 0.5520 depth_loss: 0.0194 loss: 5.5394 rmse: 0.1382\n",
      "Validation Results - Epoch: 6  dice_loss: 0.5603 depth_loss: 0.0213 loss: 5.6241 rmse: 0.1451\n",
      "Model saved with loss 5.6241 at epoch 6\n",
      "Train Epoch: 7 Iteration: 3575 - LR: [0.00030452739072453646] dice_loss: 0.5509 depth_loss: 0.0193 loss: 5.5287 rmse: 0.1380\n",
      "Train Epoch: 7 Iteration: 3580 - LR: [0.00030360685311152906] dice_loss: 0.5515 depth_loss: 0.0192 loss: 5.5343 rmse: 0.1375\n",
      "Train Epoch: 7 Iteration: 3585 - LR: [0.0003026855530308407] dice_loss: 0.5513 depth_loss: 0.0189 loss: 5.5316 rmse: 0.1367\n",
      "Train Epoch: 7 Iteration: 3590 - LR: [0.000301763503586666] dice_loss: 0.5439 depth_loss: 0.0190 loss: 5.4578 rmse: 0.1370\n",
      "Train Epoch: 7 Iteration: 3595 - LR: [0.00030084071789385837] dice_loss: 0.5360 depth_loss: 0.0189 loss: 5.3789 rmse: 0.1364\n",
      "Train Epoch: 7 Iteration: 3600 - LR: [0.0002999172090777432] dice_loss: 0.5369 depth_loss: 0.0189 loss: 5.3878 rmse: 0.1365\n",
      "Train Epoch: 7 Iteration: 3605 - LR: [0.0002989929902739314] dice_loss: 0.5337 depth_loss: 0.0192 loss: 5.3559 rmse: 0.1375\n",
      "Train Epoch: 7 Iteration: 3610 - LR: [0.00029806807462813236] dice_loss: 0.5340 depth_loss: 0.0190 loss: 5.3593 rmse: 0.1369\n",
      "Train Epoch: 7 Iteration: 3615 - LR: [0.000297142475295967] dice_loss: 0.5298 depth_loss: 0.0192 loss: 5.3173 rmse: 0.1377\n",
      "Train Epoch: 7 Iteration: 3620 - LR: [0.0002962162054427809] dice_loss: 0.5247 depth_loss: 0.0193 loss: 5.2663 rmse: 0.1381\n",
      "Train Epoch: 7 Iteration: 3625 - LR: [0.0002952892782434568] dice_loss: 0.5243 depth_loss: 0.0192 loss: 5.2620 rmse: 0.1377\n",
      "Train Epoch: 7 Iteration: 3630 - LR: [0.0002943617068822271] dice_loss: 0.5279 depth_loss: 0.0196 loss: 5.2981 rmse: 0.1389\n",
      "Train Epoch: 7 Iteration: 3635 - LR: [0.00029343350455248677] dice_loss: 0.5282 depth_loss: 0.0194 loss: 5.3010 rmse: 0.1383\n",
      "Train Epoch: 7 Iteration: 3640 - LR: [0.0002925046844566051] dice_loss: 0.5361 depth_loss: 0.0195 loss: 5.3804 rmse: 0.1389\n",
      "Train Epoch: 7 Iteration: 3645 - LR: [0.0002915752598057385] dice_loss: 0.5418 depth_loss: 0.0198 loss: 5.4376 rmse: 0.1397\n",
      "Train Epoch: 7 Iteration: 3650 - LR: [0.0002906452438196423] dice_loss: 0.5422 depth_loss: 0.0197 loss: 5.4416 rmse: 0.1395\n",
      "Train Epoch: 7 Iteration: 3655 - LR: [0.0002897146497264825] dice_loss: 0.5480 depth_loss: 0.0195 loss: 5.4992 rmse: 0.1387\n",
      "Train Epoch: 7 Iteration: 3660 - LR: [0.00028878349076264805] dice_loss: 0.5532 depth_loss: 0.0192 loss: 5.5508 rmse: 0.1376\n",
      "Train Epoch: 7 Iteration: 3665 - LR: [0.0002878517801725625] dice_loss: 0.5526 depth_loss: 0.0190 loss: 5.5451 rmse: 0.1370\n",
      "Train Epoch: 7 Iteration: 3670 - LR: [0.0002869195312084951] dice_loss: 0.5550 depth_loss: 0.0190 loss: 5.5694 rmse: 0.1369\n",
      "Train Epoch: 7 Iteration: 3675 - LR: [0.0002859867571303733] dice_loss: 0.5535 depth_loss: 0.0187 loss: 5.5534 rmse: 0.1359\n",
      "Train Epoch: 7 Iteration: 3680 - LR: [0.00028505347120559295] dice_loss: 0.5540 depth_loss: 0.0187 loss: 5.5592 rmse: 0.1361\n",
      "Train Epoch: 7 Iteration: 3685 - LR: [0.0002841196867088307] dice_loss: 0.5512 depth_loss: 0.0183 loss: 5.5307 rmse: 0.1346\n",
      "Train Epoch: 7 Iteration: 3690 - LR: [0.0002831854169218542] dice_loss: 0.5527 depth_loss: 0.0182 loss: 5.5452 rmse: 0.1341\n",
      "Train Epoch: 7 Iteration: 3695 - LR: [0.0002822506751333343] dice_loss: 0.5556 depth_loss: 0.0183 loss: 5.5741 rmse: 0.1344\n",
      "Train Epoch: 7 Iteration: 3700 - LR: [0.0002813154746386547] dice_loss: 0.5571 depth_loss: 0.0186 loss: 5.5892 rmse: 0.1357\n",
      "Train Epoch: 7 Iteration: 3705 - LR: [0.00028037982873972407] dice_loss: 0.5521 depth_loss: 0.0185 loss: 5.5394 rmse: 0.1352\n",
      "Train Epoch: 7 Iteration: 3710 - LR: [0.000279443750744786] dice_loss: 0.5489 depth_loss: 0.0186 loss: 5.5075 rmse: 0.1356\n",
      "Train Epoch: 7 Iteration: 3715 - LR: [0.0002785072539682304] dice_loss: 0.5529 depth_loss: 0.0190 loss: 5.5482 rmse: 0.1370\n",
      "Train Epoch: 7 Iteration: 3720 - LR: [0.00027757035173040325] dice_loss: 0.5549 depth_loss: 0.0186 loss: 5.5672 rmse: 0.1355\n",
      "Train Epoch: 7 Iteration: 3725 - LR: [0.0002766330573574182] dice_loss: 0.5613 depth_loss: 0.0188 loss: 5.6319 rmse: 0.1361\n",
      "Train Epoch: 7 Iteration: 3730 - LR: [0.0002756953841809661] dice_loss: 0.5669 depth_loss: 0.0187 loss: 5.6872 rmse: 0.1357\n",
      "Train Epoch: 7 Iteration: 3735 - LR: [0.0002747573455381258] dice_loss: 0.5718 depth_loss: 0.0188 loss: 5.7367 rmse: 0.1361\n",
      "Train Epoch: 7 Iteration: 3740 - LR: [0.0002738189547711745] dice_loss: 0.5686 depth_loss: 0.0191 loss: 5.7046 rmse: 0.1372\n",
      "Train Epoch: 7 Iteration: 3745 - LR: [0.00027288022522739785] dice_loss: 0.5672 depth_loss: 0.0192 loss: 5.6912 rmse: 0.1377\n",
      "Train Epoch: 7 Iteration: 3750 - LR: [0.0002719411702588999] dice_loss: 0.5623 depth_loss: 0.0192 loss: 5.6421 rmse: 0.1379\n",
      "Train Epoch: 7 Iteration: 3755 - LR: [0.00027100180322241386] dice_loss: 0.5580 depth_loss: 0.0192 loss: 5.5992 rmse: 0.1379\n",
      "Train Epoch: 7 Iteration: 3760 - LR: [0.00027006213747911116] dice_loss: 0.5541 depth_loss: 0.0191 loss: 5.5601 rmse: 0.1376\n",
      "Train Epoch: 7 Iteration: 3765 - LR: [0.00026912218639441233] dice_loss: 0.5518 depth_loss: 0.0190 loss: 5.5375 rmse: 0.1370\n",
      "Train Epoch: 7 Iteration: 3770 - LR: [0.0002681819633377962] dice_loss: 0.5477 depth_loss: 0.0186 loss: 5.4954 rmse: 0.1357\n",
      "Train Epoch: 7 Iteration: 3775 - LR: [0.0002672414816826102] dice_loss: 0.5456 depth_loss: 0.0184 loss: 5.4745 rmse: 0.1349\n",
      "Train Epoch: 7 Iteration: 3780 - LR: [0.00026630075480587966] dice_loss: 0.5392 depth_loss: 0.0182 loss: 5.4106 rmse: 0.1344\n",
      "Train Epoch: 7 Iteration: 3785 - LR: [0.0002653597960881184] dice_loss: 0.5333 depth_loss: 0.0179 loss: 5.3509 rmse: 0.1332\n",
      "Train Epoch: 7 Iteration: 3790 - LR: [0.00026441861891313725] dice_loss: 0.5296 depth_loss: 0.0179 loss: 5.3138 rmse: 0.1331\n",
      "Train Epoch: 7 Iteration: 3795 - LR: [0.0002634772366678547] dice_loss: 0.5254 depth_loss: 0.0181 loss: 5.2721 rmse: 0.1338\n",
      "Train Epoch: 7 Iteration: 3800 - LR: [0.00026253566274210576] dice_loss: 0.5226 depth_loss: 0.0181 loss: 5.2442 rmse: 0.1339\n",
      "Train Epoch: 7 Iteration: 3805 - LR: [0.00026159391052845213] dice_loss: 0.5304 depth_loss: 0.0186 loss: 5.3226 rmse: 0.1356\n",
      "Train Epoch: 7 Iteration: 3810 - LR: [0.00026065199342199124] dice_loss: 0.5387 depth_loss: 0.0189 loss: 5.4059 rmse: 0.1368\n",
      "Train Epoch: 7 Iteration: 3815 - LR: [0.00025970992482016587] dice_loss: 0.5442 depth_loss: 0.0197 loss: 5.4612 rmse: 0.1394\n",
      "Train Epoch: 7 Iteration: 3820 - LR: [0.0002587677181225737] dice_loss: 0.5429 depth_loss: 0.0202 loss: 5.4490 rmse: 0.1411\n",
      "Train Epoch: 7 Iteration: 3825 - LR: [0.0002578253867307764] dice_loss: 0.5521 depth_loss: 0.0208 loss: 5.5423 rmse: 0.1431\n",
      "Train Epoch: 7 Iteration: 3830 - LR: [0.0002568829440481097] dice_loss: 0.5573 depth_loss: 0.0210 loss: 5.5936 rmse: 0.1437\n",
      "Train Epoch: 7 Iteration: 3835 - LR: [0.00025594040347949175] dice_loss: 0.5685 depth_loss: 0.0211 loss: 5.7063 rmse: 0.1443\n",
      "Train Epoch: 7 Iteration: 3840 - LR: [0.00025499777843123345] dice_loss: 0.5708 depth_loss: 0.0212 loss: 5.7295 rmse: 0.1445\n",
      "Train Epoch: 7 Iteration: 3845 - LR: [0.0002540550823108469] dice_loss: 0.5709 depth_loss: 0.0210 loss: 5.7302 rmse: 0.1441\n",
      "Train Epoch: 7 Iteration: 3850 - LR: [0.0002531123285268554] dice_loss: 0.5751 depth_loss: 0.0215 loss: 5.7726 rmse: 0.1457\n",
      "Train Epoch: 7 Iteration: 3855 - LR: [0.00025216953048860227] dice_loss: 0.5763 depth_loss: 0.0213 loss: 5.7841 rmse: 0.1449\n",
      "Train Epoch: 7 Iteration: 3860 - LR: [0.0002512267016060605] dice_loss: 0.5778 depth_loss: 0.0213 loss: 5.7994 rmse: 0.1452\n",
      "Train Epoch: 7 Iteration: 3865 - LR: [0.00025028385528964137] dice_loss: 0.5749 depth_loss: 0.0213 loss: 5.7704 rmse: 0.1452\n",
      "Train Epoch: 7 Iteration: 3870 - LR: [0.0002493410049500046] dice_loss: 0.5733 depth_loss: 0.0215 loss: 5.7549 rmse: 0.1457\n",
      "Train Epoch: 7 Iteration: 3875 - LR: [0.0002483981639978668] dice_loss: 0.5747 depth_loss: 0.0216 loss: 5.7691 rmse: 0.1462\n",
      "Train Epoch: 7 Iteration: 3880 - LR: [0.0002474553458438114] dice_loss: 0.5711 depth_loss: 0.0216 loss: 5.7330 rmse: 0.1462\n",
      "Train Epoch: 7 Iteration: 3885 - LR: [0.0002465125638980972] dice_loss: 0.5680 depth_loss: 0.0216 loss: 5.7017 rmse: 0.1464\n",
      "Train Epoch: 7 Iteration: 3890 - LR: [0.00024556983157046814] dice_loss: 0.5707 depth_loss: 0.0215 loss: 5.7289 rmse: 0.1461\n",
      "Train Epoch: 7 Iteration: 3895 - LR: [0.0002446271622699624] dice_loss: 0.5632 depth_loss: 0.0215 loss: 5.6533 rmse: 0.1460\n",
      "Train Epoch: 7 Iteration: 3900 - LR: [0.00024368456940472178] dice_loss: 0.5648 depth_loss: 0.0214 loss: 5.6691 rmse: 0.1458\n",
      "Train Epoch: 7 Iteration: 3905 - LR: [0.00024274206638180083] dice_loss: 0.5563 depth_loss: 0.0214 loss: 5.5848 rmse: 0.1456\n",
      "Train Epoch: 7 Iteration: 3910 - LR: [0.00024179966660697606] dice_loss: 0.5554 depth_loss: 0.0211 loss: 5.5751 rmse: 0.1444\n",
      "Train Epoch: 7 Iteration: 3915 - LR: [0.00024085738348455558] dice_loss: 0.5507 depth_loss: 0.0210 loss: 5.5278 rmse: 0.1442\n",
      "Train Epoch: 7 Iteration: 3920 - LR: [0.00023991523041718842] dice_loss: 0.5519 depth_loss: 0.0208 loss: 5.5401 rmse: 0.1436\n",
      "Train Epoch: 7 Iteration: 3925 - LR: [0.00023897322080567354] dice_loss: 0.5483 depth_loss: 0.0206 loss: 5.5037 rmse: 0.1431\n",
      "Train Epoch: 7 Iteration: 3930 - LR: [0.00023803136804876945] dice_loss: 0.5410 depth_loss: 0.0207 loss: 5.4306 rmse: 0.1433\n",
      "Train Epoch: 7 Iteration: 3935 - LR: [0.0002370896855430038] dice_loss: 0.5435 depth_loss: 0.0205 loss: 5.4555 rmse: 0.1425\n",
      "Train Epoch: 7 Iteration: 3940 - LR: [0.00023614818668248242] dice_loss: 0.5429 depth_loss: 0.0204 loss: 5.4491 rmse: 0.1421\n",
      "Train Epoch: 7 Iteration: 3945 - LR: [0.00023520688485869932] dice_loss: 0.5382 depth_loss: 0.0202 loss: 5.4019 rmse: 0.1414\n",
      "Train Epoch: 7 Iteration: 3950 - LR: [0.0002342657934603456] dice_loss: 0.5398 depth_loss: 0.0199 loss: 5.4180 rmse: 0.1407\n",
      "Train Epoch: 7 Iteration: 3955 - LR: [0.00023332492587311969] dice_loss: 0.5386 depth_loss: 0.0198 loss: 5.4053 rmse: 0.1400\n",
      "Train Epoch: 7 Iteration: 3960 - LR: [0.00023238429547953643] dice_loss: 0.5415 depth_loss: 0.0199 loss: 5.4347 rmse: 0.1405\n",
      "Train Epoch: 7 Iteration: 3965 - LR: [0.0002314439156587371] dice_loss: 0.5400 depth_loss: 0.0198 loss: 5.4196 rmse: 0.1399\n",
      "Train Epoch: 7 Iteration: 3970 - LR: [0.00023050379978629853] dice_loss: 0.5381 depth_loss: 0.0198 loss: 5.4004 rmse: 0.1399\n",
      "Train Epoch: 7 Iteration: 3975 - LR: [0.0002295639612340437] dice_loss: 0.5334 depth_loss: 0.0200 loss: 5.3537 rmse: 0.1405\n",
      "Train Epoch: 7 Iteration: 3980 - LR: [0.00022862441336985085] dice_loss: 0.5300 depth_loss: 0.0207 loss: 5.3208 rmse: 0.1428\n",
      "Train Epoch: 7 Iteration: 3985 - LR: [0.00022768516955746387] dice_loss: 0.5234 depth_loss: 0.0215 loss: 5.2550 rmse: 0.1454\n",
      "Train Epoch: 7 Iteration: 3990 - LR: [0.0002267462431563014] dice_loss: 0.5199 depth_loss: 0.0217 loss: 5.2208 rmse: 0.1462\n",
      "Train Epoch: 7 Iteration: 3995 - LR: [0.00022580764752126796] dice_loss: 0.5221 depth_loss: 0.0214 loss: 5.2421 rmse: 0.1453\n",
      "Train Epoch: 7 Iteration: 4000 - LR: [0.00022486939600256298] dice_loss: 0.5230 depth_loss: 0.0213 loss: 5.2510 rmse: 0.1450\n",
      "Train Epoch: 7 Iteration: 4005 - LR: [0.00022393150194549165] dice_loss: 0.5227 depth_loss: 0.0213 loss: 5.2479 rmse: 0.1448\n",
      "Train Epoch: 7 Iteration: 4010 - LR: [0.0002229939786902743] dice_loss: 0.5226 depth_loss: 0.0208 loss: 5.2464 rmse: 0.1434\n",
      "Train Epoch: 7 Iteration: 4015 - LR: [0.0002220568395718576] dice_loss: 0.5270 depth_loss: 0.0205 loss: 5.2904 rmse: 0.1422\n",
      "Train Epoch: 7 Iteration: 4020 - LR: [0.00022112009791972417] dice_loss: 0.5275 depth_loss: 0.0204 loss: 5.2958 rmse: 0.1420\n",
      "Train Epoch: 7 Iteration: 4025 - LR: [0.00022018376705770342] dice_loss: 0.5302 depth_loss: 0.0201 loss: 5.3218 rmse: 0.1407\n",
      "Train Epoch: 7 Iteration: 4030 - LR: [0.0002192478603037815] dice_loss: 0.5369 depth_loss: 0.0202 loss: 5.3887 rmse: 0.1411\n",
      "Train Epoch: 7 Iteration: 4035 - LR: [0.00021831239096991252] dice_loss: 0.5410 depth_loss: 0.0207 loss: 5.4309 rmse: 0.1429\n",
      "Train Epoch: 7 Iteration: 4040 - LR: [0.00021737737236182886] dice_loss: 0.5484 depth_loss: 0.0206 loss: 5.5046 rmse: 0.1425\n",
      "Train Epoch: 7 Iteration: 4045 - LR: [0.00021644281777885203] dice_loss: 0.5447 depth_loss: 0.0203 loss: 5.4674 rmse: 0.1416\n",
      "Train Epoch: 7 Iteration: 4050 - LR: [0.0002155087405137031] dice_loss: 0.5407 depth_loss: 0.0199 loss: 5.4268 rmse: 0.1403\n",
      "Train Epoch: 7 Iteration: 4055 - LR: [0.0002145751538523143] dice_loss: 0.5372 depth_loss: 0.0201 loss: 5.3923 rmse: 0.1408\n",
      "Train Epoch: 7 Iteration: 4060 - LR: [0.00021364207107363962] dice_loss: 0.5373 depth_loss: 0.0209 loss: 5.3942 rmse: 0.1433\n",
      "Train Epoch: 7 Iteration: 4065 - LR: [0.0002127095054494661] dice_loss: 0.5432 depth_loss: 0.0212 loss: 5.4532 rmse: 0.1445\n",
      "Train Epoch: 7 Iteration: 4070 - LR: [0.00021177747024422487] dice_loss: 0.5429 depth_loss: 0.0212 loss: 5.4504 rmse: 0.1444\n",
      "Train Epoch: 7 Iteration: 4075 - LR: [0.00021084597871480254] dice_loss: 0.5434 depth_loss: 0.0210 loss: 5.4549 rmse: 0.1438\n",
      "Train Epoch: 7 Iteration: 4080 - LR: [0.00020991504411035285] dice_loss: 0.5435 depth_loss: 0.0213 loss: 5.4566 rmse: 0.1448\n",
      "Train Epoch: 7 Iteration: 4085 - LR: [0.0002089846796721081] dice_loss: 0.5347 depth_loss: 0.0213 loss: 5.3687 rmse: 0.1449\n",
      "Train Epoch: 7 Iteration: 4090 - LR: [0.0002080548986331905] dice_loss: 0.5330 depth_loss: 0.0209 loss: 5.3506 rmse: 0.1436\n",
      "Train Epoch: 7 Iteration: 4095 - LR: [0.0002071257142184246] dice_loss: 0.5361 depth_loss: 0.0207 loss: 5.3813 rmse: 0.1429\n",
      "Train Epoch: 7 Iteration: 4100 - LR: [0.0002061971396441486] dice_loss: 0.5399 depth_loss: 0.0212 loss: 5.4207 rmse: 0.1447\n",
      "Train Epoch: 7 Iteration: 4105 - LR: [0.00020526918811802656] dice_loss: 0.5397 depth_loss: 0.0208 loss: 5.4182 rmse: 0.1431\n",
      "Train Epoch: 7 Iteration: 4110 - LR: [0.0002043418728388606] dice_loss: 0.5434 depth_loss: 0.0206 loss: 5.4543 rmse: 0.1425\n",
      "Train Epoch: 7 Iteration: 4115 - LR: [0.00020341520699640315] dice_loss: 0.5403 depth_loss: 0.0204 loss: 5.4235 rmse: 0.1418\n",
      "Train Epoch: 7 Iteration: 4120 - LR: [0.00020248920377116932] dice_loss: 0.5291 depth_loss: 0.0203 loss: 5.3114 rmse: 0.1415\n",
      "Train Epoch: 7 Iteration: 4125 - LR: [0.00020156387633424948] dice_loss: 0.5256 depth_loss: 0.0198 loss: 5.2755 rmse: 0.1395\n",
      "Train Epoch: 7 Iteration: 4130 - LR: [0.00020063923784712162] dice_loss: 0.5190 depth_loss: 0.0196 loss: 5.2092 rmse: 0.1389\n",
      "Train Epoch: 7 Iteration: 4135 - LR: [0.00019971530146146467] dice_loss: 0.5215 depth_loss: 0.0192 loss: 5.2342 rmse: 0.1375\n",
      "Train Epoch: 7 Iteration: 4140 - LR: [0.000198792080318971] dice_loss: 0.5219 depth_loss: 0.0189 loss: 5.2380 rmse: 0.1366\n",
      "Train Epoch: 7 Iteration: 4145 - LR: [0.00019786958755115986] dice_loss: 0.5289 depth_loss: 0.0191 loss: 5.3079 rmse: 0.1372\n",
      "Train Epoch: 7 Iteration: 4150 - LR: [0.00019694783627919] dice_loss: 0.5372 depth_loss: 0.0193 loss: 5.3913 rmse: 0.1378\n",
      "Train Epoch: 7 Iteration: 4155 - LR: [0.00019602683961367375] dice_loss: 0.5404 depth_loss: 0.0193 loss: 5.4234 rmse: 0.1380\n",
      "Train Epoch: 7 Iteration: 4160 - LR: [0.00019510661065449018] dice_loss: 0.5447 depth_loss: 0.0190 loss: 5.4661 rmse: 0.1368\n",
      "Train Epoch: 7 Iteration: 4165 - LR: [0.00019418716249059893] dice_loss: 0.5473 depth_loss: 0.0186 loss: 5.4915 rmse: 0.1355\n",
      "Validation Results - Epoch: 7  dice_loss: 0.5558 depth_loss: 0.0203 loss: 5.5780 rmse: 0.1413\n",
      "Model saved with loss 5.5780 at epoch 7\n",
      "Train Epoch: 8 Iteration: 4170 - LR: [0.00019326850819985356] dice_loss: 0.5459 depth_loss: 0.0185 loss: 5.4778 rmse: 0.1351\n",
      "Train Epoch: 8 Iteration: 4175 - LR: [0.0001923506608488162] dice_loss: 0.5466 depth_loss: 0.0182 loss: 5.4840 rmse: 0.1340\n",
      "Train Epoch: 8 Iteration: 4180 - LR: [0.00019143363349257136] dice_loss: 0.5464 depth_loss: 0.0177 loss: 5.4822 rmse: 0.1321\n",
      "Train Epoch: 8 Iteration: 4185 - LR: [0.00019051743917454042] dice_loss: 0.5396 depth_loss: 0.0178 loss: 5.4138 rmse: 0.1324\n",
      "Train Epoch: 8 Iteration: 4190 - LR: [0.0001896020909262955] dice_loss: 0.5320 depth_loss: 0.0177 loss: 5.3377 rmse: 0.1320\n",
      "Train Epoch: 8 Iteration: 4195 - LR: [0.00018868760176737502] dice_loss: 0.5335 depth_loss: 0.0176 loss: 5.3522 rmse: 0.1318\n",
      "Train Epoch: 8 Iteration: 4200 - LR: [0.0001877739847050979] dice_loss: 0.5305 depth_loss: 0.0179 loss: 5.3228 rmse: 0.1328\n",
      "Train Epoch: 8 Iteration: 4205 - LR: [0.0001868612527343789] dice_loss: 0.5312 depth_loss: 0.0178 loss: 5.3295 rmse: 0.1324\n",
      "Train Epoch: 8 Iteration: 4210 - LR: [0.0001859494188375431] dice_loss: 0.5269 depth_loss: 0.0180 loss: 5.2870 rmse: 0.1333\n",
      "Train Epoch: 8 Iteration: 4215 - LR: [0.00018503849598414227] dice_loss: 0.5215 depth_loss: 0.0181 loss: 5.2334 rmse: 0.1335\n",
      "Train Epoch: 8 Iteration: 4220 - LR: [0.00018412849713076967] dice_loss: 0.5208 depth_loss: 0.0179 loss: 5.2261 rmse: 0.1329\n",
      "Train Epoch: 8 Iteration: 4225 - LR: [0.000183219435220876] dice_loss: 0.5243 depth_loss: 0.0181 loss: 5.2613 rmse: 0.1337\n",
      "Train Epoch: 8 Iteration: 4230 - LR: [0.00018231132318458516] dice_loss: 0.5245 depth_loss: 0.0179 loss: 5.2625 rmse: 0.1330\n",
      "Train Epoch: 8 Iteration: 4235 - LR: [0.00018140417393851057] dice_loss: 0.5321 depth_loss: 0.0179 loss: 5.3394 rmse: 0.1330\n",
      "Train Epoch: 8 Iteration: 4240 - LR: [0.00018049800038557118] dice_loss: 0.5377 depth_loss: 0.0182 loss: 5.3951 rmse: 0.1339\n",
      "Train Epoch: 8 Iteration: 4245 - LR: [0.00017959281541480822] dice_loss: 0.5381 depth_loss: 0.0182 loss: 5.3991 rmse: 0.1340\n",
      "Train Epoch: 8 Iteration: 4250 - LR: [0.00017868863190120165] dice_loss: 0.5440 depth_loss: 0.0179 loss: 5.4581 rmse: 0.1330\n",
      "Train Epoch: 8 Iteration: 4255 - LR: [0.00017778546270548712] dice_loss: 0.5494 depth_loss: 0.0177 loss: 5.5119 rmse: 0.1321\n",
      "Train Epoch: 8 Iteration: 4260 - LR: [0.00017688332067397302] dice_loss: 0.5491 depth_loss: 0.0177 loss: 5.5091 rmse: 0.1320\n",
      "Train Epoch: 8 Iteration: 4265 - LR: [0.00017598221863835797] dice_loss: 0.5515 depth_loss: 0.0177 loss: 5.5330 rmse: 0.1322\n",
      "Train Epoch: 8 Iteration: 4270 - LR: [0.00017508216941554774] dice_loss: 0.5503 depth_loss: 0.0174 loss: 5.5205 rmse: 0.1308\n",
      "Train Epoch: 8 Iteration: 4275 - LR: [0.0001741831858074736] dice_loss: 0.5510 depth_loss: 0.0176 loss: 5.5276 rmse: 0.1317\n",
      "Train Epoch: 8 Iteration: 4280 - LR: [0.00017328528060090993] dice_loss: 0.5482 depth_loss: 0.0172 loss: 5.4996 rmse: 0.1302\n",
      "Train Epoch: 8 Iteration: 4285 - LR: [0.0001723884665672923] dice_loss: 0.5498 depth_loss: 0.0171 loss: 5.5152 rmse: 0.1299\n",
      "Train Epoch: 8 Iteration: 4290 - LR: [0.000171492756462536] dice_loss: 0.5529 depth_loss: 0.0172 loss: 5.5461 rmse: 0.1304\n",
      "Train Epoch: 8 Iteration: 4295 - LR: [0.00017059816302685427] dice_loss: 0.5544 depth_loss: 0.0176 loss: 5.5612 rmse: 0.1318\n",
      "Train Epoch: 8 Iteration: 4300 - LR: [0.00016970469898457743] dice_loss: 0.5493 depth_loss: 0.0175 loss: 5.5108 rmse: 0.1315\n",
      "Train Epoch: 8 Iteration: 4305 - LR: [0.00016881237704397185] dice_loss: 0.5463 depth_loss: 0.0176 loss: 5.4807 rmse: 0.1320\n",
      "Train Epoch: 8 Iteration: 4310 - LR: [0.00016792120989705906] dice_loss: 0.5506 depth_loss: 0.0180 loss: 5.5245 rmse: 0.1334\n",
      "Train Epoch: 8 Iteration: 4315 - LR: [0.00016703121021943517] dice_loss: 0.5523 depth_loss: 0.0177 loss: 5.5403 rmse: 0.1322\n",
      "Train Epoch: 8 Iteration: 4320 - LR: [0.0001661423906700908] dice_loss: 0.5589 depth_loss: 0.0179 loss: 5.6068 rmse: 0.1327\n",
      "Train Epoch: 8 Iteration: 4325 - LR: [0.00016525476389123088] dice_loss: 0.5642 depth_loss: 0.0177 loss: 5.6594 rmse: 0.1322\n",
      "Train Epoch: 8 Iteration: 4330 - LR: [0.00016436834250809504] dice_loss: 0.5695 depth_loss: 0.0179 loss: 5.7130 rmse: 0.1330\n",
      "Train Epoch: 8 Iteration: 4335 - LR: [0.00016348313912877734] dice_loss: 0.5656 depth_loss: 0.0184 loss: 5.6747 rmse: 0.1347\n",
      "Train Epoch: 8 Iteration: 4340 - LR: [0.00016259916634404803] dice_loss: 0.5641 depth_loss: 0.0184 loss: 5.6591 rmse: 0.1347\n",
      "Train Epoch: 8 Iteration: 4345 - LR: [0.00016171643672717356] dice_loss: 0.5591 depth_loss: 0.0184 loss: 5.6091 rmse: 0.1349\n",
      "Train Epoch: 8 Iteration: 4350 - LR: [0.0001608349628337383] dice_loss: 0.5554 depth_loss: 0.0185 loss: 5.5727 rmse: 0.1351\n",
      "Train Epoch: 8 Iteration: 4355 - LR: [0.0001599547572014654] dice_loss: 0.5513 depth_loss: 0.0185 loss: 5.5311 rmse: 0.1351\n",
      "Train Epoch: 8 Iteration: 4360 - LR: [0.000159075832350039] dice_loss: 0.5489 depth_loss: 0.0182 loss: 5.5067 rmse: 0.1342\n",
      "Train Epoch: 8 Iteration: 4365 - LR: [0.00015819820078092595] dice_loss: 0.5448 depth_loss: 0.0179 loss: 5.4654 rmse: 0.1329\n",
      "Train Epoch: 8 Iteration: 4370 - LR: [0.00015732187497719804] dice_loss: 0.5426 depth_loss: 0.0176 loss: 5.4435 rmse: 0.1321\n",
      "Train Epoch: 8 Iteration: 4375 - LR: [0.00015644686740335409] dice_loss: 0.5365 depth_loss: 0.0174 loss: 5.3827 rmse: 0.1313\n",
      "Train Epoch: 8 Iteration: 4380 - LR: [0.00015557319050514323] dice_loss: 0.5305 depth_loss: 0.0171 loss: 5.3225 rmse: 0.1300\n",
      "Train Epoch: 8 Iteration: 4385 - LR: [0.00015470085670938753] dice_loss: 0.5273 depth_loss: 0.0171 loss: 5.2904 rmse: 0.1301\n",
      "Train Epoch: 8 Iteration: 4390 - LR: [0.00015382987842380542] dice_loss: 0.5233 depth_loss: 0.0173 loss: 5.2499 rmse: 0.1306\n",
      "Train Epoch: 8 Iteration: 4395 - LR: [0.00015296026803683486] dice_loss: 0.5211 depth_loss: 0.0172 loss: 5.2284 rmse: 0.1304\n",
      "Train Epoch: 8 Iteration: 4400 - LR: [0.00015209203791745763] dice_loss: 0.5282 depth_loss: 0.0177 loss: 5.2999 rmse: 0.1324\n",
      "Train Epoch: 8 Iteration: 4405 - LR: [0.00015122520041502296] dice_loss: 0.5363 depth_loss: 0.0179 loss: 5.3813 rmse: 0.1331\n",
      "Train Epoch: 8 Iteration: 4410 - LR: [0.0001503597678590723] dice_loss: 0.5419 depth_loss: 0.0186 loss: 5.4375 rmse: 0.1354\n",
      "Train Epoch: 8 Iteration: 4415 - LR: [0.00014949575255916343] dice_loss: 0.5405 depth_loss: 0.0192 loss: 5.4242 rmse: 0.1374\n",
      "Train Epoch: 8 Iteration: 4420 - LR: [0.00014863316680469583] dice_loss: 0.5502 depth_loss: 0.0198 loss: 5.5215 rmse: 0.1396\n",
      "Train Epoch: 8 Iteration: 4425 - LR: [0.00014777202286473572] dice_loss: 0.5553 depth_loss: 0.0201 loss: 5.5732 rmse: 0.1404\n",
      "Train Epoch: 8 Iteration: 4430 - LR: [0.00014691233298784164] dice_loss: 0.5668 depth_loss: 0.0202 loss: 5.6878 rmse: 0.1408\n",
      "Train Epoch: 8 Iteration: 4435 - LR: [0.00014605410940188976] dice_loss: 0.5690 depth_loss: 0.0203 loss: 5.7108 rmse: 0.1413\n",
      "Train Epoch: 8 Iteration: 4440 - LR: [0.0001451973643139007] dice_loss: 0.5690 depth_loss: 0.0202 loss: 5.7100 rmse: 0.1410\n",
      "Train Epoch: 8 Iteration: 4445 - LR: [0.0001443421099098653] dice_loss: 0.5724 depth_loss: 0.0205 loss: 5.7446 rmse: 0.1421\n",
      "Train Epoch: 8 Iteration: 4450 - LR: [0.0001434883583545718] dice_loss: 0.5735 depth_loss: 0.0202 loss: 5.7547 rmse: 0.1411\n",
      "Train Epoch: 8 Iteration: 4455 - LR: [0.00014263612179143232] dice_loss: 0.5747 depth_loss: 0.0202 loss: 5.7673 rmse: 0.1411\n",
      "Train Epoch: 8 Iteration: 4460 - LR: [0.00014178541234231027] dice_loss: 0.5718 depth_loss: 0.0202 loss: 5.7381 rmse: 0.1412\n",
      "Train Epoch: 8 Iteration: 4465 - LR: [0.00014093624210734815] dice_loss: 0.5706 depth_loss: 0.0202 loss: 5.7261 rmse: 0.1414\n",
      "Train Epoch: 8 Iteration: 4470 - LR: [0.00014008862316479545] dice_loss: 0.5714 depth_loss: 0.0204 loss: 5.7345 rmse: 0.1419\n",
      "Train Epoch: 8 Iteration: 4475 - LR: [0.00013924256757083645] dice_loss: 0.5677 depth_loss: 0.0204 loss: 5.6976 rmse: 0.1419\n",
      "Train Epoch: 8 Iteration: 4480 - LR: [0.00013839808735941901] dice_loss: 0.5652 depth_loss: 0.0204 loss: 5.6722 rmse: 0.1422\n",
      "Train Epoch: 8 Iteration: 4485 - LR: [0.00013755519454208373] dice_loss: 0.5681 depth_loss: 0.0204 loss: 5.7017 rmse: 0.1422\n",
      "Train Epoch: 8 Iteration: 4490 - LR: [0.00013671390110779242] dice_loss: 0.5597 depth_loss: 0.0207 loss: 5.6175 rmse: 0.1432\n",
      "Train Epoch: 8 Iteration: 4495 - LR: [0.00013587421902275799] dice_loss: 0.5606 depth_loss: 0.0204 loss: 5.6267 rmse: 0.1419\n",
      "Train Epoch: 8 Iteration: 4500 - LR: [0.0001350361602302742] dice_loss: 0.5520 depth_loss: 0.0202 loss: 5.5402 rmse: 0.1414\n",
      "Train Epoch: 8 Iteration: 4505 - LR: [0.0001341997366505459] dice_loss: 0.5506 depth_loss: 0.0200 loss: 5.5259 rmse: 0.1406\n",
      "Train Epoch: 8 Iteration: 4510 - LR: [0.00013336496018051918] dice_loss: 0.5458 depth_loss: 0.0198 loss: 5.4776 rmse: 0.1401\n",
      "Train Epoch: 8 Iteration: 4515 - LR: [0.00013253184269371233] dice_loss: 0.5469 depth_loss: 0.0197 loss: 5.4883 rmse: 0.1397\n",
      "Train Epoch: 8 Iteration: 4520 - LR: [0.00013170039604004692] dice_loss: 0.5434 depth_loss: 0.0196 loss: 5.4533 rmse: 0.1395\n",
      "Train Epoch: 8 Iteration: 4525 - LR: [0.00013087063204567952] dice_loss: 0.5358 depth_loss: 0.0197 loss: 5.3772 rmse: 0.1396\n",
      "Train Epoch: 8 Iteration: 4530 - LR: [0.00013004256251283294] dice_loss: 0.5386 depth_loss: 0.0193 loss: 5.4057 rmse: 0.1385\n",
      "Train Epoch: 8 Iteration: 4535 - LR: [0.00012921619921962878] dice_loss: 0.5382 depth_loss: 0.0191 loss: 5.4015 rmse: 0.1375\n",
      "Train Epoch: 8 Iteration: 4540 - LR: [0.00012839155391991975] dice_loss: 0.5338 depth_loss: 0.0189 loss: 5.3573 rmse: 0.1368\n",
      "Train Epoch: 8 Iteration: 4545 - LR: [0.00012756863834312263] dice_loss: 0.5358 depth_loss: 0.0187 loss: 5.3763 rmse: 0.1363\n",
      "Train Epoch: 8 Iteration: 4550 - LR: [0.00012674746419405128] dice_loss: 0.5344 depth_loss: 0.0187 loss: 5.3624 rmse: 0.1363\n",
      "Train Epoch: 8 Iteration: 4555 - LR: [0.00012592804315275012] dice_loss: 0.5374 depth_loss: 0.0190 loss: 5.3933 rmse: 0.1372\n",
      "Train Epoch: 8 Iteration: 4560 - LR: [0.00012511038687432812] dice_loss: 0.5359 depth_loss: 0.0189 loss: 5.3774 rmse: 0.1368\n",
      "Train Epoch: 8 Iteration: 4565 - LR: [0.0001242945069887931] dice_loss: 0.5340 depth_loss: 0.0189 loss: 5.3587 rmse: 0.1368\n",
      "Train Epoch: 8 Iteration: 4570 - LR: [0.00012348041510088612] dice_loss: 0.5293 depth_loss: 0.0192 loss: 5.3126 rmse: 0.1376\n",
      "Train Epoch: 8 Iteration: 4575 - LR: [0.00012266812278991646] dice_loss: 0.5262 depth_loss: 0.0198 loss: 5.2817 rmse: 0.1396\n",
      "Train Epoch: 8 Iteration: 4580 - LR: [0.00012185764160959685] dice_loss: 0.5194 depth_loss: 0.0207 loss: 5.2150 rmse: 0.1426\n",
      "Train Epoch: 8 Iteration: 4585 - LR: [0.00012104898308787963] dice_loss: 0.5160 depth_loss: 0.0207 loss: 5.1807 rmse: 0.1427\n",
      "Train Epoch: 8 Iteration: 4590 - LR: [0.000120242158726792] dice_loss: 0.5187 depth_loss: 0.0205 loss: 5.2071 rmse: 0.1423\n",
      "Train Epoch: 8 Iteration: 4595 - LR: [0.00011943718000227294] dice_loss: 0.5195 depth_loss: 0.0203 loss: 5.2150 rmse: 0.1414\n",
      "Train Epoch: 8 Iteration: 4600 - LR: [0.00011863405836400975] dice_loss: 0.5194 depth_loss: 0.0204 loss: 5.2141 rmse: 0.1417\n",
      "Train Epoch: 8 Iteration: 4605 - LR: [0.00011783280523527565] dice_loss: 0.5193 depth_loss: 0.0202 loss: 5.2135 rmse: 0.1413\n",
      "Train Epoch: 8 Iteration: 4610 - LR: [0.00011703343201276655] dice_loss: 0.5239 depth_loss: 0.0198 loss: 5.2588 rmse: 0.1399\n",
      "Train Epoch: 8 Iteration: 4615 - LR: [0.00011623595006643941] dice_loss: 0.5244 depth_loss: 0.0197 loss: 5.2641 rmse: 0.1396\n",
      "Train Epoch: 8 Iteration: 4620 - LR: [0.00011544037073935075] dice_loss: 0.5273 depth_loss: 0.0194 loss: 5.2921 rmse: 0.1383\n",
      "Train Epoch: 8 Iteration: 4625 - LR: [0.00011464670534749467] dice_loss: 0.5340 depth_loss: 0.0195 loss: 5.3590 rmse: 0.1387\n",
      "Train Epoch: 8 Iteration: 4630 - LR: [0.00011385496517964241] dice_loss: 0.5384 depth_loss: 0.0200 loss: 5.4040 rmse: 0.1406\n",
      "Train Epoch: 8 Iteration: 4635 - LR: [0.00011306516149718145] dice_loss: 0.5461 depth_loss: 0.0198 loss: 5.4813 rmse: 0.1398\n",
      "Train Epoch: 8 Iteration: 4640 - LR: [0.00011227730553395585] dice_loss: 0.5426 depth_loss: 0.0197 loss: 5.4461 rmse: 0.1393\n",
      "Train Epoch: 8 Iteration: 4645 - LR: [0.00011149140849610563] dice_loss: 0.5387 depth_loss: 0.0195 loss: 5.4061 rmse: 0.1389\n",
      "Train Epoch: 8 Iteration: 4650 - LR: [0.00011070748156190843] dice_loss: 0.5352 depth_loss: 0.0194 loss: 5.3719 rmse: 0.1385\n",
      "Train Epoch: 8 Iteration: 4655 - LR: [0.00010992553588161923] dice_loss: 0.5357 depth_loss: 0.0204 loss: 5.3774 rmse: 0.1415\n",
      "Train Epoch: 8 Iteration: 4660 - LR: [0.00010914558257731315] dice_loss: 0.5417 depth_loss: 0.0208 loss: 5.4380 rmse: 0.1429\n",
      "Train Epoch: 8 Iteration: 4665 - LR: [0.00010836763274272608] dice_loss: 0.5414 depth_loss: 0.0205 loss: 5.4349 rmse: 0.1417\n",
      "Train Epoch: 8 Iteration: 4670 - LR: [0.00010759169744309788] dice_loss: 0.5420 depth_loss: 0.0205 loss: 5.4405 rmse: 0.1418\n",
      "Train Epoch: 8 Iteration: 4675 - LR: [0.00010681778771501379] dice_loss: 0.5423 depth_loss: 0.0208 loss: 5.4442 rmse: 0.1431\n",
      "Train Epoch: 8 Iteration: 4680 - LR: [0.00010604591456624883] dice_loss: 0.5338 depth_loss: 0.0207 loss: 5.3584 rmse: 0.1427\n",
      "Train Epoch: 8 Iteration: 4685 - LR: [0.00010527608897561008] dice_loss: 0.5322 depth_loss: 0.0205 loss: 5.3427 rmse: 0.1422\n",
      "Train Epoch: 8 Iteration: 4690 - LR: [0.00010450832189278146] dice_loss: 0.5352 depth_loss: 0.0203 loss: 5.3721 rmse: 0.1414\n",
      "Train Epoch: 8 Iteration: 4695 - LR: [0.00010374262423816688] dice_loss: 0.5388 depth_loss: 0.0207 loss: 5.4084 rmse: 0.1428\n",
      "Train Epoch: 8 Iteration: 4700 - LR: [0.00010297900690273619] dice_loss: 0.5385 depth_loss: 0.0204 loss: 5.4055 rmse: 0.1418\n",
      "Train Epoch: 8 Iteration: 4705 - LR: [0.00010221748074786917] dice_loss: 0.5423 depth_loss: 0.0202 loss: 5.4431 rmse: 0.1410\n",
      "Train Epoch: 8 Iteration: 4710 - LR: [0.0001014580566052019] dice_loss: 0.5391 depth_loss: 0.0199 loss: 5.4107 rmse: 0.1401\n",
      "Train Epoch: 8 Iteration: 4715 - LR: [0.00010070074527647184] dice_loss: 0.5276 depth_loss: 0.0198 loss: 5.2958 rmse: 0.1397\n",
      "Train Epoch: 8 Iteration: 4720 - LR: [9.994555753336508e-05] dice_loss: 0.5243 depth_loss: 0.0192 loss: 5.2623 rmse: 0.1377\n",
      "Train Epoch: 8 Iteration: 4725 - LR: [9.919250411736236e-05] dice_loss: 0.5174 depth_loss: 0.0190 loss: 5.1931 rmse: 0.1368\n",
      "Train Epoch: 8 Iteration: 4730 - LR: [9.844159573958706e-05] dice_loss: 0.5199 depth_loss: 0.0187 loss: 5.2178 rmse: 0.1358\n",
      "Train Epoch: 8 Iteration: 4735 - LR: [9.769284308065179e-05] dice_loss: 0.5203 depth_loss: 0.0184 loss: 5.2213 rmse: 0.1346\n",
      "Train Epoch: 8 Iteration: 4740 - LR: [9.69462567905077e-05] dice_loss: 0.5268 depth_loss: 0.0185 loss: 5.2865 rmse: 0.1351\n",
      "Train Epoch: 8 Iteration: 4745 - LR: [9.620184748829196e-05] dice_loss: 0.5351 depth_loss: 0.0186 loss: 5.3696 rmse: 0.1353\n",
      "Train Epoch: 8 Iteration: 4750 - LR: [9.545962576217754e-05] dice_loss: 0.5386 depth_loss: 0.0187 loss: 5.4042 rmse: 0.1358\n",
      "Train Epoch: 8 Iteration: 4755 - LR: [9.471960216922193e-05] dice_loss: 0.5427 depth_loss: 0.0184 loss: 5.4451 rmse: 0.1345\n",
      "Train Epoch: 8 Iteration: 4760 - LR: [9.39817872352174e-05] dice_loss: 0.5454 depth_loss: 0.0181 loss: 5.4721 rmse: 0.1335\n",
      "Validation Results - Epoch: 8  dice_loss: 0.5531 depth_loss: 0.0192 loss: 5.5502 rmse: 0.1374\n",
      "Model saved with loss 5.5502 at epoch 8\n",
      "Train Epoch: 9 Iteration: 4765 - LR: [9.324619145454099e-05] dice_loss: 0.5438 depth_loss: 0.0179 loss: 5.4557 rmse: 0.1329\n",
      "Train Epoch: 9 Iteration: 4770 - LR: [9.251282529000577e-05] dice_loss: 0.5447 depth_loss: 0.0176 loss: 5.4650 rmse: 0.1320\n",
      "Train Epoch: 9 Iteration: 4775 - LR: [9.178169917271133e-05] dice_loss: 0.5448 depth_loss: 0.0172 loss: 5.4653 rmse: 0.1301\n",
      "Train Epoch: 9 Iteration: 4780 - LR: [9.10528235018958e-05] dice_loss: 0.5376 depth_loss: 0.0172 loss: 5.3933 rmse: 0.1301\n",
      "Train Epoch: 9 Iteration: 4785 - LR: [9.032620864478815e-05] dice_loss: 0.5301 depth_loss: 0.0170 loss: 5.3179 rmse: 0.1295\n",
      "Train Epoch: 9 Iteration: 4790 - LR: [8.960186493646011e-05] dice_loss: 0.5312 depth_loss: 0.0169 loss: 5.3285 rmse: 0.1292\n",
      "Train Epoch: 9 Iteration: 4795 - LR: [8.887980267967971e-05] dice_loss: 0.5283 depth_loss: 0.0172 loss: 5.3003 rmse: 0.1302\n",
      "Train Epoch: 9 Iteration: 4800 - LR: [8.816003214476443e-05] dice_loss: 0.5286 depth_loss: 0.0171 loss: 5.3031 rmse: 0.1298\n",
      "Train Epoch: 9 Iteration: 4805 - LR: [8.744256356943546e-05] dice_loss: 0.5239 depth_loss: 0.0174 loss: 5.2568 rmse: 0.1308\n",
      "Train Epoch: 9 Iteration: 4810 - LR: [8.672740715867156e-05] dice_loss: 0.5186 depth_loss: 0.0174 loss: 5.2036 rmse: 0.1309\n",
      "Train Epoch: 9 Iteration: 4815 - LR: [8.601457308456462e-05] dice_loss: 0.5179 depth_loss: 0.0173 loss: 5.1961 rmse: 0.1303\n",
      "Train Epoch: 9 Iteration: 4820 - LR: [8.530407148617394e-05] dice_loss: 0.5214 depth_loss: 0.0174 loss: 5.2318 rmse: 0.1310\n",
      "Train Epoch: 9 Iteration: 4825 - LR: [8.459591246938331e-05] dice_loss: 0.5221 depth_loss: 0.0173 loss: 5.2385 rmse: 0.1304\n",
      "Train Epoch: 9 Iteration: 4830 - LR: [8.389010610675614e-05] dice_loss: 0.5299 depth_loss: 0.0173 loss: 5.3159 rmse: 0.1304\n",
      "Train Epoch: 9 Iteration: 4835 - LR: [8.318666243739303e-05] dice_loss: 0.5354 depth_loss: 0.0174 loss: 5.3714 rmse: 0.1310\n",
      "Train Epoch: 9 Iteration: 4840 - LR: [8.248559146678804e-05] dice_loss: 0.5358 depth_loss: 0.0175 loss: 5.3756 rmse: 0.1312\n",
      "Train Epoch: 9 Iteration: 4845 - LR: [8.178690316668759e-05] dice_loss: 0.5418 depth_loss: 0.0172 loss: 5.4350 rmse: 0.1300\n",
      "Train Epoch: 9 Iteration: 4850 - LR: [8.109060747494743e-05] dice_loss: 0.5474 depth_loss: 0.0169 loss: 5.4911 rmse: 0.1288\n",
      "Train Epoch: 9 Iteration: 4855 - LR: [8.039671429539235e-05] dice_loss: 0.5472 depth_loss: 0.0167 loss: 5.4892 rmse: 0.1283\n",
      "Train Epoch: 9 Iteration: 4860 - LR: [7.970523349767414e-05] dice_loss: 0.5496 depth_loss: 0.0168 loss: 5.5127 rmse: 0.1286\n",
      "Train Epoch: 9 Iteration: 4865 - LR: [7.901617491713249e-05] dice_loss: 0.5483 depth_loss: 0.0165 loss: 5.4998 rmse: 0.1273\n",
      "Train Epoch: 9 Iteration: 4870 - LR: [7.8329548354654e-05] dice_loss: 0.5491 depth_loss: 0.0165 loss: 5.5075 rmse: 0.1277\n",
      "Train Epoch: 9 Iteration: 4875 - LR: [7.764536357653375e-05] dice_loss: 0.5463 depth_loss: 0.0162 loss: 5.4796 rmse: 0.1263\n",
      "Train Epoch: 9 Iteration: 4880 - LR: [7.696363031433518e-05] dice_loss: 0.5480 depth_loss: 0.0161 loss: 5.4957 rmse: 0.1261\n",
      "Train Epoch: 9 Iteration: 4885 - LR: [7.628435826475302e-05] dice_loss: 0.5508 depth_loss: 0.0163 loss: 5.5241 rmse: 0.1268\n",
      "Train Epoch: 9 Iteration: 4890 - LR: [7.560755708947429e-05] dice_loss: 0.5522 depth_loss: 0.0167 loss: 5.5388 rmse: 0.1283\n",
      "Train Epoch: 9 Iteration: 4895 - LR: [7.493323641504177e-05] dice_loss: 0.5471 depth_loss: 0.0166 loss: 5.4875 rmse: 0.1281\n",
      "Train Epoch: 9 Iteration: 4900 - LR: [7.426140583271593e-05] dice_loss: 0.5442 depth_loss: 0.0168 loss: 5.4587 rmse: 0.1287\n",
      "Train Epoch: 9 Iteration: 4905 - LR: [7.359207489833987e-05] dice_loss: 0.5485 depth_loss: 0.0172 loss: 5.5025 rmse: 0.1303\n",
      "Train Epoch: 9 Iteration: 4910 - LR: [7.292525313220228e-05] dice_loss: 0.5502 depth_loss: 0.0170 loss: 5.5188 rmse: 0.1295\n",
      "Train Epoch: 9 Iteration: 4915 - LR: [7.226095001890274e-05] dice_loss: 0.5573 depth_loss: 0.0173 loss: 5.5898 rmse: 0.1304\n",
      "Train Epoch: 9 Iteration: 4920 - LR: [7.159917500721635e-05] dice_loss: 0.5632 depth_loss: 0.0173 loss: 5.6494 rmse: 0.1306\n",
      "Train Epoch: 9 Iteration: 4925 - LR: [7.093993750995966e-05] dice_loss: 0.5682 depth_loss: 0.0175 loss: 5.6997 rmse: 0.1313\n",
      "Train Epoch: 9 Iteration: 4930 - LR: [7.028324690385647e-05] dice_loss: 0.5636 depth_loss: 0.0180 loss: 5.6540 rmse: 0.1332\n",
      "Train Epoch: 9 Iteration: 4935 - LR: [6.962911252940499e-05] dice_loss: 0.5624 depth_loss: 0.0184 loss: 5.6420 rmse: 0.1345\n",
      "Train Epoch: 9 Iteration: 4940 - LR: [6.897754369074428e-05] dice_loss: 0.5575 depth_loss: 0.0183 loss: 5.5935 rmse: 0.1344\n",
      "Train Epoch: 9 Iteration: 4945 - LR: [6.832854965552238e-05] dice_loss: 0.5533 depth_loss: 0.0182 loss: 5.5510 rmse: 0.1341\n",
      "Train Epoch: 9 Iteration: 4950 - LR: [6.768213965476454e-05] dice_loss: 0.5493 depth_loss: 0.0183 loss: 5.5111 rmse: 0.1343\n",
      "Train Epoch: 9 Iteration: 4955 - LR: [6.703832288274158e-05] dice_loss: 0.5469 depth_loss: 0.0180 loss: 5.4868 rmse: 0.1332\n",
      "Train Epoch: 9 Iteration: 4960 - LR: [6.63971084968394e-05] dice_loss: 0.5425 depth_loss: 0.0176 loss: 5.4426 rmse: 0.1319\n",
      "Train Epoch: 9 Iteration: 4965 - LR: [6.575850561742842e-05] dice_loss: 0.5406 depth_loss: 0.0174 loss: 5.4232 rmse: 0.1312\n",
      "Train Epoch: 9 Iteration: 4970 - LR: [6.512252332773445e-05] dice_loss: 0.5344 depth_loss: 0.0172 loss: 5.3614 rmse: 0.1304\n",
      "Train Epoch: 9 Iteration: 4975 - LR: [6.448917067370878e-05] dice_loss: 0.5287 depth_loss: 0.0169 loss: 5.3040 rmse: 0.1292\n",
      "Train Epoch: 9 Iteration: 4980 - LR: [6.385845666389992e-05] dice_loss: 0.5251 depth_loss: 0.0168 loss: 5.2680 rmse: 0.1286\n",
      "Train Epoch: 9 Iteration: 4985 - LR: [6.323039026932539e-05] dice_loss: 0.5215 depth_loss: 0.0169 loss: 5.2316 rmse: 0.1291\n",
      "Train Epoch: 9 Iteration: 4990 - LR: [6.260498042334422e-05] dice_loss: 0.5186 depth_loss: 0.0168 loss: 5.2024 rmse: 0.1288\n",
      "Train Epoch: 9 Iteration: 4995 - LR: [6.198223602152965e-05] dice_loss: 0.5257 depth_loss: 0.0171 loss: 5.2739 rmse: 0.1302\n",
      "Train Epoch: 9 Iteration: 5000 - LR: [6.136216592154281e-05] dice_loss: 0.5337 depth_loss: 0.0174 loss: 5.3542 rmse: 0.1311\n",
      "Train Epoch: 9 Iteration: 5005 - LR: [6.074477894300651e-05] dice_loss: 0.5389 depth_loss: 0.0182 loss: 5.4073 rmse: 0.1338\n",
      "Train Epoch: 9 Iteration: 5010 - LR: [6.013008386738022e-05] dice_loss: 0.5376 depth_loss: 0.0188 loss: 5.3945 rmse: 0.1358\n",
      "Train Epoch: 9 Iteration: 5015 - LR: [5.9518089437834656e-05] dice_loss: 0.5469 depth_loss: 0.0194 loss: 5.4886 rmse: 0.1379\n",
      "Train Epoch: 9 Iteration: 5020 - LR: [5.890880435912771e-05] dice_loss: 0.5523 depth_loss: 0.0197 loss: 5.5429 rmse: 0.1392\n",
      "Train Epoch: 9 Iteration: 5025 - LR: [5.830223729748054e-05] dice_loss: 0.5637 depth_loss: 0.0199 loss: 5.6573 rmse: 0.1398\n",
      "Train Epoch: 9 Iteration: 5030 - LR: [5.769839688045451e-05] dice_loss: 0.5661 depth_loss: 0.0200 loss: 5.6810 rmse: 0.1403\n",
      "Train Epoch: 9 Iteration: 5035 - LR: [5.709729169682813e-05] dice_loss: 0.5663 depth_loss: 0.0199 loss: 5.6827 rmse: 0.1399\n",
      "Train Epoch: 9 Iteration: 5040 - LR: [5.649893029647512e-05] dice_loss: 0.5698 depth_loss: 0.0203 loss: 5.7181 rmse: 0.1412\n",
      "Train Epoch: 9 Iteration: 5045 - LR: [5.5903321190242734e-05] dice_loss: 0.5709 depth_loss: 0.0201 loss: 5.7296 rmse: 0.1406\n",
      "Train Epoch: 9 Iteration: 5050 - LR: [5.531047284983083e-05] dice_loss: 0.5724 depth_loss: 0.0201 loss: 5.7440 rmse: 0.1408\n",
      "Train Epoch: 9 Iteration: 5055 - LR: [5.472039370767116e-05] dice_loss: 0.5697 depth_loss: 0.0201 loss: 5.7169 rmse: 0.1407\n",
      "Train Epoch: 9 Iteration: 5060 - LR: [5.413309215680757e-05] dice_loss: 0.5690 depth_loss: 0.0202 loss: 5.7100 rmse: 0.1411\n",
      "Train Epoch: 9 Iteration: 5065 - LR: [5.354857655077646e-05] dice_loss: 0.5698 depth_loss: 0.0202 loss: 5.7187 rmse: 0.1412\n",
      "Train Epoch: 9 Iteration: 5070 - LR: [5.296685520348838e-05] dice_loss: 0.5658 depth_loss: 0.0201 loss: 5.6780 rmse: 0.1410\n",
      "Train Epoch: 9 Iteration: 5075 - LR: [5.238793638910924e-05] dice_loss: 0.5632 depth_loss: 0.0202 loss: 5.6522 rmse: 0.1413\n",
      "Train Epoch: 9 Iteration: 5080 - LR: [5.181182834194291e-05] dice_loss: 0.5663 depth_loss: 0.0201 loss: 5.6831 rmse: 0.1411\n",
      "Train Epoch: 9 Iteration: 5085 - LR: [5.1238539256314204e-05] dice_loss: 0.5576 depth_loss: 0.0200 loss: 5.5958 rmse: 0.1408\n",
      "Train Epoch: 9 Iteration: 5090 - LR: [5.066807728645202e-05] dice_loss: 0.5585 depth_loss: 0.0195 loss: 5.6040 rmse: 0.1387\n",
      "Train Epoch: 9 Iteration: 5095 - LR: [5.010045054637355e-05] dice_loss: 0.5498 depth_loss: 0.0193 loss: 5.5169 rmse: 0.1380\n",
      "Train Epoch: 9 Iteration: 5100 - LR: [4.953566710976886e-05] dice_loss: 0.5484 depth_loss: 0.0190 loss: 5.5028 rmse: 0.1370\n",
      "Train Epoch: 9 Iteration: 5105 - LR: [4.897373500988614e-05] dice_loss: 0.5436 depth_loss: 0.0189 loss: 5.4545 rmse: 0.1366\n",
      "Train Epoch: 9 Iteration: 5110 - LR: [4.8414662239417114e-05] dice_loss: 0.5446 depth_loss: 0.0187 loss: 5.4650 rmse: 0.1360\n",
      "Train Epoch: 9 Iteration: 5115 - LR: [4.785845675038386e-05] dice_loss: 0.5410 depth_loss: 0.0186 loss: 5.4291 rmse: 0.1358\n",
      "Train Epoch: 9 Iteration: 5120 - LR: [4.730512645402506e-05] dice_loss: 0.5335 depth_loss: 0.0186 loss: 5.3541 rmse: 0.1358\n",
      "Train Epoch: 9 Iteration: 5125 - LR: [4.675467922068418e-05] dice_loss: 0.5365 depth_loss: 0.0184 loss: 5.3832 rmse: 0.1350\n",
      "Train Epoch: 9 Iteration: 5130 - LR: [4.620712287969692e-05] dice_loss: 0.5363 depth_loss: 0.0182 loss: 5.3807 rmse: 0.1342\n",
      "Train Epoch: 9 Iteration: 5135 - LR: [4.56624652192804e-05] dice_loss: 0.5320 depth_loss: 0.0180 loss: 5.3379 rmse: 0.1336\n",
      "Train Epoch: 9 Iteration: 5140 - LR: [4.512071398642169e-05] dice_loss: 0.5339 depth_loss: 0.0178 loss: 5.3564 rmse: 0.1328\n",
      "Train Epoch: 9 Iteration: 5145 - LR: [4.458187688676844e-05] dice_loss: 0.5327 depth_loss: 0.0178 loss: 5.3445 rmse: 0.1326\n",
      "Train Epoch: 9 Iteration: 5150 - LR: [4.404596158451853e-05] dice_loss: 0.5363 depth_loss: 0.0180 loss: 5.3809 rmse: 0.1336\n",
      "Train Epoch: 9 Iteration: 5155 - LR: [4.3512975702311764e-05] dice_loss: 0.5352 depth_loss: 0.0179 loss: 5.3701 rmse: 0.1333\n",
      "Train Epoch: 9 Iteration: 5160 - LR: [4.298292682112057e-05] dice_loss: 0.5334 depth_loss: 0.0180 loss: 5.3522 rmse: 0.1333\n",
      "Train Epoch: 9 Iteration: 5165 - LR: [4.245582248014319e-05] dice_loss: 0.5292 depth_loss: 0.0183 loss: 5.3103 rmse: 0.1345\n",
      "Train Epoch: 9 Iteration: 5170 - LR: [4.193167017669553e-05] dice_loss: 0.5264 depth_loss: 0.0190 loss: 5.2833 rmse: 0.1369\n",
      "Train Epoch: 9 Iteration: 5175 - LR: [4.141047736610531e-05] dice_loss: 0.5201 depth_loss: 0.0200 loss: 5.2210 rmse: 0.1401\n",
      "Train Epoch: 9 Iteration: 5180 - LR: [4.0892251461605195e-05] dice_loss: 0.5167 depth_loss: 0.0199 loss: 5.1867 rmse: 0.1399\n",
      "Train Epoch: 9 Iteration: 5185 - LR: [4.037699983422817e-05] dice_loss: 0.5197 depth_loss: 0.0197 loss: 5.2166 rmse: 0.1392\n",
      "Train Epoch: 9 Iteration: 5190 - LR: [3.986472981270207e-05] dice_loss: 0.5207 depth_loss: 0.0196 loss: 5.2269 rmse: 0.1387\n",
      "Train Epoch: 9 Iteration: 5195 - LR: [3.935544868334588e-05] dice_loss: 0.5206 depth_loss: 0.0196 loss: 5.2253 rmse: 0.1389\n",
      "Train Epoch: 9 Iteration: 5200 - LR: [3.88491636899654e-05] dice_loss: 0.5207 depth_loss: 0.0192 loss: 5.2267 rmse: 0.1376\n",
      "Train Epoch: 9 Iteration: 5205 - LR: [3.8345882033751e-05] dice_loss: 0.5255 depth_loss: 0.0189 loss: 5.2736 rmse: 0.1363\n",
      "Train Epoch: 9 Iteration: 5210 - LR: [3.784561087317453e-05] dice_loss: 0.5264 depth_loss: 0.0188 loss: 5.2826 rmse: 0.1360\n",
      "Train Epoch: 9 Iteration: 5215 - LR: [3.734835732388804e-05] dice_loss: 0.5294 depth_loss: 0.0185 loss: 5.3123 rmse: 0.1349\n",
      "Train Epoch: 9 Iteration: 5220 - LR: [3.685412845862213e-05] dice_loss: 0.5349 depth_loss: 0.0187 loss: 5.3680 rmse: 0.1358\n",
      "Train Epoch: 9 Iteration: 5225 - LR: [3.63629313070856e-05] dice_loss: 0.5388 depth_loss: 0.0192 loss: 5.4071 rmse: 0.1376\n",
      "Train Epoch: 9 Iteration: 5230 - LR: [3.5874772855865306e-05] dice_loss: 0.5459 depth_loss: 0.0190 loss: 5.4782 rmse: 0.1369\n",
      "Train Epoch: 9 Iteration: 5235 - LR: [3.538966004832707e-05] dice_loss: 0.5417 depth_loss: 0.0188 loss: 5.4353 rmse: 0.1361\n",
      "Train Epoch: 9 Iteration: 5240 - LR: [3.490759978451658e-05] dice_loss: 0.5371 depth_loss: 0.0186 loss: 5.3891 rmse: 0.1354\n",
      "Train Epoch: 9 Iteration: 5245 - LR: [3.442859892106136e-05] dice_loss: 0.5332 depth_loss: 0.0185 loss: 5.3506 rmse: 0.1352\n",
      "Train Epoch: 9 Iteration: 5250 - LR: [3.395266427107345e-05] dice_loss: 0.5329 depth_loss: 0.0193 loss: 5.3480 rmse: 0.1376\n",
      "Train Epoch: 9 Iteration: 5255 - LR: [3.347980260405216e-05] dice_loss: 0.5391 depth_loss: 0.0197 loss: 5.4110 rmse: 0.1390\n",
      "Train Epoch: 9 Iteration: 5260 - LR: [3.301002064578802e-05] dice_loss: 0.5388 depth_loss: 0.0193 loss: 5.4071 rmse: 0.1378\n",
      "Train Epoch: 9 Iteration: 5265 - LR: [3.254332507826699e-05] dice_loss: 0.5392 depth_loss: 0.0191 loss: 5.4110 rmse: 0.1371\n",
      "Train Epoch: 9 Iteration: 5270 - LR: [3.207972253957559e-05] dice_loss: 0.5394 depth_loss: 0.0195 loss: 5.4133 rmse: 0.1385\n",
      "Train Epoch: 9 Iteration: 5275 - LR: [3.1619219623806205e-05] dice_loss: 0.5309 depth_loss: 0.0193 loss: 5.3279 rmse: 0.1379\n",
      "Train Epoch: 9 Iteration: 5280 - LR: [3.1161822880963736e-05] dice_loss: 0.5292 depth_loss: 0.0190 loss: 5.3115 rmse: 0.1368\n",
      "Train Epoch: 9 Iteration: 5285 - LR: [3.0707538816871704e-05] dice_loss: 0.5317 depth_loss: 0.0188 loss: 5.3356 rmse: 0.1362\n",
      "Train Epoch: 9 Iteration: 5290 - LR: [3.0256373893080603e-05] dice_loss: 0.5355 depth_loss: 0.0192 loss: 5.3744 rmse: 0.1375\n",
      "Train Epoch: 9 Iteration: 5295 - LR: [2.980833452677527e-05] dice_loss: 0.5349 depth_loss: 0.0189 loss: 5.3680 rmse: 0.1364\n",
      "Train Epoch: 9 Iteration: 5300 - LR: [2.9363427090684118e-05] dice_loss: 0.5388 depth_loss: 0.0187 loss: 5.4071 rmse: 0.1359\n",
      "Train Epoch: 9 Iteration: 5305 - LR: [2.8921657912987968e-05] dice_loss: 0.5361 depth_loss: 0.0185 loss: 5.3796 rmse: 0.1351\n",
      "Train Epoch: 9 Iteration: 5310 - LR: [2.8483033277230653e-05] dice_loss: 0.5247 depth_loss: 0.0184 loss: 5.2650 rmse: 0.1348\n",
      "Train Epoch: 9 Iteration: 5315 - LR: [2.804755942222911e-05] dice_loss: 0.5212 depth_loss: 0.0180 loss: 5.2299 rmse: 0.1332\n",
      "Train Epoch: 9 Iteration: 5320 - LR: [2.7615242541985095e-05] dice_loss: 0.5144 depth_loss: 0.0179 loss: 5.1622 rmse: 0.1329\n",
      "Train Epoch: 9 Iteration: 5325 - LR: [2.718608878559651e-05] dice_loss: 0.5172 depth_loss: 0.0176 loss: 5.1899 rmse: 0.1317\n",
      "Train Epoch: 9 Iteration: 5330 - LR: [2.6760104257170576e-05] dice_loss: 0.5178 depth_loss: 0.0173 loss: 5.1957 rmse: 0.1308\n",
      "Train Epoch: 9 Iteration: 5335 - LR: [2.6337295015736574e-05] dice_loss: 0.5246 depth_loss: 0.0175 loss: 5.2631 rmse: 0.1314\n",
      "Train Epoch: 9 Iteration: 5340 - LR: [2.5917667075159934e-05] dice_loss: 0.5329 depth_loss: 0.0177 loss: 5.3464 rmse: 0.1319\n",
      "Train Epoch: 9 Iteration: 5345 - LR: [2.550122640405629e-05] dice_loss: 0.5365 depth_loss: 0.0178 loss: 5.3833 rmse: 0.1326\n",
      "Train Epoch: 9 Iteration: 5350 - LR: [2.508797892570721e-05] dice_loss: 0.5410 depth_loss: 0.0176 loss: 5.4278 rmse: 0.1316\n",
      "Train Epoch: 9 Iteration: 5355 - LR: [2.4677930517975306e-05] dice_loss: 0.5442 depth_loss: 0.0174 loss: 5.4591 rmse: 0.1309\n",
      "Validation Results - Epoch: 9  dice_loss: 0.5496 depth_loss: 0.0189 loss: 5.5153 rmse: 0.1362\n",
      "Model saved with loss 5.5153 at epoch 9\n",
      "Train Epoch: 10 Iteration: 5360 - LR: [2.4271087013221235e-05] dice_loss: 0.5423 depth_loss: 0.0172 loss: 5.4403 rmse: 0.1304\n",
      "Train Epoch: 10 Iteration: 5365 - LR: [2.386745419821998e-05] dice_loss: 0.5432 depth_loss: 0.0169 loss: 5.4488 rmse: 0.1293\n",
      "Train Epoch: 10 Iteration: 5370 - LR: [2.346703781407945e-05] dice_loss: 0.5432 depth_loss: 0.0165 loss: 5.4483 rmse: 0.1276\n",
      "Train Epoch: 10 Iteration: 5375 - LR: [2.306984355615803e-05] dice_loss: 0.5358 depth_loss: 0.0165 loss: 5.3746 rmse: 0.1275\n",
      "Train Epoch: 10 Iteration: 5380 - LR: [2.2675877073984145e-05] dice_loss: 0.5282 depth_loss: 0.0163 loss: 5.2980 rmse: 0.1269\n",
      "Train Epoch: 10 Iteration: 5385 - LR: [2.2285143971175487e-05] dice_loss: 0.5296 depth_loss: 0.0163 loss: 5.3125 rmse: 0.1268\n",
      "Train Epoch: 10 Iteration: 5390 - LR: [2.1897649805359525e-05] dice_loss: 0.5267 depth_loss: 0.0166 loss: 5.2836 rmse: 0.1278\n",
      "Train Epoch: 10 Iteration: 5395 - LR: [2.1513400088094435e-05] dice_loss: 0.5273 depth_loss: 0.0165 loss: 5.2891 rmse: 0.1274\n",
      "Train Epoch: 10 Iteration: 5400 - LR: [2.113240028479074e-05] dice_loss: 0.5227 depth_loss: 0.0167 loss: 5.2439 rmse: 0.1283\n",
      "Train Epoch: 10 Iteration: 5405 - LR: [2.0754655814633425e-05] dice_loss: 0.5176 depth_loss: 0.0168 loss: 5.1931 rmse: 0.1285\n",
      "Train Epoch: 10 Iteration: 5410 - LR: [2.038017205050499e-05] dice_loss: 0.5168 depth_loss: 0.0167 loss: 5.1847 rmse: 0.1281\n",
      "Train Epoch: 10 Iteration: 5415 - LR: [2.000895431890901e-05] dice_loss: 0.5198 depth_loss: 0.0169 loss: 5.2151 rmse: 0.1289\n",
      "Train Epoch: 10 Iteration: 5420 - LR: [1.9641007899894332e-05] dice_loss: 0.5204 depth_loss: 0.0168 loss: 5.2206 rmse: 0.1287\n",
      "Train Epoch: 10 Iteration: 5425 - LR: [1.9276338026979982e-05] dice_loss: 0.5280 depth_loss: 0.0169 loss: 5.2972 rmse: 0.1291\n",
      "Train Epoch: 10 Iteration: 5430 - LR: [1.8914949887080738e-05] dice_loss: 0.5337 depth_loss: 0.0171 loss: 5.3540 rmse: 0.1298\n",
      "Train Epoch: 10 Iteration: 5435 - LR: [1.855684862043339e-05] dice_loss: 0.5341 depth_loss: 0.0171 loss: 5.3578 rmse: 0.1298\n",
      "Train Epoch: 10 Iteration: 5440 - LR: [1.8202039320523572e-05] dice_loss: 0.5400 depth_loss: 0.0169 loss: 5.4165 rmse: 0.1290\n",
      "Train Epoch: 10 Iteration: 5445 - LR: [1.785052703401331e-05] dice_loss: 0.5454 depth_loss: 0.0167 loss: 5.4710 rmse: 0.1282\n",
      "Train Epoch: 10 Iteration: 5450 - LR: [1.750231676066922e-05] dice_loss: 0.5453 depth_loss: 0.0166 loss: 5.4696 rmse: 0.1278\n",
      "Train Epoch: 10 Iteration: 5455 - LR: [1.7157413453291584e-05] dice_loss: 0.5479 depth_loss: 0.0166 loss: 5.4958 rmse: 0.1280\n",
      "Train Epoch: 10 Iteration: 5460 - LR: [1.6815822017643672e-05] dice_loss: 0.5466 depth_loss: 0.0163 loss: 5.4820 rmse: 0.1267\n",
      "Train Epoch: 10 Iteration: 5465 - LR: [1.6477547312381985e-05] dice_loss: 0.5475 depth_loss: 0.0163 loss: 5.4910 rmse: 0.1269\n",
      "Train Epoch: 10 Iteration: 5470 - LR: [1.6142594148987224e-05] dice_loss: 0.5448 depth_loss: 0.0160 loss: 5.4642 rmse: 0.1256\n",
      "Train Epoch: 10 Iteration: 5475 - LR: [1.5810967291696014e-05] dice_loss: 0.5465 depth_loss: 0.0159 loss: 5.4805 rmse: 0.1254\n",
      "Train Epoch: 10 Iteration: 5480 - LR: [1.548267145743274e-05] dice_loss: 0.5492 depth_loss: 0.0161 loss: 5.5076 rmse: 0.1261\n",
      "Train Epoch: 10 Iteration: 5485 - LR: [1.5157711315742824e-05] dice_loss: 0.5508 depth_loss: 0.0165 loss: 5.5242 rmse: 0.1275\n",
      "Train Epoch: 10 Iteration: 5490 - LR: [1.4836091488726076e-05] dice_loss: 0.5458 depth_loss: 0.0164 loss: 5.4747 rmse: 0.1273\n",
      "Train Epoch: 10 Iteration: 5495 - LR: [1.451781655097111e-05] dice_loss: 0.5430 depth_loss: 0.0165 loss: 5.4470 rmse: 0.1279\n",
      "Train Epoch: 10 Iteration: 5500 - LR: [1.4202891029490156e-05] dice_loss: 0.5474 depth_loss: 0.0170 loss: 5.4905 rmse: 0.1294\n",
      "Train Epoch: 10 Iteration: 5505 - LR: [1.3891319403654765e-05] dice_loss: 0.5487 depth_loss: 0.0168 loss: 5.5042 rmse: 0.1288\n",
      "Train Epoch: 10 Iteration: 5510 - LR: [1.3583106105131978e-05] dice_loss: 0.5555 depth_loss: 0.0172 loss: 5.5723 rmse: 0.1301\n",
      "Train Epoch: 10 Iteration: 5515 - LR: [1.327825551782149e-05] dice_loss: 0.5612 depth_loss: 0.0173 loss: 5.6296 rmse: 0.1307\n",
      "Train Epoch: 10 Iteration: 5520 - LR: [1.2976771977793028e-05] dice_loss: 0.5668 depth_loss: 0.0176 loss: 5.6856 rmse: 0.1317\n",
      "Train Epoch: 10 Iteration: 5525 - LR: [1.2678659773224906e-05] dice_loss: 0.5609 depth_loss: 0.0179 loss: 5.6265 rmse: 0.1327\n",
      "Train Epoch: 10 Iteration: 5530 - LR: [1.2383923144342903e-05] dice_loss: 0.5592 depth_loss: 0.0180 loss: 5.6104 rmse: 0.1333\n",
      "Train Epoch: 10 Iteration: 5535 - LR: [1.2092566283360063e-05] dice_loss: 0.5543 depth_loss: 0.0181 loss: 5.5614 rmse: 0.1336\n",
      "Train Epoch: 10 Iteration: 5540 - LR: [1.1804593334416884e-05] dice_loss: 0.5503 depth_loss: 0.0181 loss: 5.5210 rmse: 0.1336\n",
      "Train Epoch: 10 Iteration: 5545 - LR: [1.1520008393522554e-05] dice_loss: 0.5463 depth_loss: 0.0180 loss: 5.4813 rmse: 0.1334\n",
      "Train Epoch: 10 Iteration: 5550 - LR: [1.12388155084967e-05] dice_loss: 0.5442 depth_loss: 0.0177 loss: 5.4594 rmse: 0.1322\n",
      "Train Epoch: 10 Iteration: 5555 - LR: [1.0961018678911566e-05] dice_loss: 0.5401 depth_loss: 0.0173 loss: 5.4183 rmse: 0.1305\n",
      "Train Epoch: 10 Iteration: 5560 - LR: [1.0686621856035425e-05] dice_loss: 0.5383 depth_loss: 0.0170 loss: 5.4003 rmse: 0.1296\n",
      "Train Epoch: 10 Iteration: 5565 - LR: [1.041562894277615e-05] dice_loss: 0.5323 depth_loss: 0.0168 loss: 5.3402 rmse: 0.1286\n",
      "Train Epoch: 10 Iteration: 5570 - LR: [1.0148043793625952e-05] dice_loss: 0.5267 depth_loss: 0.0164 loss: 5.2839 rmse: 0.1272\n",
      "Train Epoch: 10 Iteration: 5575 - LR: [9.883870214606205e-06] dice_loss: 0.5233 depth_loss: 0.0164 loss: 5.2492 rmse: 0.1271\n",
      "Train Epoch: 10 Iteration: 5580 - LR: [9.623111963213737e-06] dice_loss: 0.5201 depth_loss: 0.0166 loss: 5.2173 rmse: 0.1280\n",
      "Train Epoch: 10 Iteration: 5585 - LR: [9.365772748366871e-06] dice_loss: 0.5175 depth_loss: 0.0166 loss: 5.1916 rmse: 0.1279\n",
      "Train Epoch: 10 Iteration: 5590 - LR: [9.111856230353222e-06] dice_loss: 0.5247 depth_loss: 0.0168 loss: 5.2637 rmse: 0.1288\n",
      "Train Epoch: 10 Iteration: 5595 - LR: [8.861366020777208e-06] dice_loss: 0.5324 depth_loss: 0.0170 loss: 5.3406 rmse: 0.1297\n",
      "Train Epoch: 10 Iteration: 5600 - LR: [8.614305682508956e-06] dice_loss: 0.5375 depth_loss: 0.0177 loss: 5.3931 rmse: 0.1321\n",
      "Train Epoch: 10 Iteration: 5605 - LR: [8.370678729633363e-06] dice_loss: 0.5363 depth_loss: 0.0183 loss: 5.3811 rmse: 0.1341\n",
      "Train Epoch: 10 Iteration: 5610 - LR: [8.130488627400396e-06] dice_loss: 0.5456 depth_loss: 0.0189 loss: 5.4750 rmse: 0.1363\n",
      "Train Epoch: 10 Iteration: 5615 - LR: [7.89373879217554e-06] dice_loss: 0.5512 depth_loss: 0.0191 loss: 5.5307 rmse: 0.1370\n",
      "Train Epoch: 10 Iteration: 5620 - LR: [7.660432591391537e-06] dice_loss: 0.5625 depth_loss: 0.0192 loss: 5.6446 rmse: 0.1375\n",
      "Train Epoch: 10 Iteration: 5625 - LR: [7.430573343500032e-06] dice_loss: 0.5650 depth_loss: 0.0194 loss: 5.6696 rmse: 0.1379\n",
      "Train Epoch: 10 Iteration: 5630 - LR: [7.204164317924808e-06] dice_loss: 0.5657 depth_loss: 0.0193 loss: 5.6762 rmse: 0.1377\n",
      "Train Epoch: 10 Iteration: 5635 - LR: [6.98120873501496e-06] dice_loss: 0.5689 depth_loss: 0.0197 loss: 5.7091 rmse: 0.1391\n",
      "Train Epoch: 10 Iteration: 5640 - LR: [6.761709765999461e-06] dice_loss: 0.5699 depth_loss: 0.0194 loss: 5.7187 rmse: 0.1383\n",
      "Train Epoch: 10 Iteration: 5645 - LR: [6.545670532941477e-06] dice_loss: 0.5714 depth_loss: 0.0194 loss: 5.7336 rmse: 0.1383\n",
      "Train Epoch: 10 Iteration: 5650 - LR: [6.33309410869465e-06] dice_loss: 0.5686 depth_loss: 0.0194 loss: 5.7057 rmse: 0.1385\n",
      "Train Epoch: 10 Iteration: 5655 - LR: [6.1239835168588305e-06] dice_loss: 0.5676 depth_loss: 0.0195 loss: 5.6955 rmse: 0.1386\n",
      "Train Epoch: 10 Iteration: 5660 - LR: [5.918341731737386e-06] dice_loss: 0.5686 depth_loss: 0.0195 loss: 5.7054 rmse: 0.1390\n",
      "Train Epoch: 10 Iteration: 5665 - LR: [5.716171678294768e-06] dice_loss: 0.5648 depth_loss: 0.0195 loss: 5.6677 rmse: 0.1388\n",
      "Train Epoch: 10 Iteration: 5670 - LR: [5.51747623211493e-06] dice_loss: 0.5619 depth_loss: 0.0195 loss: 5.6387 rmse: 0.1388\n",
      "Train Epoch: 10 Iteration: 5675 - LR: [5.3222582193604456e-06] dice_loss: 0.5647 depth_loss: 0.0194 loss: 5.6664 rmse: 0.1386\n",
      "Train Epoch: 10 Iteration: 5680 - LR: [5.130520416732346e-06] dice_loss: 0.5565 depth_loss: 0.0195 loss: 5.5842 rmse: 0.1389\n",
      "Train Epoch: 10 Iteration: 5685 - LR: [4.94226555143054e-06] dice_loss: 0.5572 depth_loss: 0.0189 loss: 5.5906 rmse: 0.1367\n",
      "Train Epoch: 10 Iteration: 5690 - LR: [4.757496301115014e-06] dice_loss: 0.5486 depth_loss: 0.0188 loss: 5.5052 rmse: 0.1362\n",
      "Train Epoch: 10 Iteration: 5695 - LR: [4.5762152938678604e-06] dice_loss: 0.5473 depth_loss: 0.0186 loss: 5.4917 rmse: 0.1355\n",
      "Train Epoch: 10 Iteration: 5700 - LR: [4.3984251081558356e-06] dice_loss: 0.5424 depth_loss: 0.0185 loss: 5.4426 rmse: 0.1353\n",
      "Train Epoch: 10 Iteration: 5705 - LR: [4.224128272793668e-06] dice_loss: 0.5433 depth_loss: 0.0184 loss: 5.4513 rmse: 0.1348\n",
      "Train Epoch: 10 Iteration: 5710 - LR: [4.053327266908087e-06] dice_loss: 0.5398 depth_loss: 0.0183 loss: 5.4159 rmse: 0.1344\n",
      "Train Epoch: 10 Iteration: 5715 - LR: [3.886024519902574e-06] dice_loss: 0.5323 depth_loss: 0.0183 loss: 5.3416 rmse: 0.1345\n",
      "Train Epoch: 10 Iteration: 5720 - LR: [3.722222411422861e-06] dice_loss: 0.5353 depth_loss: 0.0181 loss: 5.3709 rmse: 0.1338\n",
      "Train Epoch: 10 Iteration: 5725 - LR: [3.5619232713229304e-06] dice_loss: 0.5351 depth_loss: 0.0179 loss: 5.3687 rmse: 0.1331\n",
      "Train Epoch: 10 Iteration: 5730 - LR: [3.405129379632098e-06] dice_loss: 0.5309 depth_loss: 0.0178 loss: 5.3269 rmse: 0.1326\n",
      "Train Epoch: 10 Iteration: 5735 - LR: [3.2518429665223997e-06] dice_loss: 0.5329 depth_loss: 0.0176 loss: 5.3466 rmse: 0.1319\n",
      "Train Epoch: 10 Iteration: 5740 - LR: [3.1020662122768944e-06] dice_loss: 0.5317 depth_loss: 0.0175 loss: 5.3348 rmse: 0.1317\n",
      "Train Epoch: 10 Iteration: 5745 - LR: [2.9558012472588292e-06] dice_loss: 0.5351 depth_loss: 0.0178 loss: 5.3690 rmse: 0.1329\n",
      "Train Epoch: 10 Iteration: 5750 - LR: [2.813050151881023e-06] dice_loss: 0.5339 depth_loss: 0.0177 loss: 5.3566 rmse: 0.1326\n",
      "Train Epoch: 10 Iteration: 5755 - LR: [2.6738149565765857e-06] dice_loss: 0.5321 depth_loss: 0.0178 loss: 5.3391 rmse: 0.1325\n",
      "Train Epoch: 10 Iteration: 5760 - LR: [2.538097641769858e-06] dice_loss: 0.5280 depth_loss: 0.0181 loss: 5.2979 rmse: 0.1336\n",
      "Train Epoch: 10 Iteration: 5765 - LR: [2.405900137848296e-06] dice_loss: 0.5253 depth_loss: 0.0188 loss: 5.2715 rmse: 0.1360\n",
      "Train Epoch: 10 Iteration: 5770 - LR: [2.2772243251349637e-06] dice_loss: 0.5191 depth_loss: 0.0198 loss: 5.2112 rmse: 0.1393\n",
      "Train Epoch: 10 Iteration: 5775 - LR: [2.1520720338619447e-06] dice_loss: 0.5158 depth_loss: 0.0197 loss: 5.1780 rmse: 0.1392\n",
      "Train Epoch: 10 Iteration: 5780 - LR: [2.030445044144057e-06] dice_loss: 0.5192 depth_loss: 0.0193 loss: 5.2111 rmse: 0.1378\n",
      "Train Epoch: 10 Iteration: 5785 - LR: [1.9123450859538187e-06] dice_loss: 0.5200 depth_loss: 0.0192 loss: 5.2195 rmse: 0.1371\n",
      "Train Epoch: 10 Iteration: 5790 - LR: [1.7977738390965493e-06] dice_loss: 0.5200 depth_loss: 0.0191 loss: 5.2186 rmse: 0.1370\n",
      "Train Epoch: 10 Iteration: 5795 - LR: [1.68673293318678e-06] dice_loss: 0.5202 depth_loss: 0.0188 loss: 5.2206 rmse: 0.1358\n",
      "Train Epoch: 10 Iteration: 5800 - LR: [1.579223947624771e-06] dice_loss: 0.5252 depth_loss: 0.0185 loss: 5.2706 rmse: 0.1348\n",
      "Train Epoch: 10 Iteration: 5805 - LR: [1.4752484115743074e-06] dice_loss: 0.5265 depth_loss: 0.0184 loss: 5.2835 rmse: 0.1347\n",
      "Train Epoch: 10 Iteration: 5810 - LR: [1.3748078039407722e-06] dice_loss: 0.5295 depth_loss: 0.0181 loss: 5.3128 rmse: 0.1336\n",
      "Train Epoch: 10 Iteration: 5815 - LR: [1.277903553350192e-06] dice_loss: 0.5347 depth_loss: 0.0183 loss: 5.3650 rmse: 0.1343\n",
      "Train Epoch: 10 Iteration: 5820 - LR: [1.1845370381288904e-06] dice_loss: 0.5382 depth_loss: 0.0189 loss: 5.4012 rmse: 0.1365\n",
      "Train Epoch: 10 Iteration: 5825 - LR: [1.0947095862839783e-06] dice_loss: 0.5453 depth_loss: 0.0187 loss: 5.4716 rmse: 0.1358\n",
      "Train Epoch: 10 Iteration: 5830 - LR: [1.008422475484228e-06] dice_loss: 0.5409 depth_loss: 0.0185 loss: 5.4275 rmse: 0.1349\n",
      "Train Epoch: 10 Iteration: 5835 - LR: [9.256769330422003e-07] dice_loss: 0.5360 depth_loss: 0.0182 loss: 5.3780 rmse: 0.1340\n",
      "Train Epoch: 10 Iteration: 5840 - LR: [8.464741358965078e-07] dice_loss: 0.5321 depth_loss: 0.0181 loss: 5.3387 rmse: 0.1337\n",
      "Train Epoch: 10 Iteration: 5845 - LR: [7.708152105953287e-07] dice_loss: 0.5316 depth_loss: 0.0189 loss: 5.3345 rmse: 0.1362\n",
      "Train Epoch: 10 Iteration: 5850 - LR: [6.9870123328017e-07] dice_loss: 0.5377 depth_loss: 0.0193 loss: 5.3965 rmse: 0.1378\n",
      "Train Epoch: 10 Iteration: 5855 - LR: [6.301332296706847e-07] dice_loss: 0.5374 depth_loss: 0.0191 loss: 5.3929 rmse: 0.1368\n",
      "Train Epoch: 10 Iteration: 5860 - LR: [5.651121750500448e-07] dice_loss: 0.5379 depth_loss: 0.0189 loss: 5.3978 rmse: 0.1361\n",
      "Train Epoch: 10 Iteration: 5865 - LR: [5.036389942511196e-07] dice_loss: 0.5385 depth_loss: 0.0192 loss: 5.4038 rmse: 0.1374\n",
      "Train Epoch: 10 Iteration: 5870 - LR: [4.457145616431801e-07] dice_loss: 0.5304 depth_loss: 0.0191 loss: 5.3227 rmse: 0.1371\n",
      "Train Epoch: 10 Iteration: 5875 - LR: [3.913397011196594e-07] dice_loss: 0.5290 depth_loss: 0.0188 loss: 5.3083 rmse: 0.1359\n",
      "Train Epoch: 10 Iteration: 5880 - LR: [3.405151860862453e-07] dice_loss: 0.5317 depth_loss: 0.0185 loss: 5.3356 rmse: 0.1349\n",
      "Train Epoch: 10 Iteration: 5885 - LR: [2.932417394500281e-07] dice_loss: 0.5356 depth_loss: 0.0188 loss: 5.3745 rmse: 0.1362\n",
      "Train Epoch: 10 Iteration: 5890 - LR: [2.495200336091198e-07] dice_loss: 0.5351 depth_loss: 0.0184 loss: 5.3693 rmse: 0.1345\n",
      "Train Epoch: 10 Iteration: 5895 - LR: [2.0935069044316207e-07] dice_loss: 0.5387 depth_loss: 0.0182 loss: 5.4049 rmse: 0.1338\n",
      "Train Epoch: 10 Iteration: 5900 - LR: [1.727342813044165e-07] dice_loss: 0.5356 depth_loss: 0.0180 loss: 5.3736 rmse: 0.1330\n",
      "Train Epoch: 10 Iteration: 5905 - LR: [1.3967132700968778e-07] dice_loss: 0.5243 depth_loss: 0.0178 loss: 5.2608 rmse: 0.1326\n",
      "Train Epoch: 10 Iteration: 5910 - LR: [1.1016229783285762e-07] dice_loss: 0.5210 depth_loss: 0.0174 loss: 5.2275 rmse: 0.1308\n",
      "Train Epoch: 10 Iteration: 5915 - LR: [8.420761349827881e-08] dice_loss: 0.5142 depth_loss: 0.0173 loss: 5.1598 rmse: 0.1304\n",
      "Train Epoch: 10 Iteration: 5920 - LR: [6.180764317472461e-08] dice_loss: 0.5171 depth_loss: 0.0170 loss: 5.1884 rmse: 0.1293\n",
      "Train Epoch: 10 Iteration: 5925 - LR: [4.2962705470226166e-08] dice_loss: 0.5179 depth_loss: 0.0168 loss: 5.1957 rmse: 0.1285\n",
      "Train Epoch: 10 Iteration: 5930 - LR: [2.7673068427437426e-08] dice_loss: 0.5245 depth_loss: 0.0170 loss: 5.2624 rmse: 0.1292\n",
      "Train Epoch: 10 Iteration: 5935 - LR: [1.5938949519888086e-08] dice_loss: 0.5329 depth_loss: 0.0172 loss: 5.3459 rmse: 0.1298\n",
      "Train Epoch: 10 Iteration: 5940 - LR: [7.76051564884725e-09] dice_loss: 0.5367 depth_loss: 0.0174 loss: 5.3841 rmse: 0.1307\n",
      "Train Epoch: 10 Iteration: 5945 - LR: [3.13788314104749e-09] dice_loss: 0.5412 depth_loss: 0.0171 loss: 5.4291 rmse: 0.1298\n",
      "Train Epoch: 10 Iteration: 5950 - LR: [2.0711177468807204e-09] dice_loss: 0.5446 depth_loss: 0.0169 loss: 5.4628 rmse: 0.1293\n",
      "Validation Results - Epoch: 10  dice_loss: 0.5493 depth_loss: 0.0186 loss: 5.5117 rmse: 0.1352\n",
      "Model saved with loss 5.5117 at epoch 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 5950\n",
       "\tepoch: 10\n",
       "\tepoch_length: 595\n",
       "\tmax_epochs: 10\n",
       "\toutput: <class 'dict'>\n",
       "\tbatch: <class 'dict'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import RunningAverage\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assume these are defined elsewhere:\n",
    "# model, optimizer, scheduler, train_loader, valid_loader, DEVICE, \n",
    "# BATCH_SIZE, NUM_CLASSES, epochs, model_root,\n",
    "# threshold_accuracy\n",
    "\n",
    "# Import Dice loss for multiclass segmentation and define MSE loss for depth.\n",
    "dice_criterion = smp.losses.DiceLoss(mode='multiclass', classes=NUM_CLASSES, smooth=1e-5)\n",
    "depth_criterion = nn.MSELoss()\n",
    "\n",
    "# ---------------------------\n",
    "# Define the training step\n",
    "# ---------------------------\n",
    "def train_step(engine, batch):\n",
    "    model.train()\n",
    "    # Unpack batch and send data to the device\n",
    "    left = batch['left'].to(device)\n",
    "    mask = batch['mask'].squeeze(1).to(device)  # Expected shape: [B, H, W]\n",
    "    depth = batch['depth'].squeeze(1).to(device)  # Expected shape: [B, H, W]\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(left)\n",
    "    pred_depth, pred_seg = output[1], output[0]\n",
    "    # print(\"Unique mask values:\", torch.unique(mask))\n",
    "    # print(\"Unique predseg values:\", torch.unique(pred_seg))\n",
    "    # print(\"pred_seg.shape:\", pred_seg.shape)  # (B, 21, H, W)\n",
    "    # print(\"mask.shape:\", mask.shape)          # (B, H, W)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Reshape segmentation output and target mask for loss computation\n",
    "    # pred_seg: [B, NUM_CLASSES, H, W] --> [B, NUM_CLASSES, H*W]\n",
    "    # pred_seg = pred_seg.reshape(BATCH_SIZE, NUM_CLASSES, -1)\n",
    "    # mask: [B, H, W] --> [B, H*W]\n",
    "    # mask = mask.view(BATCH_SIZE, -1).to(torch.long)\n",
    "    # Fix shape: [B, T, H, W] -> [B, H, W]\n",
    "    # print(mask.shape)\n",
    "    # mask = mask[:, 0, :, :]\n",
    "    # if pred_seg.shape[2:] != mask.shape[2:]:\n",
    "    #     pred_seg = F.interpolate(pred_seg, size=mask.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "    # if pred_seg.size(0) != mask.size(0):\n",
    "    #     print(f\"Skipping batch due to batch size mismatch: pred_seg {pred_seg.shape}, mask {mask.shape}\")\n",
    "    #     return 0.0\n",
    "\n",
    "    # print(\"mask\", mask.shape)\n",
    "    # print(\"pred_seg\", pred_seg.shape)\n",
    "    # print(\"Unique mask values:\", torch.unique(mask))\n",
    "    # print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "\n",
    "\n",
    "    # Compute Dice loss for segmentation and MSE loss for depth\n",
    "    dice_loss_val = dice_criterion(pred_seg, mask)\n",
    "    depth = depth.unsqueeze(1)\n",
    "    depth_loss = depth_criterion(pred_depth, depth)\n",
    "    loss = 10.0 * dice_loss_val + depth_loss\n",
    "    rmse = torch.sqrt(depth_loss)\n",
    "    accuracies = threshold_accuracy(pred_depth, depth)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if engine.state.iteration <= scheduler.total_steps:\n",
    "        scheduler.step()\n",
    "\n",
    "    # Return scalar values for logging\n",
    "    return {\n",
    "        \"dice_loss\": dice_loss_val.item(),\n",
    "        \"depth_loss\": depth_loss.item(),\n",
    "        \"loss\": loss.item(),\n",
    "        \"rmse\": rmse.item(),\n",
    "        \"accuracy_1\": accuracies[\"accuracy_1\"],\n",
    "        \"accuracy_2\": accuracies[\"accuracy_2\"],\n",
    "        \"accuracy_3\": accuracies[\"accuracy_3\"]\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Define the validation step\n",
    "# ---------------------------\n",
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        left = batch['left'].to(device)\n",
    "        mask = batch['mask'].squeeze(1).to(device)\n",
    "        depth = batch['depth'].squeeze(1).to(device)\n",
    "        \n",
    "        output = model(left)\n",
    "        pred_depth, pred_seg = output[1], output[0]\n",
    "        # pred_seg = pred_seg.reshape(BATCH_SIZE, NUM_CLASSES, -1)\n",
    "        # mask = mask[:, 0, :, :]\n",
    "        # if pred_seg.shape[2:] != mask.shape[2:]:\n",
    "        #     pred_seg = F.interpolate(pred_seg, size=mask.shape[2:], mode='bilinear', align_corners=False)\n",
    "    \n",
    "        # if pred_seg.size(0) != mask.size(0):\n",
    "        #     print(f\"Skipping batch due to batch size mismatch: pred_seg {pred_seg.shape}, mask {mask.shape}\")\n",
    "        #     return 0.0\n",
    "\n",
    "        # print(\"mask\", mask.shape)\n",
    "        # print(\"pred_seg\", pred_seg.shape)\n",
    "\n",
    "\n",
    "        dice_loss_val = dice_criterion(pred_seg, mask)\n",
    "        depth = depth.unsqueeze(1)\n",
    "        depth_loss = depth_criterion(pred_depth, depth)\n",
    "        loss = 10.0 * dice_loss_val + depth_loss\n",
    "        rmse = torch.sqrt(depth_loss)\n",
    "        accuracies = threshold_accuracy(pred_depth, depth)\n",
    "\n",
    "    return {\n",
    "        \"dice_loss\": dice_loss_val.item(),\n",
    "        \"depth_loss\": depth_loss.item(),\n",
    "        \"loss\": loss.item(),\n",
    "        \"rmse\": rmse.item(),\n",
    "        \"accuracy_1\": accuracies[\"accuracy_1\"],\n",
    "        \"accuracy_2\": accuracies[\"accuracy_2\"],\n",
    "        \"accuracy_3\": accuracies[\"accuracy_3\"]\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# Create Ignite Engines\n",
    "# ---------------------------\n",
    "trainer = Engine(train_step)\n",
    "evaluator = Engine(validation_step)\n",
    "\n",
    "# Define metric keys to track\n",
    "metric_keys = [\"dice_loss\", \"depth_loss\", \"loss\", \"rmse\", \"accuracy_1\", \"accuracy_2\", \"accuracy_3\"]\n",
    "\n",
    "# Attach RunningAverage metrics to the trainer\n",
    "for key in metric_keys:\n",
    "    RunningAverage(output_transform=lambda output, key=key: output[key]).attach(trainer, key)\n",
    "\n",
    "# Attach RunningAverage metrics to the evaluator\n",
    "for key in metric_keys:\n",
    "    RunningAverage(output_transform=lambda output, key=key: output[key]).attach(evaluator, key)\n",
    "\n",
    "# TensorBoard writer for logging\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Track best validation loss for model checkpointing\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=5))\n",
    "def log_training_metrics(engine):\n",
    "    metrics = engine.state.metrics\n",
    "    lr = scheduler.get_last_lr()  # Assumes scheduler supports get_last_lr()\n",
    "    print(\n",
    "        f\"Train Epoch: {engine.state.epoch} Iteration: {engine.state.iteration} - LR: {lr} \"\n",
    "        f\"dice_loss: {metrics['dice_loss']:.4f} depth_loss: {metrics['depth_loss']:.4f} \"\n",
    "        f\"loss: {metrics['loss']:.4f} rmse: {metrics['rmse']:.4f}\"\n",
    "    )\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def run_validation_and_log(engine):\n",
    "    evaluator.run(valid_loader)\n",
    "    train_metrics = engine.state.metrics\n",
    "    valid_metrics = evaluator.state.metrics\n",
    "    epoch = engine.state.epoch\n",
    "\n",
    "    print(\n",
    "        f\"Validation Results - Epoch: {epoch}  dice_loss: {valid_metrics['dice_loss']:.4f} \"\n",
    "        f\"depth_loss: {valid_metrics['depth_loss']:.4f} loss: {valid_metrics['loss']:.4f} \"\n",
    "        f\"rmse: {valid_metrics['rmse']:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    for key in metric_keys:\n",
    "        writer.add_scalar(f'Training/{key}', train_metrics[key], epoch)\n",
    "        writer.add_scalar(f'Validation/{key}', valid_metrics[key], epoch)\n",
    "\n",
    "    # Save the best model based on validation loss\n",
    "    global best_valid_loss\n",
    "    if valid_metrics[\"loss\"] < best_valid_loss:\n",
    "        best_valid_loss = valid_metrics[\"loss\"]\n",
    "        model_savepath = os.path.join(model_root, f\"best_model_{epoch}.pth\")\n",
    "        torch.save(model.state_dict(), model_savepath)\n",
    "        print(f\"Model saved with loss {valid_metrics['loss']:.4f} at epoch {epoch}\")\n",
    "    elif epoch %10 == 0:\n",
    "        best_valid_loss = valid_metrics[\"loss\"]\n",
    "        model_savepath = os.path.join(model_root, f\"model_{epoch}.pth\")\n",
    "        torch.save(model.state_dict(), model_savepath)\n",
    "        print(f\"Model saved with loss {valid_metrics['loss']:.4f} at epoch {epoch}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Run the training loop\n",
    "# ---------------------------\n",
    "# trainer.run(train_loader, max_epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1767359,
     "sourceId": 2885120,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6323835,
     "sourceId": 10228287,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6324415,
     "sourceId": 10229050,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 23316.045459,
   "end_time": "2025-05-19T13:46:13.890865",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-19T07:17:37.845406",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06c397fdbaf144fda54b4ef98b88c357": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0b550fbd96ea406e9a8719c07cd1aa68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "TextView",
       "continuous_update": true,
       "description": "Crop Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_f203557d61464543988ece758cca6f05",
       "placeholder": "​",
       "style": "IPY_MODEL_70955e758ba7472b99f28ea13244afd7",
       "tabbable": null,
       "tooltip": null,
       "value": "128,128"
      }
     },
     "0b77b71f2293418ab131c528be4d38d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0c7caa472def43959b84d228486d8aea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e24e805122e4fb2a29cd36e1bda4ee1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f639df3b62e4b6c876f45d2c32314e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "FloatTextView",
       "continuous_update": false,
       "description": "Lambda Loss:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_0c7caa472def43959b84d228486d8aea",
       "step": null,
       "style": "IPY_MODEL_5c52c3b7125b4f86b40890fa0b9927c7",
       "tabbable": null,
       "tooltip": null,
       "value": 0.04
      }
     },
     "3a0a4144b8e54f9088f28d52b2526379": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4937d7de0ac0468b84068fee3cc4e47d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "52fd47a60e014c54bd51532ce5ce7779": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5687db1f682743ebace84d6c668ff4bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Version:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_0e24e805122e4fb2a29cd36e1bda4ee1",
       "step": 1,
       "style": "IPY_MODEL_4937d7de0ac0468b84068fee3cc4e47d",
       "tabbable": null,
       "tooltip": null,
       "value": 0
      }
     },
     "56b3f04989c5428eab5f922e601ca787": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5c52c3b7125b4f86b40890fa0b9927c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6c92c2a97ae042fba960a9d022d1f4fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DropdownModel",
       "_options_labels": [
        "indoor",
        "outdoor",
        "reside",
        "nh"
       ],
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "DropdownView",
       "description": "Category:",
       "description_allow_html": false,
       "disabled": false,
       "index": 2,
       "layout": "IPY_MODEL_c2f03c3bf9224c4cbba66bbaec5396e7",
       "style": "IPY_MODEL_52fd47a60e014c54bd51532ce5ce7779",
       "tabbable": null,
       "tooltip": null
      }
     },
     "70955e758ba7472b99f28ea13244afd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7a42a488b2404081b251f0ebcb787645": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Growth Rate:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_fef7f5c0df614a31b70c71e62a071eaa",
       "step": 1,
       "style": "IPY_MODEL_ed83102fef9a47529dd892c6242539cd",
       "tabbable": null,
       "tooltip": null,
       "value": 16
      }
     },
     "9c15eb091c6f4ccabdca5b4b4ef3db45": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "af9fe33bf9d748d9a9b619ee34fcaa44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b6fd04b70e3346d99dbeac8d432429ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b97d9e6729d345e1b0c315d788b2fe1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Val Batch Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_9c15eb091c6f4ccabdca5b4b4ef3db45",
       "step": 1,
       "style": "IPY_MODEL_0b77b71f2293418ab131c528be4d38d9",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "c2f03c3bf9224c4cbba66bbaec5396e7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c3fbadd8f3604440a8649d44723a1bb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DropdownModel",
       "_options_labels": [
        "local",
        "kaggle"
       ],
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "DropdownView",
       "description": "Execution Env:",
       "description_allow_html": false,
       "disabled": false,
       "index": 1,
       "layout": "IPY_MODEL_c5bb6180c09245e3ad19bd7bd9be67fb",
       "style": "IPY_MODEL_af9fe33bf9d748d9a9b619ee34fcaa44",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c5bb6180c09245e3ad19bd7bd9be67fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c81919ff853a4a7faab34fa2c9deb5d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "FloatTextView",
       "continuous_update": false,
       "description": "Learning Rate:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_06c397fdbaf144fda54b4ef98b88c357",
       "step": null,
       "style": "IPY_MODEL_56b3f04989c5428eab5f922e601ca787",
       "tabbable": null,
       "tooltip": null,
       "value": 0.0001
      }
     },
     "ed83102fef9a47529dd892c6242539cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f1d78706df444077a44af44eaedfaf14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Train Batch Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_3a0a4144b8e54f9088f28d52b2526379",
       "step": 1,
       "style": "IPY_MODEL_b6fd04b70e3346d99dbeac8d432429ef",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "f203557d61464543988ece758cca6f05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fef7f5c0df614a31b70c71e62a071eaa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
