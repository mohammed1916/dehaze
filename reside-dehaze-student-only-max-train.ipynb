{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66edd4c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:41:55.315917Z",
     "iopub.status.busy": "2025-05-02T06:41:55.315294Z",
     "iopub.status.idle": "2025-05-02T06:41:55.484879Z",
     "shell.execute_reply": "2025-05-02T06:41:55.484140Z"
    },
    "papermill": {
     "duration": 0.19014,
     "end_time": "2025-05-02T06:41:55.486508",
     "exception": false,
     "start_time": "2025-05-02T06:41:55.296368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  2 06:41:55 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   33C    P0             27W /  250W |       0MiB /  16384MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from IPython import get_ipython\n",
    "\n",
    "if shutil.which(\"nvidia-smi\") is not None:\n",
    "    get_ipython().system(\"nvidia-smi\")\n",
    "else:\n",
    "    print(\"No NVIDIA GPU or driver detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1b3116",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:41:55.521517Z",
     "iopub.status.busy": "2025-05-02T06:41:55.520791Z",
     "iopub.status.idle": "2025-05-02T06:41:59.353244Z",
     "shell.execute_reply": "2025-05-02T06:41:59.352375Z"
    },
    "papermill": {
     "duration": 3.851252,
     "end_time": "2025-05-02T06:41:59.354524",
     "exception": false,
     "start_time": "2025-05-02T06:41:55.503272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c28948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:41:59.388545Z",
     "iopub.status.busy": "2025-05-02T06:41:59.388170Z",
     "iopub.status.idle": "2025-05-02T06:41:59.411017Z",
     "shell.execute_reply": "2025-05-02T06:41:59.410173Z"
    },
    "papermill": {
     "duration": 0.040988,
     "end_time": "2025-05-02T06:41:59.412184",
     "exception": false,
     "start_time": "2025-05-02T06:41:59.371196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Device IDs: [0]\n"
     ]
    }
   ],
   "source": [
    "# --- Device Setup --- #\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_ids = list(range(torch.cuda.device_count()))\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Device IDs: {device_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4bd412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:41:59.446169Z",
     "iopub.status.busy": "2025-05-02T06:41:59.445578Z",
     "iopub.status.idle": "2025-05-02T06:41:59.455096Z",
     "shell.execute_reply": "2025-05-02T06:41:59.454321Z"
    },
    "papermill": {
     "duration": 0.027636,
     "end_time": "2025-05-02T06:41:59.456157",
     "exception": false,
     "start_time": "2025-05-02T06:41:59.428521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "Current GPU: 0\n",
      "GPU Name: Tesla P100-PCIE-16GB\n",
      "GPU Memory Allocated: 0 bytes\n",
      "GPU Memory Cached: 0 bytes\n",
      "GPU Memory Allocated (Total): 0 bytes\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(device)}\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "    print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(device)} bytes\")\n",
    "    print(f\"GPU Memory Allocated (Total): {torch.cuda.memory_allocated()} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bea1a0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:41:59.490037Z",
     "iopub.status.busy": "2025-05-02T06:41:59.489487Z",
     "iopub.status.idle": "2025-05-02T06:42:09.347514Z",
     "shell.execute_reply": "2025-05-02T06:42:09.346847Z"
    },
    "papermill": {
     "duration": 9.876186,
     "end_time": "2025-05-02T06:42:09.348888",
     "exception": false,
     "start_time": "2025-05-02T06:41:59.472702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "from timm.layers import to_2tuple, trunc_normal_\n",
    "import os\n",
    "import torchvision.utils as utils\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from random import randrange\n",
    "import time\n",
    "from math import log10\n",
    "from skimage import measure\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.nn.init import trunc_normal_\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a748d042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:09.382940Z",
     "iopub.status.busy": "2025-05-02T06:42:09.382130Z",
     "iopub.status.idle": "2025-05-02T06:42:09.385784Z",
     "shell.execute_reply": "2025-05-02T06:42:09.385244Z"
    },
    "papermill": {
     "duration": 0.021529,
     "end_time": "2025-05-02T06:42:09.386844",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.365315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d66cd3",
   "metadata": {
    "papermill": {
     "duration": 0.015825,
     "end_time": "2025-05-02T06:42:09.419219",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.403394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ebc75",
   "metadata": {
    "papermill": {
     "duration": 0.016022,
     "end_time": "2025-05-02T06:42:09.508063",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.492041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RevisedLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057b1ced",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:09.541497Z",
     "iopub.status.busy": "2025-05-02T06:42:09.540876Z",
     "iopub.status.idle": "2025-05-02T06:42:09.547891Z",
     "shell.execute_reply": "2025-05-02T06:42:09.547342Z"
    },
    "papermill": {
     "duration": 0.02478,
     "end_time": "2025-05-02T06:42:09.548949",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.524169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RevisedLayerNorm(nn.Module):\n",
    "    \"\"\"Revised LayerNorm\"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n",
    "        super(RevisedLayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.detach_gradient = detach_gradient\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n",
    "        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n",
    "\n",
    "        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "\n",
    "        trunc_normal_(self.scale_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.scale_mlp.bias, 1)\n",
    "\n",
    "        trunc_normal_(self.shift_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.shift_mlp.bias, 0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n",
    "        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n",
    "\n",
    "        normalized_tensor = (input_tensor - mean_value) / std_value\n",
    "\n",
    "        if self.detach_gradient:\n",
    "            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n",
    "        else:\n",
    "            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n",
    "\n",
    "        output = normalized_tensor * self.scale + self.shift\n",
    "        return output, rescale, rebias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "357ec11d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:09.582793Z",
     "iopub.status.busy": "2025-05-02T06:42:09.582316Z",
     "iopub.status.idle": "2025-05-02T06:42:09.590818Z",
     "shell.execute_reply": "2025-05-02T06:42:09.590102Z"
    },
    "papermill": {
     "duration": 0.02653,
     "end_time": "2025-05-02T06:42:09.591905",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.565375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n",
    "        super().__init__()\n",
    "        output_channels = output_channels or input_channels\n",
    "        hidden_channels = hidden_channels or input_channels\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            gain = (8 * self.depth) ** (-1 / 4)\n",
    "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "            trunc_normal_(layer.weight, std=std)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "\n",
    "def partition_into_windows(tensor, window_size):\n",
    "    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n",
    "    batch_size, height, width, channels = tensor.shape\n",
    "    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n",
    "\n",
    "    tensor = tensor.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n",
    "    )\n",
    "    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, height, width):\n",
    "    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n",
    "    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n",
    "    tensor = windows.view(\n",
    "        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31caddd0",
   "metadata": {
    "papermill": {
     "duration": 0.015981,
     "end_time": "2025-05-02T06:42:09.624230",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.608249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5baedc9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:09.657924Z",
     "iopub.status.busy": "2025-05-02T06:42:09.657268Z",
     "iopub.status.idle": "2025-05-02T06:42:09.797453Z",
     "shell.execute_reply": "2025-05-02T06:42:09.796703Z"
    },
    "papermill": {
     "duration": 0.158363,
     "end_time": "2025-05-02T06:42:09.798718",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.640355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 16, 16]),\n",
       " torch.Size([32, 16, 64]),\n",
       " torch.Size([2, 16, 16, 64]),\n",
       " True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize the MultiLayerPerceptron with sample parameters\n",
    "depth = 4\n",
    "input_channels = 64\n",
    "hidden_channels = 128\n",
    "output_channels = 64\n",
    "\n",
    "mlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n",
    "\n",
    "# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\n",
    "input_tensor = torch.randn(2, 64, 16, 16)\n",
    "output_tensor = mlp(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "mlp_output_shape = output_tensor.shape\n",
    "\n",
    "# Test window partition and merging\n",
    "batch_size, height, width, channels = 2, 16, 16, 64\n",
    "window_size = 4\n",
    "\n",
    "# Create a random tensor for window functions (B, H, W, C) format\n",
    "input_window_tensor = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Apply partitioning and merging\n",
    "windows = partition_into_windows(input_window_tensor, window_size)\n",
    "reconstructed_tensor = merge_windows(windows, window_size, height, width)\n",
    "\n",
    "# Check shapes\n",
    "windows_shape = windows.shape\n",
    "reconstructed_shape = reconstructed_tensor.shape\n",
    "\n",
    "# Validate if the reconstruction matches the original input shape\n",
    "is_shape_correct = reconstructed_shape == input_window_tensor.shape\n",
    "\n",
    "# Output results\n",
    "mlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba334c",
   "metadata": {
    "papermill": {
     "duration": 0.016181,
     "end_time": "2025-05-02T06:42:09.832066",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.815885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### LocalWindowAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ace3e1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:09.866256Z",
     "iopub.status.busy": "2025-05-02T06:42:09.865883Z",
     "iopub.status.idle": "2025-05-02T06:42:09.873141Z",
     "shell.execute_reply": "2025-05-02T06:42:09.872392Z"
    },
    "papermill": {
     "duration": 0.025859,
     "end_time": "2025-05-02T06:42:09.874388",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.848529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, window_size, num_heads):\n",
    "        \"\"\"Self-attention mechanism within local windows.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # (height, width)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n",
    "\n",
    "        # Compute and store relative positional encodings\n",
    "        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n",
    "        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n",
    "\n",
    "        # Learnable transformation of relative position embeddings\n",
    "        self.relative_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_heads, bias=True)\n",
    "        )\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n",
    "        batch_size, num_tokens, _ = qkv.shape\n",
    "\n",
    "        # Reshape qkv into separate query, key, and value tensors\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n",
    "\n",
    "        # Scale query for stable attention computation\n",
    "        q = q * self.scaling_factor\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Compute relative position bias\n",
    "        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n",
    "        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n",
    "        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n",
    "\n",
    "        # Apply softmax and compute weighted values\n",
    "        attention_weights = self.attention_softmax(attention_scores)\n",
    "        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8675a31",
   "metadata": {
    "papermill": {
     "duration": 0.015972,
     "end_time": "2025-05-02T06:42:09.907685",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.891713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### compute_log_relative_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b5ba683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:09.941335Z",
     "iopub.status.busy": "2025-05-02T06:42:09.941017Z",
     "iopub.status.idle": "2025-05-02T06:42:09.945919Z",
     "shell.execute_reply": "2025-05-02T06:42:09.945225Z"
    },
    "papermill": {
     "duration": 0.023201,
     "end_time": "2025-05-02T06:42:09.947155",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.923954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_log_relative_positions(window_size):\n",
    "    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n",
    "    coord_range = torch.arange(window_size)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n",
    "\n",
    "    # Compute relative positions\n",
    "    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n",
    "\n",
    "    # Format and apply log transformation\n",
    "    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n",
    "    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "    return log_relative_positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4895a72",
   "metadata": {
    "papermill": {
     "duration": 0.016196,
     "end_time": "2025-05-02T06:42:09.980050",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.963854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### AdaptiveAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfb9052d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.014761Z",
     "iopub.status.busy": "2025-05-02T06:42:10.014481Z",
     "iopub.status.idle": "2025-05-02T06:42:10.028214Z",
     "shell.execute_reply": "2025-05-02T06:42:10.027444Z"
    },
    "papermill": {
     "duration": 0.032661,
     "end_time": "2025-05-02T06:42:10.029433",
     "exception": false,
     "start_time": "2025-05-02T06:42:09.996772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n",
    "        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.network_depth = network_depth\n",
    "        self.enable_attention = enable_attention\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "        # Define convolutional processing based on mode\n",
    "        if self.conv_mode == 'Conv':\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "            )\n",
    "\n",
    "        if self.conv_mode == 'DWConv':\n",
    "            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n",
    "            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight_shape = module.weight.shape\n",
    "\n",
    "            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "            else:\n",
    "                gain = (8 * self.network_depth) ** (-1/4)\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def pad_for_window_processing(self, x, shift=False):\n",
    "        \"\"\"Pads the input tensor to fit window processing requirements with left and right shifts.\"\"\"\n",
    "        _, _, height, width = x.size()\n",
    "        pad_h = (self.window_size - height % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - width % self.window_size) % self.window_size\n",
    "\n",
    "        if shift:\n",
    "            # Apply left or right shift instead of cyclic shift\n",
    "            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n",
    "                          0, (self.window_size - pad_h) % self.window_size), mode='reflect')\n",
    "        else:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            v_proj = self.value_projection(x)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            qk_proj = self.query_key_projection(x)\n",
    "            qkv = torch.cat([qk_proj, v_proj], dim=1)\n",
    "\n",
    "            # Apply padding for shifted window processing\n",
    "            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n",
    "            padded_height, padded_width = padded_qkv.shape[2:]\n",
    "\n",
    "            # Partition into windows\n",
    "            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n",
    "            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n",
    "\n",
    "            # Apply window-based attention\n",
    "            attn_windows = self.window_attention(qkv_windows)\n",
    "\n",
    "            # Merge back to original spatial dimensions\n",
    "            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n",
    "\n",
    "            # Apply left or right shift (avoid cyclic)\n",
    "            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), 0:(self.shift_size + width), :]\n",
    "            attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.conv_mode in ['Conv', 'DWConv']:\n",
    "                conv_output = self.conv_layer(v_proj)\n",
    "                print(f\"conv_output shape: {conv_output.shape}\")\n",
    "                print(f\"attn_output shape: {attn_output.shape}\")\n",
    "                # print(f\"conv_output + attn_output shape: {conv_output + attn_output.shape}\")\n",
    "                print(f\"self.output_projection: {self.output_projection}\")\n",
    "                output = self.output_projection(conv_output + attn_output)\n",
    "            else:\n",
    "                output = self.output_projection(attn_output)\n",
    "\n",
    "        else:\n",
    "            if self.conv_mode == 'Conv':\n",
    "                output = self.conv_layer(x)  # No attention, using convolution only\n",
    "            elif self.conv_mode == 'DWConv':\n",
    "                output = self.output_projection(self.conv_layer(v_proj))\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ad159",
   "metadata": {
    "papermill": {
     "duration": 0.016521,
     "end_time": "2025-05-02T06:42:10.062849",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.046328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### VisionTransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0bdee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.096951Z",
     "iopub.status.busy": "2025-05-02T06:42:10.096665Z",
     "iopub.status.idle": "2025-05-02T06:42:10.103372Z",
     "shell.execute_reply": "2025-05-02T06:42:10.102606Z"
    },
    "papermill": {
     "duration": 0.02514,
     "end_time": "2025-05-02T06:42:10.104541",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.079401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n",
    "                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n",
    "        \"\"\"\n",
    "        A transformer block that includes attention (optional) and MLP layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enable_attention = enable_attention\n",
    "        self.enable_mlp_norm = enable_mlp_norm\n",
    "\n",
    "        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n",
    "        self.attention_layer = AdaptiveAttention(\n",
    "            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n",
    "            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n",
    "        )\n",
    "\n",
    "        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n",
    "        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.enable_attention:\n",
    "            x, rescale, rebias = self.pre_norm(x)\n",
    "        x = self.attention_layer(x)\n",
    "        if self.enable_attention:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        residual = x\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x, rescale, rebias = self.post_norm(x)\n",
    "        x = self.mlp_layer(x)\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d95ba7",
   "metadata": {
    "papermill": {
     "duration": 0.016268,
     "end_time": "2025-05-02T06:42:10.137509",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.121241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PatchEmbedding and PatchReconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94dc594c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.171367Z",
     "iopub.status.busy": "2025-05-02T06:42:10.171038Z",
     "iopub.status.idle": "2025-05-02T06:42:10.177484Z",
     "shell.execute_reply": "2025-05-02T06:42:10.176754Z"
    },
    "papermill": {
     "duration": 0.024848,
     "end_time": "2025-05-02T06:42:10.178599",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.153751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch embedding module that projects input images into token embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to generate patch embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class PatchReconstruction(nn.Module):\n",
    "    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch reconstruction module that converts token embeddings back to image patches.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2, padding_mode='reflect'\n",
    "            ),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to reconstruct image from embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d61ed3",
   "metadata": {
    "papermill": {
     "duration": 0.016131,
     "end_time": "2025-05-02T06:42:10.211323",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.195192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SelectiveKernelFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f5007be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.245750Z",
     "iopub.status.busy": "2025-05-02T06:42:10.245479Z",
     "iopub.status.idle": "2025-05-02T06:42:10.252021Z",
     "shell.execute_reply": "2025-05-02T06:42:10.251289Z"
    },
    "papermill": {
     "duration": 0.025362,
     "end_time": "2025-05-02T06:42:10.253197",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.227835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelectiveKernelFusion(nn.Module):\n",
    "    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n",
    "        \"\"\"\n",
    "        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n",
    "\n",
    "        Args:\n",
    "            channels (int): Number of input channels.\n",
    "            num_branches (int): Number of feature branches to fuse.\n",
    "            reduction_ratio (int): Reduction ratio for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_branches = num_branches\n",
    "        reduced_channels = max(int(channels / reduction_ratio), 4)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Forward pass for selective kernel fusion.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (list of tensors): A list of feature maps to be fused.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adaptively fused feature map.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = feature_maps[0].shape\n",
    "        \n",
    "        # Concatenate feature maps along a new dimension (num_branches)\n",
    "        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        aggregated_features = torch.sum(stacked_features, dim=1)\n",
    "        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n",
    "        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n",
    "\n",
    "        # Weighted sum of input feature maps\n",
    "        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ebd77",
   "metadata": {
    "papermill": {
     "duration": 0.015975,
     "end_time": "2025-05-02T06:42:10.285543",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.269568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TransformerStage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d0597c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.321301Z",
     "iopub.status.busy": "2025-05-02T06:42:10.320540Z",
     "iopub.status.idle": "2025-05-02T06:42:10.327600Z",
     "shell.execute_reply": "2025-05-02T06:42:10.326995Z"
    },
    "papermill": {
     "duration": 0.025304,
     "end_time": "2025-05-02T06:42:10.328644",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.303340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, window_size=8,\n",
    "                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n",
    "        \"\"\"\n",
    "        A stage of transformer blocks with configurable attention placement.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        attention_layers = int(attention_ratio * num_layers)\n",
    "\n",
    "        if attention_placement == 'last':\n",
    "            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'first':\n",
    "            enable_attentions = [i < attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'middle':\n",
    "            enable_attentions = [\n",
    "                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(\n",
    "                network_depth=network_depth,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                enable_attention=enable_attentions[i],\n",
    "                conv_mode=conv_mode\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer stage.\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d75d3",
   "metadata": {
    "papermill": {
     "duration": 0.016074,
     "end_time": "2025-05-02T06:42:10.361578",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.345504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DehazingTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bcbba3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.395580Z",
     "iopub.status.busy": "2025-05-02T06:42:10.395238Z",
     "iopub.status.idle": "2025-05-02T06:42:10.410428Z",
     "shell.execute_reply": "2025-05-02T06:42:10.409662Z"
    },
    "papermill": {
     "duration": 0.033533,
     "end_time": "2025-05-02T06:42:10.411531",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.377998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DehazingTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "                 layer_depths=[16, 16, 16, 8, 8],\n",
    "                 num_heads=[2, 4, 6, 1, 1],\n",
    "                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding settings\n",
    "        self.patch_size = 4\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # Backbone layers\n",
    "        self.encoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[0],\n",
    "            num_layers=layer_depths[0],\n",
    "            num_heads=num_heads[0],\n",
    "            mlp_ratio=mlp_ratios[0],\n",
    "            norm_layer=norm_layers[0],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[0],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[0]\n",
    "        )\n",
    "        \n",
    "        self.downsample1 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "        \n",
    "        self.encoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[1],\n",
    "            num_layers=layer_depths[1],\n",
    "            num_heads=num_heads[1],\n",
    "            mlp_ratio=mlp_ratios[1],\n",
    "            norm_layer=norm_layers[1],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[1],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[1]\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "        \n",
    "        self.encoder_stage3 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[2],\n",
    "            num_layers=layer_depths[2],\n",
    "            num_heads=num_heads[2],\n",
    "            mlp_ratio=mlp_ratios[2],\n",
    "            norm_layer=norm_layers[2],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[2],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[2]\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n",
    "        \n",
    "        self.decoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[3],\n",
    "            num_layers=layer_depths[3],\n",
    "            num_heads=num_heads[3],\n",
    "            mlp_ratio=mlp_ratios[3],\n",
    "            norm_layer=norm_layers[3],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[3],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[3]\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n",
    "        \n",
    "        self.decoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[4],\n",
    "            num_layers=layer_depths[4],\n",
    "            num_heads=num_heads[4],\n",
    "            mlp_ratio=mlp_ratios[4],\n",
    "            norm_layer=norm_layers[4],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[4],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[4]\n",
    "        )\n",
    "\n",
    "        # Final patch reconstruction\n",
    "        self.patch_reconstruction = PatchReconstruction(\n",
    "            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def adjust_image_size(self, x):\n",
    "        # Ensures the input image size is compatible with the patch size\n",
    "        _, _, height, width = x.size()\n",
    "        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n",
    "        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder_stage1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.downsample1(x)\n",
    "        x = self.encoder_stage2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.downsample2(x)\n",
    "        x = self.encoder_stage3(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n",
    "        x = self.decoder_stage1(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n",
    "        x = self.decoder_stage2(x)\n",
    "        x = self.patch_reconstruction(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_height, original_width = x.shape[2:]\n",
    "        x = self.adjust_image_size(x)\n",
    "\n",
    "        features = self.extract_features(x)\n",
    "        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n",
    "\n",
    "        # Dehazing formula: I = J * t + A * (1 - t)\n",
    "        x = transmission_map * x - atmospheric_light + x\n",
    "        x = x[:, :, :original_height, :original_width]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1147c7",
   "metadata": {
    "papermill": {
     "duration": 0.015984,
     "end_time": "2025-05-02T06:42:10.444504",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.428520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### build_dehazing_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f051e677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.478252Z",
     "iopub.status.busy": "2025-05-02T06:42:10.477720Z",
     "iopub.status.idle": "2025-05-02T06:42:10.482224Z",
     "shell.execute_reply": "2025-05-02T06:42:10.481502Z"
    },
    "papermill": {
     "duration": 0.022572,
     "end_time": "2025-05-02T06:42:10.483346",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.460774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dehazing_transformer():\n",
    "    return DehazingTransformer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "        layer_depths=[12, 12, 12, 6, 6],\n",
    "        num_heads=[2, 4, 6, 1, 1],\n",
    "        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95c1a6",
   "metadata": {
    "papermill": {
     "duration": 0.016213,
     "end_time": "2025-05-02T06:42:10.515965",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.499752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RevisedLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13c27054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.550207Z",
     "iopub.status.busy": "2025-05-02T06:42:10.549643Z",
     "iopub.status.idle": "2025-05-02T06:42:10.556240Z",
     "shell.execute_reply": "2025-05-02T06:42:10.555660Z"
    },
    "papermill": {
     "duration": 0.024965,
     "end_time": "2025-05-02T06:42:10.557393",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.532428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RevisedLayerNorm(nn.Module):\n",
    "    \"\"\"Revised LayerNorm\"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n",
    "        super(RevisedLayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.detach_gradient = detach_gradient\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n",
    "        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n",
    "\n",
    "        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "\n",
    "        trunc_normal_(self.scale_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.scale_mlp.bias, 1)\n",
    "\n",
    "        trunc_normal_(self.shift_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.shift_mlp.bias, 0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n",
    "        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n",
    "\n",
    "        normalized_tensor = (input_tensor - mean_value) / std_value\n",
    "\n",
    "        if self.detach_gradient:\n",
    "            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n",
    "        else:\n",
    "            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n",
    "\n",
    "        output = normalized_tensor * self.scale + self.shift\n",
    "        return output, rescale, rebias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8d8f547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.591740Z",
     "iopub.status.busy": "2025-05-02T06:42:10.591185Z",
     "iopub.status.idle": "2025-05-02T06:42:10.599148Z",
     "shell.execute_reply": "2025-05-02T06:42:10.598551Z"
    },
    "papermill": {
     "duration": 0.026102,
     "end_time": "2025-05-02T06:42:10.600222",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.574120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n",
    "        super().__init__()\n",
    "        output_channels = output_channels or input_channels\n",
    "        hidden_channels = hidden_channels or input_channels\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            gain = (8 * self.depth) ** (-1 / 4)\n",
    "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "            trunc_normal_(layer.weight, std=std)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "\n",
    "def partition_into_windows(tensor, window_size):\n",
    "    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n",
    "    batch_size, height, width, channels = tensor.shape\n",
    "    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n",
    "\n",
    "    tensor = tensor.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n",
    "    )\n",
    "    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, height, width):\n",
    "    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n",
    "    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n",
    "    tensor = windows.view(\n",
    "        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3f76e",
   "metadata": {
    "papermill": {
     "duration": 0.016261,
     "end_time": "2025-05-02T06:42:10.633064",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.616803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb3a7021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.667046Z",
     "iopub.status.busy": "2025-05-02T06:42:10.666471Z",
     "iopub.status.idle": "2025-05-02T06:42:10.680676Z",
     "shell.execute_reply": "2025-05-02T06:42:10.679923Z"
    },
    "papermill": {
     "duration": 0.03251,
     "end_time": "2025-05-02T06:42:10.681937",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.649427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 16, 16]),\n",
       " torch.Size([32, 16, 64]),\n",
       " torch.Size([2, 16, 16, 64]),\n",
       " True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the MultiLayerPerceptron with sample parameters\n",
    "depth = 4\n",
    "input_channels = 64\n",
    "hidden_channels = 128\n",
    "output_channels = 64\n",
    "\n",
    "mlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n",
    "\n",
    "# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\n",
    "input_tensor = torch.randn(2, 64, 16, 16)\n",
    "output_tensor = mlp(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "mlp_output_shape = output_tensor.shape\n",
    "\n",
    "# Test window partition and merging\n",
    "batch_size, height, width, channels = 2, 16, 16, 64\n",
    "window_size = 4\n",
    "\n",
    "# Create a random tensor for window functions (B, H, W, C) format\n",
    "input_window_tensor = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Apply partitioning and merging\n",
    "windows = partition_into_windows(input_window_tensor, window_size)\n",
    "reconstructed_tensor = merge_windows(windows, window_size, height, width)\n",
    "\n",
    "# Check shapes\n",
    "windows_shape = windows.shape\n",
    "reconstructed_shape = reconstructed_tensor.shape\n",
    "\n",
    "# Validate if the reconstruction matches the original input shape\n",
    "is_shape_correct = reconstructed_shape == input_window_tensor.shape\n",
    "\n",
    "# Output results\n",
    "mlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78d774ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.718531Z",
     "iopub.status.busy": "2025-05-02T06:42:10.717937Z",
     "iopub.status.idle": "2025-05-02T06:42:10.725032Z",
     "shell.execute_reply": "2025-05-02T06:42:10.724419Z"
    },
    "papermill": {
     "duration": 0.025717,
     "end_time": "2025-05-02T06:42:10.726170",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.700453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, window_size, num_heads):\n",
    "        \"\"\"Self-attention mechanism within local windows.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # (height, width)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n",
    "\n",
    "        # Compute and store relative positional encodings\n",
    "        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n",
    "        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n",
    "\n",
    "        # Learnable transformation of relative position embeddings\n",
    "        self.relative_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_heads, bias=True)\n",
    "        )\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n",
    "        batch_size, num_tokens, _ = qkv.shape\n",
    "\n",
    "        # Reshape qkv into separate query, key, and value tensors\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n",
    "\n",
    "        # Scale query for stable attention computation\n",
    "        q = q * self.scaling_factor\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Compute relative position bias\n",
    "        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n",
    "        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n",
    "        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n",
    "\n",
    "        # Apply softmax and compute weighted values\n",
    "        attention_weights = self.attention_softmax(attention_scores)\n",
    "        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e4bbf7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.760959Z",
     "iopub.status.busy": "2025-05-02T06:42:10.760695Z",
     "iopub.status.idle": "2025-05-02T06:42:10.765574Z",
     "shell.execute_reply": "2025-05-02T06:42:10.764839Z"
    },
    "papermill": {
     "duration": 0.023404,
     "end_time": "2025-05-02T06:42:10.766705",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.743301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_log_relative_positions(window_size):\n",
    "    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n",
    "    coord_range = torch.arange(window_size)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n",
    "\n",
    "    # Compute relative positions\n",
    "    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n",
    "\n",
    "    # Format and apply log transformation\n",
    "    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n",
    "    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "    return log_relative_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "674b1386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.801998Z",
     "iopub.status.busy": "2025-05-02T06:42:10.801468Z",
     "iopub.status.idle": "2025-05-02T06:42:10.815251Z",
     "shell.execute_reply": "2025-05-02T06:42:10.814471Z"
    },
    "papermill": {
     "duration": 0.032399,
     "end_time": "2025-05-02T06:42:10.816455",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.784056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n",
    "        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.network_depth = network_depth\n",
    "        self.enable_attention = enable_attention\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "        # Define convolutional processing based on mode\n",
    "        if self.conv_mode == 'Conv':\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "            )\n",
    "\n",
    "        if self.conv_mode == 'DWConv':\n",
    "            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n",
    "            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight_shape = module.weight.shape\n",
    "\n",
    "            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "            else:\n",
    "                gain = (8 * self.network_depth) ** (-1/4)\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def pad_for_window_processing(self, x, shift=False):\n",
    "        \"\"\"Pads the input tensor to fit window processing requirements.\"\"\"\n",
    "        _, _, height, width = x.size()\n",
    "        pad_h = (self.window_size - height % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - width % self.window_size) % self.window_size\n",
    "\n",
    "        if shift:\n",
    "            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n",
    "                          self.shift_size, (self.window_size - self.shift_size + pad_h) % self.window_size), mode='reflect')\n",
    "        else:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            v_proj = self.value_projection(x)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            qk_proj = self.query_key_projection(x)\n",
    "            qkv = torch.cat([qk_proj, v_proj], dim=1)\n",
    "\n",
    "            # Apply padding for shifted window processing\n",
    "            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n",
    "            padded_height, padded_width = padded_qkv.shape[2:]\n",
    "\n",
    "            # Partition into windows\n",
    "            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n",
    "            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n",
    "\n",
    "            # Apply window-based attention\n",
    "            attn_windows = self.window_attention(qkv_windows)\n",
    "\n",
    "            # Merge back to original spatial dimensions\n",
    "            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n",
    "\n",
    "            # Reverse the cyclic shift\n",
    "            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), self.shift_size:(self.shift_size + width), :]\n",
    "            attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.conv_mode in ['Conv', 'DWConv']:\n",
    "                conv_output = self.conv_layer(v_proj)\n",
    "                output = self.output_projection(conv_output + attn_output)\n",
    "            else:\n",
    "                output = self.output_projection(attn_output)\n",
    "\n",
    "        else:\n",
    "            if self.conv_mode == 'Conv':\n",
    "                output = self.conv_layer(x)  # No attention, using convolution only\n",
    "            elif self.conv_mode == 'DWConv':\n",
    "                output = self.output_projection(self.conv_layer(v_proj))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee6db8d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.851006Z",
     "iopub.status.busy": "2025-05-02T06:42:10.850729Z",
     "iopub.status.idle": "2025-05-02T06:42:10.861166Z",
     "shell.execute_reply": "2025-05-02T06:42:10.860262Z"
    },
    "papermill": {
     "duration": 0.028989,
     "end_time": "2025-05-02T06:42:10.862348",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.833359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n",
    "                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n",
    "        \"\"\"\n",
    "        A transformer block that includes attention (optional) and MLP layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enable_attention = enable_attention\n",
    "        self.enable_mlp_norm = enable_mlp_norm\n",
    "\n",
    "        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n",
    "        self.attention_layer = AdaptiveAttention(\n",
    "            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n",
    "            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n",
    "        )\n",
    "\n",
    "        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n",
    "        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.enable_attention:\n",
    "            x, rescale, rebias = self.pre_norm(x)\n",
    "        x = self.attention_layer(x)\n",
    "        if self.enable_attention:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        residual = x\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x, rescale, rebias = self.post_norm(x)\n",
    "        x = self.mlp_layer(x)\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, window_size=8,\n",
    "                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n",
    "        \"\"\"\n",
    "        A stage of transformer blocks with configurable attention placement.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        attention_layers = int(attention_ratio * num_layers)\n",
    "\n",
    "        if attention_placement == 'last':\n",
    "            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'first':\n",
    "            enable_attentions = [i < attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'middle':\n",
    "            enable_attentions = [\n",
    "                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(\n",
    "                network_depth=network_depth,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                enable_attention=enable_attentions[i],\n",
    "                conv_mode=conv_mode\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer stage.\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7758f918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.896956Z",
     "iopub.status.busy": "2025-05-02T06:42:10.896484Z",
     "iopub.status.idle": "2025-05-02T06:42:10.902646Z",
     "shell.execute_reply": "2025-05-02T06:42:10.902146Z"
    },
    "papermill": {
     "duration": 0.024579,
     "end_time": "2025-05-02T06:42:10.903788",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.879209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch embedding module that projects input images into token embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to generate patch embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class PatchReconstruction(nn.Module):\n",
    "    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch reconstruction module that converts token embeddings back to image patches.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2, padding_mode='reflect'\n",
    "            ),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to reconstruct image from embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11bde65d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.938388Z",
     "iopub.status.busy": "2025-05-02T06:42:10.938125Z",
     "iopub.status.idle": "2025-05-02T06:42:10.944453Z",
     "shell.execute_reply": "2025-05-02T06:42:10.943900Z"
    },
    "papermill": {
     "duration": 0.024956,
     "end_time": "2025-05-02T06:42:10.945555",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.920599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelectiveKernelFusion(nn.Module):\n",
    "    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n",
    "        \"\"\"\n",
    "        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n",
    "\n",
    "        Args:\n",
    "            channels (int): Number of input channels.\n",
    "            num_branches (int): Number of feature branches to fuse.\n",
    "            reduction_ratio (int): Reduction ratio for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_branches = num_branches\n",
    "        reduced_channels = max(int(channels / reduction_ratio), 4)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Forward pass for selective kernel fusion.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (list of tensors): A list of feature maps to be fused.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adaptively fused feature map.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = feature_maps[0].shape\n",
    "        \n",
    "        # Concatenate feature maps along a new dimension (num_branches)\n",
    "        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        aggregated_features = torch.sum(stacked_features, dim=1)\n",
    "        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n",
    "        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n",
    "\n",
    "        # Weighted sum of input feature maps\n",
    "        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "950df120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:10.980606Z",
     "iopub.status.busy": "2025-05-02T06:42:10.980332Z",
     "iopub.status.idle": "2025-05-02T06:42:10.994553Z",
     "shell.execute_reply": "2025-05-02T06:42:10.993940Z"
    },
    "papermill": {
     "duration": 0.033028,
     "end_time": "2025-05-02T06:42:10.995739",
     "exception": false,
     "start_time": "2025-05-02T06:42:10.962711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DehazingTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "                 layer_depths=[16, 16, 16, 8, 8],\n",
    "                 num_heads=[2, 4, 6, 1, 1],\n",
    "                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding settings\n",
    "        self.patch_size = 4\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # Backbone layers\n",
    "        self.encoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[0],\n",
    "            num_layers=layer_depths[0],\n",
    "            num_heads=num_heads[0],\n",
    "            mlp_ratio=mlp_ratios[0],\n",
    "            norm_layer=norm_layers[0],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[0],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[0]\n",
    "        )\n",
    "        \n",
    "        self.downsample1 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "        \n",
    "        self.encoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[1],\n",
    "            num_layers=layer_depths[1],\n",
    "            num_heads=num_heads[1],\n",
    "            mlp_ratio=mlp_ratios[1],\n",
    "            norm_layer=norm_layers[1],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[1],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[1]\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "        \n",
    "        self.encoder_stage3 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[2],\n",
    "            num_layers=layer_depths[2],\n",
    "            num_heads=num_heads[2],\n",
    "            mlp_ratio=mlp_ratios[2],\n",
    "            norm_layer=norm_layers[2],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[2],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[2]\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n",
    "        \n",
    "        self.decoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[3],\n",
    "            num_layers=layer_depths[3],\n",
    "            num_heads=num_heads[3],\n",
    "            mlp_ratio=mlp_ratios[3],\n",
    "            norm_layer=norm_layers[3],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[3],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[3]\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n",
    "        \n",
    "        self.decoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[4],\n",
    "            num_layers=layer_depths[4],\n",
    "            num_heads=num_heads[4],\n",
    "            mlp_ratio=mlp_ratios[4],\n",
    "            norm_layer=norm_layers[4],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[4],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[4]\n",
    "        )\n",
    "\n",
    "        # Final patch reconstruction\n",
    "        self.patch_reconstruction = PatchReconstruction(\n",
    "            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def adjust_image_size(self, x):\n",
    "        # Ensures the input image size is compatible with the patch size\n",
    "        _, _, height, width = x.size()\n",
    "        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n",
    "        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder_stage1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.downsample1(x)\n",
    "        x = self.encoder_stage2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.downsample2(x)\n",
    "        x = self.encoder_stage3(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n",
    "        x = self.decoder_stage1(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n",
    "        x = self.decoder_stage2(x)\n",
    "        x = self.patch_reconstruction(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_height, original_width = x.shape[2:]\n",
    "        x = self.adjust_image_size(x)\n",
    "\n",
    "        features = self.extract_features(x)\n",
    "        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n",
    "\n",
    "        # Dehazing formula: I = J * t + A * (1 - t)\n",
    "        x = transmission_map * x - atmospheric_light + x\n",
    "        x = x[:, :, :original_height, :original_width]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b5f9936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:11.030622Z",
     "iopub.status.busy": "2025-05-02T06:42:11.030036Z",
     "iopub.status.idle": "2025-05-02T06:42:11.034170Z",
     "shell.execute_reply": "2025-05-02T06:42:11.033586Z"
    },
    "papermill": {
     "duration": 0.0226,
     "end_time": "2025-05-02T06:42:11.035310",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.012710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dehazing_transformer():\n",
    "    return DehazingTransformer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "        layer_depths=[12, 12, 12, 6, 6],\n",
    "        num_heads=[2, 4, 6, 1, 1],\n",
    "        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1806b4",
   "metadata": {
    "papermill": {
     "duration": 0.01667,
     "end_time": "2025-05-02T06:42:11.069012",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.052342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ConvolutionalGuidedFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d264c86a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:11.103950Z",
     "iopub.status.busy": "2025-05-02T06:42:11.103360Z",
     "iopub.status.idle": "2025-05-02T06:42:11.111187Z",
     "shell.execute_reply": "2025-05-02T06:42:11.110534Z"
    },
    "papermill": {
     "duration": 0.026574,
     "end_time": "2025-05-02T06:42:11.112367",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.085793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvolutionalGuidedFilter(nn.Module):\n",
    "    def __init__(self, radius=1, norm_layer=nn.BatchNorm2d, conv_kernel_size: int = 1):\n",
    "        super(ConvolutionalGuidedFilter, self).__init__()\n",
    "\n",
    "        self.box_filter = nn.Conv2d(\n",
    "            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n",
    "        )\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                6,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                3,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.box_filter.weight.data[...] = 1.0\n",
    "\n",
    "    def forward(self, x_low_res, y_low_res, x_high_res):\n",
    "        _, _, h_lr, w_lr = x_low_res.size()\n",
    "        _, _, h_hr, w_hr = x_high_res.size()\n",
    "\n",
    "        N = self.box_filter(x_low_res.data.new().resize_((1, 3, h_lr, w_lr)).fill_(1.0))\n",
    "        ## mean_x\n",
    "        mean_x = self.box_filter(x_low_res) / N\n",
    "        ## mean_y\n",
    "        mean_y = self.box_filter(y_low_res) / N\n",
    "        ## cov_xy\n",
    "        cov_xy = self.box_filter(x_low_res * y_low_res) / N - mean_x * mean_y\n",
    "        ## var_x\n",
    "        var_x = self.box_filter(x_low_res * x_low_res) / N - mean_x * mean_x\n",
    "\n",
    "        ## A\n",
    "        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n",
    "        ## b\n",
    "        b = mean_y - A * mean_x\n",
    "\n",
    "        ## mean_A; mean_b\n",
    "        mean_A = F.interpolate(A, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "        mean_b = F.interpolate(b, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return mean_A * x_high_res + mean_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d2b51",
   "metadata": {
    "papermill": {
     "duration": 0.016397,
     "end_time": "2025-05-02T06:42:11.145691",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.129294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PixelAttentionLayer and ChannelAttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccc77924",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:11.180312Z",
     "iopub.status.busy": "2025-05-02T06:42:11.179713Z",
     "iopub.status.idle": "2025-05-02T06:42:11.186145Z",
     "shell.execute_reply": "2025-05-02T06:42:11.185514Z"
    },
    "papermill": {
     "duration": 0.024934,
     "end_time": "2025-05-02T06:42:11.187258",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.162324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(PixelAttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, 1, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_map = self.attention(x)\n",
    "        return x * attention_map\n",
    "\n",
    "class ChannelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelAttentionLayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, channels, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = self.avg_pool(x)\n",
    "        attention_map = self.attention(pooled)\n",
    "        return x * attention_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c11e5",
   "metadata": {
    "papermill": {
     "duration": 0.029928,
     "end_time": "2025-05-02T06:42:11.234126",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.204198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## HybridResidualDenseBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "957f3459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:11.297454Z",
     "iopub.status.busy": "2025-05-02T06:42:11.297144Z",
     "iopub.status.idle": "2025-05-02T06:42:11.307468Z",
     "shell.execute_reply": "2025-05-02T06:42:11.306648Z"
    },
    "papermill": {
     "duration": 0.046179,
     "end_time": "2025-05-02T06:42:11.308762",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.262583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HybridResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_dense_layers=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.growth_rate = growth_rate\n",
    "        self.in_channels = in_channels\n",
    "        total_channels = in_channels\n",
    "\n",
    "        for i in range(num_dense_layers):\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(total_channels, growth_rate, kernel_size=3, padding=2**i,dilation=2**i)\n",
    "            )\n",
    "            total_channels += growth_rate\n",
    "\n",
    "        self.fusion = nn.Conv2d(total_channels, in_channels, kernel_size=1)\n",
    "        self.channel_attention = ChannelAttentionLayer(in_channels)\n",
    "        self.pixel_attention = PixelAttentionLayer(in_channels)\n",
    "\n",
    "        # Gated residual fusion\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"self.layers\", self.layers)\n",
    "        # print(\"HybridResidualDenseBlock: x.shape\", x.shape)\n",
    "        # x: (B, C, H, W)\n",
    "        features = [x]\n",
    "        # x: (B, C, H, W) -> (B, C, H, W) + (B, growth_rate, H, W) * num_dense_layers\n",
    "        # features: [(B, C, H, W), (B, growth_rate, H, W), ...] \n",
    "        for conv in self.layers:\n",
    "            # Apply convolution and ReLU activation\n",
    "            out = F.relu(conv(torch.cat(features, dim=1)))\n",
    "            # print(\"self.layers -> out.shape: \", out.shape)\n",
    "            features.append(out)\n",
    "        \n",
    "        # print(\"self.layers -> features: \", features)\n",
    "\n",
    "        dense_out = torch.cat(features, dim=1)\n",
    "        # print(\"self.layers -> dense_out.shape: \", dense_out.shape)\n",
    "        fused = self.fusion(dense_out)\n",
    "        # print(\"self.layers -> fused.shape: \", fused.shape)\n",
    "        # Apply channel attention and pixel attention\n",
    "        # fused: (B, C, H, W) -> (B, C, H, W) + (B, C, H, W) * 2\n",
    "        ca = self.channel_attention(fused)\n",
    "        # print(\"self.layers -> ca.shape: \", ca.shape)\n",
    "        pa = self.pixel_attention(ca)\n",
    "        # print(\"self.layers -> pa.shape: \", pa.shape)\n",
    "        # Gated residual fusion\n",
    "        # x: (B, C, H, W) + (B, C, H, W) * 2\n",
    "        gate_input = torch.cat([x, pa], dim=1)\n",
    "        # print(\"self.layers -> gate_input.shape: \", gate_input.shape)\n",
    "        gated_fusion = self.gate(gate_input)\n",
    "        # print(\"self.layers -> gated_fusion.shape: \", gated_fusion.shape)\n",
    "        # Apply gated fusion\n",
    "        return x * (1 - gated_fusion) + pa * gated_fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737188ec",
   "metadata": {
    "papermill": {
     "duration": 0.016598,
     "end_time": "2025-05-02T06:42:11.343531",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.326933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## AdaptiveInstanceNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da6c58ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:11.379439Z",
     "iopub.status.busy": "2025-05-02T06:42:11.378713Z",
     "iopub.status.idle": "2025-05-02T06:42:11.383956Z",
     "shell.execute_reply": "2025-05-02T06:42:11.383295Z"
    },
    "papermill": {
     "duration": 0.024052,
     "end_time": "2025-05-02T06:42:11.385107",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.361055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNormalization(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(AdaptiveInstanceNormalization, self).__init__()\n",
    "\n",
    "        # Learnable scaling factors\n",
    "        self.scale_x = nn.Parameter(torch.tensor(1.0))  # Identity scaling\n",
    "        self.scale_norm = nn.Parameter(torch.tensor(0.0))  # Initially no effect\n",
    "\n",
    "        # Instance normalization layer with affine transformation enabled\n",
    "        self.instance_norm = nn.InstanceNorm2d(num_channels, momentum=0.999, eps=0.001, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized_x = self.instance_norm(x)\n",
    "        return self.scale_x * x + self.scale_norm * normalized_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec7cd83",
   "metadata": {
    "papermill": {
     "duration": 0.016519,
     "end_time": "2025-05-02T06:42:11.418816",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.402297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DEEPGUIDEDNETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a400c2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:11.453235Z",
     "iopub.status.busy": "2025-05-02T06:42:11.452871Z",
     "iopub.status.idle": "2025-05-02T06:42:11.459676Z",
     "shell.execute_reply": "2025-05-02T06:42:11.459067Z"
    },
    "papermill": {
     "duration": 0.025351,
     "end_time": "2025-05-02T06:42:11.460747",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.435396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepGuidedNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=3, radius=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        norm = AdaptiveInstanceNormalization  # define this separately\n",
    "        kernel_size = 3\n",
    "        depth_rate = 64\n",
    "        growth_rate = 16\n",
    "        num_dense_layer = 4\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "        self.rdb1 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb2 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb3 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb4 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(depth_rate, 256, 3, padding=1),       # 64 → 256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 48, 3, padding=1), # 256 → 48 for 3 × 4²\n",
    "            nn.PixelShuffle(4)                              # 48 → 3, upsample ×4\n",
    "        )\n",
    "\n",
    "    def forward(self, x, sr= False):\n",
    "        # print(f\"Shape of x: {x.shape}\")\n",
    "        x_in = self.conv_in(x)\n",
    "        # print(f\"Shape of x_in: {x_in.shape}\")\n",
    "        feat1 = self.rdb1(x_in)\n",
    "        # print(f\"Shape of feat1: {feat1.shape}\")\n",
    "        feat2 = self.rdb2(feat1)\n",
    "        # print(f\"Shape of feat2: {feat2.shape}\")\n",
    "        feat3 = self.rdb3(feat2)\n",
    "        # print(f\"Shape of feat3: {feat3.shape}\")\n",
    "        feat4 = self.rdb4(feat3)\n",
    "        # print(f\"Shape of feat4: {feat4.shape}\")\n",
    "        out_feat = self.conv_out(feat4)\n",
    "        # print(\"CONV Tail final\", self.tail)\n",
    "\n",
    "        sr = self.tail(out_feat)  # shape: (B, 3, H×4, W×4)\n",
    "        return sr, sr, [feat1, feat2, feat3, feat4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17499c82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:11.495935Z",
     "iopub.status.busy": "2025-05-02T06:42:11.495691Z",
     "iopub.status.idle": "2025-05-02T06:42:11.503012Z",
     "shell.execute_reply": "2025-05-02T06:42:11.502399Z"
    },
    "papermill": {
     "duration": 0.026268,
     "end_time": "2025-05-02T06:42:11.504164",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.477896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepGuidedNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, radius=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        norm = AdaptiveInstanceNormalization  # define this separately\n",
    "        kernel_size = 3\n",
    "        depth_rate = 64\n",
    "        growth_rate = 16\n",
    "        num_dense_layer = 4\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "        self.rdb1 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb2 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb3 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb4 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(depth_rate, 256, 3, padding=1),       # 64 → 256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 48, 3, padding=1), # 256 → 48 for 3 × 4²\n",
    "            nn.PixelShuffle(4)                              # 48 → 3, upsample ×4\n",
    "        )\n",
    "        self.downsample = nn.Upsample(scale_factor=0.25, mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "        # Guided Filter & Dehazing Transformer\n",
    "        self.guided_filter = ConvolutionalGuidedFilter(radius, norm_layer=norm)\n",
    "        self.dehaze_network = build_dehazing_transformer()\n",
    "\n",
    "    def forward(self, x, sr= False):\n",
    "        # print(f\"Shape of x: {x.shape}\")\n",
    "        x_lr = self.downsample(x)\n",
    "        x_in = self.conv_in(x_lr)\n",
    "        # print(f\"Shape of x_in: {x_in.shape}\")\n",
    "        feat1 = self.rdb1(x_in)\n",
    "        # print(f\"Shape of feat1: {feat1.shape}\")\n",
    "        feat2 = self.rdb2(feat1)\n",
    "        # print(f\"Shape of feat2: {feat2.shape}\")\n",
    "        feat3 = self.rdb3(feat2)\n",
    "        # print(f\"Shape of feat3: {feat3.shape}\")\n",
    "        feat4 = self.rdb4(feat3)\n",
    "        # print(f\"Shape of feat4: {feat4.shape}\")\n",
    "        out_feat = self.conv_out(feat4)\n",
    "        # print(\"CONV Tail final\", self.tail)\n",
    "        \n",
    "        y_base = self.dehaze_network(x_lr)\n",
    "        # print(\"y_base\", y_base.shape)\n",
    "        # print(\"x_in\", x_in.shape)\n",
    "        # print(\"x\", x.shape)\n",
    "        refined_output = self.guided_filter(x_lr, y_base, x)\n",
    "        # print(\"refined_output\", refined_output.shape)\n",
    "        # print(\"out_feat\", out_feat.shape)\n",
    "\n",
    "        sr = self.tail(out_feat)  # shape: (B, 3, H×4, W×4)\n",
    "        # print(\"sr\", sr.shape)\n",
    "        y_out  = refined_output + sr\n",
    "        # print(\"y_out\", y_out.shape)\n",
    "        return sr, refined_output, [feat1, feat2, feat3, feat4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "800142b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:11.539362Z",
     "iopub.status.busy": "2025-05-02T06:42:11.538777Z",
     "iopub.status.idle": "2025-05-02T06:42:12.086446Z",
     "shell.execute_reply": "2025-05-02T06:42:12.085729Z"
    },
    "papermill": {
     "duration": 0.566352,
     "end_time": "2025-05-02T06:42:12.087729",
     "exception": false,
     "start_time": "2025-05-02T06:42:11.521377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test dehazing transformer\n",
    "d_transformer = build_dehazing_transformer()\n",
    "x = torch.randn(1, 3, 64, 64)  # Example input tensor\n",
    "t_out = d_transformer(x)\n",
    "t_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51f85678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:12.125934Z",
     "iopub.status.busy": "2025-05-02T06:42:12.125613Z",
     "iopub.status.idle": "2025-05-02T06:42:13.071490Z",
     "shell.execute_reply": "2025-05-02T06:42:13.070494Z"
    },
    "papermill": {
     "duration": 0.965075,
     "end_time": "2025-05-02T06:42:13.072693",
     "exception": false,
     "start_time": "2025-05-02T06:42:12.107618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sr shape: torch.Size([1, 3, 128, 128])\n",
      "refined_output shape: torch.Size([1, 3, 128, 128])\n",
      "feat1 shape: torch.Size([1, 64, 32, 32])\n",
      "feat2 shape: torch.Size([1, 64, 32, 32])\n",
      "feat3 shape: torch.Size([1, 64, 32, 32])\n",
      "feat4 shape: torch.Size([1, 64, 32, 32])\n",
      "t_out shape: torch.Size([1, 3, 64, 64])\n",
      "guided_filter output shape: torch.Size([1, 3, 128, 128])\n",
      "PixelAttentionLayer output shape: torch.Size([1, 64, 32, 32])\n",
      "ChannelAttentionLayer output shape: torch.Size([1, 64, 32, 32])\n",
      "HybridResidualDenseBlock output shape: torch.Size([1, 64, 32, 32])\n",
      "AdaptiveInstanceNormalization output shape: torch.Size([1, 64, 32, 32])\n",
      "PatchEmbedding output shape: torch.Size([1, 96, 32, 32])\n",
      "PatchReconstruction output shape: torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "test_net = DeepGuidedNet()\n",
    "x = torch.randn(1, 3, 128, 128)  # Example input tensor\n",
    "y_t = test_net(x)\n",
    "sr, refined_output, [feat1, feat2, feat3, feat4] = y_t\n",
    "print(\"sr shape:\", sr.shape)  # Output shape after the network\n",
    "print(\"refined_output shape:\", refined_output.shape)  # Output shape after the network\n",
    "print(\"feat1 shape:\", feat1.shape)  # Output shape after the network\n",
    "print(\"feat2 shape:\", feat2.shape)  # Output shape after the network\n",
    "print(\"feat3 shape:\", feat3.shape)  # Output shape after the network\n",
    "print(\"feat4 shape:\", feat4.shape)  # Output shape after the network\n",
    "# test the dehazing transformer\n",
    "x = torch.randn(1, 3, 64, 64)  # Example input tensor\n",
    "t_out = d_transformer(x)\n",
    "print(\"t_out shape:\", t_out.shape)  # Output shape after the network\n",
    "# test the convolutional guided filter\n",
    "x_low_res = torch.randn(1, 3, 32, 32)  # Example low-resolution input tensor\n",
    "y_low_res = torch.randn(1, 3, 32, 32)  # Example low-resolution input tensor\n",
    "x_high_res = torch.randn(1, 3, 128, 128)  # Example high-resolution input tensor\n",
    "guided_filter = ConvolutionalGuidedFilter(radius=1)\n",
    "output = guided_filter(x_low_res, y_low_res, x_high_res)\n",
    "print(\"guided_filter output shape:\", output.shape)  # Output shape after the network\n",
    "# test the pixel attention layer\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "pixel_attention_layer = PixelAttentionLayer(channels=64)\n",
    "output = pixel_attention_layer(x)\n",
    "print(\"PixelAttentionLayer output shape:\", output.shape)  # Output shape after the network\n",
    "# test the channel attention layer\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "channel_attention_layer = ChannelAttentionLayer(channels=64)\n",
    "output = channel_attention_layer(x)\n",
    "print(\"ChannelAttentionLayer output shape:\", output.shape)  # Output shape after the network\n",
    "# test the hybrid residual dense block\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "hybrid_residual_dense_block = HybridResidualDenseBlock(in_channels=64, growth_rate=16, num_dense_layers=4)\n",
    "output = hybrid_residual_dense_block(x)\n",
    "print(\"HybridResidualDenseBlock output shape:\", output.shape)  # Output shape after the network\n",
    "# test the adaptive instance normalization\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "adaptive_instance_norm = AdaptiveInstanceNormalization(num_channels=64)\n",
    "output = adaptive_instance_norm(x)\n",
    "print(\"AdaptiveInstanceNormalization output shape:\", output.shape)  # Output shape after the network\n",
    "# test the patch embedding\n",
    "x = torch.randn(1, 3, 128, 128)  # Example input tensor\n",
    "# (batch_size, channels, height, width)\n",
    "patch_embedding = PatchEmbedding(patch_size=4, input_channels=3, embedding_dim=96, kernel_size=3)\n",
    "output = patch_embedding(x)\n",
    "print(\"PatchEmbedding output shape:\", output.shape)  # Output shape after the network\n",
    "# test the patch reconstruction\n",
    "x = torch.randn(1, 96, 32, 32)  # Example input tensor\n",
    "# (batch_size, channels, height, width)\n",
    "patch_reconstruction = PatchReconstruction(patch_size=4, output_channels=3, embedding_dim=96, kernel_size=3)\n",
    "output = patch_reconstruction(x)\n",
    "print(\"PatchReconstruction output shape:\", output.shape)  # Output shape after the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b438f",
   "metadata": {
    "papermill": {
     "duration": 0.016628,
     "end_time": "2025-05-02T06:42:13.109547",
     "exception": false,
     "start_time": "2025-05-02T06:42:13.092919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With pixle shuffle 2 and conv out 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f82748ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:13.144851Z",
     "iopub.status.busy": "2025-05-02T06:42:13.144340Z",
     "iopub.status.idle": "2025-05-02T06:42:13.449316Z",
     "shell.execute_reply": "2025-05-02T06:42:13.448410Z"
    },
    "papermill": {
     "duration": 0.323832,
     "end_time": "2025-05-02T06:42:13.450515",
     "exception": false,
     "start_time": "2025-05-02T06:42:13.126683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3, 512, 512])\n",
      "Feature shapes: [torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128])]\n"
     ]
    }
   ],
   "source": [
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, 3, 128, 128)  # Batch size of 1, 3 channels, 256x256 image\n",
    "\n",
    "# Initialize the DeepGuidedNetwork\n",
    "model = DeepGuidedNetwork(radius=1)\n",
    "   \n",
    "# Forward pass through the network\n",
    "output_tensor, dummy_out, features = model(input_tensor)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output_tensor.shape)\n",
    "print(\"Feature shapes:\", [f.shape for f in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a0c8d8",
   "metadata": {
    "papermill": {
     "duration": 0.016849,
     "end_time": "2025-05-02T06:42:13.485667",
     "exception": false,
     "start_time": "2025-05-02T06:42:13.468818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With pixle shuffle 2 and conv out 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11fb5e72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:13.521557Z",
     "iopub.status.busy": "2025-05-02T06:42:13.521283Z",
     "iopub.status.idle": "2025-05-02T06:42:13.801628Z",
     "shell.execute_reply": "2025-05-02T06:42:13.800750Z"
    },
    "papermill": {
     "duration": 0.300157,
     "end_time": "2025-05-02T06:42:13.803152",
     "exception": false,
     "start_time": "2025-05-02T06:42:13.502995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3, 512, 512])\n",
      "Feature shapes: [torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128])]\n"
     ]
    }
   ],
   "source": [
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, 3, 128, 128)  # Batch size of 1, 3 channels, 256x256 image\n",
    "\n",
    "# Initialize the DeepGuidedNetwork\n",
    "model = DeepGuidedNetwork(radius=1)\n",
    "\n",
    "# Forward pass through the network\n",
    "output_tensor, dummy_out, features = model(input_tensor)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output_tensor.shape)\n",
    "print(\"Feature shapes:\", [f.shape for f in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a56c95b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:13.840814Z",
     "iopub.status.busy": "2025-05-02T06:42:13.840124Z",
     "iopub.status.idle": "2025-05-02T06:42:13.843441Z",
     "shell.execute_reply": "2025-05-02T06:42:13.842863Z"
    },
    "papermill": {
     "duration": 0.022325,
     "end_time": "2025-05-02T06:42:13.844432",
     "exception": false,
     "start_time": "2025-05-02T06:42:13.822107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test to_psnr\n",
    "# to_psnr(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaedae3",
   "metadata": {
    "papermill": {
     "duration": 0.016766,
     "end_time": "2025-05-02T06:42:13.878381",
     "exception": false,
     "start_time": "2025-05-02T06:42:13.861615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1affeadf",
   "metadata": {
    "papermill": {
     "duration": 0.016709,
     "end_time": "2025-05-02T06:42:13.912297",
     "exception": false,
     "start_time": "2025-05-02T06:42:13.895588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b23adcac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:13.947764Z",
     "iopub.status.busy": "2025-05-02T06:42:13.947489Z",
     "iopub.status.idle": "2025-05-02T06:42:13.952048Z",
     "shell.execute_reply": "2025-05-02T06:42:13.951478Z"
    },
    "papermill": {
     "duration": 0.023653,
     "end_time": "2025-05-02T06:42:13.953125",
     "exception": false,
     "start_time": "2025-05-02T06:42:13.929472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_psnr(dehaze, gt):\n",
    "    \"\"\"\n",
    "    Compute PSNR (Peak Signal-to-Noise Ratio) between dehazed and ground truth images.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: PSNR values for each image in the batch.\n",
    "    \"\"\"\n",
    "    # print(\"Shapes: \", dehaze.shape, gt.shape)\n",
    "    mse = F.mse_loss(dehaze, gt, reduction='none').mean(dim=[1, 2, 3])  # Compute MSE per image\n",
    "    intensity_max = 1.0\n",
    "\n",
    "    # Compute PSNR safely, avoiding division by zero and extreme values\n",
    "    psnr_list = [10.0 * log10(intensity_max / max(mse_val.item(), 1e-6)) for mse_val in mse]\n",
    "\n",
    "    return psnr_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4076002",
   "metadata": {
    "papermill": {
     "duration": 0.016936,
     "end_time": "2025-05-02T06:42:13.987621",
     "exception": false,
     "start_time": "2025-05-02T06:42:13.970685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc17bbf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:14.023565Z",
     "iopub.status.busy": "2025-05-02T06:42:14.023282Z",
     "iopub.status.idle": "2025-05-02T06:42:17.317505Z",
     "shell.execute_reply": "2025-05-02T06:42:17.316732Z"
    },
    "papermill": {
     "duration": 3.313691,
     "end_time": "2025-05-02T06:42:17.318981",
     "exception": false,
     "start_time": "2025-05-02T06:42:14.005290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "# Define SSIM metric\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0, reduction='none')\n",
    "\n",
    "def to_ssim(dehaze: torch.Tensor, gt: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute SSIM directly on the GPU using torchmetrics.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: SSIM values for each image in the batch.\n",
    "    \"\"\"\n",
    "    ssim_values = ssim_metric(dehaze, gt)  # Shape: [B]\n",
    "    # print(\"1\",ssim_values)\n",
    "    # print(\"2\",[ssim_values])\n",
    "    ssim_values = ssim_values.tolist() \n",
    "    # print(type(ssim_values))\n",
    "    if isinstance(ssim_values, float):  # Correct way to check for a float\n",
    "        return [ssim_values]  # Convert single float to a list\n",
    "    return ssim_values  # Otherwise, return as is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7941c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:17.355930Z",
     "iopub.status.busy": "2025-05-02T06:42:17.355199Z",
     "iopub.status.idle": "2025-05-02T06:42:17.398384Z",
     "shell.execute_reply": "2025-05-02T06:42:17.397341Z"
    },
    "papermill": {
     "duration": 0.062623,
     "end_time": "2025-05-02T06:42:17.399750",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.337127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.006723514758050442]\n"
     ]
    }
   ],
   "source": [
    "# Test with a dummy tensor\n",
    "dehaze = torch.rand(1, 3, 360, 360)  # Random batch of images\n",
    "gt = torch.rand(1, 3, 360, 360)  # Random ground truth images\n",
    "\n",
    "ssim_scores = to_ssim(dehaze, gt)\n",
    "print(ssim_scores)  # Should print a list of 6 SSIM values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26a97a9",
   "metadata": {
    "papermill": {
     "duration": 0.017114,
     "end_time": "2025-05-02T06:42:17.437233",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.420119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Validation Haze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2783665",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:17.472870Z",
     "iopub.status.busy": "2025-05-02T06:42:17.472572Z",
     "iopub.status.idle": "2025-05-02T06:42:17.478111Z",
     "shell.execute_reply": "2025-05-02T06:42:17.477363Z"
    },
    "papermill": {
     "duration": 0.024786,
     "end_time": "2025-05-02T06:42:17.479352",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.454566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validationB(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: Your deep learning model\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: GPU/CPU device\n",
    "    :param category: dataset type (indoor/outdoor)\n",
    "    :param save_tag: whether to save images\n",
    "    :return: average PSNR & SSIM values\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    \n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            haze, gt = val_data\n",
    "            haze, gt = haze.to(device), gt.to(device)\n",
    "            dehaze, _, _ = net(haze)\n",
    "\n",
    "        # --- Compute PSNR & SSIM --- #\n",
    "        batch_psnr = to_psnr(dehaze, gt)  # This returns a list\n",
    "        # print(batch_psnr)\n",
    "        batch_ssim = to_ssim(dehaze, gt)  # This returns a list\n",
    "        # print(batch_ssim)\n",
    "\n",
    "        psnr_list.extend(batch_psnr)  # Flatten the list\n",
    "        ssim_list.extend(batch_ssim)  # Flatten the list\n",
    "\n",
    "    # --- Ensure lists are not empty to avoid division by zero --- #\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a717f39c",
   "metadata": {
    "papermill": {
     "duration": 0.017225,
     "end_time": "2025-05-02T06:42:17.514369",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.497144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Validation SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef874a82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:17.550353Z",
     "iopub.status.busy": "2025-05-02T06:42:17.549779Z",
     "iopub.status.idle": "2025-05-02T06:42:17.555036Z",
     "shell.execute_reply": "2025-05-02T06:42:17.554332Z"
    },
    "papermill": {
     "duration": 0.02453,
     "end_time": "2025-05-02T06:42:17.556269",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.531739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation_sr(net, sr_val_loader, device):\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    for lr, hr in sr_val_loader:\n",
    "        with torch.no_grad():\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr_out, _, _ = net(lr, sr = False)\n",
    "            hr = F.interpolate(hr, size=sr_out.shape[2:], mode='bilinear', align_corners=False)\n",
    "        # print(\"Shapes 1: \", sr_out.shape, hr.shape)\n",
    "        psnr_list.extend(to_psnr(sr_out, hr))\n",
    "        ssim_list.extend(to_ssim(sr_out, hr))\n",
    "\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88260cd",
   "metadata": {
    "papermill": {
     "duration": 0.017016,
     "end_time": "2025-05-02T06:42:17.590720",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.573704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "962d6d07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:17.626235Z",
     "iopub.status.busy": "2025-05-02T06:42:17.625645Z",
     "iopub.status.idle": "2025-05-02T06:42:17.629056Z",
     "shell.execute_reply": "2025-05-02T06:42:17.628346Z"
    },
    "papermill": {
     "duration": 0.022418,
     "end_time": "2025-05-02T06:42:17.630301",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.607883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e26bf43f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:17.666319Z",
     "iopub.status.busy": "2025-05-02T06:42:17.665608Z",
     "iopub.status.idle": "2025-05-02T06:42:17.668885Z",
     "shell.execute_reply": "2025-05-02T06:42:17.668331Z"
    },
    "papermill": {
     "duration": 0.022513,
     "end_time": "2025-05-02T06:42:17.670052",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.647539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# psnr, ssim = validationB(net, val_data_loader, device, category)\n",
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # psnr, ssim = validationB(model, val_loader, device, \"indoor\", save_tag=True)\n",
    "# print(f\"Validation PSNR: {psnr:.2f}, SSIM: {ssim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529a816",
   "metadata": {
    "papermill": {
     "duration": 0.016831,
     "end_time": "2025-05-02T06:42:17.704459",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.687628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115dc447",
   "metadata": {
    "papermill": {
     "duration": 0.016942,
     "end_time": "2025-05-02T06:42:17.738492",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.721550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exec Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd12c02f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:17.773973Z",
     "iopub.status.busy": "2025-05-02T06:42:17.773700Z",
     "iopub.status.idle": "2025-05-02T06:42:17.781511Z",
     "shell.execute_reply": "2025-05-02T06:42:17.780969Z"
    },
    "papermill": {
     "duration": 0.027123,
     "end_time": "2025-05-02T06:42:17.782586",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.755463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c718e4a0a0dd4f6a9822e5f30389ea68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Execution Env:', index=1, options=('local', 'kaggle'), value='kaggle')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execution_env_widget = widgets.Dropdown(options=['local', 'kaggle'], value='kaggle', description='Execution Env:')\n",
    "display(execution_env_widget)\n",
    "\n",
    "# check if not in windows and not in kaggle\n",
    "if os.path.exists('/kaggle') and os.path.exists('/mnt'):\n",
    "    execution_env_widget.value = 'kaggle' \n",
    "else:\n",
    "    execution_env_widget.value = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afff6e39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:17.819160Z",
     "iopub.status.busy": "2025-05-02T06:42:17.818851Z",
     "iopub.status.idle": "2025-05-02T06:42:17.852215Z",
     "shell.execute_reply": "2025-05-02T06:42:17.851406Z"
    },
    "papermill": {
     "duration": 0.053147,
     "end_time": "2025-05-02T06:42:17.853506",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.800359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc099343080423980f6824deaa6a26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Learning Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b38c40a90b4baaa177b485d53dd498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='128,128', description='Crop Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e5dcc25ea049a38ef6079212c92b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Train Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed2c7e001f644ba814bb6176e5ec3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=0, description='Version:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfca32a1e024bbf8e1934496755c235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='Growth Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e1fac7d5a94f31a55b6410895b8cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.04, description='Lambda Loss:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb50ddac489e4bc3b73c613faf1eccaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Val Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb35a2bb023e46f4a6ef4b9fb2b09328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', index=2, options=('indoor', 'outdoor', 'reside', 'nh'), value='reside')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters set:\n",
      "learning_rate: 0.0001\n",
      "crop_size: [128, 128]\n",
      "train_batch_size: 2\n",
      "version: 0\n",
      "growth_rate: 16\n",
      "lambda_loss: 0.04\n",
      "val_batch_size: 2\n",
      "category: reside\n",
      "execution_env: kaggle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Create widgets for each hyper-parameter ---\n",
    "learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n",
    "crop_size_widget = widgets.Text(value='128,128', description='Crop Size:')\n",
    "train_batch_size_widget = widgets.IntText(value=2, description='Train Batch Size:')\n",
    "version_widget = widgets.IntText(value=0, description='Version:')\n",
    "growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n",
    "lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n",
    "val_batch_size_widget = widgets.IntText(value=2, description='Val Batch Size:')\n",
    "category_widget = widgets.Dropdown(options=['indoor', 'outdoor', 'reside', 'nh'], value='reside', description='Category:')\n",
    "\n",
    "# --- Display the widgets ---\n",
    "display(\n",
    "    learning_rate_widget, crop_size_widget, train_batch_size_widget, version_widget,\n",
    "    growth_rate_widget, lambda_loss_widget, \n",
    "    val_batch_size_widget, category_widget\n",
    ")\n",
    "\n",
    "# --- Function to parse crop size ---\n",
    "def parse_crop_size(crop_size_str):\n",
    "    return [int(x) for x in crop_size_str.split(',')]\n",
    "\n",
    "# --- Assign the widget values to variables ---\n",
    "learning_rate = learning_rate_widget.value\n",
    "crop_size = parse_crop_size(crop_size_widget.value)\n",
    "train_batch_size = train_batch_size_widget.value\n",
    "version = version_widget.value\n",
    "growth_rate = growth_rate_widget.value\n",
    "lambda_loss = lambda_loss_widget.value\n",
    "val_batch_size = val_batch_size_widget.value\n",
    "category = category_widget.value\n",
    "\n",
    "execution_env = execution_env_widget.value  # Local or Kaggle\n",
    "\n",
    "\n",
    "print('\\nHyper-parameters set:')\n",
    "print(f'learning_rate: {learning_rate}')\n",
    "print(f'crop_size: {crop_size}')\n",
    "print(f'train_batch_size: {train_batch_size}')\n",
    "print(f'version: {version}')\n",
    "print(f'growth_rate: {growth_rate}')\n",
    "print(f'lambda_loss: {lambda_loss}')\n",
    "print(f'val_batch_size: {val_batch_size}')\n",
    "print(f'category: {category}')\n",
    "print(f'execution_env: {execution_env}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd85aa",
   "metadata": {
    "papermill": {
     "duration": 0.017663,
     "end_time": "2025-05-02T06:42:17.889767",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.872104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Paths Dehaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f28c2df4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:17.926853Z",
     "iopub.status.busy": "2025-05-02T06:42:17.926588Z",
     "iopub.status.idle": "2025-05-02T06:42:17.932609Z",
     "shell.execute_reply": "2025-05-02T06:42:17.931765Z"
    },
    "papermill": {
     "duration": 0.026176,
     "end_time": "2025-05-02T06:42:17.933796",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.907620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RESIDE dataset\n",
      "\n",
      "Final dataset paths:\n",
      "Training directory: /kaggle/input/reside128r/cropped_t\n",
      "Validation directory: /kaggle/input/reside128r/cropped_t\n",
      "Number of epochs: 10\n"
     ]
    }
   ],
   "source": [
    "# --- Set category-specific hyper-parameters ---\n",
    "if category == 'indoor':\n",
    "    num_epochs = 1500\n",
    "    train_data_dir = './data/train/indoor/'\n",
    "    val_data_dir = './data/test/SOTS/indoor/'\n",
    "elif category == 'outdoor':\n",
    "    num_epochs = 10\n",
    "    train_data_dir = './data/train/outdoor/'\n",
    "    val_data_dir = './data/test/SOTS/outdoor/'\n",
    "elif category == 'reside':\n",
    "    print('Using RESIDE dataset')\n",
    "    num_epochs = 10\n",
    "    # train_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n",
    "    # train_data_dir = '/kaggle/input/reside-processed/kaggle/working/cropped_train'\n",
    "    train_data_dir = '/kaggle/input/reside128r/cropped_t'\n",
    "    val_data_dir = '/kaggle/input/reside128r/cropped_t'\n",
    "    test_data_dir = '/kaggle/input/reside128r/cropped_v'\n",
    "    if execution_env == 'local':\n",
    "        print('Using local RESIDE dataset')\n",
    "        # train_data_dir = './dataset/reside_processed/kaggle/working/cropped_train'\n",
    "        train_data_dir = './dataset/reside_processed/kaggle/working/cropped_t'\n",
    "        val_data_dir = './dataset/reside_processed/kaggle/working/cropped_v'\n",
    "        # train_data_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/reside_processed/kaggle/working/cropped_train/'\n",
    "elif category == 'nh':\n",
    "    num_epochs = 50\n",
    "    train_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "    val_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "else:\n",
    "    raise Exception('Wrong image category. Set it to indoor or outdoor for RESIDE dataset.')\n",
    "\n",
    "# --- Adjust paths based on execution environment ---\n",
    "# if execution_env == 'kaggle':\n",
    "    # train_data_dir = '/kaggle/input/reside-dataset/' + train_data_dir.strip('./')\n",
    "    # val_data_dir = '/kaggle/input/reside-dataset/' + val_data_dir.strip('./')\n",
    "    # train_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T'\n",
    "    # val_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V' \n",
    "    # train_data_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "    # val_data_dir = '/kaggle/input/o-haze/O-HAZY/GT' \n",
    "print('\\nFinal dataset paths:')\n",
    "print(f'Training directory: {train_data_dir}')\n",
    "print(f'Validation directory: {val_data_dir}')\n",
    "print(f'Number of epochs: {num_epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3c5ebda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:17.975287Z",
     "iopub.status.busy": "2025-05-02T06:42:17.974963Z",
     "iopub.status.idle": "2025-05-02T06:42:17.979568Z",
     "shell.execute_reply": "2025-05-02T06:42:17.978837Z"
    },
    "papermill": {
     "duration": 0.025087,
     "end_time": "2025-05-02T06:42:17.980830",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.955743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "91dc453e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:18.019595Z",
     "iopub.status.busy": "2025-05-02T06:42:18.018897Z",
     "iopub.status.idle": "2025-05-02T06:42:18.022664Z",
     "shell.execute_reply": "2025-05-02T06:42:18.022106Z"
    },
    "papermill": {
     "duration": 0.023912,
     "end_time": "2025-05-02T06:42:18.023670",
     "exception": false,
     "start_time": "2025-05-02T06:42:17.999758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hazeeffected_images_dir_train = f\"{train_data_dir}/IN\"\n",
    "hazeeffected_images_dir_train = f\"{train_data_dir}/hazy\"\n",
    "hazefree_images_dir_train = f\"{train_data_dir}/GT\"\n",
    "\n",
    "# hazeeffected_images_dir_valid = f\"{val_data_dir}/IN\"\n",
    "hazeeffected_images_dir_valid = f\"{val_data_dir}/hazy\"\n",
    "hazefree_images_dir_valid = f\"{val_data_dir}/GT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a64ae636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:18.062562Z",
     "iopub.status.busy": "2025-05-02T06:42:18.061962Z",
     "iopub.status.idle": "2025-05-02T06:42:18.065324Z",
     "shell.execute_reply": "2025-05-02T06:42:18.064744Z"
    },
    "papermill": {
     "duration": 0.024076,
     "end_time": "2025-05-02T06:42:18.066387",
     "exception": false,
     "start_time": "2025-05-02T06:42:18.042311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # hazeeffected_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "# # hazefree_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "# # hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "# # hazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/GT'\n",
    "\n",
    "# hazeeffected_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN'\n",
    "# hazefree_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT'\n",
    "# hazeeffected_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN'\n",
    "# hazefree_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/GT'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38bdee",
   "metadata": {
    "papermill": {
     "duration": 0.01798,
     "end_time": "2025-05-02T06:42:18.102634",
     "exception": false,
     "start_time": "2025-05-02T06:42:18.084654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Paths SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84b7c6bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:18.140125Z",
     "iopub.status.busy": "2025-05-02T06:42:18.139453Z",
     "iopub.status.idle": "2025-05-02T06:42:18.143446Z",
     "shell.execute_reply": "2025-05-02T06:42:18.142838Z"
    },
    "papermill": {
     "duration": 0.023993,
     "end_time": "2025-05-02T06:42:18.144576",
     "exception": false,
     "start_time": "2025-05-02T06:42:18.120583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sr_enabled = True\n",
    "    # sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n",
    "    # sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X4'\n",
    "if sr_enabled:\n",
    "    sr_hr_dir = '/kaggle/input/flickr-p/working/filtered_HR'\n",
    "    sr_lr_dir = '/kaggle/input/flickr-p/working/filtered_LR_2'\n",
    "    # sr_hr_dir = '/kaggle/input/sr-flickr/kaggle/working/filtered_HR'\n",
    "    # sr_lr_dir = '/kaggle/input/sr-flickr/kaggle/working/filtered_LR_2'\n",
    "    # sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n",
    "    # sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X4'\n",
    "    if execution_env == 'local':\n",
    "        # sr_hr_dir = './dataset/Flickr2K/Flickr2K_HR'\n",
    "        # sr_lr_dir = './dataset/Flickr2K/Flickr2K_LR_unknown/X4'\n",
    "        sr_hr_dir = './dataset/SR_flickr/kaggle/working/filtered_HR'\n",
    "        sr_lr_dir = './dataset/SR_flickr/kaggle/working/filtered_LR_2'\n",
    "        # sr_lr_dir = './dataset/SR_flickr/kaggle/working/filtered_LR/X4'\n",
    "        \n",
    "        # sr_hr_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/SR_flickr/kaggle/working/filtered_HR'\n",
    "        # sr_lr_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/SR_flickr/kaggle/working/filtered_LR/X4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "525d737d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:18.182553Z",
     "iopub.status.busy": "2025-05-02T06:42:18.182291Z",
     "iopub.status.idle": "2025-05-02T06:42:18.186130Z",
     "shell.execute_reply": "2025-05-02T06:42:18.185525Z"
    },
    "papermill": {
     "duration": 0.024245,
     "end_time": "2025-05-02T06:42:18.187272",
     "exception": false,
     "start_time": "2025-05-02T06:42:18.163027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Define paths\n",
    "# input_dir = 'dataset/SR_flickr/kaggle/working/filtered_HR'\n",
    "# output_dir = 'dataset/SR_flickr/kaggle/working/filtered_LR_2'\n",
    "\n",
    "# # Create output directory if it doesn't exist\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Function to resize images to half their resolution\n",
    "# def resize_image(input_path, output_path, target_size=(128, 128)):\n",
    "#     img = Image.open(input_path)\n",
    "#     img_resized = img.resize(target_size, Image.LANCZOS)\n",
    "#     img_resized.save(output_path)\n",
    "\n",
    "# # Get list of images in input directory\n",
    "# try:\n",
    "#     images = [f for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "# except FileNotFoundError as e:\n",
    "#     images = []\n",
    "#     error_message = str(e)\n",
    "\n",
    "# # Resize each image and save to output directory using tqdm\n",
    "# if images:\n",
    "#     for image_name in tqdm(images, desc=\"Resizing images\"):\n",
    "#         input_path = os.path.join(input_dir, image_name)\n",
    "#         output_path = os.path.join(output_dir, image_name)\n",
    "#         resize_image(input_path, output_path, target_size=(128, 128))\n",
    "#     result = \"Resizing completed\"\n",
    "# else:\n",
    "#     result = f\"Error: {error_message}\"\n",
    "\n",
    "# result = \"Resizing completed\" if images else f\"Error: {error_message}\"\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42b807ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:18.225931Z",
     "iopub.status.busy": "2025-05-02T06:42:18.225426Z",
     "iopub.status.idle": "2025-05-02T06:42:18.230636Z",
     "shell.execute_reply": "2025-05-02T06:42:18.229926Z"
    },
    "papermill": {
     "duration": 0.025929,
     "end_time": "2025-05-02T06:42:18.231742",
     "exception": false,
     "start_time": "2025-05-02T06:42:18.205813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_log(epoch, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, category):\n",
    "    log_dir = \"./training_log\"\n",
    "    os.makedirs(log_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    log_path = os.path.join(log_dir, f\"{category}_log.txt\")\n",
    "\n",
    "    print('({0:.0f}s) Epoch [{1}/{2}], Train_PSNR:{3:.2f}, Val_PSNR:{4:.2f}, Val_SSIM:{5:.4f}'\n",
    "          .format(one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim))\n",
    "\n",
    "    # --- Write the training log --- #\n",
    "    with open(log_path, 'a') as f:\n",
    "        print('Date: {0}, Time_Cost: {1:.0f}s, Epoch: [{2}/{3}], Train_PSNR: {4:.2f}, Val_PSNR: {5:.2f}, Val_SSIM: {6:.4f}'\n",
    "              .format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "                      one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d23161ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:18.269543Z",
     "iopub.status.busy": "2025-05-02T06:42:18.269278Z",
     "iopub.status.idle": "2025-05-02T06:42:18.272762Z",
     "shell.execute_reply": "2025-05-02T06:42:18.272129Z"
    },
    "papermill": {
     "duration": 0.023438,
     "end_time": "2025-05-02T06:42:18.273788",
     "exception": false,
     "start_time": "2025-05-02T06:42:18.250350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def adjust_learning_rate(optimizer, epoch, category, lr_decay=0.90):\n",
    "#     \"\"\"\n",
    "#     Adjusts the learning rate based on the epoch and dataset category.\n",
    "\n",
    "#     :param optimizer: The optimizer (e.g., Adam, SGD).\n",
    "#     :param epoch: Current epoch number.\n",
    "#     :param category: Dataset category ('indoor', 'outdoor', or 'NH').\n",
    "#     :param lr_decay: Multiplicative factor for learning rate decay.\n",
    "#     \"\"\"\n",
    "#     # Define learning rate decay steps based on category\n",
    "#     step_dict = {'indoor': 18, 'outdoor': 3, 'NH': 20}\n",
    "#     step = step_dict.get(category, 3)  # Default step size if category is unknown\n",
    "\n",
    "#     # Decay learning rate at the specified step\n",
    "#     if epoch > 0 and epoch % step == 0:\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] *= lr_decay\n",
    "#             print(f\"Epoch {epoch}: Learning rate adjusted to {param_group['lr']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a40ca",
   "metadata": {
    "papermill": {
     "duration": 0.018721,
     "end_time": "2025-05-02T06:42:18.310847",
     "exception": false,
     "start_time": "2025-05-02T06:42:18.292126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f7d5586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:18.348252Z",
     "iopub.status.busy": "2025-05-02T06:42:18.347902Z",
     "iopub.status.idle": "2025-05-02T06:42:18.357890Z",
     "shell.execute_reply": "2025-05-02T06:42:18.357049Z"
    },
    "papermill": {
     "duration": 0.030042,
     "end_time": "2025-05-02T06:42:18.359026",
     "exception": false,
     "start_time": "2025-05-02T06:42:18.328984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Perceptual Feature Loss Network --- #\n",
    "class PerceptualLossNet(nn.Module):\n",
    "    def __init__(self, vgg_model, use_style_loss=False, style_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = vgg_model\n",
    "        self.feature_layers = {'3': \"low_level\", '8': \"mid_level\", '15': \"high_level\"}\n",
    "        self.use_style_loss = use_style_loss\n",
    "        self.style_weight = style_weight\n",
    "\n",
    "        # Freeze VGG\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def get_feature_maps(self, x):\n",
    "        feature_maps = []\n",
    "        for layer_id, layer in self.feature_extractor.named_children():\n",
    "            x = layer(x)\n",
    "            if layer_id in self.feature_layers:\n",
    "                feature_maps.append(x)\n",
    "        return feature_maps\n",
    "\n",
    "    def compute_gram(self, feature):\n",
    "        b, c, h, w = feature.size()\n",
    "        feature = feature.view(b, c, -1)\n",
    "        gram = torch.bmm(feature, feature.transpose(1, 2)) / (c * h * w)\n",
    "        return gram\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        pred_feats = self.get_feature_maps(predicted)\n",
    "        target_feats = self.get_feature_maps(target)\n",
    "\n",
    "        # Perceptual loss (normalized MSE)\n",
    "        perceptual_loss = torch.stack([\n",
    "            F.mse_loss(F.normalize(p, dim=1), F.normalize(t, dim=1))\n",
    "            for p, t in zip(pred_feats, target_feats)\n",
    "        ]).mean()\n",
    "\n",
    "        if self.use_style_loss:\n",
    "            style_loss = torch.stack([\n",
    "                F.mse_loss(self.compute_gram(p), self.compute_gram(t))\n",
    "                for p, t in zip(pred_feats, target_feats)\n",
    "            ]).mean()\n",
    "            return perceptual_loss + self.style_weight * style_loss\n",
    "\n",
    "        return perceptual_loss\n",
    "\n",
    "\n",
    "class SSFM(nn.Module):\n",
    "    def __init__(self, loss_type='l1'):\n",
    "        super(SSFM, self).__init__()\n",
    "        assert loss_type in ['l1', 'l2'], \"loss_type must be 'l1' or 'l2'\"\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def forward(self, student_feats, teacher_feats):\n",
    "        \"\"\"\n",
    "        student_feats: List of feature maps from student RDBs [rdb1, rdb2, rdb3, rdb4]\n",
    "        teacher_feats: List of corresponding feature maps from teacher\n",
    "        \"\"\"\n",
    "        assert len(student_feats) == len(teacher_feats), \"Feature lists must match\"\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for s_feat, t_feat in zip(student_feats, teacher_feats):\n",
    "            # Match resolution\n",
    "            if s_feat.shape != t_feat.shape:\n",
    "                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            if self.loss_type == 'l1':\n",
    "                loss = F.l1_loss(s_feat, t_feat)\n",
    "            else:\n",
    "                loss = F.mse_loss(s_feat, t_feat)\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss / len(student_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30d09ab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:18.397352Z",
     "iopub.status.busy": "2025-05-02T06:42:18.396777Z",
     "iopub.status.idle": "2025-05-02T06:42:23.152960Z",
     "shell.execute_reply": "2025-05-02T06:42:23.152129Z"
    },
    "papermill": {
     "duration": 4.776302,
     "end_time": "2025-05-02T06:42:23.154152",
     "exception": false,
     "start_time": "2025-05-02T06:42:18.377850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:02<00:00, 191MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No pretrained weights found at /kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\n",
      "📊 Total Trainable Parameters: 5,210,051\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "\n",
    "# --- Initialize Model --- #\n",
    "net = DeepGuidedNet().to(device)\n",
    "\n",
    "# --- Enable Multi-GPU (if available) --- #\n",
    "if len(device_ids) > 1:\n",
    "    net = nn.DataParallel(net, device_ids=device_ids)\n",
    "\n",
    "# --- Optimizer --- #\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Load Pretrained VGG16 for Perceptual Loss --- #\n",
    "vgg_features = vgg16(pretrained=True).features[:16].to(device)\n",
    "for param in vgg_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "loss_network = PerceptualLossNet(vgg_features)\n",
    "loss_network.eval()\n",
    "\n",
    "# --- Load Model Weights (if available) --- #\n",
    "checkpoint_path = \"/kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\" \n",
    "\n",
    "try:\n",
    "    net.load_state_dict(torch.load(checkpoint_path, weights_only=False, map_location=torch.device('cpu')))\n",
    "    print(f\"✅ Model weights loaded from {checkpoint_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️ No pretrained weights found at {checkpoint_path}\")\n",
    "\n",
    "# --- Compute Total Trainable Parameters --- #\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"📊 Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbb669fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:23.196476Z",
     "iopub.status.busy": "2025-05-02T06:42:23.195837Z",
     "iopub.status.idle": "2025-05-02T06:42:23.201887Z",
     "shell.execute_reply": "2025-05-02T06:42:23.201307Z"
    },
    "papermill": {
     "duration": 0.028265,
     "end_time": "2025-05-02T06:42:23.203041",
     "exception": false,
     "start_time": "2025-05-02T06:42:23.174776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeatureAffinityModule(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(FeatureAffinityModule, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def forward(self, student_features, teacher_features):\n",
    "        # Debug: Print input shapes\n",
    "        # print(\"Input student_features shape:\", student_features.shape)\n",
    "        # print(\"Input teacher_features shape:\", teacher_features.shape)\n",
    "\n",
    "        # # Normalize features\n",
    "        # print(\"\\nBefore normalization:\")\n",
    "        # print(\"Student features:\", student_features.shape)  # Example for the first feature of the first batch\n",
    "        # print(\"Teacher features:\", teacher_features.shape)  # Example for the first feature of the first batch\n",
    "\n",
    "        student_norm = F.normalize(student_features.view(student_features.size(0), self.channels, -1), dim=2)\n",
    "        # print(\"Student normalized features:\", student_norm.shape)  # Checking first normalized value\n",
    "\n",
    "        # print(\"\\nother:\")\n",
    "        # print(\"student_features.size(0)\",student_features.size(0))\n",
    "        # print(\"teacher_features.size(0)\",teacher_features.size(0))\n",
    "        # print(\"self.channels\",self.channels)\n",
    "        # print(\"student_features.view(student_features.size(0), self.channels, -1)\",student_features.view(student_features.size(0), self.channels, -1).shape)\n",
    "        # print(\"teacher_features.view(teacher_features.size(0), self.channels, -1)\",teacher_features.view(teacher_features.size(0), self.channels, -1).shape)\n",
    "        \n",
    "        teacher_norm = F.normalize(teacher_features.view(teacher_features.size(0), self.channels, -1), dim=2)\n",
    "        # print(\"Teacher normalized features:\", teacher_norm[0, 0, :2])  # Checking first normalized value\n",
    "\n",
    "        # Compute affinity matrices\n",
    "        student_affinity = torch.bmm(student_norm, student_norm.transpose(1, 2))\n",
    "        teacher_affinity = torch.bmm(teacher_norm, teacher_norm.transpose(1, 2))\n",
    "\n",
    "        # Debug: Print affinity matrix shapes\n",
    "        # print(\"\\nStudent affinity matrix shape:\", student_affinity.shape)\n",
    "        # print(\"Teacher affinity matrix shape:\", teacher_affinity.shape)\n",
    "\n",
    "        # Compute KL divergence\n",
    "        loss = F.kl_div(F.log_softmax(student_affinity, dim=-1),\n",
    "                        F.softmax(teacher_affinity, dim=-1),\n",
    "                        reduction='batchmean')\n",
    "\n",
    "        # Debug: Print computed loss\n",
    "        # print(\"\\nComputed loss:\", loss.item())\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f8be352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:23.244871Z",
     "iopub.status.busy": "2025-05-02T06:42:23.244223Z",
     "iopub.status.idle": "2025-05-02T06:42:23.247565Z",
     "shell.execute_reply": "2025-05-02T06:42:23.246950Z"
    },
    "papermill": {
     "duration": 0.025272,
     "end_time": "2025-05-02T06:42:23.248640",
     "exception": false,
     "start_time": "2025-05-02T06:42:23.223368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae5637",
   "metadata": {
    "papermill": {
     "duration": 0.019228,
     "end_time": "2025-05-02T06:42:23.288121",
     "exception": false,
     "start_time": "2025-05-02T06:42:23.268893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c0b77a",
   "metadata": {
    "papermill": {
     "duration": 0.020801,
     "end_time": "2025-05-02T06:42:23.328430",
     "exception": false,
     "start_time": "2025-05-02T06:42:23.307629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Haze Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5685989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:23.369965Z",
     "iopub.status.busy": "2025-05-02T06:42:23.369415Z",
     "iopub.status.idle": "2025-05-02T06:42:23.380467Z",
     "shell.execute_reply": "2025-05-02T06:42:23.379656Z"
    },
    "papermill": {
     "duration": 0.032888,
     "end_time": "2025-05-02T06:42:23.381647",
     "exception": false,
     "start_time": "2025-05-02T06:42:23.348759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class HazeDataset(Dataset):\n",
    "    def __init__(self, hazeeffected_images_dir, hazefree_images_dir, split=\"train\", split_ratio=(0.8, 0.1, 0.1), random_seed=42):\n",
    "        \"\"\"\n",
    "        Dataset class for handling hazy and corresponding ground-truth images (already aligned and cropped).\n",
    "\n",
    "        Args:\n",
    "            hazeeffected_images_dir (str): Directory for hazy images.\n",
    "            hazefree_images_dir (str): Directory for ground-truth images.\n",
    "            split (str): \"train\", \"valid\", or \"test\" (determines data split).\n",
    "            split_ratio (tuple): A tuple of three floats representing the train, validation, and test splits.\n",
    "                                 (default 80% train, 10% validation, 10% test).\n",
    "            random_seed (int): Random seed for reproducibility in shuffling data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if sum(split_ratio) != 1.0:\n",
    "            raise ValueError(\"The sum of split_ratio must be 1.0\")\n",
    "\n",
    "        self.train_ratio, self.valid_ratio, self.test_ratio = split_ratio\n",
    "\n",
    "        valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        # print(\"Haze affected images dir: \", hazeeffected_images_dir)\n",
    "        hazy_data = [\n",
    "            f for f in glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_extensions)\n",
    "        ]\n",
    "\n",
    "        if not hazy_data:\n",
    "            raise ValueError(f\"No valid images found in {hazeeffected_images_dir}\")\n",
    "\n",
    "        hazy_data.sort()\n",
    "\n",
    "        # Shuffle the dataset for randomization\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(hazy_data)\n",
    "\n",
    "        # Split dataset based on provided ratios\n",
    "        total_images = len(hazy_data)\n",
    "        train_size = int(total_images * self.train_ratio)\n",
    "        valid_size = int(total_images * self.valid_ratio)\n",
    "        test_size = total_images - train_size - valid_size\n",
    "\n",
    "        if split == \"train\":\n",
    "            hazy_data = hazy_data[:train_size]\n",
    "        elif split == \"valid\":\n",
    "            hazy_data = hazy_data[train_size:train_size+valid_size]\n",
    "        elif split == \"test\":\n",
    "            hazy_data = hazy_data[train_size+valid_size:]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split value: {split}. Choose from ['train', 'valid', 'test'].\")\n",
    "\n",
    "        self.haze_names = []\n",
    "        self.gt_names = []\n",
    "\n",
    "        for h_image in hazy_data:\n",
    "            filename = os.path.basename(h_image)\n",
    "            haze_path = os.path.join(hazeeffected_images_dir, filename)\n",
    "            gt_path = os.path.join(hazefree_images_dir, filename)\n",
    "\n",
    "            if not os.path.exists(gt_path):\n",
    "                print(f\"Warning: Ground-truth missing for {filename}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            self.haze_names.append(haze_path)\n",
    "            self.gt_names.append(gt_path)\n",
    "\n",
    "        if not self.haze_names:\n",
    "            raise ValueError(\"No matching ground-truth images found.\")\n",
    "\n",
    "        # Define transforms\n",
    "        self.transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        self.transform_gt = Compose([ToTensor()])\n",
    "\n",
    "    def get_images(self, index):\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        try:\n",
    "            haze_img = Image.open(haze_name).convert('RGB')\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Invalid image format: {haze_name} or {gt_name}\")\n",
    "\n",
    "        haze = self.transform_haze(haze_img)\n",
    "        gt = self.transform_gt(gt_img)\n",
    "\n",
    "        if haze.shape[0] != 3 or gt.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {haze_name}\")\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_images(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a13941",
   "metadata": {
    "papermill": {
     "duration": 0.019494,
     "end_time": "2025-05-02T06:42:23.421257",
     "exception": false,
     "start_time": "2025-05-02T06:42:23.401763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Haze DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "323480b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:23.461667Z",
     "iopub.status.busy": "2025-05-02T06:42:23.461375Z",
     "iopub.status.idle": "2025-05-02T06:42:31.068898Z",
     "shell.execute_reply": "2025-05-02T06:42:31.068312Z"
    },
    "papermill": {
     "duration": 7.629479,
     "end_time": "2025-05-02T06:42:31.070450",
     "exception": false,
     "start_time": "2025-05-02T06:42:23.440971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For training:\n",
    "train_dataset = HazeDataset(hazeeffected_images_dir=hazeeffected_images_dir_train, \n",
    "                             hazefree_images_dir=hazefree_images_dir_train, \n",
    "                             split=\"train\", \n",
    "                             split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "# For validation:\n",
    "val_dataset = HazeDataset(hazeeffected_images_dir=hazeeffected_images_dir_train, \n",
    "                             hazefree_images_dir=hazefree_images_dir_train, \n",
    "                             split=\"valid\", \n",
    "                             split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "# For testing:\n",
    "test_dataset = HazeDataset(hazeeffected_images_dir=hazeeffected_images_dir_train, \n",
    "                            hazefree_images_dir=hazefree_images_dir_train, \n",
    "                            split=\"test\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdbba5a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:31.112172Z",
     "iopub.status.busy": "2025-05-02T06:42:31.111467Z",
     "iopub.status.idle": "2025-05-02T06:42:31.115759Z",
     "shell.execute_reply": "2025-05-02T06:42:31.115142Z"
    },
    "papermill": {
     "duration": 0.026002,
     "end_time": "2025-05-02T06:42:31.116823",
     "exception": false,
     "start_time": "2025-05-02T06:42:31.090821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4800, Validation samples: 600, Test samples: 600\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0bb9cc4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:31.157355Z",
     "iopub.status.busy": "2025-05-02T06:42:31.157055Z",
     "iopub.status.idle": "2025-05-02T06:42:31.161110Z",
     "shell.execute_reply": "2025-05-02T06:42:31.160578Z"
    },
    "papermill": {
     "duration": 0.025521,
     "end_time": "2025-05-02T06:42:31.162302",
     "exception": false,
     "start_time": "2025-05-02T06:42:31.136781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "346f16a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:31.203933Z",
     "iopub.status.busy": "2025-05-02T06:42:31.203442Z",
     "iopub.status.idle": "2025-05-02T06:42:31.206848Z",
     "shell.execute_reply": "2025-05-02T06:42:31.206126Z"
    },
    "papermill": {
     "duration": 0.025453,
     "end_time": "2025-05-02T06:42:31.208028",
     "exception": false,
     "start_time": "2025-05-02T06:42:31.182575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(glob.glob( \"/kaggle/working/cropped_train/hazy/*\")), len(glob.glob(\"/kaggle/working/cropped_train/GT/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbefa5b",
   "metadata": {
    "papermill": {
     "duration": 0.020013,
     "end_time": "2025-05-02T06:42:31.248860",
     "exception": false,
     "start_time": "2025-05-02T06:42:31.228847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e20601e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:31.290263Z",
     "iopub.status.busy": "2025-05-02T06:42:31.289792Z",
     "iopub.status.idle": "2025-05-02T06:42:31.299877Z",
     "shell.execute_reply": "2025-05-02T06:42:31.299192Z"
    },
    "papermill": {
     "duration": 0.032152,
     "end_time": "2025-05-02T06:42:31.301152",
     "exception": false,
     "start_time": "2025-05-02T06:42:31.269000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, scale='x4', split='train', split_ratio=(0.8, 0.1, 0.1), random_seed=42):\n",
    "        \"\"\"\n",
    "        Super-Resolution dataset that matches LR and HR image pairs based on naming pattern.\n",
    "        Assumes images are already aligned and correctly scaled. No cropping is applied.\n",
    "\n",
    "        Args:\n",
    "            lr_dir (str): Directory containing low-resolution images (e.g., x2, x3, x4).\n",
    "            hr_dir (str): Directory containing high-resolution images.\n",
    "            scale (str): Scale suffix in LR filenames (e.g., 'x2', 'x3', 'x4').\n",
    "            split (str): 'train', 'val', or 'test' (determines data split).\n",
    "            split_ratio (tuple): A tuple of three floats representing the train, validation, and test splits.\n",
    "                                 (default 80% train, 10% validation, 10% test).\n",
    "            random_seed (int): Random seed for reproducibility in shuffling data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if sum(split_ratio) != 1.0:\n",
    "            raise ValueError(\"The sum of split_ratio must be 1.0\")\n",
    "\n",
    "        self.train_ratio, self.valid_ratio, self.test_ratio = split_ratio\n",
    "\n",
    "        valid_ext = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        lr_images = sorted([\n",
    "            f for f in glob.glob(os.path.join(lr_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_ext)\n",
    "        ])\n",
    "\n",
    "        lr_hr_pairs = []\n",
    "        for lr_path in lr_images:\n",
    "            lr_name = os.path.basename(lr_path)\n",
    "            hr_name = lr_name.replace(scale, '')  # assumes naming like image_x4.png -> image.png\n",
    "            hr_path = os.path.join(hr_dir, hr_name)\n",
    "\n",
    "            if not os.path.exists(hr_path):\n",
    "                print(f\"Warning: Ground-truth missing for {lr_name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            lr_hr_pairs.append((lr_path, hr_path))\n",
    "\n",
    "        if not lr_hr_pairs:\n",
    "            raise ValueError(\"No matching LR-HR image pairs found.\")\n",
    "\n",
    "        # Shuffle the dataset for randomization\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(lr_hr_pairs)\n",
    "\n",
    "        # Split dataset based on provided ratios\n",
    "        total_pairs = len(lr_hr_pairs)\n",
    "        train_size = int(total_pairs * self.train_ratio)\n",
    "        valid_size = int(total_pairs * self.valid_ratio)\n",
    "        test_size = total_pairs - train_size - valid_size\n",
    "\n",
    "        # Splitting the data based on the chosen split\n",
    "        if split == 'train':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[:train_size]\n",
    "        elif split == 'val':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[train_size:train_size+valid_size]\n",
    "        elif split == 'test':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[train_size+valid_size:]\n",
    "        else:\n",
    "            raise ValueError(\"split must be either 'train', 'val', or 'test'\")\n",
    "\n",
    "        # Define common transform\n",
    "        self.transform = Compose([ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_hr_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_path, hr_path = self.lr_hr_pairs[idx]\n",
    "\n",
    "        try:\n",
    "            lr_img = Image.open(lr_path).convert('RGB')\n",
    "            hr_img = Image.open(hr_path).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Unidentified image at {lr_path} or {hr_path}\")\n",
    "\n",
    "        lr_tensor = self.transform(lr_img)\n",
    "        hr_tensor = self.transform(hr_img)\n",
    "\n",
    "        if lr_tensor.shape[0] != 3 or hr_tensor.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {lr_path}\")\n",
    "\n",
    "        return lr_tensor, hr_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65096b",
   "metadata": {
    "papermill": {
     "duration": 0.019748,
     "end_time": "2025-05-02T06:42:31.341436",
     "exception": false,
     "start_time": "2025-05-02T06:42:31.321688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SR DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1269e558",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:31.385066Z",
     "iopub.status.busy": "2025-05-02T06:42:31.384405Z",
     "iopub.status.idle": "2025-05-02T06:42:31.389520Z",
     "shell.execute_reply": "2025-05-02T06:42:31.388956Z"
    },
    "papermill": {
     "duration": 0.026934,
     "end_time": "2025-05-02T06:42:31.390581",
     "exception": false,
     "start_time": "2025-05-02T06:42:31.363647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def custom_collate_fn(batch):\n",
    "#     min_height = min([x[0].shape[1] for x in batch])\n",
    "#     min_width = min([x[0].shape[2] for x in batch])\n",
    "#     resized_batch = [(TF.resize(x[0], [min_height, min_width]), TF.resize(x[1], [min_height, min_width])) for x in batch]\n",
    "#     return torch.utils.data.dataloader.default_collate(resized_batch)\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Assumes each item in batch is a tuple: (input_tensor, gt_tensor)\n",
    "    # Input is already 64x64, ground truth is 256x256\n",
    "    resized_batch = []\n",
    "    for input_tensor, gt_tensor in batch:\n",
    "        # Keep input as-is, downsample ground truth to 64x64\n",
    "        resized_gt = TF.resize(gt_tensor, [64, 64], interpolation=TF.InterpolationMode.NEAREST)\n",
    "        resized_batch.append((input_tensor, resized_gt))\n",
    "    return torch.utils.data.dataloader.default_collate(resized_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "020fa0f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:31.432422Z",
     "iopub.status.busy": "2025-05-02T06:42:31.431726Z",
     "iopub.status.idle": "2025-05-02T06:42:35.612695Z",
     "shell.execute_reply": "2025-05-02T06:42:35.612117Z"
    },
    "papermill": {
     "duration": 4.203287,
     "end_time": "2025-05-02T06:42:35.613990",
     "exception": false,
     "start_time": "2025-05-02T06:42:31.410703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- SR Dataset Setup --- #\n",
    "\n",
    "if sr_enabled:\n",
    "\n",
    "    # Dataset for Super-Resolution (SR) task\n",
    "    # For training:\n",
    "    sr_train_dataset = SRDataset(lr_dir=sr_lr_dir, \n",
    "                            hr_dir=sr_hr_dir,\n",
    "                            scale=\"x4\", \n",
    "                            split=\"train\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "    # For validation:\n",
    "    sr_val_dataset = SRDataset(lr_dir=sr_lr_dir, \n",
    "                            hr_dir=sr_hr_dir, \n",
    "                            scale=\"x4\", \n",
    "                            split=\"val\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "    # For testing:\n",
    "    sr_test_dataset = SRDataset(lr_dir=sr_lr_dir, \n",
    "                            hr_dir=sr_hr_dir, \n",
    "                            scale=\"x4\", \n",
    "                            split=\"test\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n",
    "    \n",
    "    # DataLoader for training with drop_last=True to avoid smaller batches\n",
    "    sr_train_loader = DataLoader(\n",
    "        sr_train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        # collate_fn=custom_collate_fn,\n",
    "        drop_last=True  # Ensure the final batch is dropped if not full\n",
    "    )\n",
    "    \n",
    "    # DataLoader for validation with drop_last=True for consistency (optional)\n",
    "    sr_val_loader = DataLoader(\n",
    "        sr_val_dataset,\n",
    "        batch_size=val_batch_size,\n",
    "        shuffle=False,  # Don't shuffle the validation set\n",
    "        # collate_fn=custom_collate_fn,\n",
    "        drop_last=True  # Optional: consider it for consistency across train/val\n",
    "    )\n",
    "\n",
    "    # DataLoader for testing (No shuffling and no drop_last)\n",
    "    sr_test_loader = DataLoader(\n",
    "        sr_test_dataset,\n",
    "        batch_size=val_batch_size,  # Define your test batch size\n",
    "        shuffle=False,  # No shuffling needed for testing\n",
    "        # collate_fn=custom_collate_fn,  # Use custom collate function if needed\n",
    "        drop_last=False  # We typically do not drop the last batch for testing\n",
    "    )\n",
    "\n",
    "    # Create an iterator for the training set\n",
    "    sr_iter = iter(sr_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39088a87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:35.655913Z",
     "iopub.status.busy": "2025-05-02T06:42:35.655231Z",
     "iopub.status.idle": "2025-05-02T06:42:35.659662Z",
     "shell.execute_reply": "2025-05-02T06:42:35.658855Z"
    },
    "papermill": {
     "duration": 0.026254,
     "end_time": "2025-05-02T06:42:35.660848",
     "exception": false,
     "start_time": "2025-05-02T06:42:35.634594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR Train samples: 2120, SR Validation samples: 265, SR Test samples: 265\n"
     ]
    }
   ],
   "source": [
    "print(f\"SR Train samples: {len(sr_train_dataset)}, SR Validation samples: {len(sr_val_dataset)}, SR Test samples: {len(sr_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35244298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:35.701808Z",
     "iopub.status.busy": "2025-05-02T06:42:35.701227Z",
     "iopub.status.idle": "2025-05-02T06:42:35.757507Z",
     "shell.execute_reply": "2025-05-02T06:42:35.756761Z"
    },
    "papermill": {
     "duration": 0.077875,
     "end_time": "2025-05-02T06:42:35.758747",
     "exception": false,
     "start_time": "2025-05-02T06:42:35.680872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128]) torch.Size([2, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for i,o in train_data_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "52777735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:35.801249Z",
     "iopub.status.busy": "2025-05-02T06:42:35.800653Z",
     "iopub.status.idle": "2025-05-02T06:42:35.819635Z",
     "shell.execute_reply": "2025-05-02T06:42:35.818778Z"
    },
    "papermill": {
     "duration": 0.041285,
     "end_time": "2025-05-02T06:42:35.820881",
     "exception": false,
     "start_time": "2025-05-02T06:42:35.779596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128]) torch.Size([2, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for i,o in val_data_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "30cc3d06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:35.864596Z",
     "iopub.status.busy": "2025-05-02T06:42:35.864052Z",
     "iopub.status.idle": "2025-05-02T06:42:35.899950Z",
     "shell.execute_reply": "2025-05-02T06:42:35.899100Z"
    },
    "papermill": {
     "duration": 0.057551,
     "end_time": "2025-05-02T06:42:35.901320",
     "exception": false,
     "start_time": "2025-05-02T06:42:35.843769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128]) torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i,o in sr_train_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2fa5df4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:35.943473Z",
     "iopub.status.busy": "2025-05-02T06:42:35.943207Z",
     "iopub.status.idle": "2025-05-02T06:42:35.977384Z",
     "shell.execute_reply": "2025-05-02T06:42:35.976486Z"
    },
    "papermill": {
     "duration": 0.056199,
     "end_time": "2025-05-02T06:42:35.978665",
     "exception": false,
     "start_time": "2025-05-02T06:42:35.922466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128]) torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i,o in sr_val_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "10323503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:36.020692Z",
     "iopub.status.busy": "2025-05-02T06:42:36.020441Z",
     "iopub.status.idle": "2025-05-02T06:42:36.025482Z",
     "shell.execute_reply": "2025-05-02T06:42:36.024745Z"
    },
    "papermill": {
     "duration": 0.027316,
     "end_time": "2025-05-02T06:42:36.026763",
     "exception": false,
     "start_time": "2025-05-02T06:42:35.999447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2120, 265, 265)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sr_train_dataset), len(sr_val_dataset), len(sr_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd89eee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:36.069216Z",
     "iopub.status.busy": "2025-05-02T06:42:36.068916Z",
     "iopub.status.idle": "2025-05-02T06:42:36.205372Z",
     "shell.execute_reply": "2025-05-02T06:42:36.204552Z"
    },
    "papermill": {
     "duration": 0.158822,
     "end_time": "2025-05-02T06:42:36.206717",
     "exception": false,
     "start_time": "2025-05-02T06:42:36.047895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 1: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 2: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 3: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 4: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# enumerate\n",
    "for i, (lr, hr) in enumerate(sr_train_loader):\n",
    "    print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "    if i == 4:  # Just to limit the output\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3b61d",
   "metadata": {
    "papermill": {
     "duration": 0.020603,
     "end_time": "2025-05-02T06:42:36.253599",
     "exception": false,
     "start_time": "2025-05-02T06:42:36.232996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Init Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "137d4c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:36.295142Z",
     "iopub.status.busy": "2025-05-02T06:42:36.294844Z",
     "iopub.status.idle": "2025-05-02T06:42:36.314252Z",
     "shell.execute_reply": "2025-05-02T06:42:36.313668Z"
    },
    "papermill": {
     "duration": 0.041627,
     "end_time": "2025-05-02T06:42:36.315462",
     "exception": false,
     "start_time": "2025-05-02T06:42:36.273835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Teacher Network --- #\n",
    "sr_net = DeepGuidedNetwork(radius=1).to(device)\n",
    "# pretrained_path = r'C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run5_200_teacher_rettrain\\teacher_net_sr_final_199.pth'\n",
    "# pretrained = torch.load(pretrained_path, map_location=device, weights_only=False)\n",
    "# sr_net.load_state_dict(pretrained)\n",
    "\n",
    "# Remove tail weights that do not match the new ×2 architecture\n",
    "# for key in list(pretrained.keys()):\n",
    "#     if key.startswith('tail.2') or key.startswith('tail.3'):\n",
    "#         del pretrained[key]\n",
    "\n",
    "# Load partial weights into the new model\n",
    "# teacher_net.eval()\n",
    "# for param in teacher_net.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "\n",
    "# Apply Xavier initialization to the new tail.2 Conv2d (output 12 channels for PixelShuffle(2))\n",
    "# with torch.no_grad():\n",
    "#     nn.init.xavier_uniform_(teacher_net.tail[2].weight)\n",
    "#     nn.init.zeros_(teacher_net.tail[2].bias)\n",
    "# teacher_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "72db6650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:36.377769Z",
     "iopub.status.busy": "2025-05-02T06:42:36.377407Z",
     "iopub.status.idle": "2025-05-02T06:42:36.871003Z",
     "shell.execute_reply": "2025-05-02T06:42:36.870218Z"
    },
    "papermill": {
     "duration": 0.534763,
     "end_time": "2025-05-02T06:42:36.872245",
     "exception": false,
     "start_time": "2025-05-02T06:42:36.337482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_net_path = r\"/kaggle/input/reside-dehaze-from-student/pytorch/default/1/net_haze_iter_85.pth\"\n",
    "pretrained_net = torch.load(pretrained_net_path, map_location=device, weights_only=False)\n",
    "net.load_state_dict(pretrained_net, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d6025551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:42:36.915359Z",
     "iopub.status.busy": "2025-05-02T06:42:36.914722Z",
     "iopub.status.idle": "2025-05-02T06:43:03.587994Z",
     "shell.execute_reply": "2025-05-02T06:43:03.587127Z"
    },
    "papermill": {
     "duration": 26.695982,
     "end_time": "2025-05-02T06:43:03.589360",
     "exception": false,
     "start_time": "2025-05-02T06:42:36.893378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Init Val] PSNR: 21.21, SSIM: 0.6802\n",
      "[SR Init Val] PSNR: 6.80, SSIM: 0.0619\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Feature Affinity Module --- #\n",
    "fam = FeatureAffinityModule(channels=16).to(device)\n",
    "ssfm_loss = SSFM(loss_type='l1') \n",
    "\n",
    "\n",
    "# --- Initial Validation --- #\n",
    "old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)\n",
    "print(f\"[Dehazing Init Val] PSNR: {old_val_psnr:.2f}, SSIM: {old_val_ssim:.4f}\")\n",
    "if sr_enabled:\n",
    "    sr_val_psnr, sr_val_ssim = validation_sr(sr_net, sr_val_loader, device)\n",
    "    print(f\"[SR Init Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "# --- Training Loop --- #\n",
    "best_psnr = old_val_psnr\n",
    "best_psnr_sr = sr_val_psnr\n",
    "train_psnr_prev = 0\n",
    "distillation_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "88bf090f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:43:03.632797Z",
     "iopub.status.busy": "2025-05-02T06:43:03.632073Z",
     "iopub.status.idle": "2025-05-02T06:43:03.644288Z",
     "shell.execute_reply": "2025-05-02T06:43:03.643720Z"
    },
    "papermill": {
     "duration": 0.034948,
     "end_time": "2025-05-02T06:43:03.645423",
     "exception": false,
     "start_time": "2025-05-02T06:43:03.610475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "sr_net = sr_net.to(device)\n",
    "fam = fam.to(device)\n",
    "loss_network = loss_network.to(device)\n",
    "ssfm_loss = ssfm_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3709eb",
   "metadata": {
    "papermill": {
     "duration": 0.020188,
     "end_time": "2025-05-02T06:43:03.686505",
     "exception": false,
     "start_time": "2025-05-02T06:43:03.666317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c2893fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T06:43:03.728952Z",
     "iopub.status.busy": "2025-05-02T06:43:03.728274Z",
     "iopub.status.idle": "2025-05-02T07:50:41.111312Z",
     "shell.execute_reply": "2025-05-02T07:50:41.110546Z"
    },
    "papermill": {
     "duration": 4059.571039,
     "end_time": "2025-05-02T07:50:43.277983",
     "exception": false,
     "start_time": "2025-05-02T06:43:03.706944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 06:43:05.378526: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746168185.557477      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746168185.610584      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - net saved in epoch 0.\n",
      "[Dehazing Val] PSNR: 21.00, SSIM: 0.6762\n",
      "(415s) Epoch [1/10], Train_PSNR:21.81, Val_PSNR:21.00, Val_SSIM:0.6762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 19.97, SSIM: 0.6655\n",
      "(396s) Epoch [2/10], Train_PSNR:21.85, Val_PSNR:19.97, Val_SSIM:0.6655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 20.99, SSIM: 0.6784\n",
      "(397s) Epoch [3/10], Train_PSNR:21.84, Val_PSNR:20.99, Val_SSIM:0.6784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 20.81, SSIM: 0.6761\n",
      "(401s) Epoch [4/10], Train_PSNR:21.89, Val_PSNR:20.81, Val_SSIM:0.6761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 21.04, SSIM: 0.6779\n",
      "(404s) Epoch [5/10], Train_PSNR:21.84, Val_PSNR:21.04, Val_SSIM:0.6779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - net saved in epoch 5.\n",
      "[Dehazing Val] PSNR: 20.62, SSIM: 0.6716\n",
      "(404s) Epoch [6/10], Train_PSNR:22.01, Val_PSNR:20.62, Val_SSIM:0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 20.86, SSIM: 0.6771\n",
      "(403s) Epoch [7/10], Train_PSNR:22.01, Val_PSNR:20.86, Val_SSIM:0.6771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 20.87, SSIM: 0.6711\n",
      "(406s) Epoch [8/10], Train_PSNR:22.05, Val_PSNR:20.87, Val_SSIM:0.6711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 20.84, SSIM: 0.6752\n",
      "(407s) Epoch [9/10], Train_PSNR:22.03, Val_PSNR:20.84, Val_SSIM:0.6752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 20.94, SSIM: 0.6768\n",
      "(410s) Epoch [10/10], Train_PSNR:22.41, Val_PSNR:20.94, Val_SSIM:0.6768\n",
      "Model - net saved in epoch 9.\n",
      "🏁 Training completed in 67.38 minutes.\n",
      "TensorBoard logs saved in: runs/net_haze_1\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# Create a directory for TensorBoard logs\n",
    "log_dir = \"runs/net_haze_1\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Initialize global variables\n",
    "global_iter = 0\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Optimizer for net only\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# LR scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# --- Training Loop --- #\n",
    "best_psnr = old_val_psnr\n",
    "train_psnr_prev = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    psnr_list = []\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # Training loop with tqdm progress bar\n",
    "    train_loader_tqdm = tqdm(train_data_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "    for batch_id, (haze, gt) in enumerate(train_loader_tqdm):\n",
    "        haze, gt = haze.to(device), gt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass - student (net)\n",
    "        dehaze, base, s = net(haze)\n",
    "\n",
    "        # Student losses\n",
    "        base_loss = F.smooth_l1_loss(base, gt)\n",
    "        smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "        perceptual_loss = loss_network(dehaze, gt)\n",
    "        total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        psnr_list.extend(to_psnr(dehaze, gt))\n",
    "        train_loader_tqdm.set_postfix(loss=total_loss.item(), iter=global_iter)\n",
    "        global_iter += 1\n",
    "\n",
    "    train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "    # Save models every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        iter_model_path = f\"net_haze_iter_{epoch}.pth\"\n",
    "        torch.save(net.state_dict(), iter_model_path)\n",
    "        print(f\"Model - net saved in epoch {epoch}.\")\n",
    "\n",
    "    # === Validation ===\n",
    "    net.eval()\n",
    "    val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "    print(f\"[Dehazing Val] PSNR: {val_psnr:.2f}, SSIM: {val_ssim:.4f}\")\n",
    "    writer.add_scalar(\"Metrics/Val_PSNR_Dehaze\", val_psnr, epoch)\n",
    "    writer.add_scalar(\"Metrics/Val_SSIM_Dehaze\", val_ssim, epoch)\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    model_path = f\"net_haze_{version}.pth\"\n",
    "    print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "    # Learning rate scheduler based on validation PSNR\n",
    "    scheduler.step(val_psnr)\n",
    "\n",
    "    # Best model save\n",
    "    if val_psnr >= best_psnr:\n",
    "        best_model_path = f\"net_haze_best_{version}.pth\"\n",
    "        torch.save(net.state_dict(), best_model_path)\n",
    "        best_psnr = val_psnr\n",
    "\n",
    "    train_psnr_prev = train_psnr\n",
    "\n",
    "# Final save\n",
    "final_path = f\"net_final_{epoch}.pth\"\n",
    "torch.save(net.state_dict(), final_path)\n",
    "print(f\"Model - net saved in epoch {epoch}.\")\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"🏁 Training completed in {total_time/60:.2f} minutes.\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "print(\"TensorBoard logs saved in:\", log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c4570d43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:50:47.557223Z",
     "iopub.status.busy": "2025-05-02T07:50:47.556455Z",
     "iopub.status.idle": "2025-05-02T07:50:47.565396Z",
     "shell.execute_reply": "2025-05-02T07:50:47.564663Z"
    },
    "papermill": {
     "duration": 2.232974,
     "end_time": "2025-05-02T07:50:47.566594",
     "exception": false,
     "start_time": "2025-05-02T07:50:45.333620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Create a directory for TensorBoard logs\n",
    "# log_dir = \"runs/co_distillation_logs\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# # TensorBoard writer\n",
    "# writer = SummaryWriter(log_dir)\n",
    "\n",
    "# # Initialize global variables\n",
    "# global_iter = 0\n",
    "# total_start_time = time.time()\n",
    "\n",
    "# # Optimizer for net and teacher_net (joint training)\n",
    "# optimizer = torch.optim.Adam(list(net.parameters()) + list(sr_net.parameters()), lr=learning_rate)\n",
    "\n",
    "# # LR scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# # Apply Xavier initialization to the tail of the teacher_net (for PixelShuffle)\n",
    "# with torch.no_grad():\n",
    "#     nn.init.xavier_uniform_(sr_net.tail[2].weight)\n",
    "#     nn.init.zeros_(sr_net.tail[2].bias)\n",
    "\n",
    "# # --- Feature Affinity Module and SSFM loss --- #\n",
    "# fam = FeatureAffinityModule(channels=16).to(device)\n",
    "# ssfm_loss = SSFM(loss_type='l1')\n",
    "\n",
    "# # --- Training Loop --- #\n",
    "# best_psnr = old_val_psnr\n",
    "# best_psnr_sr = sr_val_psnr\n",
    "# train_psnr_prev = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_start_time = time.time()\n",
    "#     psnr_list = []\n",
    "\n",
    "#     net.train()\n",
    "#     sr_net.train()\n",
    "\n",
    "#     # Training loop with tqdm progress bar\n",
    "#     train_loader_tqdm = tqdm(train_data_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "#     for batch_id, (haze, gt) in enumerate(train_loader_tqdm):\n",
    "#         haze, gt = haze.to(device), gt.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass - student (net)\n",
    "#         dehaze, base, s = net(haze)\n",
    "#         s1, s2, s3, s4 = s\n",
    "\n",
    "#         # Student losses\n",
    "#         base_loss = F.smooth_l1_loss(base, gt)\n",
    "#         smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "#         perceptual_loss = loss_network(dehaze, gt)\n",
    "#         total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "\n",
    "#         # Teacher SR logic (if SR is enabled)\n",
    "#         if sr_enabled:\n",
    "#             try:\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             except StopIteration:\n",
    "#                 sr_iter = iter(sr_train_loader)\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n",
    "\n",
    "#             # Teacher forward (SR)\n",
    "#             sr_out, _, t = sr_net(sr_lr)\n",
    "#             sr_hr = F.interpolate(sr_hr, size=sr_out.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "#             sr_loss = F.l1_loss(sr_out, sr_hr)\n",
    "#             total_loss += sr_loss\n",
    "\n",
    "#             # SSFM loss (for feature matching between student and teacher)\n",
    "#             s_loss = ssfm_loss(s, t)\n",
    "#             total_loss += s_loss\n",
    "\n",
    "#             # Distillation loss\n",
    "#             distillation_loss = fam(dehaze, sr_out)\n",
    "#             total_loss += distillation_loss\n",
    "\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         psnr_list.extend(to_psnr(dehaze, gt))\n",
    "#         train_loader_tqdm.set_postfix(loss=total_loss.item(), iter=global_iter)\n",
    "#         global_iter += 1\n",
    "\n",
    "#     train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "#     # Save models every 5 epochs\n",
    "#     if epoch % 5 == 0:\n",
    "#         iter_model_path = f\"net_haze_iter_{epoch}.pth\"\n",
    "#         torch.save(net.state_dict(), iter_model_path)\n",
    "#         print(f\"Model - net saved in epoch {epoch}.\")\n",
    "#         if sr_enabled:\n",
    "#             iter_model_path = f\"teacher_net_haze_iter_{epoch}.pth\"\n",
    "#             torch.save(sr_net.state_dict(), iter_model_path)\n",
    "#             print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "#     # === Validation ===\n",
    "#     net.eval()\n",
    "#     val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "#     print(f\"[Dehazing Val] PSNR: {val_psnr:.2f}, SSIM: {val_ssim:.4f}\")\n",
    "#     writer.add_scalar(\"Metrics/Val_PSNR_Dehaze\", val_psnr, epoch)\n",
    "#     writer.add_scalar(\"Metrics/Val_SSIM_Dehaze\", val_ssim, epoch)\n",
    "\n",
    "#     # Optional SR validation (if enabled)\n",
    "#     if sr_enabled:\n",
    "#         sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#         print(f\"[SR Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "#         writer.add_scalar(\"Metrics/Val_PSNR_SR\", sr_val_psnr, epoch)\n",
    "#         writer.add_scalar(\"Metrics/Val_SSIM_SR\", sr_val_ssim, epoch)\n",
    "\n",
    "#     epoch_duration = time.time() - epoch_start_time\n",
    "#     model_path = f\"net_haze_{version}.pth\"\n",
    "#     print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "#     # Learning rate scheduler based on validation PSNR\n",
    "#     scheduler.step(val_psnr)\n",
    "\n",
    "#     # Best model save\n",
    "#     if val_psnr >= best_psnr:\n",
    "#         best_model_path = f\"net_haze_best_{version}.pth\"\n",
    "#         torch.save(net.state_dict(), best_model_path)\n",
    "#         best_psnr = val_psnr\n",
    "\n",
    "#     # SR best model save (if enabled)\n",
    "#     if sr_enabled and sr_val_psnr >= best_psnr_sr:\n",
    "#         best_sr_model_path = f\"teacher_net_haze_best_sr_{version}.pth\"\n",
    "#         torch.save(sr_net.state_dict(), best_sr_model_path)\n",
    "#         best_psnr_sr = sr_val_psnr\n",
    "\n",
    "#     train_psnr_prev = train_psnr\n",
    "\n",
    "# # Final save\n",
    "# final_path = f\"net_final_{epoch}.pth\"\n",
    "# torch.save(net.state_dict(), final_path)\n",
    "# print(f\"Model - net saved in epoch {epoch}.\")\n",
    "# if sr_enabled:\n",
    "#     final_path = f\"teacher_net_final_{epoch}.pth\"\n",
    "#     torch.save(sr_net.state_dict(), final_path)\n",
    "#     print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "# total_time = time.time() - total_start_time\n",
    "# print(f\"🏁 Training completed in {total_time/60:.2f} minutes.\")\n",
    "\n",
    "# # Close the TensorBoard writer\n",
    "# writer.close()\n",
    "# print(\"TensorBoard logs saved in:\", log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f82bde3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:50:51.828164Z",
     "iopub.status.busy": "2025-05-02T07:50:51.827842Z",
     "iopub.status.idle": "2025-05-02T07:50:51.833719Z",
     "shell.execute_reply": "2025-05-02T07:50:51.832954Z"
    },
    "papermill": {
     "duration": 2.172408,
     "end_time": "2025-05-02T07:50:51.834859",
     "exception": false,
     "start_time": "2025-05-02T07:50:49.662451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Create a directory for TensorBoard logs\n",
    "# log_dir = \"runs/co_distillation_logs\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# # TensorBoard writer\n",
    "# writer = SummaryWriter(log_dir)\n",
    "\n",
    "# # Initialize global variables\n",
    "# global_iter = 0\n",
    "# total_start_time = time.time()\n",
    "\n",
    "# # Optimizer for net and teacher_net (joint training)\n",
    "# optimizer = torch.optim.Adam(list(net.parameters()) + list(sr_net.parameters()), lr=learning_rate)\n",
    "\n",
    "# # LR scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# # Apply Xavier initialization to the tail of the teacher_net (for PixelShuffle)\n",
    "# with torch.no_grad():\n",
    "#     nn.init.xavier_uniform_(sr_net.tail[2].weight)\n",
    "#     nn.init.zeros_(sr_net.tail[2].bias)\n",
    "\n",
    "# # --- Feature Affinity Module and SSFM loss --- #\n",
    "# fam = FeatureAffinityModule(channels=16).to(device)\n",
    "# ssfm_loss = SSFM(loss_type='l1')\n",
    "\n",
    "# # --- Training Loop --- #\n",
    "# best_psnr = old_val_psnr\n",
    "# best_psnr_sr = sr_val_psnr\n",
    "# train_psnr_prev = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_start_time = time.time()\n",
    "#     psnr_list = []\n",
    "\n",
    "#     net.train()\n",
    "#     sr_net.train()\n",
    "\n",
    "#     # Training loop with tqdm progress bar\n",
    "#     train_loader_tqdm = tqdm(train_data_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "#     for batch_id, (haze, gt) in enumerate(train_loader_tqdm):\n",
    "#         haze, gt = haze.to(device), gt.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass - student (net)\n",
    "#         dehaze, base, s = net(haze)\n",
    "#         s1, s2, s3, s4 = s\n",
    "\n",
    "#         # Student losses\n",
    "#         base_loss = F.smooth_l1_loss(base, gt)\n",
    "#         smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "#         perceptual_loss = loss_network(dehaze, gt)\n",
    "#         total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "\n",
    "#         # Teacher SR logic (if SR is enabled)\n",
    "#         if sr_enabled:\n",
    "#             try:\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             except StopIteration:\n",
    "#                 sr_iter = iter(sr_train_loader)\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n",
    "\n",
    "#             # Teacher forward (SR)\n",
    "#             sr_out, _, t = sr_net(sr_lr)\n",
    "#             sr_hr = F.interpolate(sr_hr, size=sr_out.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "#             sr_loss = F.l1_loss(sr_out, sr_hr)\n",
    "#             total_loss += sr_loss\n",
    "\n",
    "#             # SSFM loss (for feature matching between student and teacher)\n",
    "#             s_loss = ssfm_loss(s, t)\n",
    "#             total_loss += s_loss\n",
    "\n",
    "#             # Distillation loss\n",
    "#             distillation_loss = fam(dehaze, sr_out)\n",
    "#             total_loss += distillation_loss\n",
    "\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         psnr_list.extend(to_psnr(dehaze, gt))\n",
    "#         train_loader_tqdm.set_postfix(loss=total_loss.item(), iter=global_iter)\n",
    "#         global_iter += 1\n",
    "\n",
    "#     train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "#     # Save models every 5 epochs\n",
    "#     if epoch % 5 == 0:\n",
    "#         iter_model_path = f\"net_haze_iter_{epoch}.pth\"\n",
    "#         torch.save(net.state_dict(), iter_model_path)\n",
    "#         print(f\"Model - net saved in epoch {epoch}.\")\n",
    "#         if sr_enabled:\n",
    "#             iter_model_path = f\"teacher_net_haze_iter_{epoch}.pth\"\n",
    "#             torch.save(sr_net.state_dict(), iter_model_path)\n",
    "#             print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "#     # === Validation ===\n",
    "#     net.eval()\n",
    "#     val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "#     print(f\"[Dehazing Val] PSNR: {val_psnr:.2f}, SSIM: {val_ssim:.4f}\")\n",
    "#     writer.add_scalar(\"Metrics/Val_PSNR_Dehaze\", val_psnr, epoch)\n",
    "#     writer.add_scalar(\"Metrics/Val_SSIM_Dehaze\", val_ssim, epoch)\n",
    "\n",
    "#     # Optional SR validation (if enabled)\n",
    "#     if sr_enabled:\n",
    "#         sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#         print(f\"[SR Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "#         writer.add_scalar(\"Metrics/Val_PSNR_SR\", sr_val_psnr, epoch)\n",
    "#         writer.add_scalar(\"Metrics/Val_SSIM_SR\", sr_val_ssim, epoch)\n",
    "\n",
    "#     epoch_duration = time.time() - epoch_start_time\n",
    "#     model_path = f\"net_haze_{version}.pth\"\n",
    "#     print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "#     # Learning rate scheduler based on validation PSNR\n",
    "#     scheduler.step(val_psnr)\n",
    "\n",
    "#     # Best model save\n",
    "#     if val_psnr >= best_psnr:\n",
    "#         best_model_path = f\"net_haze_best_{version}.pth\"\n",
    "#         torch.save(net.state_dict(), best_model_path)\n",
    "#         best_psnr = val_psnr\n",
    "\n",
    "#     # SR best model save (if enabled)\n",
    "#     if sr_enabled and sr_val_psnr >= best_psnr_sr:\n",
    "#         best_sr_model_path = f\"teacher_net_haze_best_sr_{version}.pth\"\n",
    "#         torch.save(sr_net.state_dict(), best_sr_model_path)\n",
    "#         best_psnr_sr = sr_val_psnr\n",
    "\n",
    "#     train_psnr_prev = train_psnr\n",
    "\n",
    "# # Final save\n",
    "# final_path = f\"net_final_{epoch}.pth\"\n",
    "# torch.save(net.state_dict(), final_path)\n",
    "# print(f\"Model - net saved in epoch {epoch}.\")\n",
    "# if sr_enabled:\n",
    "#     final_path = f\"teacher_net_final_{epoch}.pth\"\n",
    "#     torch.save(sr_net.state_dict(), final_path)\n",
    "#     print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "# total_time = time.time() - total_start_time\n",
    "# print(f\"🏁 Training completed in {total_time/60:.2f} minutes.\")\n",
    "\n",
    "# # Close the TensorBoard writer\n",
    "# writer.close()\n",
    "# print(\"TensorBoard logs saved in:\", log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ac59a6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:50:56.091409Z",
     "iopub.status.busy": "2025-05-02T07:50:56.090735Z",
     "iopub.status.idle": "2025-05-02T07:50:56.094275Z",
     "shell.execute_reply": "2025-05-02T07:50:56.093543Z"
    },
    "papermill": {
     "duration": 2.16956,
     "end_time": "2025-05-02T07:50:56.095547",
     "exception": false,
     "start_time": "2025-05-02T07:50:53.925987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# validation_sr(sr_net , sr_val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b892c31b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:51:00.250534Z",
     "iopub.status.busy": "2025-05-02T07:51:00.249958Z",
     "iopub.status.idle": "2025-05-02T07:51:32.891859Z",
     "shell.execute_reply": "2025-05-02T07:51:32.891069Z"
    },
    "papermill": {
     "duration": 36.862432,
     "end_time": "2025-05-02T07:51:35.051858",
     "exception": false,
     "start_time": "2025-05-02T07:50:58.189426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.940930919753928, 0.6768226163337628)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationB(net, val_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a38d50e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:51:39.325314Z",
     "iopub.status.busy": "2025-05-02T07:51:39.324489Z",
     "iopub.status.idle": "2025-05-02T07:51:39.329732Z",
     "shell.execute_reply": "2025-05-02T07:51:39.329193Z"
    },
    "papermill": {
     "duration": 2.075397,
     "end_time": "2025-05-02T07:51:39.330991",
     "exception": false,
     "start_time": "2025-05-02T07:51:37.255594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# global_iter = 0\n",
    "# total_start_time = time.time()\n",
    "\n",
    "# # Joint optimizer for net and teacher_net\n",
    "# optimizer = torch.optim.Adam(list(net.parameters()) + list(sr_net.parameters()), lr=learning_rate)\n",
    "\n",
    "# # Initialize ReduceLROnPlateau scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_start_time = time.time()\n",
    "#     psnr_list = []\n",
    "\n",
    "#     net.train()\n",
    "#     if sr_enabled:\n",
    "#         sr_net.train()\n",
    "\n",
    "#     train_loader_tqdm = tqdm(train_data_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "#     for batch_id, (haze, gt) in enumerate(train_loader_tqdm):\n",
    "#         haze, gt = haze.to(device), gt.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass - student\n",
    "#         dehaze, base, s = net(haze)\n",
    "#         s1, s2, s3, s4 = s\n",
    "\n",
    "#         # Student losses\n",
    "#         base_loss = F.smooth_l1_loss(base, gt)\n",
    "#         smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "#         perceptual_loss = loss_network(dehaze, gt)\n",
    "#         total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "\n",
    "#         # Teacher SR logic\n",
    "#         if sr_enabled:\n",
    "#             try:\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             except StopIteration:\n",
    "#                 sr_iter = iter(sr_train_loader)\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n",
    "\n",
    "#             # Teacher forward\n",
    "#             sr_out, _, t = sr_net(sr_lr)\n",
    "\n",
    "#             sr_loss = F.l1_loss(sr_out, sr_hr)\n",
    "#             total_loss += sr_loss\n",
    "\n",
    "#             s_loss = ssfm_loss(s, t)\n",
    "#             total_loss += s_loss\n",
    "\n",
    "#             distillation_loss = fam(dehaze, sr_out)\n",
    "#             total_loss += distillation_loss\n",
    "\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         psnr_list.extend(to_psnr(dehaze, gt))\n",
    "#         train_loader_tqdm.set_postfix(loss=total_loss.item(), iter=global_iter)\n",
    "#         global_iter += 1\n",
    "\n",
    "#     train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "#     # Save model every 5 epochs\n",
    "#     if epoch % 5 == 0:\n",
    "#         iter_model_path = f\"net_haze_iter_{epoch}.pth\"\n",
    "#         torch.save(net.state_dict(), iter_model_path)\n",
    "#         print(f\"Model - net saved in epoch {epoch}.\")\n",
    "#         if sr_enabled:\n",
    "#             iter_model_path = f\"teacher_net_haze_iter_{epoch}.pth\"\n",
    "#             torch.save(sr_net.state_dict(), iter_model_path)\n",
    "#             print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "#     # Validation\n",
    "#     net.eval()\n",
    "#     val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "#     if sr_enabled:\n",
    "#         sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#         print(f\"[SR Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "#     epoch_duration = time.time() - epoch_start_time\n",
    "#     model_path = f\"net_haze_{version}.pth\"\n",
    "#     print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "#     elapsed = time.time() - total_start_time\n",
    "#     epochs_left = num_epochs - (epoch + 1)\n",
    "#     avg_epoch_time = elapsed / (epoch + 1)\n",
    "#     eta = avg_epoch_time * epochs_left\n",
    "#     print(f\"✅ Epoch [{epoch+1}/{num_epochs}] completed in {epoch_duration:.2f}s — ETA for {epochs_left} more: ~{eta/60:.2f} min\")\n",
    "\n",
    "#     # Adjust the learning rate based on validation PSNR\n",
    "#     scheduler.step(val_psnr)\n",
    "\n",
    "#     # Best model save\n",
    "#     if val_psnr >= best_psnr:\n",
    "#         best_model_path = f\"net_haze_best_{version}.pth\"\n",
    "#         torch.save(net.state_dict(), best_model_path)\n",
    "#         best_psnr = val_psnr\n",
    "\n",
    "#     train_psnr_prev = train_psnr\n",
    "\n",
    "# # Final save\n",
    "# final_path = f\"net_final_{epoch}.pth\"\n",
    "# torch.save(net.state_dict(), final_path)\n",
    "# print(f\"Model - net saved in epoch {epoch}.\")\n",
    "# if sr_enabled:\n",
    "#     final_path = f\"teacher_net_final_{epoch}.pth\"\n",
    "#     torch.save(sr_net.state_dict(), final_path)\n",
    "#     print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "# total_time = time.time() - total_start_time\n",
    "# print(f\"🏁 Training completed in {total_time/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec1b058a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:51:43.466103Z",
     "iopub.status.busy": "2025-05-02T07:51:43.465405Z",
     "iopub.status.idle": "2025-05-02T07:51:43.468826Z",
     "shell.execute_reply": "2025-05-02T07:51:43.468119Z"
    },
    "papermill": {
     "duration": 2.074987,
     "end_time": "2025-05-02T07:51:43.470062",
     "exception": false,
     "start_time": "2025-05-02T07:51:41.395075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # --- Final Validation --- #\n",
    "# net.eval()\n",
    "# val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "# print(f\"[Dehazing Final Val] PSNR: {val_psnr:.2f}, SSIM: {val_ssim:.4f}\")\n",
    "# if sr_enabled:\n",
    "#     sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#     print(f\"[SR Final Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ed0d35da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:51:47.778439Z",
     "iopub.status.busy": "2025-05-02T07:51:47.778164Z",
     "iopub.status.idle": "2025-05-02T07:51:47.781584Z",
     "shell.execute_reply": "2025-05-02T07:51:47.780872Z"
    },
    "papermill": {
     "duration": 2.138439,
     "end_time": "2025-05-02T07:51:47.782723",
     "exception": false,
     "start_time": "2025-05-02T07:51:45.644284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initialize model\n",
    "# # model_path = r\"C:\\Users\\abd\\d\\ai\\dehaze\\teacher_net_sr_final_24.pth\"\n",
    "# model_path = r\"C:\\Users\\abd\\d\\ai\\dehaze\\teacher_net_sr_iter_100.pth\"\n",
    "# # model = DehazingNet().to(device)\n",
    "# teacher_net.load_state_dict(torch.load(model_path, map_location=device, weights_only=False), strict=False)\n",
    "# # net.load_state_dict(torch.load(model_path, map_location=device))\n",
    "# # net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "154440f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:51:52.106721Z",
     "iopub.status.busy": "2025-05-02T07:51:52.106240Z",
     "iopub.status.idle": "2025-05-02T07:51:52.110824Z",
     "shell.execute_reply": "2025-05-02T07:51:52.110123Z"
    },
    "papermill": {
     "duration": 2.262394,
     "end_time": "2025-05-02T07:51:52.112044",
     "exception": false,
     "start_time": "2025-05-02T07:51:49.849650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Visualize using test sr_test_loader\n",
    "# # model_path1 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\teacher_net_sr_final_199.pth\"\n",
    "# # model_path2 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run2\\teacher_net_sr_best_0.pth\"\n",
    "# # Load models\n",
    "# # sr_net.load_state_dict(torch.load(model_path1, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net1 = sr_net\n",
    "\n",
    "# # sr_net.load_state_dict(torch.load(model_path2, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net2 = net\n",
    "\n",
    "# # Iterate through test loader\n",
    "# for i, (lr, hr) in enumerate(sr_test_loader):\n",
    "#     print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "#     lr, hr = lr.to(device), hr.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         sr_out1, _, _ = teacher_net1(lr)\n",
    "#         sr_out2, _, _ = teacher_net2(lr)\n",
    "#         print(f\"SR Output shape: {sr_out1.shape}\")\n",
    "#         print(f\"SR Output shape: {sr_out2.shape}\")\n",
    "#         # Downsample to match the size of hr\n",
    "#         sr_out1 = F.interpolate(sr_out1, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "#         # sr_out2 = F.interpolate(sr_out2, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "#         # Plot using matplotlib\n",
    "#         fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "#         axes[0].imshow(lr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[0].set_title(\"Input (Low-Res)\")\n",
    "#         axes[0].axis(\"off\")\n",
    "        \n",
    "#         axes[1].imshow(hr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[1].set_title(\"Ground Truth\")\n",
    "#         axes[1].axis(\"off\")\n",
    "\n",
    "#         axes[2].imshow(sr_out1[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[2].set_title(\"Model Path 1 Output\")\n",
    "#         axes[2].axis(\"off\")\n",
    "\n",
    "#         axes[3].imshow(sr_out2[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[3].set_title(\"Model Path 2 Output\")\n",
    "#         axes[3].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     if i == 4:  # Just to limit the output\n",
    "#         break\n",
    "# # do same for dehaze\n",
    "# # Iterate through test loader\n",
    "# for i, (haze, gt) in enumerate(test_data_loader):\n",
    "#     print(f\"Batch {i}: Haze shape: {haze.shape}, GT shape: {gt.shape}\")\n",
    "#     haze, gt = haze.to(device), gt.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         dehaze, _, _ = net(haze)\n",
    "#         print(f\"Dehaze Output shape: {dehaze.shape}\")\n",
    "        \n",
    "#         # Plot using matplotlib\n",
    "#         fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "#         axes[0].imshow(haze[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[0].set_title(\"Input (Hazy)\")\n",
    "#         axes[0].axis(\"off\")\n",
    "        \n",
    "#         axes[1].imshow(gt[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[1].set_title(\"Ground Truth\")\n",
    "#         axes[1].axis(\"off\")\n",
    "\n",
    "#         axes[2].imshow(dehaze[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[2].set_title(\"Dehazed Output\")\n",
    "#         axes[2].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     if i == 4:  # Just to limit the output\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "369b3748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:51:56.356842Z",
     "iopub.status.busy": "2025-05-02T07:51:56.356300Z",
     "iopub.status.idle": "2025-05-02T07:51:56.360457Z",
     "shell.execute_reply": "2025-05-02T07:51:56.359733Z"
    },
    "papermill": {
     "duration": 2.180874,
     "end_time": "2025-05-02T07:51:56.361867",
     "exception": false,
     "start_time": "2025-05-02T07:51:54.180993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Visualize using test sr_test_loader\n",
    "# model_path1 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run2\\teacher_net_sr_iter_100.pth\"\n",
    "# model_path2 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run2\\teacher_net_sr_best_0.pth\"\n",
    "# # Load models\n",
    "# sr_net.load_state_dict(torch.load(model_path1, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net1 = sr_net\n",
    "\n",
    "# sr_net.load_state_dict(torch.load(model_path2, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net2 = sr_net\n",
    "\n",
    "# # Iterate through test loader\n",
    "# for i, (lr, hr) in enumerate(sr_test_loader):\n",
    "#     print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "#     lr, hr = lr.to(device), hr.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         sr_out1, _, _ = teacher_net1(lr)\n",
    "#         sr_out2, _, _ = teacher_net2(lr)\n",
    "#         print(f\"SR Output shape: {sr_out1.shape}\")\n",
    "#         print(f\"SR Output shape: {sr_out2.shape}\")\n",
    "#         # Downsample to match the size of hr\n",
    "#         sr_out1 = F.interpolate(sr_out1, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "#         # sr_out2 = F.interpolate(sr_out2, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "#         # Plot using matplotlib\n",
    "#         fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "#         axes[0].imshow(lr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[0].set_title(\"Input (Low-Res)\")\n",
    "#         axes[0].axis(\"off\")\n",
    "        \n",
    "#         axes[1].imshow(hr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[1].set_title(\"Ground Truth\")\n",
    "#         axes[1].axis(\"off\")\n",
    "\n",
    "#         axes[2].imshow(sr_out1[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[2].set_title(\"Model Path 1 Output\")\n",
    "#         axes[2].axis(\"off\")\n",
    "\n",
    "#         axes[3].imshow(sr_out2[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[3].set_title(\"Model Path 2 Output\")\n",
    "#         axes[3].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     if i == 4:  # Just to limit the output\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b0741dcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:52:00.612418Z",
     "iopub.status.busy": "2025-05-02T07:52:00.611813Z",
     "iopub.status.idle": "2025-05-02T07:52:00.615963Z",
     "shell.execute_reply": "2025-05-02T07:52:00.615253Z"
    },
    "papermill": {
     "duration": 2.172696,
     "end_time": "2025-05-02T07:52:00.617211",
     "exception": false,
     "start_time": "2025-05-02T07:51:58.444515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Visualize using test sr_test_loader\n",
    "# model_path1 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run2\\teacher_net_sr_iter_100.pth\"\n",
    "# model_path2 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\teacher_net_sr_iter_5.pth\"\n",
    "# # Load models\n",
    "# sr_net.load_state_dict(torch.load(model_path1, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net1 = sr_net\n",
    "# sr_net.load_state_dict(torch.load(model_path2, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net2 = sr_net\n",
    "\n",
    "# # Iterate through test loader\n",
    "# for i, (lr, hr) in enumerate(sr_test_loader):\n",
    "#     print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "#     lr, hr = lr.to(device), hr.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         sr_out1, _, _ = teacher_net1(lr)\n",
    "#         sr_out2, _, _ = teacher_net2(lr)\n",
    "        \n",
    "#         # Plot using matplotlib\n",
    "#         fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "#         axes[0].imshow(lr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[0].set_title(\"Input (Low-Res)\")\n",
    "#         axes[0].axis(\"off\")\n",
    "        \n",
    "#         axes[1].imshow(hr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[1].set_title(\"Ground Truth\")\n",
    "#         axes[1].axis(\"off\")\n",
    "\n",
    "#         axes[2].imshow(sr_out1[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[2].set_title(\"Model Path 1 Output\")\n",
    "#         axes[2].axis(\"off\")\n",
    "\n",
    "#         axes[3].imshow(sr_out2[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[3].set_title(\"Model Path 2 Output\")\n",
    "#         axes[3].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     if i == 4:  # Just to limit the output\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b372f476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:52:04.803394Z",
     "iopub.status.busy": "2025-05-02T07:52:04.802668Z",
     "iopub.status.idle": "2025-05-02T07:52:04.806438Z",
     "shell.execute_reply": "2025-05-02T07:52:04.805867Z"
    },
    "papermill": {
     "duration": 2.063119,
     "end_time": "2025-05-02T07:52:04.807580",
     "exception": false,
     "start_time": "2025-05-02T07:52:02.744461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # LOAD TEST DATA\n",
    "# # -----------------------------\n",
    "# # check is exec env is local or kaggle\n",
    "# if execution_env == 'kaggle':\n",
    "#     test_hazy_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/hazy\"\n",
    "#     test_gt_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/GT\"\n",
    "# else:\n",
    "#     test_hazy_dir = \"./dehaze/dataset/RESIDE-6K/test/hazy\"\n",
    "#     test_gt_dir = \"./dehaze/dataset/RESIDE-6K/test/GT\"\n",
    "#     # test_hazy_dir = \"/Volumes/S/dev/project/code/Aphase/dehaze/dataset/RESIDE-6K/test/hazy\"\n",
    "#     # test_gt_dir = \"/Volumes/S/dev/project/code/Aphase/dehaze/dataset/RESIDE-6K/test/GT\"\n",
    "\n",
    "# # test_hazy_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN\"\n",
    "# # test_gt_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT\"\n",
    "\n",
    "# hazy_images = sorted(glob.glob(os.path.join(test_hazy_dir, \"*.*\")))\n",
    "# gt_images = sorted(glob.glob(os.path.join(test_gt_dir, \"*.*\")))\n",
    "\n",
    "# transform = Compose([\n",
    "#     ToTensor(),\n",
    "#     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# to_pil = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d9c4c237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:52:09.063973Z",
     "iopub.status.busy": "2025-05-02T07:52:09.063413Z",
     "iopub.status.idle": "2025-05-02T07:52:09.067310Z",
     "shell.execute_reply": "2025-05-02T07:52:09.066545Z"
    },
    "papermill": {
     "duration": 2.161581,
     "end_time": "2025-05-02T07:52:09.068555",
     "exception": false,
     "start_time": "2025-05-02T07:52:06.906974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [11, 12, 13,14]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e80705c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:52:13.171737Z",
     "iopub.status.busy": "2025-05-02T07:52:13.171251Z",
     "iopub.status.idle": "2025-05-02T07:52:13.175044Z",
     "shell.execute_reply": "2025-05-02T07:52:13.174319Z"
    },
    "papermill": {
     "duration": 2.061042,
     "end_time": "2025-05-02T07:52:13.176356",
     "exception": false,
     "start_time": "2025-05-02T07:52:11.115314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [70, 75, 90, 100]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f1c2ceff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T07:52:17.534494Z",
     "iopub.status.busy": "2025-05-02T07:52:17.533798Z",
     "iopub.status.idle": "2025-05-02T07:52:17.537602Z",
     "shell.execute_reply": "2025-05-02T07:52:17.537067Z"
    },
    "papermill": {
     "duration": 2.21666,
     "end_time": "2025-05-02T07:52:17.538624",
     "exception": false,
     "start_time": "2025-05-02T07:52:15.321964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [1, 3, 5]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7302936,
     "sourceId": 11638847,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7303093,
     "sourceId": 11639061,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 325527,
     "modelInstanceId": 305084,
     "sourceId": 368260,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 326116,
     "modelInstanceId": 305661,
     "sourceId": 369089,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4231.520881,
   "end_time": "2025-05-02T07:52:22.562049",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-02T06:41:51.041168",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07115070e404430db6530a6885538124": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "17e5dcc25ea049a38ef6079212c92b9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Train Batch Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_4833e67e5fb844ff9a52e418c99b72f0",
       "step": 1,
       "style": "IPY_MODEL_4a6446d4e8a041a984ba489f721af54a",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "1f2e8171f1cd4bf3a925bbd6cd966037": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "30f0246a6a05456aa7d9e32282dada5b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3dc099343080423980f6824deaa6a26d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "FloatTextView",
       "continuous_update": false,
       "description": "Learning Rate:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_c88c66daa90e49468b14464ebaebb5e2",
       "step": null,
       "style": "IPY_MODEL_07115070e404430db6530a6885538124",
       "tabbable": null,
       "tooltip": null,
       "value": 0.0001
      }
     },
     "4833e67e5fb844ff9a52e418c99b72f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4a6446d4e8a041a984ba489f721af54a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4bfca32a1e024bbf8e1934496755c235": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Growth Rate:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_1f2e8171f1cd4bf3a925bbd6cd966037",
       "step": 1,
       "style": "IPY_MODEL_c18712d2355b422cb2745c5565edfda2",
       "tabbable": null,
       "tooltip": null,
       "value": 16
      }
     },
     "50e1fac7d5a94f31a55b6410895b8cbe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "FloatTextView",
       "continuous_update": false,
       "description": "Lambda Loss:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_ee2681d82107440d96690bcbcf82ffef",
       "step": null,
       "style": "IPY_MODEL_a1b57bc409d64d5fb5e9bef59e4dea38",
       "tabbable": null,
       "tooltip": null,
       "value": 0.04
      }
     },
     "5512acd7698b43febd667e1c8d6f0f38": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "70396a756d6141bf8611478e2ad38557": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "71a72b5e4e684d62b06c630e617f158b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8ed2c7e001f644ba814bb6176e5ec3ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Version:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_30f0246a6a05456aa7d9e32282dada5b",
       "step": 1,
       "style": "IPY_MODEL_a79bfd7f118d4ed786af764633f237ef",
       "tabbable": null,
       "tooltip": null,
       "value": 0
      }
     },
     "a1b57bc409d64d5fb5e9bef59e4dea38": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a79bfd7f118d4ed786af764633f237ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b7b38c40a90b4baaa177b485d53dd498": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "TextView",
       "continuous_update": true,
       "description": "Crop Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_e5f582086e6a4cb3b8261ceea4f740f3",
       "placeholder": "​",
       "style": "IPY_MODEL_cd580100a2034876b428a78a899e9b09",
       "tabbable": null,
       "tooltip": null,
       "value": "128,128"
      }
     },
     "c18712d2355b422cb2745c5565edfda2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c6fc3d51f1b94e8bb23030bd6c15c51a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c718e4a0a0dd4f6a9822e5f30389ea68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DropdownModel",
       "_options_labels": [
        "local",
        "kaggle"
       ],
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "DropdownView",
       "description": "Execution Env:",
       "description_allow_html": false,
       "disabled": false,
       "index": 1,
       "layout": "IPY_MODEL_ff686a10f6d24c549698ee8a3f5f9a8d",
       "style": "IPY_MODEL_70396a756d6141bf8611478e2ad38557",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c88c66daa90e49468b14464ebaebb5e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cb35a2bb023e46f4a6ef4b9fb2b09328": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DropdownModel",
       "_options_labels": [
        "indoor",
        "outdoor",
        "reside",
        "nh"
       ],
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "DropdownView",
       "description": "Category:",
       "description_allow_html": false,
       "disabled": false,
       "index": 2,
       "layout": "IPY_MODEL_5512acd7698b43febd667e1c8d6f0f38",
       "style": "IPY_MODEL_cc6b2c90475b4bf0a357d7ec3789554d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "cc6b2c90475b4bf0a357d7ec3789554d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cd580100a2034876b428a78a899e9b09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e5f582086e6a4cb3b8261ceea4f740f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb50ddac489e4bc3b73c613faf1eccaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Val Batch Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_c6fc3d51f1b94e8bb23030bd6c15c51a",
       "step": 1,
       "style": "IPY_MODEL_71a72b5e4e684d62b06c630e617f158b",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "ee2681d82107440d96690bcbcf82ffef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ff686a10f6d24c549698ee8a3f5f9a8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
