{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:01.883179Z",
     "iopub.status.busy": "2025-04-21T16:01:01.882223Z",
     "iopub.status.idle": "2025-04-21T16:01:16.592457Z",
     "shell.execute_reply": "2025-04-21T16:01:16.591461Z",
     "shell.execute_reply.started": "2025-04-21T16:01:01.883139Z"
    },
    "papermill": {
     "duration": 11.276571,
     "end_time": "2025-04-12T13:09:32.125878",
     "exception": false,
     "start_time": "2025-04-12T13:09:20.849307",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "from timm.layers import to_2tuple, trunc_normal_\n",
    "import os\n",
    "import torchvision.utils as utils\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from random import randrange\n",
    "import time\n",
    "from math import log10\n",
    "from skimage import measure\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.nn.init import trunc_normal_\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.593951Z",
     "iopub.status.busy": "2025-04-21T16:01:16.593512Z",
     "iopub.status.idle": "2025-04-21T16:01:16.598530Z",
     "shell.execute_reply": "2025-04-21T16:01:16.597465Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.593926Z"
    },
    "papermill": {
     "duration": 0.015315,
     "end_time": "2025-04-12T13:09:32.152405",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.137090",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009983,
     "end_time": "2025-04-12T13:09:32.172516",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.162533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RevisedLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.601109Z",
     "iopub.status.busy": "2025-04-21T16:01:16.600708Z",
     "iopub.status.idle": "2025-04-21T16:01:16.719788Z",
     "shell.execute_reply": "2025-04-21T16:01:16.718584Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.601080Z"
    },
    "papermill": {
     "duration": 0.01803,
     "end_time": "2025-04-12T13:09:32.200726",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.182696",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RevisedLayerNorm(nn.Module):\n",
    "    \"\"\"Revised LayerNorm\"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n",
    "        super(RevisedLayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.detach_gradient = detach_gradient\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n",
    "        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n",
    "\n",
    "        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "\n",
    "        trunc_normal_(self.scale_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.scale_mlp.bias, 1)\n",
    "\n",
    "        trunc_normal_(self.shift_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.shift_mlp.bias, 0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n",
    "        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n",
    "\n",
    "        normalized_tensor = (input_tensor - mean_value) / std_value\n",
    "\n",
    "        if self.detach_gradient:\n",
    "            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n",
    "        else:\n",
    "            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n",
    "\n",
    "        output = normalized_tensor * self.scale + self.shift\n",
    "        return output, rescale, rebias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.720946Z",
     "iopub.status.busy": "2025-04-21T16:01:16.720710Z",
     "iopub.status.idle": "2025-04-21T16:01:16.744866Z",
     "shell.execute_reply": "2025-04-21T16:01:16.743982Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.720927Z"
    },
    "papermill": {
     "duration": 0.019569,
     "end_time": "2025-04-12T13:09:32.230395",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.210826",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n",
    "        super().__init__()\n",
    "        output_channels = output_channels or input_channels\n",
    "        hidden_channels = hidden_channels or input_channels\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            gain = (8 * self.depth) ** (-1 / 4)\n",
    "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "            trunc_normal_(layer.weight, std=std)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "\n",
    "def partition_into_windows(tensor, window_size):\n",
    "    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n",
    "    batch_size, height, width, channels = tensor.shape\n",
    "    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n",
    "\n",
    "    tensor = tensor.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n",
    "    )\n",
    "    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, height, width):\n",
    "    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n",
    "    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n",
    "    tensor = windows.view(\n",
    "        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010551,
     "end_time": "2025-04-12T13:09:32.251201",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.240650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.746699Z",
     "iopub.status.busy": "2025-04-21T16:01:16.746311Z",
     "iopub.status.idle": "2025-04-21T16:01:16.935555Z",
     "shell.execute_reply": "2025-04-21T16:01:16.934584Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.746668Z"
    },
    "papermill": {
     "duration": 0.141806,
     "end_time": "2025-04-12T13:09:32.403159",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.261353",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 16, 16]),\n",
       " torch.Size([32, 16, 64]),\n",
       " torch.Size([2, 16, 16, 64]),\n",
       " True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the MultiLayerPerceptron with sample parameters\n",
    "depth = 4\n",
    "input_channels = 64\n",
    "hidden_channels = 128\n",
    "output_channels = 64\n",
    "\n",
    "mlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n",
    "\n",
    "# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\n",
    "input_tensor = torch.randn(2, 64, 16, 16)\n",
    "output_tensor = mlp(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "mlp_output_shape = output_tensor.shape\n",
    "\n",
    "# Test window partition and merging\n",
    "batch_size, height, width, channels = 2, 16, 16, 64\n",
    "window_size = 4\n",
    "\n",
    "# Create a random tensor for window functions (B, H, W, C) format\n",
    "input_window_tensor = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Apply partitioning and merging\n",
    "windows = partition_into_windows(input_window_tensor, window_size)\n",
    "reconstructed_tensor = merge_windows(windows, window_size, height, width)\n",
    "\n",
    "# Check shapes\n",
    "windows_shape = windows.shape\n",
    "reconstructed_shape = reconstructed_tensor.shape\n",
    "\n",
    "# Validate if the reconstruction matches the original input shape\n",
    "is_shape_correct = reconstructed_shape == input_window_tensor.shape\n",
    "\n",
    "# Output results\n",
    "mlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.937244Z",
     "iopub.status.busy": "2025-04-21T16:01:16.936980Z",
     "iopub.status.idle": "2025-04-21T16:01:16.947402Z",
     "shell.execute_reply": "2025-04-21T16:01:16.946296Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.937222Z"
    },
    "papermill": {
     "duration": 0.019079,
     "end_time": "2025-04-12T13:09:32.432815",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.413736",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, window_size, num_heads):\n",
    "        \"\"\"Self-attention mechanism within local windows.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # (height, width)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n",
    "\n",
    "        # Compute and store relative positional encodings\n",
    "        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n",
    "        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n",
    "\n",
    "        # Learnable transformation of relative position embeddings\n",
    "        self.relative_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_heads, bias=True)\n",
    "        )\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n",
    "        batch_size, num_tokens, _ = qkv.shape\n",
    "\n",
    "        # Reshape qkv into separate query, key, and value tensors\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n",
    "\n",
    "        # Scale query for stable attention computation\n",
    "        q = q * self.scaling_factor\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Compute relative position bias\n",
    "        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n",
    "        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n",
    "        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n",
    "\n",
    "        # Apply softmax and compute weighted values\n",
    "        attention_weights = self.attention_softmax(attention_scores)\n",
    "        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.948815Z",
     "iopub.status.busy": "2025-04-21T16:01:16.948476Z",
     "iopub.status.idle": "2025-04-21T16:01:16.974830Z",
     "shell.execute_reply": "2025-04-21T16:01:16.973956Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.948785Z"
    },
    "papermill": {
     "duration": 0.015719,
     "end_time": "2025-04-12T13:09:32.458806",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.443087",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_log_relative_positions(window_size):\n",
    "    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n",
    "    coord_range = torch.arange(window_size)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n",
    "\n",
    "    # Compute relative positions\n",
    "    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n",
    "\n",
    "    # Format and apply log transformation\n",
    "    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n",
    "    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "    return log_relative_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.976208Z",
     "iopub.status.busy": "2025-04-21T16:01:16.975861Z",
     "iopub.status.idle": "2025-04-21T16:01:17.000363Z",
     "shell.execute_reply": "2025-04-21T16:01:16.999498Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.976184Z"
    },
    "papermill": {
     "duration": 0.025485,
     "end_time": "2025-04-12T13:09:32.494578",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.469093",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n",
    "        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.network_depth = network_depth\n",
    "        self.enable_attention = enable_attention\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "        # Define convolutional processing based on mode\n",
    "        if self.conv_mode == 'Conv':\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "            )\n",
    "\n",
    "        if self.conv_mode == 'DWConv':\n",
    "            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n",
    "            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight_shape = module.weight.shape\n",
    "\n",
    "            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "            else:\n",
    "                gain = (8 * self.network_depth) ** (-1/4)\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def pad_for_window_processing(self, x, shift=False):\n",
    "        \"\"\"Pads the input tensor to fit window processing requirements.\"\"\"\n",
    "        _, _, height, width = x.size()\n",
    "        pad_h = (self.window_size - height % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - width % self.window_size) % self.window_size\n",
    "\n",
    "        if shift:\n",
    "            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n",
    "                          self.shift_size, (self.window_size - self.shift_size + pad_h) % self.window_size), mode='reflect')\n",
    "        else:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            v_proj = self.value_projection(x)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            qk_proj = self.query_key_projection(x)\n",
    "            qkv = torch.cat([qk_proj, v_proj], dim=1)\n",
    "\n",
    "            # Apply padding for shifted window processing\n",
    "            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n",
    "            padded_height, padded_width = padded_qkv.shape[2:]\n",
    "\n",
    "            # Partition into windows\n",
    "            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n",
    "            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n",
    "\n",
    "            # Apply window-based attention\n",
    "            attn_windows = self.window_attention(qkv_windows)\n",
    "\n",
    "            # Merge back to original spatial dimensions\n",
    "            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n",
    "\n",
    "            # Reverse the cyclic shift\n",
    "            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), self.shift_size:(self.shift_size + width), :]\n",
    "            attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.conv_mode in ['Conv', 'DWConv']:\n",
    "                conv_output = self.conv_layer(v_proj)\n",
    "                output = self.output_projection(conv_output + attn_output)\n",
    "            else:\n",
    "                output = self.output_projection(attn_output)\n",
    "\n",
    "        else:\n",
    "            if self.conv_mode == 'Conv':\n",
    "                output = self.conv_layer(x)  # No attention, using convolution only\n",
    "            elif self.conv_mode == 'DWConv':\n",
    "                output = self.output_projection(self.conv_layer(v_proj))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.003995Z",
     "iopub.status.busy": "2025-04-21T16:01:17.003674Z",
     "iopub.status.idle": "2025-04-21T16:01:17.024865Z",
     "shell.execute_reply": "2025-04-21T16:01:17.023832Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.003973Z"
    },
    "papermill": {
     "duration": 0.020653,
     "end_time": "2025-04-12T13:09:32.525353",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.504700",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n",
    "                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n",
    "        \"\"\"\n",
    "        A transformer block that includes attention (optional) and MLP layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enable_attention = enable_attention\n",
    "        self.enable_mlp_norm = enable_mlp_norm\n",
    "\n",
    "        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n",
    "        self.attention_layer = AdaptiveAttention(\n",
    "            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n",
    "            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n",
    "        )\n",
    "\n",
    "        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n",
    "        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.enable_attention:\n",
    "            x, rescale, rebias = self.pre_norm(x)\n",
    "        x = self.attention_layer(x)\n",
    "        if self.enable_attention:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        residual = x\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x, rescale, rebias = self.post_norm(x)\n",
    "        x = self.mlp_layer(x)\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, window_size=8,\n",
    "                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n",
    "        \"\"\"\n",
    "        A stage of transformer blocks with configurable attention placement.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        attention_layers = int(attention_ratio * num_layers)\n",
    "\n",
    "        if attention_placement == 'last':\n",
    "            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'first':\n",
    "            enable_attentions = [i < attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'middle':\n",
    "            enable_attentions = [\n",
    "                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(\n",
    "                network_depth=network_depth,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                enable_attention=enable_attentions[i],\n",
    "                conv_mode=conv_mode\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer stage.\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.026567Z",
     "iopub.status.busy": "2025-04-21T16:01:17.026179Z",
     "iopub.status.idle": "2025-04-21T16:01:17.055499Z",
     "shell.execute_reply": "2025-04-21T16:01:17.054636Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.026462Z"
    },
    "papermill": {
     "duration": 0.017283,
     "end_time": "2025-04-12T13:09:32.553948",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.536665",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch embedding module that projects input images into token embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to generate patch embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class PatchReconstruction(nn.Module):\n",
    "    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch reconstruction module that converts token embeddings back to image patches.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2, padding_mode='reflect'\n",
    "            ),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to reconstruct image from embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.056678Z",
     "iopub.status.busy": "2025-04-21T16:01:17.056406Z",
     "iopub.status.idle": "2025-04-21T16:01:17.078704Z",
     "shell.execute_reply": "2025-04-21T16:01:17.077764Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.056658Z"
    },
    "papermill": {
     "duration": 0.017407,
     "end_time": "2025-04-12T13:09:32.581526",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.564119",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SelectiveKernelFusion(nn.Module):\n",
    "    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n",
    "        \"\"\"\n",
    "        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n",
    "\n",
    "        Args:\n",
    "            channels (int): Number of input channels.\n",
    "            num_branches (int): Number of feature branches to fuse.\n",
    "            reduction_ratio (int): Reduction ratio for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_branches = num_branches\n",
    "        reduced_channels = max(int(channels / reduction_ratio), 4)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Forward pass for selective kernel fusion.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (list of tensors): A list of feature maps to be fused.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adaptively fused feature map.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = feature_maps[0].shape\n",
    "        \n",
    "        # Concatenate feature maps along a new dimension (num_branches)\n",
    "        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        aggregated_features = torch.sum(stacked_features, dim=1)\n",
    "        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n",
    "        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n",
    "\n",
    "        # Weighted sum of input feature maps\n",
    "        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.080101Z",
     "iopub.status.busy": "2025-04-21T16:01:17.079760Z",
     "iopub.status.idle": "2025-04-21T16:01:17.103347Z",
     "shell.execute_reply": "2025-04-21T16:01:17.102401Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.080073Z"
    },
    "papermill": {
     "duration": 0.025553,
     "end_time": "2025-04-12T13:09:32.617120",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.591567",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DehazingTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "                 layer_depths=[16, 16, 16, 8, 8],\n",
    "                 num_heads=[2, 4, 6, 1, 1],\n",
    "                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding settings\n",
    "        self.patch_size = 4\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # Backbone layers\n",
    "        self.encoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[0],\n",
    "            num_layers=layer_depths[0],\n",
    "            num_heads=num_heads[0],\n",
    "            mlp_ratio=mlp_ratios[0],\n",
    "            norm_layer=norm_layers[0],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[0],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[0]\n",
    "        )\n",
    "        \n",
    "        self.downsample1 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "        \n",
    "        self.encoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[1],\n",
    "            num_layers=layer_depths[1],\n",
    "            num_heads=num_heads[1],\n",
    "            mlp_ratio=mlp_ratios[1],\n",
    "            norm_layer=norm_layers[1],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[1],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[1]\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "        \n",
    "        self.encoder_stage3 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[2],\n",
    "            num_layers=layer_depths[2],\n",
    "            num_heads=num_heads[2],\n",
    "            mlp_ratio=mlp_ratios[2],\n",
    "            norm_layer=norm_layers[2],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[2],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[2]\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n",
    "        \n",
    "        self.decoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[3],\n",
    "            num_layers=layer_depths[3],\n",
    "            num_heads=num_heads[3],\n",
    "            mlp_ratio=mlp_ratios[3],\n",
    "            norm_layer=norm_layers[3],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[3],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[3]\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n",
    "        \n",
    "        self.decoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[4],\n",
    "            num_layers=layer_depths[4],\n",
    "            num_heads=num_heads[4],\n",
    "            mlp_ratio=mlp_ratios[4],\n",
    "            norm_layer=norm_layers[4],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[4],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[4]\n",
    "        )\n",
    "\n",
    "        # Final patch reconstruction\n",
    "        self.patch_reconstruction = PatchReconstruction(\n",
    "            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def adjust_image_size(self, x):\n",
    "        # Ensures the input image size is compatible with the patch size\n",
    "        _, _, height, width = x.size()\n",
    "        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n",
    "        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder_stage1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.downsample1(x)\n",
    "        x = self.encoder_stage2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.downsample2(x)\n",
    "        x = self.encoder_stage3(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n",
    "        x = self.decoder_stage1(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n",
    "        x = self.decoder_stage2(x)\n",
    "        x = self.patch_reconstruction(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_height, original_width = x.shape[2:]\n",
    "        x = self.adjust_image_size(x)\n",
    "\n",
    "        features = self.extract_features(x)\n",
    "        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n",
    "\n",
    "        # Dehazing formula: I = J * t + A * (1 - t)\n",
    "        x = transmission_map * x - atmospheric_light + x\n",
    "        x = x[:, :, :original_height, :original_width]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.104568Z",
     "iopub.status.busy": "2025-04-21T16:01:17.104291Z",
     "iopub.status.idle": "2025-04-21T16:01:17.126572Z",
     "shell.execute_reply": "2025-04-21T16:01:17.125559Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.104548Z"
    },
    "papermill": {
     "duration": 0.015434,
     "end_time": "2025-04-12T13:09:32.642796",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.627362",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_dehazing_transformer():\n",
    "    return DehazingTransformer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "        layer_depths=[12, 12, 12, 6, 6],\n",
    "        num_heads=[2, 4, 6, 1, 1],\n",
    "        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.127839Z",
     "iopub.status.busy": "2025-04-21T16:01:17.127597Z",
     "iopub.status.idle": "2025-04-21T16:01:17.152140Z",
     "shell.execute_reply": "2025-04-21T16:01:17.151236Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.127821Z"
    },
    "papermill": {
     "duration": 0.018977,
     "end_time": "2025-04-12T13:09:32.671995",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.653018",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvolutionalGuidedFilter(nn.Module):\n",
    "    def __init__(self, radius=1, norm_layer=nn.BatchNorm2d, conv_kernel_size: int = 1):\n",
    "        super(ConvolutionalGuidedFilter, self).__init__()\n",
    "\n",
    "        self.box_filter = nn.Conv2d(\n",
    "            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n",
    "        )\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                6,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                3,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.box_filter.weight.data[...] = 1.0\n",
    "\n",
    "    def forward(self, x_low_res, y_low_res, x_high_res):\n",
    "        _, _, h_lr, w_lr = x_low_res.size()\n",
    "        _, _, h_hr, w_hr = x_high_res.size()\n",
    "\n",
    "        N = self.box_filter(x_low_res.data.new().resize_((1, 3, h_lr, w_lr)).fill_(1.0))\n",
    "        ## mean_x\n",
    "        mean_x = self.box_filter(x_low_res) / N\n",
    "        ## mean_y\n",
    "        mean_y = self.box_filter(y_low_res) / N\n",
    "        ## cov_xy\n",
    "        cov_xy = self.box_filter(x_low_res * y_low_res) / N - mean_x * mean_y\n",
    "        ## var_x\n",
    "        var_x = self.box_filter(x_low_res * x_low_res) / N - mean_x * mean_x\n",
    "\n",
    "        ## A\n",
    "        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n",
    "        ## b\n",
    "        b = mean_y - A * mean_x\n",
    "\n",
    "        ## mean_A; mean_b\n",
    "        mean_A = F.interpolate(A, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "        mean_b = F.interpolate(b, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return mean_A * x_high_res + mean_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.153291Z",
     "iopub.status.busy": "2025-04-21T16:01:17.153032Z",
     "iopub.status.idle": "2025-04-21T16:01:17.175609Z",
     "shell.execute_reply": "2025-04-21T16:01:17.174643Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.153270Z"
    },
    "papermill": {
     "duration": 0.016895,
     "end_time": "2025-04-12T13:09:32.699032",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.682137",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PixelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(PixelAttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, 1, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_map = self.attention(x)\n",
    "        return x * attention_map\n",
    "\n",
    "class ChannelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelAttentionLayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, channels, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = self.avg_pool(x)\n",
    "        attention_map = self.attention(pooled)\n",
    "        return x * attention_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.176736Z",
     "iopub.status.busy": "2025-04-21T16:01:17.176483Z",
     "iopub.status.idle": "2025-04-21T16:01:17.194108Z",
     "shell.execute_reply": "2025-04-21T16:01:17.193205Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.176717Z"
    },
    "papermill": {
     "duration": 0.018495,
     "end_time": "2025-04-12T13:09:32.727852",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.709357",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SuperResolutionDilationBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_dense_layers, growth_rate):\n",
    "        super(SuperResolutionDilationBlock, self).__init__()\n",
    "\n",
    "        self.split_channels = in_channels // 4\n",
    "        kernel_size = 3\n",
    "\n",
    "        # Dilated convolutions with increasing dilation rates\n",
    "        self.conv1 = nn.Conv2d(self.split_channels, self.split_channels, kernel_size=kernel_size, padding=1, dilation=1)\n",
    "        self.conv2 = nn.Conv2d(self.split_channels * 2, self.split_channels, kernel_size=kernel_size, padding=2, dilation=2)\n",
    "        self.conv3 = nn.Conv2d(self.split_channels * 3, self.split_channels, kernel_size=kernel_size, padding=4, dilation=4)\n",
    "        self.conv4 = nn.Conv2d(self.split_channels * 4, self.split_channels, kernel_size=kernel_size, padding=8, dilation=8)\n",
    "\n",
    "        # Attention mechanisms\n",
    "        self.channel_attention = ChannelAttentionLayer(in_channels)\n",
    "        self.pixel_attention = PixelAttentionLayer(in_channels)\n",
    "\n",
    "        # Final 1x1 convolution for feature fusion\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into 4 equal parts along channel dimension\n",
    "        split_features = torch.split(x, self.split_channels, dim=1)\n",
    "\n",
    "        x0 = F.relu(self.conv1(split_features[0]))\n",
    "        tmp = torch.cat((split_features[1], x0), dim=1)\n",
    "        x1 = F.relu(self.conv2(tmp))\n",
    "\n",
    "        tmp = torch.cat((split_features[2], x0, x1), dim=1)\n",
    "        x2 = F.relu(self.conv3(tmp))\n",
    "\n",
    "        tmp = torch.cat((split_features[3], x0, x1, x2), dim=1)\n",
    "        x3 = F.relu(self.conv4(tmp))\n",
    "\n",
    "        # Concatenate all outputs\n",
    "        merged_features = torch.cat((x0, x1, x2, x3), dim=1)\n",
    "\n",
    "        # Apply 1x1 convolution for feature refinement\n",
    "        out = self.conv_1x1(merged_features)\n",
    "\n",
    "        # Apply attention mechanisms\n",
    "        out = self.channel_attention(out)\n",
    "        out = self.pixel_attention(out)\n",
    "\n",
    "        # Residual connection\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.195300Z",
     "iopub.status.busy": "2025-04-21T16:01:17.195064Z",
     "iopub.status.idle": "2025-04-21T16:01:17.218308Z",
     "shell.execute_reply": "2025-04-21T16:01:17.217401Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.195283Z"
    },
    "papermill": {
     "duration": 0.015456,
     "end_time": "2025-04-12T13:09:32.753293",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.737837",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNormalization(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(AdaptiveInstanceNormalization, self).__init__()\n",
    "\n",
    "        # Learnable scaling factors\n",
    "        self.scale_x = nn.Parameter(torch.tensor(1.0))  # Identity scaling\n",
    "        self.scale_norm = nn.Parameter(torch.tensor(0.0))  # Initially no effect\n",
    "\n",
    "        # Instance normalization layer with affine transformation enabled\n",
    "        self.instance_norm = nn.InstanceNorm2d(num_channels, momentum=0.999, eps=0.001, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized_x = self.instance_norm(x)\n",
    "        return self.scale_x * x + self.scale_norm * normalized_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.219705Z",
     "iopub.status.busy": "2025-04-21T16:01:17.219346Z",
     "iopub.status.idle": "2025-04-21T16:01:17.243565Z",
     "shell.execute_reply": "2025-04-21T16:01:17.242600Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.219677Z"
    },
    "papermill": {
     "duration": 0.01754,
     "end_time": "2025-04-12T13:09:32.780956",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.763416",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class DeepGuidedNetwork(nn.Module):\n",
    "#     def __init__(self, radius=1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Adaptive Normalization for Guided Filtering\n",
    "#         norm = AdaptiveInstanceNormalization\n",
    "#         kernel_size = 3\n",
    "#         depth_rate = 16\n",
    "#         in_channels = 3\n",
    "#         num_dense_layer = 4\n",
    "#         growth_rate = 16\n",
    "\n",
    "#         # Initial convolution layers\n",
    "#         self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "#         self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "#         # Residual Dense Blocks (RDBs)\n",
    "#         self.rdb1 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "#         self.rdb2 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "#         self.rdb3 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "#         self.rdb4 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "\n",
    "#         # Guided Filter & Dehazing Transformer\n",
    "#         self.guided_filter = ConvolutionalGuidedFilter(radius, norm_layer=norm)\n",
    "#         self.dehaze_network = build_dehazing_transformer()\n",
    "\n",
    "#         # Downsampling & Upsampling Layers\n",
    "#         self.downsample = nn.Upsample(scale_factor=0.5, mode=\"bilinear\", align_corners=True)\n",
    "#         self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "#     def forward(self, x_hr):\n",
    "#         # Low-resolution processing\n",
    "#         x_lr = self.downsample(x_hr)\n",
    "\n",
    "#         # Detail extraction through Residual Dense Blocks\n",
    "#         y_features = self.conv_in(x_lr)\n",
    "#         y_features = self.rdb1(y_features)\n",
    "#         y_features = self.rdb2(y_features)\n",
    "#         y_features = self.rdb3(y_features)\n",
    "#         y_features = self.rdb4(y_features)\n",
    "#         y_detail = self.conv_out(y_features)\n",
    "\n",
    "#         y_base_hr = self.upsample(y_detail)\n",
    "#         y_lr = y_base_hr\n",
    "#         # Final guided filtering refinement\n",
    "#         refined_output = self.guided_filter(x_lr, y_lr, x_hr)\n",
    "        \n",
    "#         return refined_output, y_base_hr\n",
    "\n",
    "class DeepGuidedNetwork(nn.Module):\n",
    "    def __init__(self, radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Adaptive Normalization for Guided Filtering\n",
    "        norm = AdaptiveInstanceNormalization\n",
    "        kernel_size = 3\n",
    "        depth_rate = 16\n",
    "        in_channels = 3\n",
    "        num_dense_layer = 4\n",
    "        growth_rate = 16\n",
    "\n",
    "        # Initial convolution layers\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "        # Residual Dense Blocks (RDBs)\n",
    "        self.rdb1 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb2 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb3 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb4 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "\n",
    "        # Guided Filter & Dehazing Transformer\n",
    "        self.guided_filter = ConvolutionalGuidedFilter(radius, norm_layer=norm)\n",
    "        self.dehaze_network = build_dehazing_transformer()\n",
    "\n",
    "        # Downsampling & Upsampling Layers\n",
    "        self.downsample = nn.Upsample(scale_factor=0.5, mode=\"bilinear\", align_corners=True)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "    def forward(self, x_hr, sr=False):\n",
    "        x_lr = self.downsample(x_hr)\n",
    "    \n",
    "        # Initial conv\n",
    "        y_features = self.conv_in(x_lr)\n",
    "    \n",
    "        # RDBs + collect features\n",
    "        feat1 = self.rdb1(y_features)\n",
    "        feat2 = self.rdb2(feat1)\n",
    "        feat3 = self.rdb3(feat2)\n",
    "        feat4 = self.rdb4(feat3)\n",
    "        y_detail = self.conv_out(feat4)\n",
    "    \n",
    "        # Base image\n",
    "        y_base = self.dehaze_network(x_lr)\n",
    "    \n",
    "        # Combine\n",
    "        y_lr = y_base + y_detail\n",
    "        y_base_hr = self.upsample(y_base)\n",
    "    \n",
    "        # Guided output\n",
    "        refined_output = self.guided_filter(x_lr, y_lr, x_hr)\n",
    "    \n",
    "        return refined_output, y_base_hr, [feat1, feat2, feat3, feat4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.244744Z",
     "iopub.status.busy": "2025-04-21T16:01:17.244482Z",
     "iopub.status.idle": "2025-04-21T16:01:17.268358Z",
     "shell.execute_reply": "2025-04-21T16:01:17.267479Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.244725Z"
    },
    "papermill": {
     "duration": 0.015224,
     "end_time": "2025-04-12T13:09:32.835610",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.820386",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_crop_size(crop_size_str):\n",
    "    try:\n",
    "        return [int(x.strip()) for x in crop_size_str.split(',')]\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid crop size format: '{crop_size_str}'. Expected comma-separated integers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.269654Z",
     "iopub.status.busy": "2025-04-21T16:01:17.269385Z",
     "iopub.status.idle": "2025-04-21T16:01:17.290053Z",
     "shell.execute_reply": "2025-04-21T16:01:17.289037Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.269634Z"
    },
    "papermill": {
     "duration": 0.020548,
     "end_time": "2025-04-12T13:09:32.866232",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.845684",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from random import randrange\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self, crop_size, hazeeffected_images_dir, hazefree_images_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Ensure valid file extensions --- #\n",
    "        valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        hazy_data = [\n",
    "            f for f in glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_extensions)\n",
    "        ]\n",
    "\n",
    "        if not hazy_data:\n",
    "            raise ValueError(f\"No valid images found in {hazeeffected_images_dir}\")\n",
    "\n",
    "        self.hazeeffected_images_dir = hazeeffected_images_dir\n",
    "        self.hazefree_images_dir = hazefree_images_dir\n",
    "\n",
    "        self.haze_names = []\n",
    "        self.gt_names = []\n",
    "        \n",
    "        for h_image in hazy_data:\n",
    "            filename = os.path.basename(h_image)\n",
    "            haze_path = os.path.join(self.hazeeffected_images_dir, filename)\n",
    "            gt_path = os.path.join(self.hazefree_images_dir, filename)\n",
    "\n",
    "            if not os.path.exists(gt_path):\n",
    "                print(f\"Warning: Ground-truth missing for {filename}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            self.haze_names.append(haze_path)\n",
    "            self.gt_names.append(gt_path)\n",
    "\n",
    "        if not self.haze_names:\n",
    "            raise ValueError(\"No matching ground-truth images found.\")\n",
    "\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def get_images(self, index):\n",
    "        crop_width, crop_height = self.crop_size\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        try:\n",
    "            haze_img = Image.open(haze_name).convert('RGB')\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Invalid image format: {haze_name} or {gt_name}\")\n",
    "\n",
    "        width, height = haze_img.size\n",
    "\n",
    "        # --- Handle small images --- #\n",
    "        if width < crop_width or height < crop_height:\n",
    "            raise ValueError(f\"Image too small for cropping: {haze_name}\")\n",
    "\n",
    "        # --- Random crop --- #\n",
    "        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n",
    "        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "\n",
    "        # --- Transform to tensor --- #\n",
    "        transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        transform_gt = Compose([ToTensor()])\n",
    "        haze = transform_haze(haze_crop_img)\n",
    "        gt = transform_gt(gt_crop_img)\n",
    "\n",
    "        # --- Check channels --- #\n",
    "        if haze.shape[0] != 3 or gt.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {haze_name}\")\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_images(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.291708Z",
     "iopub.status.busy": "2025-04-21T16:01:17.291237Z",
     "iopub.status.idle": "2025-04-21T16:01:17.316254Z",
     "shell.execute_reply": "2025-04-21T16:01:17.315425Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.291673Z"
    },
    "papermill": {
     "duration": 0.021732,
     "end_time": "2025-04-12T13:09:32.898060",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.876328",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "class HazeDataset(Dataset):\n",
    "    def __init__(self, hazeeffected_images_dir, hazefree_images_dir, split=\"train\", split_ratio=0.8):\n",
    "        \"\"\"\n",
    "        Dataset class for handling hazy and corresponding ground-truth images (already aligned and cropped).\n",
    "\n",
    "        Args:\n",
    "            hazeeffected_images_dir (str): Directory for hazy images.\n",
    "            hazefree_images_dir (str): Directory for ground-truth images.\n",
    "            split (str): \"train\" or \"valid\" (determines data split).\n",
    "            split_ratio (float): Percentage of images to use for training (default 80% train, 20% validation).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        hazy_data = [\n",
    "            f for f in glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_extensions)\n",
    "        ]\n",
    "\n",
    "        if not hazy_data:\n",
    "            raise ValueError(f\"No valid images found in {hazeeffected_images_dir}\")\n",
    "\n",
    "        hazy_data.sort()\n",
    "\n",
    "        split_idx = int(len(hazy_data) * split_ratio)\n",
    "        if split == \"train\":\n",
    "            hazy_data = hazy_data[:split_idx]\n",
    "        else:  # \"valid\"\n",
    "            hazy_data = hazy_data[split_idx:]\n",
    "\n",
    "        self.haze_names = []\n",
    "        self.gt_names = []\n",
    "\n",
    "        for h_image in hazy_data:\n",
    "            filename = os.path.basename(h_image)\n",
    "            haze_path = os.path.join(hazeeffected_images_dir, filename)\n",
    "            gt_path = os.path.join(hazefree_images_dir, filename)\n",
    "\n",
    "            if not os.path.exists(gt_path):\n",
    "                print(f\"Warning: Ground-truth missing for {filename}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            self.haze_names.append(haze_path)\n",
    "            self.gt_names.append(gt_path)\n",
    "\n",
    "        if not self.haze_names:\n",
    "            raise ValueError(\"No matching ground-truth images found.\")\n",
    "\n",
    "        # Define transforms\n",
    "        self.transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        self.transform_gt = Compose([ToTensor()])\n",
    "\n",
    "    def get_images(self, index):\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        try:\n",
    "            haze_img = Image.open(haze_name).convert('RGB')\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Invalid image format: {haze_name} or {gt_name}\")\n",
    "\n",
    "        haze = self.transform_haze(haze_img)\n",
    "        gt = self.transform_gt(gt_img)\n",
    "\n",
    "        if haze.shape[0] != 3 or gt.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {haze_name}\")\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_images(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010053,
     "end_time": "2025-04-12T13:09:32.918333",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.908280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.317546Z",
     "iopub.status.busy": "2025-04-21T16:01:17.317245Z",
     "iopub.status.idle": "2025-04-21T16:01:17.339955Z",
     "shell.execute_reply": "2025-04-21T16:01:17.338827Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.317524Z"
    },
    "papermill": {
     "duration": 0.017326,
     "end_time": "2025-04-12T13:09:32.945817",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.928491",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def to_psnr(dehaze, gt):\n",
    "    \"\"\"\n",
    "    Compute PSNR (Peak Signal-to-Noise Ratio) between dehazed and ground truth images.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: PSNR values for each image in the batch.\n",
    "    \"\"\"\n",
    "    mse = F.mse_loss(dehaze, gt, reduction='none').mean(dim=[1, 2, 3])  # Compute MSE per image\n",
    "    intensity_max = 1.0\n",
    "\n",
    "    # Compute PSNR safely, avoiding division by zero and extreme values\n",
    "    psnr_list = [10.0 * log10(intensity_max / max(mse_val.item(), 1e-6)) for mse_val in mse]\n",
    "\n",
    "    return psnr_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.341295Z",
     "iopub.status.busy": "2025-04-21T16:01:17.340966Z",
     "iopub.status.idle": "2025-04-21T16:01:22.663023Z",
     "shell.execute_reply": "2025-04-21T16:01:22.661971Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.341266Z"
    },
    "papermill": {
     "duration": 2.341767,
     "end_time": "2025-04-12T13:09:35.297645",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.955878",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "# Define SSIM metric\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0, reduction='none')\n",
    "\n",
    "def to_ssim(dehaze: torch.Tensor, gt: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute SSIM directly on the GPU using torchmetrics.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: SSIM values for each image in the batch.\n",
    "    \"\"\"\n",
    "    ssim_values = ssim_metric(dehaze, gt)  # Shape: [B]\n",
    "    # print(\"1\",ssim_values)\n",
    "    # print(\"2\",[ssim_values])\n",
    "    ssim_values = ssim_values.tolist() \n",
    "    # print(type(ssim_values))\n",
    "    if isinstance(ssim_values, float):  # Correct way to check for a float\n",
    "        return [ssim_values]  # Convert single float to a list\n",
    "    return ssim_values  # Otherwise, return as is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.664561Z",
     "iopub.status.busy": "2025-04-21T16:01:22.664015Z",
     "iopub.status.idle": "2025-04-21T16:01:22.804857Z",
     "shell.execute_reply": "2025-04-21T16:01:22.804035Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.664534Z"
    },
    "papermill": {
     "duration": 0.183935,
     "end_time": "2025-04-12T13:09:35.493308",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.309373",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00673126894980669]\n"
     ]
    }
   ],
   "source": [
    "# Test with a dummy tensor\n",
    "dehaze = torch.rand(1, 3, 360, 360)  # Random batch of images\n",
    "gt = torch.rand(1, 3, 360, 360)  # Random ground truth images\n",
    "\n",
    "ssim_scores = to_ssim(dehaze, gt)\n",
    "print(ssim_scores)  # Should print a list of 6 SSIM values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.805937Z",
     "iopub.status.busy": "2025-04-21T16:01:22.805707Z",
     "iopub.status.idle": "2025-04-21T16:01:22.814610Z",
     "shell.execute_reply": "2025-04-21T16:01:22.813471Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.805921Z"
    },
    "papermill": {
     "duration": 0.018069,
     "end_time": "2025-04-12T13:09:35.522246",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.504177",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validationB(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: Your deep learning model\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: GPU/CPU device\n",
    "    :param category: dataset type (indoor/outdoor)\n",
    "    :param save_tag: whether to save images\n",
    "    :return: average PSNR & SSIM values\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    \n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            haze, gt = val_data\n",
    "            haze, gt = haze.to(device), gt.to(device)\n",
    "            dehaze, _, _ = net(haze)\n",
    "\n",
    "        # --- Compute PSNR & SSIM --- #\n",
    "        batch_psnr = to_psnr(dehaze, gt)  # This returns a list\n",
    "        # print(batch_psnr)\n",
    "        batch_ssim = to_ssim(dehaze, gt)  # This returns a list\n",
    "        # print(batch_ssim)\n",
    "\n",
    "        psnr_list.extend(batch_psnr)  # Flatten the list\n",
    "        ssim_list.extend(batch_ssim)  # Flatten the list\n",
    "\n",
    "    # --- Ensure lists are not empty to avoid division by zero --- #\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.816571Z",
     "iopub.status.busy": "2025-04-21T16:01:22.816059Z",
     "iopub.status.idle": "2025-04-21T16:01:22.840957Z",
     "shell.execute_reply": "2025-04-21T16:01:22.840120Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.816534Z"
    },
    "papermill": {
     "duration": 0.01909,
     "end_time": "2025-04-12T13:09:35.551459",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.532369",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validation_sr(net, sr_val_loader, device):\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    for lr, hr in sr_val_loader:\n",
    "        with torch.no_grad():\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr_out, _, _ = net(lr, sr = False)\n",
    "        psnr_list.extend(to_psnr(sr_out, hr))\n",
    "        ssim_list.extend(to_ssim(sr_out, hr))\n",
    "\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009925,
     "end_time": "2025-04-12T13:09:35.578339",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.568414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.846831Z",
     "iopub.status.busy": "2025-04-21T16:01:22.846514Z",
     "iopub.status.idle": "2025-04-21T16:01:22.862331Z",
     "shell.execute_reply": "2025-04-21T16:01:22.861073Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.846807Z"
    },
    "papermill": {
     "duration": 0.016023,
     "end_time": "2025-04-12T13:09:35.604481",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.588458",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.863635Z",
     "iopub.status.busy": "2025-04-21T16:01:22.863314Z",
     "iopub.status.idle": "2025-04-21T16:01:22.880993Z",
     "shell.execute_reply": "2025-04-21T16:01:22.880020Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.863607Z"
    },
    "papermill": {
     "duration": 0.015183,
     "end_time": "2025-04-12T13:09:35.637575",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.622392",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# psnr, ssim = validationB(net, val_data_loader, device, category)\n",
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # psnr, ssim = validationB(model, val_loader, device, \"indoor\", save_tag=True)\n",
    "# print(f\"Validation PSNR: {psnr:.2f}, SSIM: {ssim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.882823Z",
     "iopub.status.busy": "2025-04-21T16:01:22.882494Z",
     "iopub.status.idle": "2025-04-21T16:01:22.912339Z",
     "shell.execute_reply": "2025-04-21T16:01:22.911133Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.882793Z"
    },
    "papermill": {
     "duration": 0.021172,
     "end_time": "2025-04-12T13:09:35.676615",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.655443",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0169940db6452b86d3d937c1d35cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Execution Env:', options=('local', 'kaggle'), value='local')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execution_env_widget = widgets.Dropdown(options=['local', 'kaggle'], value='local', description='Execution Env:')\n",
    "display(execution_env_widget)\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "    execution_env_widget.value = 'kaggle' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.913804Z",
     "iopub.status.busy": "2025-04-21T16:01:22.913475Z",
     "iopub.status.idle": "2025-04-21T16:01:22.965654Z",
     "shell.execute_reply": "2025-04-21T16:01:22.964307Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.913775Z"
    },
    "papermill": {
     "duration": 0.054368,
     "end_time": "2025-04-12T13:09:35.750507",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.696139",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c7bfda5dc44bb69957761ac5627649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Learning Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a818798f65445289c9b13158267849f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='128,128', description='Crop Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4b23f17b364b1792ed3100b98bc26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Train Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875dd4f7cea94946bc38e9c81738fda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=0, description='Version:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e274102eb274b8482fcb1c5096593af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='Growth Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223b23b1654a4a68aefcca0be3ab3c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.04, description='Lambda Loss:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd1327edb4545818cae581baaa799f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Val Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393ccc12b39347cab6ff89133353f707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', index=2, options=('indoor', 'outdoor', 'reside', 'nh'), value='reside')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters set:\n",
      "learning_rate: 0.0001\n",
      "crop_size: [128, 128]\n",
      "train_batch_size: 2\n",
      "version: 0\n",
      "growth_rate: 16\n",
      "lambda_loss: 0.04\n",
      "val_batch_size: 2\n",
      "category: reside\n",
      "execution_env: local\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Create widgets for each hyper-parameter ---\n",
    "learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n",
    "crop_size_widget = widgets.Text(value='128,128', description='Crop Size:')\n",
    "train_batch_size_widget = widgets.IntText(value=2, description='Train Batch Size:')\n",
    "version_widget = widgets.IntText(value=0, description='Version:')\n",
    "growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n",
    "lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n",
    "val_batch_size_widget = widgets.IntText(value=2, description='Val Batch Size:')\n",
    "category_widget = widgets.Dropdown(options=['indoor', 'outdoor', 'reside', 'nh'], value='reside', description='Category:')\n",
    "\n",
    "# --- Display the widgets ---\n",
    "display(\n",
    "    learning_rate_widget, crop_size_widget, train_batch_size_widget, version_widget,\n",
    "    growth_rate_widget, lambda_loss_widget, \n",
    "    val_batch_size_widget, category_widget\n",
    ")\n",
    "\n",
    "# --- Function to parse crop size ---\n",
    "def parse_crop_size(crop_size_str):\n",
    "    return [int(x) for x in crop_size_str.split(',')]\n",
    "\n",
    "# --- Assign the widget values to variables ---\n",
    "learning_rate = learning_rate_widget.value\n",
    "crop_size = parse_crop_size(crop_size_widget.value)\n",
    "train_batch_size = train_batch_size_widget.value\n",
    "version = version_widget.value\n",
    "growth_rate = growth_rate_widget.value\n",
    "lambda_loss = lambda_loss_widget.value\n",
    "val_batch_size = val_batch_size_widget.value\n",
    "category = category_widget.value\n",
    "\n",
    "execution_env = execution_env_widget.value  # Local or Kaggle\n",
    "\n",
    "\n",
    "print('\\nHyper-parameters set:')\n",
    "print(f'learning_rate: {learning_rate}')\n",
    "print(f'crop_size: {crop_size}')\n",
    "print(f'train_batch_size: {train_batch_size}')\n",
    "print(f'version: {version}')\n",
    "print(f'growth_rate: {growth_rate}')\n",
    "print(f'lambda_loss: {lambda_loss}')\n",
    "print(f'val_batch_size: {val_batch_size}')\n",
    "print(f'category: {category}')\n",
    "print(f'execution_env: {execution_env}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RESIDE dataset\n",
      "Using local RESIDE dataset\n",
      "\n",
      "Final dataset paths:\n",
      "Training directory: /Volumes/S/dev/project/code/Aphase/dehaze/dataset/reside_processed/kaggle/working/cropped_train/\n",
      "Validation directory: /kaggle/input/reside6k/RESIDE-6K/train\n",
      "Number of epochs: 20\n"
     ]
    }
   ],
   "source": [
    "# --- Set category-specific hyper-parameters ---\n",
    "if category == 'indoor':\n",
    "    num_epochs = 1500\n",
    "    train_data_dir = './data/train/indoor/'\n",
    "    val_data_dir = './data/test/SOTS/indoor/'\n",
    "elif category == 'outdoor':\n",
    "    num_epochs = 10\n",
    "    train_data_dir = './data/train/outdoor/'\n",
    "    val_data_dir = './data/test/SOTS/outdoor/'\n",
    "elif category == 'reside':\n",
    "    print('Using RESIDE dataset')\n",
    "    num_epochs = 20\n",
    "    train_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n",
    "    val_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n",
    "    test_data_dir = '/kaggle/input/reside6k/RESIDE-6K/test'\n",
    "    if execution_env == 'local':\n",
    "        print('Using local RESIDE dataset')\n",
    "        train_data_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/reside_processed/kaggle/working/cropped_train/'\n",
    "elif category == 'nh':\n",
    "    num_epochs = 50\n",
    "    train_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "    val_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "else:\n",
    "    raise Exception('Wrong image category. Set it to indoor or outdoor for RESIDE dataset.')\n",
    "\n",
    "# --- Adjust paths based on execution environment ---\n",
    "# if execution_env == 'kaggle':\n",
    "    # train_data_dir = '/kaggle/input/reside-dataset/' + train_data_dir.strip('./')\n",
    "    # val_data_dir = '/kaggle/input/reside-dataset/' + val_data_dir.strip('./')\n",
    "    # train_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T'\n",
    "    # val_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V' \n",
    "    # train_data_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "    # val_data_dir = '/kaggle/input/o-haze/O-HAZY/GT' \n",
    "print('\\nFinal dataset paths:')\n",
    "print(f'Training directory: {train_data_dir}')\n",
    "print(f'Validation directory: {val_data_dir}')\n",
    "print(f'Number of epochs: {num_epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_enabled = True\n",
    "    # sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n",
    "    # sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X4'\n",
    "if sr_enabled:\n",
    "    sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n",
    "    sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X4'\n",
    "    if execution_env == 'local':\n",
    "        sr_hr_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/SR_flickr/kaggle/working/filtered_HR'\n",
    "        sr_lr_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/SR_flickr/kaggle/working/filtered_LR/X4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.967040Z",
     "iopub.status.busy": "2025-04-21T16:01:22.966747Z",
     "iopub.status.idle": "2025-04-21T16:01:22.972795Z",
     "shell.execute_reply": "2025-04-21T16:01:22.971698Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.967012Z"
    },
    "papermill": {
     "duration": 0.017958,
     "end_time": "2025-04-12T13:09:35.817103",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.799145",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# hazeeffected_images_dir_train = f\"{train_data_dir}/IN\"\n",
    "hazeeffected_images_dir_train = f\"{train_data_dir}/hazy\"\n",
    "hazefree_images_dir_train = f\"{train_data_dir}/GT\"\n",
    "\n",
    "# hazeeffected_images_dir_valid = f\"{val_data_dir}/IN\"\n",
    "hazeeffected_images_dir_valid = f\"{val_data_dir}/hazy\"\n",
    "hazefree_images_dir_valid = f\"{val_data_dir}/GT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.011082,
     "end_time": "2025-04-12T13:09:35.839407",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.828325",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.974080Z",
     "iopub.status.busy": "2025-04-21T16:01:22.973790Z",
     "iopub.status.idle": "2025-04-21T16:01:22.997749Z",
     "shell.execute_reply": "2025-04-21T16:01:22.996914Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.974058Z"
    },
    "papermill": {
     "duration": 0.016364,
     "end_time": "2025-04-12T13:09:35.867068",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.850704",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.999035Z",
     "iopub.status.busy": "2025-04-21T16:01:22.998710Z",
     "iopub.status.idle": "2025-04-21T16:01:23.018558Z",
     "shell.execute_reply": "2025-04-21T16:01:23.017604Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.999002Z"
    },
    "papermill": {
     "duration": 0.017253,
     "end_time": "2025-04-12T13:09:35.895882",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.878629",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# hazeeffected_images_dir_train = f\"{train_data_dir}/IN\"\n",
    "# hazefree_images_dir_train = f\"{train_data_dir}/GT\"\n",
    "\n",
    "# hazeeffected_images_dir_valid = f\"{val_data_dir}/IN\"\n",
    "# hazefree_images_dir_valid = f\"{val_data_dir}/GT\"\n",
    "\n",
    "# # Create validation directories if they don't exist\n",
    "# os.makedirs(hazeeffected_images_dir_valid, exist_ok=True)\n",
    "# os.makedirs(hazefree_images_dir_valid, exist_ok=True)\n",
    "\n",
    "# # List all hazy and clean images\n",
    "# hazy_images = sorted(glob.glob(f\"{hazeeffected_images_dir_train}/*\"))\n",
    "# clean_images = sorted(glob.glob(f\"{hazefree_images_dir_train}/*\"))\n",
    "\n",
    "# # Ensure matching hazy-clean pairs\n",
    "# assert len(hazy_images) == len(clean_images), \"Mismatch in hazy and clean images count!\"\n",
    "\n",
    "# # Shuffle while keeping the hazy-clean correspondence\n",
    "# paired_images = list(zip(hazy_images, clean_images))\n",
    "# # random.shuffle(paired_images)\n",
    "\n",
    "# # Define split ratio (e.g., 80% train, 20% validation)\n",
    "# split_ratio = 0.8\n",
    "# split_idx = int(len(paired_images) * split_ratio)\n",
    "\n",
    "# # Split into train and validation\n",
    "# train_pairs = paired_images[:split_idx]\n",
    "# valid_pairs = paired_images[split_idx:]\n",
    "\n",
    "# # Move validation images\n",
    "# for hazy_path, clean_path in valid_pairs:\n",
    "#     shutil.move(hazy_path, hazeeffected_images_dir_valid)\n",
    "#     shutil.move(clean_path, hazefree_images_dir_valid)\n",
    "\n",
    "# print(f\"Moved {len(valid_pairs)} image pairs to validation set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.019692Z",
     "iopub.status.busy": "2025-04-21T16:01:23.019456Z",
     "iopub.status.idle": "2025-04-21T16:01:23.039010Z",
     "shell.execute_reply": "2025-04-21T16:01:23.037920Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.019674Z"
    },
    "papermill": {
     "duration": 0.016445,
     "end_time": "2025-04-12T13:09:35.923418",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.906973",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # hazeeffected_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "# # hazefree_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "# # hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "# # hazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/GT'\n",
    "\n",
    "# hazeeffected_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN'\n",
    "# hazefree_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT'\n",
    "# hazeeffected_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN'\n",
    "# hazefree_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/GT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.040582Z",
     "iopub.status.busy": "2025-04-21T16:01:23.040208Z",
     "iopub.status.idle": "2025-04-21T16:01:23.062332Z",
     "shell.execute_reply": "2025-04-21T16:01:23.061417Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.040550Z"
    },
    "papermill": {
     "duration": 0.019086,
     "end_time": "2025-04-12T13:09:35.953847",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.934761",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_log(epoch, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, category):\n",
    "    log_dir = \"./training_log\"\n",
    "    os.makedirs(log_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    log_path = os.path.join(log_dir, f\"{category}_log.txt\")\n",
    "\n",
    "    print('({0:.0f}s) Epoch [{1}/{2}], Train_PSNR:{3:.2f}, Val_PSNR:{4:.2f}, Val_SSIM:{5:.4f}'\n",
    "          .format(one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim))\n",
    "\n",
    "    # --- Write the training log --- #\n",
    "    with open(log_path, 'a') as f:\n",
    "        print('Date: {0}, Time_Cost: {1:.0f}s, Epoch: [{2}/{3}], Train_PSNR: {4:.2f}, Val_PSNR: {5:.2f}, Val_SSIM: {6:.4f}'\n",
    "              .format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "                      one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.063670Z",
     "iopub.status.busy": "2025-04-21T16:01:23.063338Z",
     "iopub.status.idle": "2025-04-21T16:01:23.081053Z",
     "shell.execute_reply": "2025-04-21T16:01:23.080012Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.063633Z"
    },
    "papermill": {
     "duration": 0.01846,
     "end_time": "2025-04-12T13:09:35.983598",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.965138",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, category, lr_decay=0.90):\n",
    "    \"\"\"\n",
    "    Adjusts the learning rate based on the epoch and dataset category.\n",
    "\n",
    "    :param optimizer: The optimizer (e.g., Adam, SGD).\n",
    "    :param epoch: Current epoch number.\n",
    "    :param category: Dataset category ('indoor', 'outdoor', or 'NH').\n",
    "    :param lr_decay: Multiplicative factor for learning rate decay.\n",
    "    \"\"\"\n",
    "    # Define learning rate decay steps based on category\n",
    "    step_dict = {'indoor': 18, 'outdoor': 3, 'NH': 20}\n",
    "    step = step_dict.get(category, 3)  # Default step size if category is unknown\n",
    "\n",
    "    # Decay learning rate at the specified step\n",
    "    if epoch > 0 and epoch % step == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= lr_decay\n",
    "            print(f\"Epoch {epoch}: Learning rate adjusted to {param_group['lr']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011722,
     "end_time": "2025-04-12T13:09:36.006955",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.995233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.082506Z",
     "iopub.status.busy": "2025-04-21T16:01:23.082047Z",
     "iopub.status.idle": "2025-04-21T16:01:23.105640Z",
     "shell.execute_reply": "2025-04-21T16:01:23.104649Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.082473Z"
    },
    "papermill": {
     "duration": 0.020068,
     "end_time": "2025-04-12T13:09:36.038381",
     "exception": false,
     "start_time": "2025-04-12T13:09:36.018313",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Perceptual Feature Loss Network --- #\n",
    "class PerceptualLossNet(nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = vgg_model\n",
    "        self.feature_layers = {'3': \"low_level\", '8': \"mid_level\", '15': \"high_level\"}\n",
    "\n",
    "    def get_feature_maps(self, x):\n",
    "        feature_maps = []\n",
    "        for layer_id, layer in self.feature_extractor.named_children():\n",
    "            x = layer(x)\n",
    "            if layer_id in self.feature_layers:\n",
    "                feature_maps.append(x)\n",
    "        return feature_maps\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        pred_features = self.get_feature_maps(predicted)\n",
    "        target_features = self.get_feature_maps(target)\n",
    "        \n",
    "        # Compute perceptual loss as mean squared error across feature maps\n",
    "        loss = torch.stack([F.mse_loss(p, t) for p, t in zip(pred_features, target_features)]).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "class SSFM(nn.Module):\n",
    "    def __init__(self, loss_type='l1'):\n",
    "        super(SSFM, self).__init__()\n",
    "        assert loss_type in ['l1', 'l2'], \"loss_type must be 'l1' or 'l2'\"\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def forward(self, student_feats, teacher_feats):\n",
    "        \"\"\"\n",
    "        student_feats: List of feature maps from student RDBs [rdb1, rdb2, rdb3, rdb4]\n",
    "        teacher_feats: List of corresponding feature maps from teacher\n",
    "        \"\"\"\n",
    "        assert len(student_feats) == len(teacher_feats), \"Feature lists must match\"\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for s_feat, t_feat in zip(student_feats, teacher_feats):\n",
    "            # Match resolution\n",
    "            if s_feat.shape != t_feat.shape:\n",
    "                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            if self.loss_type == 'l1':\n",
    "                loss = F.l1_loss(s_feat, t_feat)\n",
    "            else:\n",
    "                loss = F.mse_loss(s_feat, t_feat)\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss / len(student_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.106898Z",
     "iopub.status.busy": "2025-04-21T16:01:23.106596Z",
     "iopub.status.idle": "2025-04-21T16:01:29.590895Z",
     "shell.execute_reply": "2025-04-21T16:01:29.589811Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.106873Z"
    },
    "papermill": {
     "duration": 5.336581,
     "end_time": "2025-04-12T13:09:41.386705",
     "exception": false,
     "start_time": "2025-04-12T13:09:36.050124",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No pretrained weights found at /kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\n",
      "📊 Total Trainable Parameters: 4,645,694\n"
     ]
    }
   ],
   "source": [
    "# --- Imports --- #\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "# --- Device Setup --- #\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_ids = list(range(torch.cuda.device_count()))\n",
    "\n",
    "# --- Initialize Model --- #\n",
    "net = DeepGuidedNetwork().to(device)\n",
    "\n",
    "# --- Enable Multi-GPU (if available) --- #\n",
    "if len(device_ids) > 1:\n",
    "    net = nn.DataParallel(net, device_ids=device_ids)\n",
    "\n",
    "# --- Optimizer --- #\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Load Pretrained VGG16 for Perceptual Loss --- #\n",
    "vgg_features = vgg16(pretrained=True).features[:16].to(device)\n",
    "for param in vgg_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "loss_network = PerceptualLossNet(vgg_features)\n",
    "loss_network.eval()\n",
    "\n",
    "# --- Load Model Weights (if available) --- #\n",
    "checkpoint_path = \"/kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\" \n",
    "\n",
    "try:\n",
    "    net.load_state_dict(torch.load(checkpoint_path, weights_only=False, map_location=torch.device('cpu')))\n",
    "    print(f\"✅ Model weights loaded from {checkpoint_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️ No pretrained weights found at {checkpoint_path}\")\n",
    "\n",
    "# --- Compute Total Trainable Parameters --- #\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"📊 Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:29.600137Z",
     "iopub.status.busy": "2025-04-21T16:01:29.599357Z",
     "iopub.status.idle": "2025-04-21T16:01:29.780968Z",
     "shell.execute_reply": "2025-04-21T16:01:29.779763Z",
     "shell.execute_reply.started": "2025-04-21T16:01:29.599860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeatureAffinityModule(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(FeatureAffinityModule, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def forward(self, student_features, teacher_features):\n",
    "        # Debug: Print input shapes\n",
    "        print(\"Input student_features shape:\", student_features.shape)\n",
    "        print(\"Input teacher_features shape:\", teacher_features.shape)\n",
    "\n",
    "        # Normalize features\n",
    "        print(\"\\nBefore normalization:\")\n",
    "        print(\"Student features:\", student_features.shape)  # Example for the first feature of the first batch\n",
    "        print(\"Teacher features:\", teacher_features.shape)  # Example for the first feature of the first batch\n",
    "\n",
    "        student_norm = F.normalize(student_features.view(student_features.size(0), self.channels, -1), dim=2)\n",
    "        print(\"Student normalized features:\", student_norm.shape)  # Checking first normalized value\n",
    "\n",
    "        print(\"\\nother:\")\n",
    "        print(\"student_features.size(0)\",student_features.size(0))\n",
    "        print(\"teacher_features.size(0)\",teacher_features.size(0))\n",
    "        print(\"self.channels\",self.channels)\n",
    "        print(\"student_features.view(student_features.size(0), self.channels, -1)\",student_features.view(student_features.size(0), self.channels, -1).shape)\n",
    "        print(\"teacher_features.view(teacher_features.size(0), self.channels, -1)\",teacher_features.view(teacher_features.size(0), self.channels, -1).shape)\n",
    "        \n",
    "        teacher_norm = F.normalize(teacher_features.view(teacher_features.size(0), self.channels, -1), dim=2)\n",
    "        print(\"Teacher normalized features:\", teacher_norm[0, 0, :2])  # Checking first normalized value\n",
    "\n",
    "        # Compute affinity matrices\n",
    "        student_affinity = torch.bmm(student_norm, student_norm.transpose(1, 2))\n",
    "        teacher_affinity = torch.bmm(teacher_norm, teacher_norm.transpose(1, 2))\n",
    "\n",
    "        # Debug: Print affinity matrix shapes\n",
    "        print(\"\\nStudent affinity matrix shape:\", student_affinity.shape)\n",
    "        print(\"Teacher affinity matrix shape:\", teacher_affinity.shape)\n",
    "\n",
    "        # Compute KL divergence\n",
    "        loss = F.kl_div(F.log_softmax(student_affinity, dim=-1),\n",
    "                        F.softmax(teacher_affinity, dim=-1),\n",
    "                        reduction='batchmean')\n",
    "\n",
    "        # Debug: Print computed loss\n",
    "        print(\"\\nComputed loss:\", loss.item())\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:29.782444Z",
     "iopub.status.busy": "2025-04-21T16:01:29.782054Z",
     "iopub.status.idle": "2025-04-21T16:01:55.153827Z",
     "shell.execute_reply": "2025-04-21T16:01:55.152853Z",
     "shell.execute_reply.started": "2025-04-21T16:01:29.782414Z"
    },
    "papermill": {
     "duration": 23.313227,
     "end_time": "2025-04-12T13:10:04.746196",
     "exception": false,
     "start_time": "2025-04-12T13:09:41.432969",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4800, Validation samples: 1200\n"
     ]
    }
   ],
   "source": [
    "# Create train and validation datasets\n",
    "train_dataset = HazeDataset(\n",
    "                            hazeeffected_images_dir=hazeeffected_images_dir_train,\n",
    "                            hazefree_images_dir=hazefree_images_dir_train,\n",
    "                            split=\"train\")\n",
    "\n",
    "val_dataset = HazeDataset(\n",
    "                          hazeeffected_images_dir=hazeeffected_images_dir_train,\n",
    "                          hazefree_images_dir=hazefree_images_dir_train,\n",
    "                          split=\"valid\")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:55.155297Z",
     "iopub.status.busy": "2025-04-21T16:01:55.154949Z",
     "iopub.status.idle": "2025-04-21T16:01:55.160764Z",
     "shell.execute_reply": "2025-04-21T16:01:55.159843Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.155268Z"
    },
    "papermill": {
     "duration": 0.018274,
     "end_time": "2025-04-12T13:10:04.777834",
     "exception": false,
     "start_time": "2025-04-12T13:10:04.759560",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob( \"/kaggle/working/cropped_train/hazy/*\")), len(glob.glob(\"/kaggle/working/cropped_train/GT/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:05.581577Z",
     "iopub.status.busy": "2025-04-21T16:09:05.581181Z",
     "iopub.status.idle": "2025-04-21T16:09:05.585769Z",
     "shell.execute_reply": "2025-04-21T16:09:05.584708Z",
     "shell.execute_reply.started": "2025-04-21T16:09:05.581550Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:11.972197Z",
     "iopub.status.busy": "2025-04-21T16:09:11.971341Z",
     "iopub.status.idle": "2025-04-21T16:09:11.984636Z",
     "shell.execute_reply": "2025-04-21T16:09:11.983552Z",
     "shell.execute_reply.started": "2025-04-21T16:09:11.972167Z"
    },
    "papermill": {
     "duration": 0.022717,
     "end_time": "2025-04-12T13:10:04.813435",
     "exception": false,
     "start_time": "2025-04-12T13:10:04.790718",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, scale='x4', split='train', split_ratio=0.9):\n",
    "        \"\"\"\n",
    "        Super-Resolution dataset that matches LR and HR image pairs based on naming pattern.\n",
    "        Assumes images are already aligned and correctly scaled. No cropping is applied.\n",
    "\n",
    "        Args:\n",
    "            lr_dir (str): Directory containing low-resolution images (e.g., x2, x3, x4).\n",
    "            hr_dir (str): Directory containing high-resolution images.\n",
    "            scale (str): Scale suffix in LR filenames (e.g., 'x2', 'x3', 'x4').\n",
    "            split (str): 'train' or 'val'.\n",
    "            split_ratio (float): Ratio of training data (e.g., 0.9 = 90% train).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.scale = scale\n",
    "        self.split = split.lower()\n",
    "\n",
    "        valid_ext = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        lr_images = sorted([\n",
    "            f for f in glob.glob(os.path.join(lr_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_ext)\n",
    "        ])\n",
    "\n",
    "        lr_hr_pairs = []\n",
    "        for lr_path in lr_images:\n",
    "            lr_name = os.path.basename(lr_path)\n",
    "            hr_name = lr_name.replace(scale, '')  # assumes naming like image_x4.png -> image.png\n",
    "            hr_path = os.path.join(hr_dir, hr_name)\n",
    "\n",
    "            if not os.path.exists(hr_path):\n",
    "                print(f\"Warning: Ground-truth missing for {lr_name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            lr_hr_pairs.append((lr_path, hr_path))\n",
    "\n",
    "        if not lr_hr_pairs:\n",
    "            raise ValueError(\"No matching LR-HR image pairs found.\")\n",
    "\n",
    "        split_idx = int(len(lr_hr_pairs) * split_ratio)\n",
    "        if self.split == 'train':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[:split_idx]\n",
    "        elif self.split == 'val':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[split_idx:]\n",
    "        else:\n",
    "            raise ValueError(\"split must be either 'train' or 'val'\")\n",
    "\n",
    "        # Define common transform\n",
    "        self.transform = Compose([ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_hr_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_path, hr_path = self.lr_hr_pairs[idx]\n",
    "\n",
    "        try:\n",
    "            lr_img = Image.open(lr_path).convert('RGB')\n",
    "            hr_img = Image.open(hr_path).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Unidentified image at {lr_path} or {hr_path}\")\n",
    "\n",
    "        lr_tensor = self.transform(lr_img)\n",
    "        hr_tensor = self.transform(hr_img)\n",
    "\n",
    "        if lr_tensor.shape[0] != 3 or hr_tensor.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {lr_path}\")\n",
    "\n",
    "        return lr_tensor, hr_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:16.873360Z",
     "iopub.status.busy": "2025-04-21T16:09:16.873060Z",
     "iopub.status.idle": "2025-04-21T16:09:16.879312Z",
     "shell.execute_reply": "2025-04-21T16:09:16.878403Z",
     "shell.execute_reply.started": "2025-04-21T16:09:16.873340Z"
    },
    "papermill": {
     "duration": 0.018734,
     "end_time": "2025-04-12T13:10:04.844884",
     "exception": false,
     "start_time": "2025-04-12T13:10:04.826150",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    min_height = min([x[0].shape[1] for x in batch])//4\n",
    "    min_width = min([x[0].shape[2] for x in batch])//4\n",
    "    resized_batch = [(TF.resize(x[0], [min_height, min_width]), TF.resize(x[1], [min_height, min_width])) for x in batch]\n",
    "    return torch.utils.data.dataloader.default_collate(resized_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:24.350816Z",
     "iopub.status.busy": "2025-04-21T16:09:24.350518Z",
     "iopub.status.idle": "2025-04-21T16:09:36.552704Z",
     "shell.execute_reply": "2025-04-21T16:09:36.551702Z",
     "shell.execute_reply.started": "2025-04-21T16:09:24.350794Z"
    },
    "papermill": {
     "duration": 6.842293,
     "end_time": "2025-04-12T13:10:11.789404",
     "exception": false,
     "start_time": "2025-04-12T13:10:04.947111",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- SR Dataset Setup --- #\n",
    "\n",
    "if sr_enabled:\n",
    "\n",
    "    sr_train_dataset = SRDataset(lr_dir=sr_lr_dir, hr_dir=sr_hr_dir, scale='x4', split='train')\n",
    "    sr_val_dataset = SRDataset(lr_dir=sr_lr_dir, hr_dir=sr_hr_dir, scale='x4', split='val')\n",
    "    sr_train_loader = DataLoader(sr_train_dataset, batch_size=batch_size, shuffle=True, custom_collate_fn=custom_collate_fn )\n",
    "    sr_val_loader = DataLoader(sr_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    sr_iter = iter(sr_train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:36.554275Z",
     "iopub.status.busy": "2025-04-21T16:09:36.554010Z",
     "iopub.status.idle": "2025-04-21T16:09:36.665975Z",
     "shell.execute_reply": "2025-04-21T16:09:36.665053Z",
     "shell.execute_reply.started": "2025-04-21T16:09:36.554253Z"
    },
    "papermill": {
     "duration": 0.272846,
     "end_time": "2025-04-12T13:10:12.082208",
     "exception": false,
     "start_time": "2025-04-12T13:10:11.809362",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64]) torch.Size([2, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for i,o in train_data_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64]) torch.Size([2, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for i,o in val_data_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:10:00.985315Z",
     "iopub.status.busy": "2025-04-21T16:10:00.985016Z",
     "iopub.status.idle": "2025-04-21T16:10:01.651391Z",
     "shell.execute_reply": "2025-04-21T16:10:01.650352Z",
     "shell.execute_reply.started": "2025-04-21T16:10:00.985293Z"
    },
    "papermill": {
     "duration": 2.453133,
     "end_time": "2025-04-12T13:10:14.580358",
     "exception": false,
     "start_time": "2025-04-12T13:10:12.127225",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64]) torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i,o in sr_train_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:51.026594Z",
     "iopub.status.busy": "2025-04-21T16:09:51.026121Z",
     "iopub.status.idle": "2025-04-21T16:09:51.473913Z",
     "shell.execute_reply": "2025-04-21T16:09:51.472989Z",
     "shell.execute_reply.started": "2025-04-21T16:09:51.026568Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64]) torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i,o in sr_val_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2385, 265)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sr_train_dataset), len(sr_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: LR shape: torch.Size([2, 3, 64, 64]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 1: LR shape: torch.Size([2, 3, 64, 64]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 2: LR shape: torch.Size([2, 3, 64, 64]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 3: LR shape: torch.Size([2, 3, 64, 64]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 4: LR shape: torch.Size([2, 3, 64, 64]), HR shape: torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# enumerate\n",
    "for i, (lr, hr) in enumerate(sr_train_loader):\n",
    "    print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "    if i == 4:  # Just to limit the output\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/c7x3kkg169s8zlyh_hfm0_wm0000gp/T/ipykernel_6119/3491781775.py:12: UserWarning: Using a target size (torch.Size([2, 3, 256, 256])) that is different to the input size (torch.Size([2, 3, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mse = F.mse_loss(dehaze, gt, reduction='none').mean(dim=[1, 2, 3])  # Compute MSE per image\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[239], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# --- Initial Validation --- #\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(f\"[Dehazing Init Val] PSNR: {old_val_psnr:.2f}, SSIM: {old_val_ssim:.4f}\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr_enabled:\n\u001b[0;32m---> 15\u001b[0m     sr_val_psnr, sr_val_ssim \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_sr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SR Init Val] PSNR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msr_val_psnr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, SSIM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msr_val_ssim\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# --- Training Loop --- #\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[164], line 8\u001b[0m, in \u001b[0;36mvalidation_sr\u001b[0;34m(net, sr_val_loader, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m         lr, hr \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mto(device), hr\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m         sr_out, _, _ \u001b[38;5;241m=\u001b[39m net(lr, sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m     psnr_list\u001b[38;5;241m.\u001b[39mextend(\u001b[43mto_psnr\u001b[49m\u001b[43m(\u001b[49m\u001b[43msr_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m     ssim_list\u001b[38;5;241m.\u001b[39mextend(to_ssim(sr_out, hr))\n\u001b[1;32m     11\u001b[0m avr_psnr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(psnr_list) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(psnr_list) \u001b[38;5;28;01mif\u001b[39;00m psnr_list \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n",
      "Cell \u001b[0;32mIn[160], line 12\u001b[0m, in \u001b[0;36mto_psnr\u001b[0;34m(dehaze, gt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_psnr\u001b[39m(dehaze, gt):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Compute PSNR (Peak Signal-to-Noise Ratio) between dehazed and ground truth images.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m        List[float]: PSNR values for each image in the batch.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     mse \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdehaze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])  \u001b[38;5;66;03m# Compute MSE per image\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     intensity_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Compute PSNR safely, avoiding division by zero and extreme values\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[1;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3794\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# --- Teacher Network --- #\n",
    "teacher_net = DeepGuidedNetwork(radius=1).to(device)\n",
    "# teacher_net.load_state_dict(torch.load('teacher_model.pth'))\n",
    "# teacher_net.eval()\n",
    "\n",
    "# --- Feature Affinity Module --- #\n",
    "fam = FeatureAffinityModule(channels=64).to(device)\n",
    "ssfm_loss = SSFM(loss_type='l1') \n",
    "\n",
    "\n",
    "# --- Initial Validation --- #\n",
    "# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)\n",
    "# print(f\"[Dehazing Init Val] PSNR: {old_val_psnr:.2f}, SSIM: {old_val_ssim:.4f}\")\n",
    "if sr_enabled:\n",
    "    sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "    print(f\"[SR Init Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "# --- Training Loop --- #\n",
    "best_psnr = old_val_psnr\n",
    "train_psnr_prev = 0\n",
    "distillation_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.205467Z",
     "iopub.status.idle": "2025-04-21T16:01:55.205773Z",
     "shell.execute_reply": "2025-04-21T16:01:55.205660Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.205648Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-04-12T13:10:14.625824",
     "status": "running"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Init Val] PSNR: 10.85, SSIM: 0.3050\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (0, 4) at dimension 3 of input [2, 144, 4, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[202], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Dehazing Init Val] PSNR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_val_psnr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, SSIM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_val_ssim\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr_enabled:\n\u001b[0;32m---> 15\u001b[0m     sr_val_psnr, sr_val_ssim \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_sr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SR Init Val] PSNR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msr_val_psnr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, SSIM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msr_val_ssim\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# --- Training Loop --- #\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[164], line 7\u001b[0m, in \u001b[0;36mvalidation_sr\u001b[0;34m(net, sr_val_loader, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m     lr, hr \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mto(device), hr\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     sr_out, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m psnr_list\u001b[38;5;241m.\u001b[39mextend(to_psnr(sr_out, hr))\n\u001b[1;32m      9\u001b[0m ssim_list\u001b[38;5;241m.\u001b[39mextend(to_ssim(sr_out, hr))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[18], line 94\u001b[0m, in \u001b[0;36mDeepGuidedNetwork.forward\u001b[0;34m(self, x_hr, sr)\u001b[0m\n\u001b[1;32m     91\u001b[0m y_detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_out(feat4)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Base image\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m y_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdehaze_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Combine\u001b[39;00m\n\u001b[1;32m     97\u001b[0m y_lr \u001b[38;5;241m=\u001b[39m y_base \u001b[38;5;241m+\u001b[39m y_detail\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 150\u001b[0m, in \u001b[0;36mDehazingTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m original_height, original_width \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    148\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjust_image_size(x)\n\u001b[0;32m--> 150\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m transmission_map, atmospheric_light \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(features, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Dehazing formula: I = J * t + A * (1 - t)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 130\u001b[0m, in \u001b[0;36mDehazingTransformer.extract_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m skip1 \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    129\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample1(x)\n\u001b[0;32m--> 130\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_stage2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m skip2 \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    133\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample2(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 87\u001b[0m, in \u001b[0;36mTransformerStage.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03mForward pass through the transformer stage.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 87\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m, in \u001b[0;36mVisionTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_attention:\n\u001b[1;32m     27\u001b[0m     x, rescale, rebias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_norm(x)\n\u001b[0;32m---> 28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_attention:\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m rescale \u001b[38;5;241m+\u001b[39m rebias\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 80\u001b[0m, in \u001b[0;36mAdaptiveAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m qkv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([qk_proj, v_proj], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Apply padding for shifted window processing\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m padded_qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_for_window_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshift_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m padded_height, padded_width \u001b[38;5;241m=\u001b[39m padded_qkv\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Partition into windows\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 65\u001b[0m, in \u001b[0;36mAdaptiveAttention.pad_for_window_processing\u001b[0;34m(self, x, shift)\u001b[0m\n\u001b[1;32m     62\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift_size, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift_size \u001b[38;5;241m+\u001b[39m pad_w) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size,\n\u001b[1;32m     63\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift_size, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift_size \u001b[38;5;241m+\u001b[39m pad_h) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_h\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:5096\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   5089\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   5090\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[1;32m   5091\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   5092\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   5093\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\n\u001b[1;32m   5094\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5095\u001b[0m             )\u001b[38;5;241m.\u001b[39m_replication_pad(\u001b[38;5;28minput\u001b[39m, pad)\n\u001b[0;32m-> 5096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (0, 4) at dimension 3 of input [2, 144, 4, 4]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    psnr_list = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch, category=category)\n",
    "    net.train()\n",
    "\n",
    "    for batch_id, (haze, gt) in enumerate(train_data_loader):\n",
    "        haze, gt = haze.to(device), gt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass - Student\n",
    "        dehaze, base, s = net(haze)\n",
    "        s1, s2, s3, s4 = s\n",
    "\n",
    "        # Losses\n",
    "        base_loss = F.smooth_l1_loss(base, gt)\n",
    "        smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "        perceptual_loss = loss_network(dehaze, gt)\n",
    "        total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "        \n",
    "        # --- SR Training --- #\n",
    "        if sr_enabled:\n",
    "            try:\n",
    "                sr_lr, sr_hr = next(sr_iter)\n",
    "            except StopIteration:\n",
    "                sr_iter = iter(sr_train_loader)\n",
    "                sr_lr, sr_hr = next(sr_iter)\n",
    "            sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n",
    "            # sr_out, _, _ = net(sr_lr)\n",
    "            sr_out, _, t = teacher_net(sr_lr)\n",
    "            t1, t2, t3, t4 = t\n",
    "            sr_loss = F.l1_loss(sr_out, sr_hr)\n",
    "            total_loss += sr_loss\n",
    "            s_loss = ssfm_loss(s, t)\n",
    "            total_loss += s_loss\n",
    "            distillation_loss = fam(dehaze, sr_out)\n",
    "            total_loss += distillation_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        psnr_list.extend(to_psnr(dehaze, gt))\n",
    "\n",
    "        if batch_id % num_epochs == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Iteration [{batch_id}]\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if epoch % 5 == 0:\n",
    "        iter_model_path = f\"net_haze_iter_{epoch}.pth\"\n",
    "        torch.save(net.state_dict(), iter_model_path)\n",
    "        print(f\"Model - net saved in epoch {epoch}.\")\n",
    "        if sr_enabled:\n",
    "            iter_model_path = f\"teacher_net_haze_iter_{epoch}.pth\"\n",
    "            torch.save(teacher_net.state_dict(), iter_model_path)\n",
    "            print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "    train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "    # --- Validation --- #\n",
    "    net.eval()\n",
    "    val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "    if sr_enabled:\n",
    "        sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "        print(f\"[SR Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    model_path = f\"net_haze_{version}.pth\"\n",
    "    print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "    if train_psnr < train_psnr_prev:\n",
    "        adjust_learning_rate(optimizer, num_epochs, category=category)\n",
    "\n",
    "    if val_psnr >= best_psnr:\n",
    "        best_model_path = f\"net_haze_best_{version}.pth\"\n",
    "        torch.save(net.state_dict(), best_model_path)\n",
    "        best_psnr = val_psnr\n",
    "\n",
    "    train_psnr_prev = train_psnr\n",
    "\n",
    "# Final save\n",
    "final_path = f\"net_final_{epoch}.pth\"\n",
    "torch.save(net.state_dict(), final_path)\n",
    "if sr_enabled:\n",
    "    final_path = f\"teacher_net_final_{epoch}.pth\"\n",
    "    torch.save(teacher_net.state_dict(), final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.206947Z",
     "iopub.status.idle": "2025-04-21T16:01:55.207230Z",
     "shell.execute_reply": "2025-04-21T16:01:55.207122Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.207107Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "# model_path = \"/kaggle/input/rdb-and-transformer/pytorch/default/1/formernewnh_final_49.pth\"\n",
    "model_path = \"/kaggle/input/reside-dehaze/pytorch/default/3/formernewreside_haze_best_0.pth\"\n",
    "# model = DehazingNet().to(device)\n",
    "# model = SR_model(upscale_factor=1).to(device)\n",
    "# net.load_state_dict(torch.load(model_path, map_location=device))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.208619Z",
     "iopub.status.idle": "2025-04-21T16:01:55.209247Z",
     "shell.execute_reply": "2025-04-21T16:01:55.209064Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.209046Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# LOAD TEST DATA\n",
    "# -----------------------------\n",
    "test_hazy_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/hazy\"\n",
    "test_gt_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/GT\"\n",
    "# test_hazy_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN\"\n",
    "# test_gt_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT\"\n",
    "\n",
    "hazy_images = sorted(glob.glob(os.path.join(test_hazy_dir, \"*.*\")))\n",
    "gt_images = sorted(glob.glob(os.path.join(test_gt_dir, \"*.*\")))\n",
    "\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "to_pil = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.210801Z",
     "iopub.status.idle": "2025-04-21T16:01:55.211425Z",
     "shell.execute_reply": "2025-04-21T16:01:55.211229Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.211212Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# -----------------------------\n",
    "image_indices = [11, 12, 13,14]  # Indices of images to visualize\n",
    "\n",
    "plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "for idx, i in enumerate(image_indices):\n",
    "    hazy_img = Image.open(hazy_images[i+1])\n",
    "    gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "    # Transform for model input\n",
    "    input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        res = net(input_tensor)\n",
    "        print(res[0].shape)\n",
    "        output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "    # Convert back to image\n",
    "    output_img = to_pil(output_tensor)\n",
    "\n",
    "    # Display results\n",
    "    plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "    plt.imshow(hazy_img)\n",
    "    plt.title(f\"Hazy Input \")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "    plt.imshow(output_img)\n",
    "    plt.title(f\"Dehazed Output \")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "    plt.imshow(gt_img)\n",
    "    plt.title(f\"Ground Truth \")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.212728Z",
     "iopub.status.idle": "2025-04-21T16:01:55.213012Z",
     "shell.execute_reply": "2025-04-21T16:01:55.212903Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.212889Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# -----------------------------\n",
    "image_indices = [70, 75, 90, 100]  # Indices of images to visualize\n",
    "\n",
    "plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "for idx, i in enumerate(image_indices):\n",
    "    hazy_img = Image.open(hazy_images[i+1])\n",
    "    gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "    # Transform for model input\n",
    "    input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        res = net(input_tensor)\n",
    "        print(res[0].shape)\n",
    "        output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "    # Convert back to image\n",
    "    output_img = to_pil(output_tensor)\n",
    "\n",
    "    # Display results\n",
    "    plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "    plt.imshow(hazy_img)\n",
    "    plt.title(f\"Hazy Input {i}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "    plt.imshow(output_img)\n",
    "    plt.title(f\"Dehazed Output {i}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "    plt.imshow(gt_img)\n",
    "    plt.title(f\"Ground Truth {i}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.214998Z",
     "iopub.status.idle": "2025-04-21T16:01:55.215653Z",
     "shell.execute_reply": "2025-04-21T16:01:55.215495Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.215481Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# -----------------------------\n",
    "image_indices = [1, 3, 5]  # Indices of images to visualize\n",
    "\n",
    "plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "for idx, i in enumerate(image_indices):\n",
    "    hazy_img = Image.open(hazy_images[i+1])\n",
    "    gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "    # Transform for model input\n",
    "    input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        res = net(input_tensor)\n",
    "        print(res[0].shape)\n",
    "        output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "    # Convert back to image\n",
    "    output_img = to_pil(output_tensor)\n",
    "\n",
    "    # Display results\n",
    "    plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "    plt.imshow(hazy_img)\n",
    "    plt.title(f\"Hazy Input {i}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "    plt.imshow(output_img)\n",
    "    plt.title(f\"Dehazed Output {i}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "    plt.imshow(gt_img)\n",
    "    plt.title(f\"Ground Truth {i}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 937211,
     "sourceId": 1587463,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2813430,
     "sourceId": 4853613,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6456606,
     "sourceId": 10417877,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6464114,
     "sourceId": 10443410,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 222875724,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 268224,
     "modelInstanceId": 246650,
     "sourceId": 287852,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 288973,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 297672,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 299643,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-12T13:09:18.306161",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
