{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:01.883179Z",
     "iopub.status.busy": "2025-04-21T16:01:01.882223Z",
     "iopub.status.idle": "2025-04-21T16:01:16.592457Z",
     "shell.execute_reply": "2025-04-21T16:01:16.591461Z",
     "shell.execute_reply.started": "2025-04-21T16:01:01.883139Z"
    },
    "papermill": {
     "duration": 11.276571,
     "end_time": "2025-04-12T13:09:32.125878",
     "exception": false,
     "start_time": "2025-04-12T13:09:20.849307",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "from timm.layers import to_2tuple, trunc_normal_\n",
    "import os\n",
    "import torchvision.utils as utils\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from random import randrange\n",
    "import time\n",
    "from math import log10\n",
    "from skimage import measure\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.nn.init import trunc_normal_\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Device Setup --- #\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_ids = list(range(torch.cuda.device_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.593951Z",
     "iopub.status.busy": "2025-04-21T16:01:16.593512Z",
     "iopub.status.idle": "2025-04-21T16:01:16.598530Z",
     "shell.execute_reply": "2025-04-21T16:01:16.597465Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.593926Z"
    },
    "papermill": {
     "duration": 0.015315,
     "end_time": "2025-04-12T13:09:32.152405",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.137090",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009983,
     "end_time": "2025-04-12T13:09:32.172516",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.162533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RevisedLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.601109Z",
     "iopub.status.busy": "2025-04-21T16:01:16.600708Z",
     "iopub.status.idle": "2025-04-21T16:01:16.719788Z",
     "shell.execute_reply": "2025-04-21T16:01:16.718584Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.601080Z"
    },
    "papermill": {
     "duration": 0.01803,
     "end_time": "2025-04-12T13:09:32.200726",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.182696",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RevisedLayerNorm(nn.Module):\n",
    "    \"\"\"Revised LayerNorm\"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n",
    "        super(RevisedLayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.detach_gradient = detach_gradient\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n",
    "        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n",
    "\n",
    "        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "\n",
    "        trunc_normal_(self.scale_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.scale_mlp.bias, 1)\n",
    "\n",
    "        trunc_normal_(self.shift_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.shift_mlp.bias, 0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n",
    "        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n",
    "\n",
    "        normalized_tensor = (input_tensor - mean_value) / std_value\n",
    "\n",
    "        if self.detach_gradient:\n",
    "            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n",
    "        else:\n",
    "            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n",
    "\n",
    "        output = normalized_tensor * self.scale + self.shift\n",
    "        return output, rescale, rebias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.720946Z",
     "iopub.status.busy": "2025-04-21T16:01:16.720710Z",
     "iopub.status.idle": "2025-04-21T16:01:16.744866Z",
     "shell.execute_reply": "2025-04-21T16:01:16.743982Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.720927Z"
    },
    "papermill": {
     "duration": 0.019569,
     "end_time": "2025-04-12T13:09:32.230395",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.210826",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n",
    "        super().__init__()\n",
    "        output_channels = output_channels or input_channels\n",
    "        hidden_channels = hidden_channels or input_channels\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            gain = (8 * self.depth) ** (-1 / 4)\n",
    "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "            trunc_normal_(layer.weight, std=std)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "\n",
    "def partition_into_windows(tensor, window_size):\n",
    "    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n",
    "    batch_size, height, width, channels = tensor.shape\n",
    "    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n",
    "\n",
    "    tensor = tensor.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n",
    "    )\n",
    "    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, height, width):\n",
    "    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n",
    "    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n",
    "    tensor = windows.view(\n",
    "        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010551,
     "end_time": "2025-04-12T13:09:32.251201",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.240650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.746699Z",
     "iopub.status.busy": "2025-04-21T16:01:16.746311Z",
     "iopub.status.idle": "2025-04-21T16:01:16.935555Z",
     "shell.execute_reply": "2025-04-21T16:01:16.934584Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.746668Z"
    },
    "papermill": {
     "duration": 0.141806,
     "end_time": "2025-04-12T13:09:32.403159",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.261353",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 16, 16]),\n",
       " torch.Size([32, 16, 64]),\n",
       " torch.Size([2, 16, 16, 64]),\n",
       " True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize the MultiLayerPerceptron with sample parameters\n",
    "depth = 4\n",
    "input_channels = 64\n",
    "hidden_channels = 128\n",
    "output_channels = 64\n",
    "\n",
    "mlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n",
    "\n",
    "# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\n",
    "input_tensor = torch.randn(2, 64, 16, 16)\n",
    "output_tensor = mlp(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "mlp_output_shape = output_tensor.shape\n",
    "\n",
    "# Test window partition and merging\n",
    "batch_size, height, width, channels = 2, 16, 16, 64\n",
    "window_size = 4\n",
    "\n",
    "# Create a random tensor for window functions (B, H, W, C) format\n",
    "input_window_tensor = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Apply partitioning and merging\n",
    "windows = partition_into_windows(input_window_tensor, window_size)\n",
    "reconstructed_tensor = merge_windows(windows, window_size, height, width)\n",
    "\n",
    "# Check shapes\n",
    "windows_shape = windows.shape\n",
    "reconstructed_shape = reconstructed_tensor.shape\n",
    "\n",
    "# Validate if the reconstruction matches the original input shape\n",
    "is_shape_correct = reconstructed_shape == input_window_tensor.shape\n",
    "\n",
    "# Output results\n",
    "mlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LocalWindowAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.937244Z",
     "iopub.status.busy": "2025-04-21T16:01:16.936980Z",
     "iopub.status.idle": "2025-04-21T16:01:16.947402Z",
     "shell.execute_reply": "2025-04-21T16:01:16.946296Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.937222Z"
    },
    "papermill": {
     "duration": 0.019079,
     "end_time": "2025-04-12T13:09:32.432815",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.413736",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, window_size, num_heads):\n",
    "        \"\"\"Self-attention mechanism within local windows.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # (height, width)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n",
    "\n",
    "        # Compute and store relative positional encodings\n",
    "        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n",
    "        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n",
    "\n",
    "        # Learnable transformation of relative position embeddings\n",
    "        self.relative_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_heads, bias=True)\n",
    "        )\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n",
    "        batch_size, num_tokens, _ = qkv.shape\n",
    "\n",
    "        # Reshape qkv into separate query, key, and value tensors\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n",
    "\n",
    "        # Scale query for stable attention computation\n",
    "        q = q * self.scaling_factor\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Compute relative position bias\n",
    "        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n",
    "        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n",
    "        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n",
    "\n",
    "        # Apply softmax and compute weighted values\n",
    "        attention_weights = self.attention_softmax(attention_scores)\n",
    "        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute_log_relative_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.948815Z",
     "iopub.status.busy": "2025-04-21T16:01:16.948476Z",
     "iopub.status.idle": "2025-04-21T16:01:16.974830Z",
     "shell.execute_reply": "2025-04-21T16:01:16.973956Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.948785Z"
    },
    "papermill": {
     "duration": 0.015719,
     "end_time": "2025-04-12T13:09:32.458806",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.443087",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_log_relative_positions(window_size):\n",
    "    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n",
    "    coord_range = torch.arange(window_size)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n",
    "\n",
    "    # Compute relative positions\n",
    "    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n",
    "\n",
    "    # Format and apply log transformation\n",
    "    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n",
    "    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "    return log_relative_positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaptiveAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.976208Z",
     "iopub.status.busy": "2025-04-21T16:01:16.975861Z",
     "iopub.status.idle": "2025-04-21T16:01:17.000363Z",
     "shell.execute_reply": "2025-04-21T16:01:16.999498Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.976184Z"
    },
    "papermill": {
     "duration": 0.025485,
     "end_time": "2025-04-12T13:09:32.494578",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.469093",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n",
    "        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.network_depth = network_depth\n",
    "        self.enable_attention = enable_attention\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "        # Define convolutional processing based on mode\n",
    "        if self.conv_mode == 'Conv':\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "            )\n",
    "\n",
    "        if self.conv_mode == 'DWConv':\n",
    "            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n",
    "            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight_shape = module.weight.shape\n",
    "\n",
    "            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "            else:\n",
    "                gain = (8 * self.network_depth) ** (-1/4)\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def pad_for_window_processing(self, x, shift=False):\n",
    "        \"\"\"Pads the input tensor to fit window processing requirements.\"\"\"\n",
    "        _, _, height, width = x.size()\n",
    "        pad_h = (self.window_size - height % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - width % self.window_size) % self.window_size\n",
    "\n",
    "        if shift:\n",
    "            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n",
    "                          self.shift_size, (self.window_size - self.shift_size + pad_h) % self.window_size), mode='reflect')\n",
    "        else:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            v_proj = self.value_projection(x)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            qk_proj = self.query_key_projection(x)\n",
    "            qkv = torch.cat([qk_proj, v_proj], dim=1)\n",
    "\n",
    "            # Apply padding for shifted window processing\n",
    "            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n",
    "            padded_height, padded_width = padded_qkv.shape[2:]\n",
    "\n",
    "            # Partition into windows\n",
    "            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n",
    "            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n",
    "\n",
    "            # Apply window-based attention\n",
    "            attn_windows = self.window_attention(qkv_windows)\n",
    "\n",
    "            # Merge back to original spatial dimensions\n",
    "            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n",
    "\n",
    "            # Reverse the cyclic shift\n",
    "            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), self.shift_size:(self.shift_size + width), :]\n",
    "            attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.conv_mode in ['Conv', 'DWConv']:\n",
    "                conv_output = self.conv_layer(v_proj)\n",
    "                output = self.output_projection(conv_output + attn_output)\n",
    "            else:\n",
    "                output = self.output_projection(attn_output)\n",
    "\n",
    "        else:\n",
    "            if self.conv_mode == 'Conv':\n",
    "                output = self.conv_layer(x)  # No attention, using convolution only\n",
    "            elif self.conv_mode == 'DWConv':\n",
    "                output = self.output_projection(self.conv_layer(v_proj))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisionTransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.003995Z",
     "iopub.status.busy": "2025-04-21T16:01:17.003674Z",
     "iopub.status.idle": "2025-04-21T16:01:17.024865Z",
     "shell.execute_reply": "2025-04-21T16:01:17.023832Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.003973Z"
    },
    "papermill": {
     "duration": 0.020653,
     "end_time": "2025-04-12T13:09:32.525353",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.504700",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n",
    "                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n",
    "        \"\"\"\n",
    "        A transformer block that includes attention (optional) and MLP layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enable_attention = enable_attention\n",
    "        self.enable_mlp_norm = enable_mlp_norm\n",
    "\n",
    "        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n",
    "        self.attention_layer = AdaptiveAttention(\n",
    "            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n",
    "            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n",
    "        )\n",
    "\n",
    "        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n",
    "        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.enable_attention:\n",
    "            x, rescale, rebias = self.pre_norm(x)\n",
    "        x = self.attention_layer(x)\n",
    "        if self.enable_attention:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        residual = x\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x, rescale, rebias = self.post_norm(x)\n",
    "        x = self.mlp_layer(x)\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchEmbedding and PatchReconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.026567Z",
     "iopub.status.busy": "2025-04-21T16:01:17.026179Z",
     "iopub.status.idle": "2025-04-21T16:01:17.055499Z",
     "shell.execute_reply": "2025-04-21T16:01:17.054636Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.026462Z"
    },
    "papermill": {
     "duration": 0.017283,
     "end_time": "2025-04-12T13:09:32.553948",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.536665",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch embedding module that projects input images into token embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to generate patch embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class PatchReconstruction(nn.Module):\n",
    "    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch reconstruction module that converts token embeddings back to image patches.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2, padding_mode='reflect'\n",
    "            ),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to reconstruct image from embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectiveKernelFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.056678Z",
     "iopub.status.busy": "2025-04-21T16:01:17.056406Z",
     "iopub.status.idle": "2025-04-21T16:01:17.078704Z",
     "shell.execute_reply": "2025-04-21T16:01:17.077764Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.056658Z"
    },
    "papermill": {
     "duration": 0.017407,
     "end_time": "2025-04-12T13:09:32.581526",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.564119",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SelectiveKernelFusion(nn.Module):\n",
    "    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n",
    "        \"\"\"\n",
    "        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n",
    "\n",
    "        Args:\n",
    "            channels (int): Number of input channels.\n",
    "            num_branches (int): Number of feature branches to fuse.\n",
    "            reduction_ratio (int): Reduction ratio for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_branches = num_branches\n",
    "        reduced_channels = max(int(channels / reduction_ratio), 4)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Forward pass for selective kernel fusion.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (list of tensors): A list of feature maps to be fused.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adaptively fused feature map.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = feature_maps[0].shape\n",
    "        \n",
    "        # Concatenate feature maps along a new dimension (num_branches)\n",
    "        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        aggregated_features = torch.sum(stacked_features, dim=1)\n",
    "        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n",
    "        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n",
    "\n",
    "        # Weighted sum of input feature maps\n",
    "        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerStage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, window_size=8,\n",
    "                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n",
    "        \"\"\"\n",
    "        A stage of transformer blocks with configurable attention placement.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        attention_layers = int(attention_ratio * num_layers)\n",
    "\n",
    "        if attention_placement == 'last':\n",
    "            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'first':\n",
    "            enable_attentions = [i < attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'middle':\n",
    "            enable_attentions = [\n",
    "                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(\n",
    "                network_depth=network_depth,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                enable_attention=enable_attentions[i],\n",
    "                conv_mode=conv_mode\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer stage.\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DehazingTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.080101Z",
     "iopub.status.busy": "2025-04-21T16:01:17.079760Z",
     "iopub.status.idle": "2025-04-21T16:01:17.103347Z",
     "shell.execute_reply": "2025-04-21T16:01:17.102401Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.080073Z"
    },
    "papermill": {
     "duration": 0.025553,
     "end_time": "2025-04-12T13:09:32.617120",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.591567",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DehazingTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "                 layer_depths=[16, 16, 16, 8, 8],\n",
    "                 num_heads=[2, 4, 6, 1, 1],\n",
    "                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding settings\n",
    "        self.patch_size = 4\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # Backbone layers\n",
    "        self.encoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[0],\n",
    "            num_layers=layer_depths[0],\n",
    "            num_heads=num_heads[0],\n",
    "            mlp_ratio=mlp_ratios[0],\n",
    "            norm_layer=norm_layers[0],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[0],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[0]\n",
    "        )\n",
    "        \n",
    "        self.downsample1 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "        \n",
    "        self.encoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[1],\n",
    "            num_layers=layer_depths[1],\n",
    "            num_heads=num_heads[1],\n",
    "            mlp_ratio=mlp_ratios[1],\n",
    "            norm_layer=norm_layers[1],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[1],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[1]\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "        \n",
    "        self.encoder_stage3 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[2],\n",
    "            num_layers=layer_depths[2],\n",
    "            num_heads=num_heads[2],\n",
    "            mlp_ratio=mlp_ratios[2],\n",
    "            norm_layer=norm_layers[2],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[2],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[2]\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n",
    "        \n",
    "        self.decoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[3],\n",
    "            num_layers=layer_depths[3],\n",
    "            num_heads=num_heads[3],\n",
    "            mlp_ratio=mlp_ratios[3],\n",
    "            norm_layer=norm_layers[3],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[3],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[3]\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n",
    "        \n",
    "        self.decoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[4],\n",
    "            num_layers=layer_depths[4],\n",
    "            num_heads=num_heads[4],\n",
    "            mlp_ratio=mlp_ratios[4],\n",
    "            norm_layer=norm_layers[4],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[4],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[4]\n",
    "        )\n",
    "\n",
    "        # Final patch reconstruction\n",
    "        self.patch_reconstruction = PatchReconstruction(\n",
    "            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def adjust_image_size(self, x):\n",
    "        # Ensures the input image size is compatible with the patch size\n",
    "        _, _, height, width = x.size()\n",
    "        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n",
    "        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder_stage1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.downsample1(x)\n",
    "        x = self.encoder_stage2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.downsample2(x)\n",
    "        x = self.encoder_stage3(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n",
    "        x = self.decoder_stage1(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n",
    "        x = self.decoder_stage2(x)\n",
    "        x = self.patch_reconstruction(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_height, original_width = x.shape[2:]\n",
    "        x = self.adjust_image_size(x)\n",
    "\n",
    "        features = self.extract_features(x)\n",
    "        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n",
    "\n",
    "        # Dehazing formula: I = J * t + A * (1 - t)\n",
    "        x = transmission_map * x - atmospheric_light + x\n",
    "        x = x[:, :, :original_height, :original_width]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build_dehazing_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.104568Z",
     "iopub.status.busy": "2025-04-21T16:01:17.104291Z",
     "iopub.status.idle": "2025-04-21T16:01:17.126572Z",
     "shell.execute_reply": "2025-04-21T16:01:17.125559Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.104548Z"
    },
    "papermill": {
     "duration": 0.015434,
     "end_time": "2025-04-12T13:09:32.642796",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.627362",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_dehazing_transformer():\n",
    "    return DehazingTransformer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "        layer_depths=[12, 12, 12, 6, 6],\n",
    "        num_heads=[2, 4, 6, 1, 1],\n",
    "        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvolutionalGuidedFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.127839Z",
     "iopub.status.busy": "2025-04-21T16:01:17.127597Z",
     "iopub.status.idle": "2025-04-21T16:01:17.152140Z",
     "shell.execute_reply": "2025-04-21T16:01:17.151236Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.127821Z"
    },
    "papermill": {
     "duration": 0.018977,
     "end_time": "2025-04-12T13:09:32.671995",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.653018",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvolutionalGuidedFilter(nn.Module):\n",
    "    def __init__(self, radius=1, norm_layer=nn.BatchNorm2d, conv_kernel_size: int = 1):\n",
    "        super(ConvolutionalGuidedFilter, self).__init__()\n",
    "\n",
    "        self.box_filter = nn.Conv2d(\n",
    "            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n",
    "        )\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                6,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                3,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.box_filter.weight.data[...] = 1.0\n",
    "\n",
    "    def forward(self, x_low_res, y_low_res, x_high_res):\n",
    "        _, _, h_lr, w_lr = x_low_res.size()\n",
    "        _, _, h_hr, w_hr = x_high_res.size()\n",
    "\n",
    "        N = self.box_filter(x_low_res.data.new().resize_((1, 3, h_lr, w_lr)).fill_(1.0))\n",
    "        ## mean_x\n",
    "        mean_x = self.box_filter(x_low_res) / N\n",
    "        ## mean_y\n",
    "        mean_y = self.box_filter(y_low_res) / N\n",
    "        ## cov_xy\n",
    "        cov_xy = self.box_filter(x_low_res * y_low_res) / N - mean_x * mean_y\n",
    "        ## var_x\n",
    "        var_x = self.box_filter(x_low_res * x_low_res) / N - mean_x * mean_x\n",
    "\n",
    "        ## A\n",
    "        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n",
    "        ## b\n",
    "        b = mean_y - A * mean_x\n",
    "\n",
    "        ## mean_A; mean_b\n",
    "        mean_A = F.interpolate(A, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "        mean_b = F.interpolate(b, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return mean_A * x_high_res + mean_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PixelAttentionLayer and ChannelAttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.153291Z",
     "iopub.status.busy": "2025-04-21T16:01:17.153032Z",
     "iopub.status.idle": "2025-04-21T16:01:17.175609Z",
     "shell.execute_reply": "2025-04-21T16:01:17.174643Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.153270Z"
    },
    "papermill": {
     "duration": 0.016895,
     "end_time": "2025-04-12T13:09:32.699032",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.682137",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PixelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(PixelAttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, 1, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_map = self.attention(x)\n",
    "        return x * attention_map\n",
    "\n",
    "class ChannelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelAttentionLayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, channels, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = self.avg_pool(x)\n",
    "        attention_map = self.attention(pooled)\n",
    "        return x * attention_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HybridResidualDenseBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.176736Z",
     "iopub.status.busy": "2025-04-21T16:01:17.176483Z",
     "iopub.status.idle": "2025-04-21T16:01:17.194108Z",
     "shell.execute_reply": "2025-04-21T16:01:17.193205Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.176717Z"
    },
    "papermill": {
     "duration": 0.018495,
     "end_time": "2025-04-12T13:09:32.727852",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.709357",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HybridResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_dense_layers=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.growth_rate = growth_rate\n",
    "        self.in_channels = in_channels\n",
    "        total_channels = in_channels\n",
    "\n",
    "        for i in range(num_dense_layers):\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(total_channels, growth_rate, kernel_size=3, padding=2**i, dilation=2**i)\n",
    "            )\n",
    "            total_channels += growth_rate\n",
    "\n",
    "        self.fusion = nn.Conv2d(total_channels, in_channels, kernel_size=1)\n",
    "        self.channel_attention = ChannelAttentionLayer(in_channels)\n",
    "        self.pixel_attention = PixelAttentionLayer(in_channels)\n",
    "\n",
    "        # Gated residual fusion\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for conv in self.layers:\n",
    "            out = F.relu(conv(torch.cat(features, dim=1)))\n",
    "            features.append(out)\n",
    "\n",
    "        dense_out = torch.cat(features, dim=1)\n",
    "        fused = self.fusion(dense_out)\n",
    "        ca = self.channel_attention(fused)\n",
    "        pa = self.pixel_attention(ca)\n",
    "\n",
    "        gate_input = torch.cat([x, pa], dim=1)\n",
    "        gated_fusion = self.gate(gate_input)\n",
    "        return x * (1 - gated_fusion) + pa * gated_fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaptiveInstanceNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.195300Z",
     "iopub.status.busy": "2025-04-21T16:01:17.195064Z",
     "iopub.status.idle": "2025-04-21T16:01:17.218308Z",
     "shell.execute_reply": "2025-04-21T16:01:17.217401Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.195283Z"
    },
    "papermill": {
     "duration": 0.015456,
     "end_time": "2025-04-12T13:09:32.753293",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.737837",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNormalization(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(AdaptiveInstanceNormalization, self).__init__()\n",
    "\n",
    "        # Learnable scaling factors\n",
    "        self.scale_x = nn.Parameter(torch.tensor(1.0))  # Identity scaling\n",
    "        self.scale_norm = nn.Parameter(torch.tensor(0.0))  # Initially no effect\n",
    "\n",
    "        # Instance normalization layer with affine transformation enabled\n",
    "        self.instance_norm = nn.InstanceNorm2d(num_channels, momentum=0.999, eps=0.001, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized_x = self.instance_norm(x)\n",
    "        return self.scale_x * x + self.scale_norm * normalized_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEPGUIDEDNETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.219705Z",
     "iopub.status.busy": "2025-04-21T16:01:17.219346Z",
     "iopub.status.idle": "2025-04-21T16:01:17.243565Z",
     "shell.execute_reply": "2025-04-21T16:01:17.242600Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.219677Z"
    },
    "papermill": {
     "duration": 0.01754,
     "end_time": "2025-04-12T13:09:32.780956",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.763416",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepGuidedNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=3, radius=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        norm = AdaptiveInstanceNormalization  # define this separately\n",
    "        kernel_size = 3\n",
    "        depth_rate = 64\n",
    "        growth_rate = 16\n",
    "        num_dense_layer = 4\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "        self.rdb1 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb2 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb3 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb4 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(depth_rate, 256, 3, padding=1),       # 64 → 256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, in_channels * 16, 3, padding=1), # 256 → 48 for 3 × 4²\n",
    "            nn.PixelShuffle(4)                              # 48 → 3, upsample ×4\n",
    "        )\n",
    "\n",
    "    def forward(self, x, sr= False):\n",
    "        x_in = self.conv_in(x)\n",
    "        feat1 = self.rdb1(x_in)\n",
    "        feat2 = self.rdb2(feat1)\n",
    "        feat3 = self.rdb3(feat2)\n",
    "        feat4 = self.rdb4(feat3)\n",
    "        out_feat = self.conv_out(feat4)\n",
    "\n",
    "        sr = self.tail(out_feat)  # shape: (B, 3, H×4, W×4)\n",
    "        return sr, sr, [feat1, feat2, feat3, feat4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3, 256, 256])\n",
      "Feature shapes: [torch.Size([1, 64, 64, 64]), torch.Size([1, 64, 64, 64]), torch.Size([1, 64, 64, 64]), torch.Size([1, 64, 64, 64])]\n"
     ]
    }
   ],
   "source": [
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, 3, 64, 64)  # Batch size of 1, 3 channels, 256x256 image\n",
    "\n",
    "# Initialize the DeepGuidedNetwork\n",
    "model = DeepGuidedNetwork(radius=1)\n",
    "\n",
    "# Forward pass through the network\n",
    "output_tensor, dummy_out, features = model(input_tensor)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output_tensor.shape)\n",
    "print(\"Feature shapes:\", [f.shape for f in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3, 256, 256])\n",
      "Base HR shape: torch.Size([2, 3, 256, 256])\n",
      "Features shape: [torch.Size([2, 64, 64, 64]), torch.Size([2, 64, 64, 64]), torch.Size([2, 64, 64, 64]), torch.Size([2, 64, 64, 64])]\n"
     ]
    }
   ],
   "source": [
    "teacher_net = DeepGuidedNetwork(radius=1).to(device)\n",
    "# test teacher output shape\n",
    "input_tensor = torch.randn(2, 3, 64, 64).to(device)\n",
    "output_tensor, base_hr, features = teacher_net(input_tensor)\n",
    "# Check output shape\n",
    "output_shape = output_tensor.shape\n",
    "base_hr_shape = base_hr.shape\n",
    "features_shape = [feat.shape for feat in features]\n",
    "print(\"Output shape:\", output_shape)\n",
    "print(\"Base HR shape:\", base_hr_shape)\n",
    "print(\"Features shape:\", features_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to_psnr\n",
    "# to_psnr(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010053,
     "end_time": "2025-04-12T13:09:32.918333",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.908280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.317546Z",
     "iopub.status.busy": "2025-04-21T16:01:17.317245Z",
     "iopub.status.idle": "2025-04-21T16:01:17.339955Z",
     "shell.execute_reply": "2025-04-21T16:01:17.338827Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.317524Z"
    },
    "papermill": {
     "duration": 0.017326,
     "end_time": "2025-04-12T13:09:32.945817",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.928491",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def to_psnr(dehaze, gt):\n",
    "    \"\"\"\n",
    "    Compute PSNR (Peak Signal-to-Noise Ratio) between dehazed and ground truth images.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: PSNR values for each image in the batch.\n",
    "    \"\"\"\n",
    "    print(\"Shapes: \", dehaze.shape, gt.shape)\n",
    "    mse = F.mse_loss(dehaze, gt, reduction='none').mean(dim=[1, 2, 3])  # Compute MSE per image\n",
    "    intensity_max = 1.0\n",
    "\n",
    "    # Compute PSNR safely, avoiding division by zero and extreme values\n",
    "    psnr_list = [10.0 * log10(intensity_max / max(mse_val.item(), 1e-6)) for mse_val in mse]\n",
    "\n",
    "    return psnr_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.341295Z",
     "iopub.status.busy": "2025-04-21T16:01:17.340966Z",
     "iopub.status.idle": "2025-04-21T16:01:22.663023Z",
     "shell.execute_reply": "2025-04-21T16:01:22.661971Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.341266Z"
    },
    "papermill": {
     "duration": 2.341767,
     "end_time": "2025-04-12T13:09:35.297645",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.955878",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "# Define SSIM metric\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0, reduction='none')\n",
    "\n",
    "def to_ssim(dehaze: torch.Tensor, gt: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute SSIM directly on the GPU using torchmetrics.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: SSIM values for each image in the batch.\n",
    "    \"\"\"\n",
    "    ssim_values = ssim_metric(dehaze, gt)  # Shape: [B]\n",
    "    # print(\"1\",ssim_values)\n",
    "    # print(\"2\",[ssim_values])\n",
    "    ssim_values = ssim_values.tolist() \n",
    "    # print(type(ssim_values))\n",
    "    if isinstance(ssim_values, float):  # Correct way to check for a float\n",
    "        return [ssim_values]  # Convert single float to a list\n",
    "    return ssim_values  # Otherwise, return as is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.664561Z",
     "iopub.status.busy": "2025-04-21T16:01:22.664015Z",
     "iopub.status.idle": "2025-04-21T16:01:22.804857Z",
     "shell.execute_reply": "2025-04-21T16:01:22.804035Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.664534Z"
    },
    "papermill": {
     "duration": 0.183935,
     "end_time": "2025-04-12T13:09:35.493308",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.309373",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007388765458017588]\n"
     ]
    }
   ],
   "source": [
    "# Test with a dummy tensor\n",
    "dehaze = torch.rand(1, 3, 360, 360)  # Random batch of images\n",
    "gt = torch.rand(1, 3, 360, 360)  # Random ground truth images\n",
    "\n",
    "ssim_scores = to_ssim(dehaze, gt)\n",
    "print(ssim_scores)  # Should print a list of 6 SSIM values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Haze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.805937Z",
     "iopub.status.busy": "2025-04-21T16:01:22.805707Z",
     "iopub.status.idle": "2025-04-21T16:01:22.814610Z",
     "shell.execute_reply": "2025-04-21T16:01:22.813471Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.805921Z"
    },
    "papermill": {
     "duration": 0.018069,
     "end_time": "2025-04-12T13:09:35.522246",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.504177",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validationB(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: Your deep learning model\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: GPU/CPU device\n",
    "    :param category: dataset type (indoor/outdoor)\n",
    "    :param save_tag: whether to save images\n",
    "    :return: average PSNR & SSIM values\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    \n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            haze, gt = val_data\n",
    "            haze, gt = haze.to(device), gt.to(device)\n",
    "            dehaze, _, _ = net(haze)\n",
    "\n",
    "        # --- Compute PSNR & SSIM --- #\n",
    "        batch_psnr = to_psnr(dehaze, gt)  # This returns a list\n",
    "        # print(batch_psnr)\n",
    "        batch_ssim = to_ssim(dehaze, gt)  # This returns a list\n",
    "        # print(batch_ssim)\n",
    "\n",
    "        psnr_list.extend(batch_psnr)  # Flatten the list\n",
    "        ssim_list.extend(batch_ssim)  # Flatten the list\n",
    "\n",
    "    # --- Ensure lists are not empty to avoid division by zero --- #\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.816571Z",
     "iopub.status.busy": "2025-04-21T16:01:22.816059Z",
     "iopub.status.idle": "2025-04-21T16:01:22.840957Z",
     "shell.execute_reply": "2025-04-21T16:01:22.840120Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.816534Z"
    },
    "papermill": {
     "duration": 0.01909,
     "end_time": "2025-04-12T13:09:35.551459",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.532369",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validation_sr(net, sr_val_loader, device):\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    for lr, hr in sr_val_loader:\n",
    "        with torch.no_grad():\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr_out, _, _ = net(lr, sr = False)\n",
    "        print(\"Shapes 1: \", sr_out.shape, hr.shape)\n",
    "        psnr_list.extend(to_psnr(sr_out, hr))\n",
    "        ssim_list.extend(to_ssim(sr_out, hr))\n",
    "\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009925,
     "end_time": "2025-04-12T13:09:35.578339",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.568414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.846831Z",
     "iopub.status.busy": "2025-04-21T16:01:22.846514Z",
     "iopub.status.idle": "2025-04-21T16:01:22.862331Z",
     "shell.execute_reply": "2025-04-21T16:01:22.861073Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.846807Z"
    },
    "papermill": {
     "duration": 0.016023,
     "end_time": "2025-04-12T13:09:35.604481",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.588458",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.863635Z",
     "iopub.status.busy": "2025-04-21T16:01:22.863314Z",
     "iopub.status.idle": "2025-04-21T16:01:22.880993Z",
     "shell.execute_reply": "2025-04-21T16:01:22.880020Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.863607Z"
    },
    "papermill": {
     "duration": 0.015183,
     "end_time": "2025-04-12T13:09:35.637575",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.622392",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# psnr, ssim = validationB(net, val_data_loader, device, category)\n",
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # psnr, ssim = validationB(model, val_loader, device, \"indoor\", save_tag=True)\n",
    "# print(f\"Validation PSNR: {psnr:.2f}, SSIM: {ssim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.882823Z",
     "iopub.status.busy": "2025-04-21T16:01:22.882494Z",
     "iopub.status.idle": "2025-04-21T16:01:22.912339Z",
     "shell.execute_reply": "2025-04-21T16:01:22.911133Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.882793Z"
    },
    "papermill": {
     "duration": 0.021172,
     "end_time": "2025-04-12T13:09:35.676615",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.655443",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5114b80f91c146118824914771df0e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Execution Env:', options=('local', 'kaggle'), value='local')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execution_env_widget = widgets.Dropdown(options=['local', 'kaggle'], value='local', description='Execution Env:')\n",
    "display(execution_env_widget)\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "    execution_env_widget.value = 'kaggle' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.913804Z",
     "iopub.status.busy": "2025-04-21T16:01:22.913475Z",
     "iopub.status.idle": "2025-04-21T16:01:22.965654Z",
     "shell.execute_reply": "2025-04-21T16:01:22.964307Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.913775Z"
    },
    "papermill": {
     "duration": 0.054368,
     "end_time": "2025-04-12T13:09:35.750507",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.696139",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ae975d86624532a2edc76246977bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Learning Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c8406a185d4ce7b4860a4c29e19e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='128,128', description='Crop Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81eae8886ea34979ab03403263ac4408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Train Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f595647f9c7847109f6cf9b6f4f28b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=0, description='Version:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c7ae393a0d4d65be53a6864faaeabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='Growth Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef9eaf8096c41f39063513645b4c9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.04, description='Lambda Loss:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbff1a61385479ea1a8ff2b256b519f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Val Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c590e7bfc6b4eee8a8c1f1b037867d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', index=2, options=('indoor', 'outdoor', 'reside', 'nh'), value='reside')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters set:\n",
      "learning_rate: 0.0001\n",
      "crop_size: [128, 128]\n",
      "train_batch_size: 2\n",
      "version: 0\n",
      "growth_rate: 16\n",
      "lambda_loss: 0.04\n",
      "val_batch_size: 2\n",
      "category: reside\n",
      "execution_env: local\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Create widgets for each hyper-parameter ---\n",
    "learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n",
    "crop_size_widget = widgets.Text(value='128,128', description='Crop Size:')\n",
    "train_batch_size_widget = widgets.IntText(value=2, description='Train Batch Size:')\n",
    "version_widget = widgets.IntText(value=0, description='Version:')\n",
    "growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n",
    "lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n",
    "val_batch_size_widget = widgets.IntText(value=2, description='Val Batch Size:')\n",
    "category_widget = widgets.Dropdown(options=['indoor', 'outdoor', 'reside', 'nh'], value='reside', description='Category:')\n",
    "\n",
    "# --- Display the widgets ---\n",
    "display(\n",
    "    learning_rate_widget, crop_size_widget, train_batch_size_widget, version_widget,\n",
    "    growth_rate_widget, lambda_loss_widget, \n",
    "    val_batch_size_widget, category_widget\n",
    ")\n",
    "\n",
    "# --- Function to parse crop size ---\n",
    "def parse_crop_size(crop_size_str):\n",
    "    return [int(x) for x in crop_size_str.split(',')]\n",
    "\n",
    "# --- Assign the widget values to variables ---\n",
    "learning_rate = learning_rate_widget.value\n",
    "crop_size = parse_crop_size(crop_size_widget.value)\n",
    "train_batch_size = train_batch_size_widget.value\n",
    "version = version_widget.value\n",
    "growth_rate = growth_rate_widget.value\n",
    "lambda_loss = lambda_loss_widget.value\n",
    "val_batch_size = val_batch_size_widget.value\n",
    "category = category_widget.value\n",
    "\n",
    "execution_env = execution_env_widget.value  # Local or Kaggle\n",
    "\n",
    "\n",
    "print('\\nHyper-parameters set:')\n",
    "print(f'learning_rate: {learning_rate}')\n",
    "print(f'crop_size: {crop_size}')\n",
    "print(f'train_batch_size: {train_batch_size}')\n",
    "print(f'version: {version}')\n",
    "print(f'growth_rate: {growth_rate}')\n",
    "print(f'lambda_loss: {lambda_loss}')\n",
    "print(f'val_batch_size: {val_batch_size}')\n",
    "print(f'category: {category}')\n",
    "print(f'execution_env: {execution_env}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths Dehaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RESIDE dataset\n",
      "Using local RESIDE dataset\n",
      "\n",
      "Final dataset paths:\n",
      "Training directory: /Volumes/S/dev/project/code/Aphase/dehaze/dataset/reside_processed/kaggle/working/cropped_train/\n",
      "Validation directory: /kaggle/input/reside6k/RESIDE-6K/train\n",
      "Number of epochs: 25\n"
     ]
    }
   ],
   "source": [
    "# --- Set category-specific hyper-parameters ---\n",
    "if category == 'indoor':\n",
    "    num_epochs = 1500\n",
    "    train_data_dir = './data/train/indoor/'\n",
    "    val_data_dir = './data/test/SOTS/indoor/'\n",
    "elif category == 'outdoor':\n",
    "    num_epochs = 10\n",
    "    train_data_dir = './data/train/outdoor/'\n",
    "    val_data_dir = './data/test/SOTS/outdoor/'\n",
    "elif category == 'reside':\n",
    "    print('Using RESIDE dataset')\n",
    "    num_epochs = 25\n",
    "    # train_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n",
    "    train_data_dir = '/kaggle/input/reside-processed/kaggle/working/cropped_train'\n",
    "    val_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n",
    "    test_data_dir = '/kaggle/input/reside6k/RESIDE-6K/test'\n",
    "    if execution_env == 'local':\n",
    "        print('Using local RESIDE dataset')\n",
    "        train_data_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/reside_processed/kaggle/working/cropped_train/'\n",
    "elif category == 'nh':\n",
    "    num_epochs = 50\n",
    "    train_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "    val_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "else:\n",
    "    raise Exception('Wrong image category. Set it to indoor or outdoor for RESIDE dataset.')\n",
    "\n",
    "# --- Adjust paths based on execution environment ---\n",
    "# if execution_env == 'kaggle':\n",
    "    # train_data_dir = '/kaggle/input/reside-dataset/' + train_data_dir.strip('./')\n",
    "    # val_data_dir = '/kaggle/input/reside-dataset/' + val_data_dir.strip('./')\n",
    "    # train_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T'\n",
    "    # val_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V' \n",
    "    # train_data_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "    # val_data_dir = '/kaggle/input/o-haze/O-HAZY/GT' \n",
    "print('\\nFinal dataset paths:')\n",
    "print(f'Training directory: {train_data_dir}')\n",
    "print(f'Validation directory: {val_data_dir}')\n",
    "print(f'Number of epochs: {num_epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.967040Z",
     "iopub.status.busy": "2025-04-21T16:01:22.966747Z",
     "iopub.status.idle": "2025-04-21T16:01:22.972795Z",
     "shell.execute_reply": "2025-04-21T16:01:22.971698Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.967012Z"
    },
    "papermill": {
     "duration": 0.017958,
     "end_time": "2025-04-12T13:09:35.817103",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.799145",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# hazeeffected_images_dir_train = f\"{train_data_dir}/IN\"\n",
    "hazeeffected_images_dir_train = f\"{train_data_dir}/hazy\"\n",
    "hazefree_images_dir_train = f\"{train_data_dir}/GT\"\n",
    "\n",
    "# hazeeffected_images_dir_valid = f\"{val_data_dir}/IN\"\n",
    "hazeeffected_images_dir_valid = f\"{val_data_dir}/hazy\"\n",
    "hazefree_images_dir_valid = f\"{val_data_dir}/GT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.019692Z",
     "iopub.status.busy": "2025-04-21T16:01:23.019456Z",
     "iopub.status.idle": "2025-04-21T16:01:23.039010Z",
     "shell.execute_reply": "2025-04-21T16:01:23.037920Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.019674Z"
    },
    "papermill": {
     "duration": 0.016445,
     "end_time": "2025-04-12T13:09:35.923418",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.906973",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # hazeeffected_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "# # hazefree_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "# # hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "# # hazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/GT'\n",
    "\n",
    "# hazeeffected_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN'\n",
    "# hazefree_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT'\n",
    "# hazeeffected_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN'\n",
    "# hazefree_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/GT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_enabled = True\n",
    "    # sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n",
    "    # sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X4'\n",
    "if sr_enabled:\n",
    "    sr_hr_dir = '/kaggle/input/sr-flickr/kaggle/working/filtered_HR'\n",
    "    sr_lr_dir = '/kaggle/input/sr-flickr/kaggle/working/filtered_LR/X4'\n",
    "    # sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n",
    "    # sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X4'\n",
    "    if execution_env == 'local':\n",
    "        sr_hr_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/SR_flickr/kaggle/working/filtered_HR'\n",
    "        sr_lr_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/SR_flickr/kaggle/working/filtered_LR/X4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.040582Z",
     "iopub.status.busy": "2025-04-21T16:01:23.040208Z",
     "iopub.status.idle": "2025-04-21T16:01:23.062332Z",
     "shell.execute_reply": "2025-04-21T16:01:23.061417Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.040550Z"
    },
    "papermill": {
     "duration": 0.019086,
     "end_time": "2025-04-12T13:09:35.953847",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.934761",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_log(epoch, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, category):\n",
    "    log_dir = \"./training_log\"\n",
    "    os.makedirs(log_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    log_path = os.path.join(log_dir, f\"{category}_log.txt\")\n",
    "\n",
    "    print('({0:.0f}s) Epoch [{1}/{2}], Train_PSNR:{3:.2f}, Val_PSNR:{4:.2f}, Val_SSIM:{5:.4f}'\n",
    "          .format(one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim))\n",
    "\n",
    "    # --- Write the training log --- #\n",
    "    with open(log_path, 'a') as f:\n",
    "        print('Date: {0}, Time_Cost: {1:.0f}s, Epoch: [{2}/{3}], Train_PSNR: {4:.2f}, Val_PSNR: {5:.2f}, Val_SSIM: {6:.4f}'\n",
    "              .format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "                      one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.063670Z",
     "iopub.status.busy": "2025-04-21T16:01:23.063338Z",
     "iopub.status.idle": "2025-04-21T16:01:23.081053Z",
     "shell.execute_reply": "2025-04-21T16:01:23.080012Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.063633Z"
    },
    "papermill": {
     "duration": 0.01846,
     "end_time": "2025-04-12T13:09:35.983598",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.965138",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def adjust_learning_rate(optimizer, epoch, category, lr_decay=0.90):\n",
    "#     \"\"\"\n",
    "#     Adjusts the learning rate based on the epoch and dataset category.\n",
    "\n",
    "#     :param optimizer: The optimizer (e.g., Adam, SGD).\n",
    "#     :param epoch: Current epoch number.\n",
    "#     :param category: Dataset category ('indoor', 'outdoor', or 'NH').\n",
    "#     :param lr_decay: Multiplicative factor for learning rate decay.\n",
    "#     \"\"\"\n",
    "#     # Define learning rate decay steps based on category\n",
    "#     step_dict = {'indoor': 18, 'outdoor': 3, 'NH': 20}\n",
    "#     step = step_dict.get(category, 3)  # Default step size if category is unknown\n",
    "\n",
    "#     # Decay learning rate at the specified step\n",
    "#     if epoch > 0 and epoch % step == 0:\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] *= lr_decay\n",
    "#             print(f\"Epoch {epoch}: Learning rate adjusted to {param_group['lr']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011722,
     "end_time": "2025-04-12T13:09:36.006955",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.995233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.082506Z",
     "iopub.status.busy": "2025-04-21T16:01:23.082047Z",
     "iopub.status.idle": "2025-04-21T16:01:23.105640Z",
     "shell.execute_reply": "2025-04-21T16:01:23.104649Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.082473Z"
    },
    "papermill": {
     "duration": 0.020068,
     "end_time": "2025-04-12T13:09:36.038381",
     "exception": false,
     "start_time": "2025-04-12T13:09:36.018313",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Perceptual Feature Loss Network --- #\n",
    "class PerceptualLossNet(nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = vgg_model\n",
    "        self.feature_layers = {'3': \"low_level\", '8': \"mid_level\", '15': \"high_level\"}\n",
    "\n",
    "    def get_feature_maps(self, x):\n",
    "        feature_maps = []\n",
    "        for layer_id, layer in self.feature_extractor.named_children():\n",
    "            x = layer(x)\n",
    "            if layer_id in self.feature_layers:\n",
    "                feature_maps.append(x)\n",
    "        return feature_maps\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        pred_features = self.get_feature_maps(predicted)\n",
    "        target_features = self.get_feature_maps(target)\n",
    "        \n",
    "        # Compute perceptual loss as mean squared error across feature maps\n",
    "        loss = torch.stack([F.mse_loss(p, t) for p, t in zip(pred_features, target_features)]).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "class SSFM(nn.Module):\n",
    "    def __init__(self, loss_type='l1'):\n",
    "        super(SSFM, self).__init__()\n",
    "        assert loss_type in ['l1', 'l2'], \"loss_type must be 'l1' or 'l2'\"\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def forward(self, student_feats, teacher_feats):\n",
    "        \"\"\"\n",
    "        student_feats: List of feature maps from student RDBs [rdb1, rdb2, rdb3, rdb4]\n",
    "        teacher_feats: List of corresponding feature maps from teacher\n",
    "        \"\"\"\n",
    "        assert len(student_feats) == len(teacher_feats), \"Feature lists must match\"\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for s_feat, t_feat in zip(student_feats, teacher_feats):\n",
    "            # Match resolution\n",
    "            if s_feat.shape != t_feat.shape:\n",
    "                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            if self.loss_type == 'l1':\n",
    "                loss = F.l1_loss(s_feat, t_feat)\n",
    "            else:\n",
    "                loss = F.mse_loss(s_feat, t_feat)\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss / len(student_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.106898Z",
     "iopub.status.busy": "2025-04-21T16:01:23.106596Z",
     "iopub.status.idle": "2025-04-21T16:01:29.590895Z",
     "shell.execute_reply": "2025-04-21T16:01:29.589811Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.106873Z"
    },
    "papermill": {
     "duration": 5.336581,
     "end_time": "2025-04-12T13:09:41.386705",
     "exception": false,
     "start_time": "2025-04-12T13:09:36.050124",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No pretrained weights found at /kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\n",
      "📊 Total Trainable Parameters: 572,628\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "\n",
    "# --- Initialize Model --- #\n",
    "net = DeepGuidedNetwork().to(device)\n",
    "\n",
    "# --- Enable Multi-GPU (if available) --- #\n",
    "if len(device_ids) > 1:\n",
    "    net = nn.DataParallel(net, device_ids=device_ids)\n",
    "\n",
    "# --- Optimizer --- #\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Load Pretrained VGG16 for Perceptual Loss --- #\n",
    "vgg_features = vgg16(pretrained=True).features[:16].to(device)\n",
    "for param in vgg_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "loss_network = PerceptualLossNet(vgg_features)\n",
    "loss_network.eval()\n",
    "\n",
    "# --- Load Model Weights (if available) --- #\n",
    "checkpoint_path = \"/kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\" \n",
    "\n",
    "try:\n",
    "    net.load_state_dict(torch.load(checkpoint_path, weights_only=False, map_location=torch.device('cpu')))\n",
    "    print(f\"✅ Model weights loaded from {checkpoint_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️ No pretrained weights found at {checkpoint_path}\")\n",
    "\n",
    "# --- Compute Total Trainable Parameters --- #\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"📊 Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:29.600137Z",
     "iopub.status.busy": "2025-04-21T16:01:29.599357Z",
     "iopub.status.idle": "2025-04-21T16:01:29.780968Z",
     "shell.execute_reply": "2025-04-21T16:01:29.779763Z",
     "shell.execute_reply.started": "2025-04-21T16:01:29.599860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeatureAffinityModule(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(FeatureAffinityModule, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def forward(self, student_features, teacher_features):\n",
    "        # Debug: Print input shapes\n",
    "        # print(\"Input student_features shape:\", student_features.shape)\n",
    "        # print(\"Input teacher_features shape:\", teacher_features.shape)\n",
    "\n",
    "        # # Normalize features\n",
    "        # print(\"\\nBefore normalization:\")\n",
    "        # print(\"Student features:\", student_features.shape)  # Example for the first feature of the first batch\n",
    "        # print(\"Teacher features:\", teacher_features.shape)  # Example for the first feature of the first batch\n",
    "\n",
    "        student_norm = F.normalize(student_features.view(student_features.size(0), self.channels, -1), dim=2)\n",
    "        # print(\"Student normalized features:\", student_norm.shape)  # Checking first normalized value\n",
    "\n",
    "        # print(\"\\nother:\")\n",
    "        # print(\"student_features.size(0)\",student_features.size(0))\n",
    "        # print(\"teacher_features.size(0)\",teacher_features.size(0))\n",
    "        # print(\"self.channels\",self.channels)\n",
    "        # print(\"student_features.view(student_features.size(0), self.channels, -1)\",student_features.view(student_features.size(0), self.channels, -1).shape)\n",
    "        # print(\"teacher_features.view(teacher_features.size(0), self.channels, -1)\",teacher_features.view(teacher_features.size(0), self.channels, -1).shape)\n",
    "        \n",
    "        teacher_norm = F.normalize(teacher_features.view(teacher_features.size(0), self.channels, -1), dim=2)\n",
    "        # print(\"Teacher normalized features:\", teacher_norm[0, 0, :2])  # Checking first normalized value\n",
    "\n",
    "        # Compute affinity matrices\n",
    "        student_affinity = torch.bmm(student_norm, student_norm.transpose(1, 2))\n",
    "        teacher_affinity = torch.bmm(teacher_norm, teacher_norm.transpose(1, 2))\n",
    "\n",
    "        # Debug: Print affinity matrix shapes\n",
    "        # print(\"\\nStudent affinity matrix shape:\", student_affinity.shape)\n",
    "        # print(\"Teacher affinity matrix shape:\", teacher_affinity.shape)\n",
    "\n",
    "        # Compute KL divergence\n",
    "        loss = F.kl_div(F.log_softmax(student_affinity, dim=-1),\n",
    "                        F.softmax(teacher_affinity, dim=-1),\n",
    "                        reduction='batchmean')\n",
    "\n",
    "        # Debug: Print computed loss\n",
    "        # print(\"\\nComputed loss:\", loss.item())\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haze Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class HazeDataset(Dataset):\n",
    "    def __init__(self, hazeeffected_images_dir, hazefree_images_dir, split=\"train\", split_ratio=(0.8, 0.1, 0.1), random_seed=42):\n",
    "        \"\"\"\n",
    "        Dataset class for handling hazy and corresponding ground-truth images (already aligned and cropped).\n",
    "\n",
    "        Args:\n",
    "            hazeeffected_images_dir (str): Directory for hazy images.\n",
    "            hazefree_images_dir (str): Directory for ground-truth images.\n",
    "            split (str): \"train\", \"valid\", or \"test\" (determines data split).\n",
    "            split_ratio (tuple): A tuple of three floats representing the train, validation, and test splits.\n",
    "                                 (default 80% train, 10% validation, 10% test).\n",
    "            random_seed (int): Random seed for reproducibility in shuffling data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if sum(split_ratio) != 1.0:\n",
    "            raise ValueError(\"The sum of split_ratio must be 1.0\")\n",
    "\n",
    "        self.train_ratio, self.valid_ratio, self.test_ratio = split_ratio\n",
    "\n",
    "        valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        hazy_data = [\n",
    "            f for f in glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_extensions)\n",
    "        ]\n",
    "\n",
    "        if not hazy_data:\n",
    "            raise ValueError(f\"No valid images found in {hazeeffected_images_dir}\")\n",
    "\n",
    "        hazy_data.sort()\n",
    "\n",
    "        # Shuffle the dataset for randomization\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(hazy_data)\n",
    "\n",
    "        # Split dataset based on provided ratios\n",
    "        total_images = len(hazy_data)\n",
    "        train_size = int(total_images * self.train_ratio)\n",
    "        valid_size = int(total_images * self.valid_ratio)\n",
    "        test_size = total_images - train_size - valid_size\n",
    "\n",
    "        if split == \"train\":\n",
    "            hazy_data = hazy_data[:train_size]\n",
    "        elif split == \"valid\":\n",
    "            hazy_data = hazy_data[train_size:train_size+valid_size]\n",
    "        elif split == \"test\":\n",
    "            hazy_data = hazy_data[train_size+valid_size:]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split value: {split}. Choose from ['train', 'valid', 'test'].\")\n",
    "\n",
    "        self.haze_names = []\n",
    "        self.gt_names = []\n",
    "\n",
    "        for h_image in hazy_data:\n",
    "            filename = os.path.basename(h_image)\n",
    "            haze_path = os.path.join(hazeeffected_images_dir, filename)\n",
    "            gt_path = os.path.join(hazefree_images_dir, filename)\n",
    "\n",
    "            if not os.path.exists(gt_path):\n",
    "                print(f\"Warning: Ground-truth missing for {filename}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            self.haze_names.append(haze_path)\n",
    "            self.gt_names.append(gt_path)\n",
    "\n",
    "        if not self.haze_names:\n",
    "            raise ValueError(\"No matching ground-truth images found.\")\n",
    "\n",
    "        # Define transforms\n",
    "        self.transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        self.transform_gt = Compose([ToTensor()])\n",
    "\n",
    "    def get_images(self, index):\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        try:\n",
    "            haze_img = Image.open(haze_name).convert('RGB')\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Invalid image format: {haze_name} or {gt_name}\")\n",
    "\n",
    "        haze = self.transform_haze(haze_img)\n",
    "        gt = self.transform_gt(gt_img)\n",
    "\n",
    "        if haze.shape[0] != 3 or gt.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {haze_name}\")\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_images(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haze DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training:\n",
    "train_dataset = HazeDataset(hazeeffected_images_dir=hazeeffected_images_dir_train, \n",
    "                             hazefree_images_dir=hazefree_images_dir_train, \n",
    "                             split=\"train\", \n",
    "                             split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "# For validation:\n",
    "val_dataset = HazeDataset(hazeeffected_images_dir=hazeeffected_images_dir_train, \n",
    "                             hazefree_images_dir=hazefree_images_dir_train, \n",
    "                             split=\"valid\", \n",
    "                             split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "# For testing:\n",
    "test_dataset = HazeDataset(hazeeffected_images_dir=hazeeffected_images_dir_train, \n",
    "                            hazefree_images_dir=hazefree_images_dir_train, \n",
    "                            split=\"test\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:29.782444Z",
     "iopub.status.busy": "2025-04-21T16:01:29.782054Z",
     "iopub.status.idle": "2025-04-21T16:01:55.153827Z",
     "shell.execute_reply": "2025-04-21T16:01:55.152853Z",
     "shell.execute_reply.started": "2025-04-21T16:01:29.782414Z"
    },
    "papermill": {
     "duration": 23.313227,
     "end_time": "2025-04-12T13:10:04.746196",
     "exception": false,
     "start_time": "2025-04-12T13:09:41.432969",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4800, Validation samples: 600, Test samples: 600\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:55.155297Z",
     "iopub.status.busy": "2025-04-21T16:01:55.154949Z",
     "iopub.status.idle": "2025-04-21T16:01:55.160764Z",
     "shell.execute_reply": "2025-04-21T16:01:55.159843Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.155268Z"
    },
    "papermill": {
     "duration": 0.018274,
     "end_time": "2025-04-12T13:10:04.777834",
     "exception": false,
     "start_time": "2025-04-12T13:10:04.759560",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(glob.glob( \"/kaggle/working/cropped_train/hazy/*\")), len(glob.glob(\"/kaggle/working/cropped_train/GT/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:11.972197Z",
     "iopub.status.busy": "2025-04-21T16:09:11.971341Z",
     "iopub.status.idle": "2025-04-21T16:09:11.984636Z",
     "shell.execute_reply": "2025-04-21T16:09:11.983552Z",
     "shell.execute_reply.started": "2025-04-21T16:09:11.972167Z"
    },
    "papermill": {
     "duration": 0.022717,
     "end_time": "2025-04-12T13:10:04.813435",
     "exception": false,
     "start_time": "2025-04-12T13:10:04.790718",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, scale='x4', split='train', split_ratio=(0.8, 0.1, 0.1), random_seed=42):\n",
    "        \"\"\"\n",
    "        Super-Resolution dataset that matches LR and HR image pairs based on naming pattern.\n",
    "        Assumes images are already aligned and correctly scaled. No cropping is applied.\n",
    "\n",
    "        Args:\n",
    "            lr_dir (str): Directory containing low-resolution images (e.g., x2, x3, x4).\n",
    "            hr_dir (str): Directory containing high-resolution images.\n",
    "            scale (str): Scale suffix in LR filenames (e.g., 'x2', 'x3', 'x4').\n",
    "            split (str): 'train', 'val', or 'test' (determines data split).\n",
    "            split_ratio (tuple): A tuple of three floats representing the train, validation, and test splits.\n",
    "                                 (default 80% train, 10% validation, 10% test).\n",
    "            random_seed (int): Random seed for reproducibility in shuffling data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if sum(split_ratio) != 1.0:\n",
    "            raise ValueError(\"The sum of split_ratio must be 1.0\")\n",
    "\n",
    "        self.train_ratio, self.valid_ratio, self.test_ratio = split_ratio\n",
    "\n",
    "        valid_ext = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        lr_images = sorted([\n",
    "            f for f in glob.glob(os.path.join(lr_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_ext)\n",
    "        ])\n",
    "\n",
    "        lr_hr_pairs = []\n",
    "        for lr_path in lr_images:\n",
    "            lr_name = os.path.basename(lr_path)\n",
    "            hr_name = lr_name.replace(scale, '')  # assumes naming like image_x4.png -> image.png\n",
    "            hr_path = os.path.join(hr_dir, hr_name)\n",
    "\n",
    "            if not os.path.exists(hr_path):\n",
    "                print(f\"Warning: Ground-truth missing for {lr_name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            lr_hr_pairs.append((lr_path, hr_path))\n",
    "\n",
    "        if not lr_hr_pairs:\n",
    "            raise ValueError(\"No matching LR-HR image pairs found.\")\n",
    "\n",
    "        # Shuffle the dataset for randomization\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(lr_hr_pairs)\n",
    "\n",
    "        # Split dataset based on provided ratios\n",
    "        total_pairs = len(lr_hr_pairs)\n",
    "        train_size = int(total_pairs * self.train_ratio)\n",
    "        valid_size = int(total_pairs * self.valid_ratio)\n",
    "        test_size = total_pairs - train_size - valid_size\n",
    "\n",
    "        # Splitting the data based on the chosen split\n",
    "        if split == 'train':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[:train_size]\n",
    "        elif split == 'val':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[train_size:train_size+valid_size]\n",
    "        elif split == 'test':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[train_size+valid_size:]\n",
    "        else:\n",
    "            raise ValueError(\"split must be either 'train', 'val', or 'test'\")\n",
    "\n",
    "        # Define common transform\n",
    "        self.transform = Compose([ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_hr_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_path, hr_path = self.lr_hr_pairs[idx]\n",
    "\n",
    "        try:\n",
    "            lr_img = Image.open(lr_path).convert('RGB')\n",
    "            hr_img = Image.open(hr_path).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Unidentified image at {lr_path} or {hr_path}\")\n",
    "\n",
    "        lr_tensor = self.transform(lr_img)\n",
    "        hr_tensor = self.transform(hr_img)\n",
    "\n",
    "        if lr_tensor.shape[0] != 3 or hr_tensor.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {lr_path}\")\n",
    "\n",
    "        return lr_tensor, hr_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SR DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:16.873360Z",
     "iopub.status.busy": "2025-04-21T16:09:16.873060Z",
     "iopub.status.idle": "2025-04-21T16:09:16.879312Z",
     "shell.execute_reply": "2025-04-21T16:09:16.878403Z",
     "shell.execute_reply.started": "2025-04-21T16:09:16.873340Z"
    },
    "papermill": {
     "duration": 0.018734,
     "end_time": "2025-04-12T13:10:04.844884",
     "exception": false,
     "start_time": "2025-04-12T13:10:04.826150",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def custom_collate_fn(batch):\n",
    "#     min_height = min([x[0].shape[1] for x in batch])\n",
    "#     min_width = min([x[0].shape[2] for x in batch])\n",
    "#     resized_batch = [(TF.resize(x[0], [min_height, min_width]), TF.resize(x[1], [min_height, min_width])) for x in batch]\n",
    "#     return torch.utils.data.dataloader.default_collate(resized_batch)\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Assumes each item in batch is a tuple: (input_tensor, gt_tensor)\n",
    "    # Input is already 64x64, ground truth is 256x256\n",
    "    resized_batch = []\n",
    "    for input_tensor, gt_tensor in batch:\n",
    "        # Keep input as-is, downsample ground truth to 64x64\n",
    "        resized_gt = TF.resize(gt_tensor, [64, 64], interpolation=TF.InterpolationMode.NEAREST)\n",
    "        resized_batch.append((input_tensor, resized_gt))\n",
    "    return torch.utils.data.dataloader.default_collate(resized_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:24.350816Z",
     "iopub.status.busy": "2025-04-21T16:09:24.350518Z",
     "iopub.status.idle": "2025-04-21T16:09:36.552704Z",
     "shell.execute_reply": "2025-04-21T16:09:36.551702Z",
     "shell.execute_reply.started": "2025-04-21T16:09:24.350794Z"
    },
    "papermill": {
     "duration": 6.842293,
     "end_time": "2025-04-12T13:10:11.789404",
     "exception": false,
     "start_time": "2025-04-12T13:10:04.947111",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- SR Dataset Setup --- #\n",
    "\n",
    "if sr_enabled:\n",
    "\n",
    "    # Dataset for Super-Resolution (SR) task\n",
    "    # For training:\n",
    "    sr_train_dataset = SRDataset(lr_dir=sr_lr_dir, \n",
    "                            hr_dir=sr_hr_dir,\n",
    "                            scale=\"x4\", \n",
    "                            split=\"train\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "    # For validation:\n",
    "    sr_val_dataset = SRDataset(lr_dir=sr_lr_dir, \n",
    "                            hr_dir=sr_hr_dir, \n",
    "                            scale=\"x4\", \n",
    "                            split=\"val\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "    # For testing:\n",
    "    sr_test_dataset = SRDataset(lr_dir=sr_lr_dir, \n",
    "                            hr_dir=sr_hr_dir, \n",
    "                            scale=\"x4\", \n",
    "                            split=\"test\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n",
    "    \n",
    "    # DataLoader for training with drop_last=True to avoid smaller batches\n",
    "    sr_train_loader = DataLoader(\n",
    "        sr_train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        # collate_fn=custom_collate_fn,\n",
    "        drop_last=True  # Ensure the final batch is dropped if not full\n",
    "    )\n",
    "    \n",
    "    # DataLoader for validation with drop_last=True for consistency (optional)\n",
    "    sr_val_loader = DataLoader(\n",
    "        sr_val_dataset,\n",
    "        batch_size=val_batch_size,\n",
    "        shuffle=False,  # Don't shuffle the validation set\n",
    "        # collate_fn=custom_collate_fn,\n",
    "        drop_last=True  # Optional: consider it for consistency across train/val\n",
    "    )\n",
    "\n",
    "    # DataLoader for testing (No shuffling and no drop_last)\n",
    "    sr_test_loader = DataLoader(\n",
    "        sr_test_dataset,\n",
    "        batch_size=val_batch_size,  # Define your test batch size\n",
    "        shuffle=False,  # No shuffling needed for testing\n",
    "        # collate_fn=custom_collate_fn,  # Use custom collate function if needed\n",
    "        drop_last=False  # We typically do not drop the last batch for testing\n",
    "    )\n",
    "\n",
    "    # Create an iterator for the training set\n",
    "    sr_iter = iter(sr_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR Train samples: 2120, SR Validation samples: 265, SR Test samples: 265\n"
     ]
    }
   ],
   "source": [
    "print(f\"SR Train samples: {len(sr_train_dataset)}, SR Validation samples: {len(sr_val_dataset)}, SR Test samples: {len(sr_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:36.554275Z",
     "iopub.status.busy": "2025-04-21T16:09:36.554010Z",
     "iopub.status.idle": "2025-04-21T16:09:36.665975Z",
     "shell.execute_reply": "2025-04-21T16:09:36.665053Z",
     "shell.execute_reply.started": "2025-04-21T16:09:36.554253Z"
    },
    "papermill": {
     "duration": 0.272846,
     "end_time": "2025-04-12T13:10:12.082208",
     "exception": false,
     "start_time": "2025-04-12T13:10:11.809362",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64]) torch.Size([2, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for i,o in train_data_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64]) torch.Size([2, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for i,o in val_data_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:10:00.985315Z",
     "iopub.status.busy": "2025-04-21T16:10:00.985016Z",
     "iopub.status.idle": "2025-04-21T16:10:01.651391Z",
     "shell.execute_reply": "2025-04-21T16:10:01.650352Z",
     "shell.execute_reply.started": "2025-04-21T16:10:00.985293Z"
    },
    "papermill": {
     "duration": 2.453133,
     "end_time": "2025-04-12T13:10:14.580358",
     "exception": false,
     "start_time": "2025-04-12T13:10:12.127225",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64]) torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i,o in sr_train_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:51.026594Z",
     "iopub.status.busy": "2025-04-21T16:09:51.026121Z",
     "iopub.status.idle": "2025-04-21T16:09:51.473913Z",
     "shell.execute_reply": "2025-04-21T16:09:51.472989Z",
     "shell.execute_reply.started": "2025-04-21T16:09:51.026568Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64]) torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i,o in sr_val_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2120, 265)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sr_train_dataset), len(sr_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: LR shape: torch.Size([2, 3, 64, 64]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 1: LR shape: torch.Size([2, 3, 64, 64]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 2: LR shape: torch.Size([2, 3, 64, 64]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 3: LR shape: torch.Size([2, 3, 64, 64]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 4: LR shape: torch.Size([2, 3, 64, 64]), HR shape: torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# enumerate\n",
    "for i, (lr, hr) in enumerate(sr_train_loader):\n",
    "    print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "    if i == 4:  # Just to limit the output\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes 1:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n",
      "[SR Init Val] PSNR: 6.68, SSIM: 0.0332\n"
     ]
    }
   ],
   "source": [
    "# --- Teacher Network --- #\n",
    "teacher_net = DeepGuidedNetwork(radius=1).to(device)\n",
    "# teacher_net.load_state_dict(torch.load('teacher_model.pth'))\n",
    "# teacher_net.eval()\n",
    "\n",
    "# --- Feature Affinity Module --- #\n",
    "fam = FeatureAffinityModule(channels=16).to(device)\n",
    "ssfm_loss = SSFM(loss_type='l1') \n",
    "\n",
    "\n",
    "# --- Initial Validation --- #\n",
    "# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)\n",
    "# print(f\"[Dehazing Init Val] PSNR: {old_val_psnr:.2f}, SSIM: {old_val_ssim:.4f}\")\n",
    "if sr_enabled:\n",
    "    sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "    print(f\"[SR Init Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "# --- Training Loop --- #\n",
    "# best_psnr = old_val_psnr\n",
    "best_psnr = sr_val_psnr\n",
    "train_psnr_prev = 0\n",
    "distillation_weight = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   0%|          | 1/1060 [00:00<10:51,  1.63it/s, iter=0, loss=0.847]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   0%|          | 2/1060 [00:01<11:01,  1.60it/s, iter=1, loss=0.835]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   0%|          | 3/1060 [00:01<10:43,  1.64it/s, iter=2, loss=0.611]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   0%|          | 4/1060 [00:02<10:54,  1.61it/s, iter=3, loss=0.664]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   0%|          | 5/1060 [00:03<10:33,  1.67it/s, iter=4, loss=0.608]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   1%|          | 6/1060 [00:03<10:28,  1.68it/s, iter=5, loss=0.846]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   1%|          | 7/1060 [00:04<10:40,  1.64it/s, iter=6, loss=1.38] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   1%|          | 8/1060 [00:04<10:26,  1.68it/s, iter=7, loss=0.994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   1%|          | 9/1060 [00:05<10:17,  1.70it/s, iter=8, loss=1.06] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   1%|          | 10/1060 [00:05<10:17,  1.70it/s, iter=9, loss=1.29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   1%|          | 11/1060 [00:06<10:12,  1.71it/s, iter=10, loss=1.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   1%|          | 12/1060 [00:07<10:17,  1.70it/s, iter=11, loss=1.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   1%|          | 13/1060 [00:07<10:12,  1.71it/s, iter=12, loss=1.62]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   1%|▏         | 14/1060 [00:08<10:11,  1.71it/s, iter=13, loss=1.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   1%|▏         | 15/1060 [00:08<10:07,  1.72it/s, iter=14, loss=1.55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   2%|▏         | 16/1060 [00:09<09:53,  1.76it/s, iter=15, loss=1.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   2%|▏         | 17/1060 [00:09<09:50,  1.77it/s, iter=16, loss=1.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   2%|▏         | 18/1060 [00:10<09:44,  1.78it/s, iter=17, loss=1.88]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   2%|▏         | 19/1060 [00:11<10:06,  1.72it/s, iter=18, loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   2%|▏         | 20/1060 [00:11<09:57,  1.74it/s, iter=19, loss=1.66]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   2%|▏         | 21/1060 [00:12<09:53,  1.75it/s, iter=20, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   2%|▏         | 22/1060 [00:12<10:08,  1.71it/s, iter=21, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   2%|▏         | 23/1060 [00:13<10:14,  1.69it/s, iter=22, loss=1.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   2%|▏         | 24/1060 [00:14<10:18,  1.68it/s, iter=23, loss=1.78]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   2%|▏         | 25/1060 [00:14<10:19,  1.67it/s, iter=24, loss=1.84]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   2%|▏         | 26/1060 [00:15<10:13,  1.69it/s, iter=25, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   3%|▎         | 27/1060 [00:15<10:08,  1.70it/s, iter=26, loss=1.8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   3%|▎         | 28/1060 [00:16<09:54,  1.73it/s, iter=27, loss=1.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   3%|▎         | 29/1060 [00:17<10:14,  1.68it/s, iter=28, loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   3%|▎         | 30/1060 [00:17<10:01,  1.71it/s, iter=29, loss=1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   3%|▎         | 31/1060 [00:18<10:10,  1.69it/s, iter=30, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   3%|▎         | 32/1060 [00:18<09:55,  1.73it/s, iter=31, loss=1.78]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   3%|▎         | 33/1060 [00:19<09:48,  1.75it/s, iter=32, loss=1.84]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   3%|▎         | 34/1060 [00:19<09:44,  1.76it/s, iter=33, loss=1.82]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   3%|▎         | 35/1060 [00:20<09:39,  1.77it/s, iter=34, loss=1.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   3%|▎         | 36/1060 [00:21<09:36,  1.78it/s, iter=35, loss=1.81]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   3%|▎         | 37/1060 [00:21<09:34,  1.78it/s, iter=36, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   4%|▎         | 38/1060 [00:22<09:30,  1.79it/s, iter=37, loss=1.85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   4%|▎         | 39/1060 [00:22<09:33,  1.78it/s, iter=38, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   4%|▍         | 40/1060 [00:23<09:41,  1.75it/s, iter=39, loss=1.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   4%|▍         | 41/1060 [00:23<09:51,  1.72it/s, iter=40, loss=1.81]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   4%|▍         | 42/1060 [00:24<09:41,  1.75it/s, iter=41, loss=1.8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   4%|▍         | 43/1060 [00:25<09:57,  1.70it/s, iter=42, loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   4%|▍         | 44/1060 [00:25<09:41,  1.75it/s, iter=43, loss=1.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   4%|▍         | 45/1060 [00:26<09:50,  1.72it/s, iter=44, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   4%|▍         | 46/1060 [00:26<09:43,  1.74it/s, iter=45, loss=1.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   4%|▍         | 47/1060 [00:27<09:38,  1.75it/s, iter=46, loss=1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   5%|▍         | 48/1060 [00:27<09:28,  1.78it/s, iter=47, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   5%|▍         | 49/1060 [00:28<09:26,  1.78it/s, iter=48, loss=1.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   5%|▍         | 50/1060 [00:28<09:25,  1.79it/s, iter=49, loss=1.78]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   5%|▍         | 51/1060 [00:29<09:20,  1.80it/s, iter=50, loss=1.8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   5%|▍         | 52/1060 [00:30<09:14,  1.82it/s, iter=51, loss=1.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   5%|▌         | 53/1060 [00:30<09:18,  1.80it/s, iter=52, loss=1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   5%|▌         | 54/1060 [00:31<09:17,  1.81it/s, iter=53, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   5%|▌         | 55/1060 [00:31<09:14,  1.81it/s, iter=54, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   5%|▌         | 56/1060 [00:32<09:15,  1.81it/s, iter=55, loss=1.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   5%|▌         | 57/1060 [00:32<09:15,  1.80it/s, iter=56, loss=1.8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   5%|▌         | 58/1060 [00:33<09:21,  1.78it/s, iter=57, loss=1.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   6%|▌         | 59/1060 [00:34<09:42,  1.72it/s, iter=58, loss=1.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   6%|▌         | 60/1060 [00:34<09:52,  1.69it/s, iter=59, loss=1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   6%|▌         | 61/1060 [00:35<09:59,  1.67it/s, iter=60, loss=1.81]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   6%|▌         | 62/1060 [00:35<09:46,  1.70it/s, iter=61, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   6%|▌         | 63/1060 [00:36<09:29,  1.75it/s, iter=62, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   6%|▌         | 64/1060 [00:36<09:15,  1.79it/s, iter=63, loss=1.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   6%|▌         | 65/1060 [00:37<09:19,  1.78it/s, iter=64, loss=1.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   6%|▌         | 66/1060 [00:38<09:18,  1.78it/s, iter=65, loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   6%|▋         | 67/1060 [00:38<09:14,  1.79it/s, iter=66, loss=1.8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   6%|▋         | 68/1060 [00:39<09:15,  1.78it/s, iter=67, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   7%|▋         | 69/1060 [00:39<09:15,  1.78it/s, iter=68, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   7%|▋         | 70/1060 [00:40<09:08,  1.80it/s, iter=69, loss=1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   7%|▋         | 71/1060 [00:40<09:33,  1.73it/s, iter=70, loss=1.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   7%|▋         | 72/1060 [00:41<09:29,  1.73it/s, iter=71, loss=1.83]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   7%|▋         | 73/1060 [00:42<09:24,  1.75it/s, iter=72, loss=1.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   7%|▋         | 74/1060 [00:42<09:34,  1.72it/s, iter=73, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   7%|▋         | 75/1060 [00:43<09:54,  1.66it/s, iter=74, loss=1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   7%|▋         | 76/1060 [00:43<10:02,  1.63it/s, iter=75, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   7%|▋         | 77/1060 [00:44<10:34,  1.55it/s, iter=76, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   7%|▋         | 78/1060 [00:45<10:14,  1.60it/s, iter=77, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   7%|▋         | 79/1060 [00:45<10:01,  1.63it/s, iter=78, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   8%|▊         | 80/1060 [00:46<10:03,  1.62it/s, iter=79, loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   8%|▊         | 81/1060 [00:47<10:04,  1.62it/s, iter=80, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   8%|▊         | 82/1060 [00:47<09:55,  1.64it/s, iter=81, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   8%|▊         | 83/1060 [00:48<09:39,  1.69it/s, iter=82, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   8%|▊         | 84/1060 [00:48<09:28,  1.72it/s, iter=83, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   8%|▊         | 85/1060 [00:49<10:01,  1.62it/s, iter=84, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   8%|▊         | 86/1060 [00:50<09:49,  1.65it/s, iter=85, loss=1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   8%|▊         | 87/1060 [00:50<10:06,  1.60it/s, iter=86, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   8%|▊         | 88/1060 [00:51<10:21,  1.56it/s, iter=87, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   8%|▊         | 89/1060 [00:52<10:26,  1.55it/s, iter=88, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   8%|▊         | 90/1060 [00:52<10:21,  1.56it/s, iter=89, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   9%|▊         | 91/1060 [00:53<10:08,  1.59it/s, iter=90, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   9%|▊         | 92/1060 [00:53<10:04,  1.60it/s, iter=91, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   9%|▉         | 93/1060 [00:54<10:09,  1.59it/s, iter=92, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   9%|▉         | 94/1060 [00:55<09:58,  1.62it/s, iter=93, loss=1.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   9%|▉         | 95/1060 [00:55<09:44,  1.65it/s, iter=94, loss=1.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   9%|▉         | 96/1060 [00:56<09:34,  1.68it/s, iter=95, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   9%|▉         | 97/1060 [00:56<09:56,  1.62it/s, iter=96, loss=1.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   9%|▉         | 98/1060 [00:57<10:17,  1.56it/s, iter=97, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   9%|▉         | 99/1060 [00:58<10:04,  1.59it/s, iter=98, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:   9%|▉         | 100/1060 [00:58<10:25,  1.54it/s, iter=99, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  10%|▉         | 101/1060 [00:59<10:06,  1.58it/s, iter=100, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  10%|▉         | 102/1060 [01:00<10:21,  1.54it/s, iter=101, loss=1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  10%|▉         | 103/1060 [01:00<10:41,  1.49it/s, iter=102, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  10%|▉         | 104/1060 [01:01<10:50,  1.47it/s, iter=103, loss=1.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  10%|▉         | 105/1060 [01:02<11:16,  1.41it/s, iter=104, loss=1.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  10%|█         | 106/1060 [01:02<10:41,  1.49it/s, iter=105, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  10%|█         | 107/1060 [01:03<10:15,  1.55it/s, iter=106, loss=1.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  10%|█         | 108/1060 [01:04<09:51,  1.61it/s, iter=107, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  10%|█         | 109/1060 [01:04<09:59,  1.59it/s, iter=108, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  10%|█         | 110/1060 [01:05<09:40,  1.64it/s, iter=109, loss=1.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  10%|█         | 111/1060 [01:05<09:29,  1.67it/s, iter=110, loss=1.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  11%|█         | 112/1060 [01:06<09:41,  1.63it/s, iter=111, loss=1.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  11%|█         | 113/1060 [01:07<09:34,  1.65it/s, iter=112, loss=1.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  11%|█         | 114/1060 [01:07<09:39,  1.63it/s, iter=113, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  11%|█         | 115/1060 [01:08<09:27,  1.67it/s, iter=114, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  11%|█         | 116/1060 [01:09<09:47,  1.61it/s, iter=115, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  11%|█         | 117/1060 [01:09<09:40,  1.62it/s, iter=116, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  11%|█         | 118/1060 [01:10<09:31,  1.65it/s, iter=117, loss=1.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  11%|█         | 119/1060 [01:10<09:46,  1.61it/s, iter=118, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  11%|█▏        | 120/1060 [01:11<09:41,  1.62it/s, iter=119, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  11%|█▏        | 121/1060 [01:12<09:38,  1.62it/s, iter=120, loss=1.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  12%|█▏        | 122/1060 [01:12<10:02,  1.56it/s, iter=121, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  12%|█▏        | 123/1060 [01:13<10:06,  1.54it/s, iter=122, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  12%|█▏        | 124/1060 [01:14<09:56,  1.57it/s, iter=123, loss=1.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  12%|█▏        | 125/1060 [01:14<09:47,  1.59it/s, iter=124, loss=1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  12%|█▏        | 126/1060 [01:15<09:30,  1.64it/s, iter=125, loss=1.74]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  12%|█▏        | 127/1060 [01:15<09:49,  1.58it/s, iter=126, loss=1.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  12%|█▏        | 128/1060 [01:16<09:52,  1.57it/s, iter=127, loss=1.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  12%|█▏        | 129/1060 [01:17<09:39,  1.61it/s, iter=128, loss=1.81]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  12%|█▏        | 130/1060 [01:17<09:33,  1.62it/s, iter=129, loss=1.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  12%|█▏        | 131/1060 [01:18<09:38,  1.61it/s, iter=130, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  12%|█▏        | 132/1060 [01:19<09:31,  1.63it/s, iter=131, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  13%|█▎        | 133/1060 [01:19<09:37,  1.60it/s, iter=132, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  13%|█▎        | 134/1060 [01:20<09:27,  1.63it/s, iter=133, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  13%|█▎        | 135/1060 [01:20<09:16,  1.66it/s, iter=134, loss=1.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  13%|█▎        | 136/1060 [01:21<09:18,  1.66it/s, iter=135, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  13%|█▎        | 137/1060 [01:22<09:28,  1.62it/s, iter=136, loss=1.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  13%|█▎        | 138/1060 [01:22<09:36,  1.60it/s, iter=137, loss=1.67]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  13%|█▎        | 139/1060 [01:23<09:56,  1.55it/s, iter=138, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  13%|█▎        | 140/1060 [01:24<09:56,  1.54it/s, iter=139, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  13%|█▎        | 141/1060 [01:24<10:10,  1.51it/s, iter=140, loss=1.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  13%|█▎        | 142/1060 [01:25<10:06,  1.51it/s, iter=141, loss=1.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  13%|█▎        | 143/1060 [01:26<10:15,  1.49it/s, iter=142, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  14%|█▎        | 144/1060 [01:26<10:00,  1.52it/s, iter=143, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  14%|█▎        | 145/1060 [01:27<10:01,  1.52it/s, iter=144, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  14%|█▍        | 146/1060 [01:28<09:53,  1.54it/s, iter=145, loss=1.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  14%|█▍        | 147/1060 [01:28<09:36,  1.58it/s, iter=146, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  14%|█▍        | 148/1060 [01:29<09:37,  1.58it/s, iter=147, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  14%|█▍        | 149/1060 [01:29<09:21,  1.62it/s, iter=148, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  14%|█▍        | 150/1060 [01:30<09:45,  1.56it/s, iter=149, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  14%|█▍        | 151/1060 [01:31<09:57,  1.52it/s, iter=150, loss=1.66]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  14%|█▍        | 152/1060 [01:31<09:51,  1.53it/s, iter=151, loss=1.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  14%|█▍        | 153/1060 [01:32<09:30,  1.59it/s, iter=152, loss=1.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  15%|█▍        | 154/1060 [01:33<09:45,  1.55it/s, iter=153, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  15%|█▍        | 155/1060 [01:33<09:52,  1.53it/s, iter=154, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  15%|█▍        | 156/1060 [01:34<09:48,  1.54it/s, iter=155, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  15%|█▍        | 157/1060 [01:35<09:41,  1.55it/s, iter=156, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  15%|█▍        | 158/1060 [01:35<09:57,  1.51it/s, iter=157, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  15%|█▌        | 159/1060 [01:36<10:01,  1.50it/s, iter=158, loss=1.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  15%|█▌        | 160/1060 [01:37<09:45,  1.54it/s, iter=159, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  15%|█▌        | 161/1060 [01:37<09:43,  1.54it/s, iter=160, loss=1.85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  15%|█▌        | 162/1060 [01:38<09:50,  1.52it/s, iter=161, loss=1.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  15%|█▌        | 163/1060 [01:39<09:39,  1.55it/s, iter=162, loss=1.73]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  15%|█▌        | 164/1060 [01:39<09:39,  1.55it/s, iter=163, loss=1.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  16%|█▌        | 165/1060 [01:40<09:43,  1.53it/s, iter=164, loss=1.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  16%|█▌        | 166/1060 [01:41<10:00,  1.49it/s, iter=165, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  16%|█▌        | 167/1060 [01:41<10:57,  1.36it/s, iter=166, loss=1.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  16%|█▌        | 168/1060 [01:42<11:34,  1.28it/s, iter=167, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  16%|█▌        | 169/1060 [01:43<11:58,  1.24it/s, iter=168, loss=1.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  16%|█▌        | 170/1060 [01:44<11:39,  1.27it/s, iter=169, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  16%|█▌        | 171/1060 [01:45<11:07,  1.33it/s, iter=170, loss=1.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  16%|█▌        | 172/1060 [01:45<10:44,  1.38it/s, iter=171, loss=1.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  16%|█▋        | 173/1060 [01:46<10:43,  1.38it/s, iter=172, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  16%|█▋        | 174/1060 [01:47<10:30,  1.40it/s, iter=173, loss=1.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  17%|█▋        | 175/1060 [01:47<10:25,  1.41it/s, iter=174, loss=1.7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  17%|█▋        | 176/1060 [01:48<11:34,  1.27it/s, iter=175, loss=1.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  17%|█▋        | 177/1060 [01:49<11:10,  1.32it/s, iter=176, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  17%|█▋        | 178/1060 [01:50<10:17,  1.43it/s, iter=177, loss=1.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  17%|█▋        | 179/1060 [01:50<09:46,  1.50it/s, iter=178, loss=1.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  17%|█▋        | 180/1060 [01:51<09:34,  1.53it/s, iter=179, loss=1.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  17%|█▋        | 181/1060 [01:51<09:16,  1.58it/s, iter=180, loss=1.69]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  17%|█▋        | 182/1060 [01:52<09:01,  1.62it/s, iter=181, loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SR Epoch 1/25]:  17%|█▋        | 183/1060 [01:52<08:36,  1.70it/s, iter=182, loss=1.77]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  torch.Size([2, 3, 256, 256]) torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Create a directory for TensorBoard logs\n",
    "log_dir = \"runs/sr_training_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "global_iter = 0\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Optimizer for net and teacher_net (joint training)\n",
    "optimizer = torch.optim.Adam(list(net.parameters()) + list(teacher_net.parameters()), lr=learning_rate)\n",
    "\n",
    "# LR scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    psnr_list = []\n",
    "\n",
    "    net.train()\n",
    "    teacher_net.train()\n",
    "\n",
    "    sr_loader_tqdm = tqdm(sr_train_loader, desc=f\"[SR Epoch {epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "    for batch_id, (sr_lr, sr_hr) in enumerate(sr_loader_tqdm):\n",
    "        sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through teacher_net for SR\n",
    "        sr_out, _, t = teacher_net(sr_lr)\n",
    "\n",
    "        # SR reconstruction loss\n",
    "        sr_loss = F.l1_loss(sr_out, sr_hr)\n",
    "        total_loss = sr_loss\n",
    "\n",
    "        # Optional: forward pass through net (student) for distillation\n",
    "        with torch.no_grad():\n",
    "            haze_like_input = sr_lr  # simulate student input for distillation if applicable\n",
    "            dehaze, base, s = net(haze_like_input)\n",
    "            distillation_loss = fam(dehaze, sr_out)\n",
    "            s_loss = ssfm_loss(s, t)\n",
    "            total_loss += distillation_loss + s_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        psnr_list.extend(to_psnr(sr_out, sr_hr))\n",
    "        sr_loader_tqdm.set_postfix(loss=total_loss.item(), iter=global_iter)\n",
    "\n",
    "        # TensorBoard logging (per iteration)\n",
    "        writer.add_scalar(\"Loss/Total\", total_loss.item(), global_iter)\n",
    "        writer.add_scalar(\"Loss/SR\", sr_loss.item(), global_iter)\n",
    "        if 'distillation_loss' in locals():\n",
    "            writer.add_scalar(\"Loss/Distillation\", distillation_loss.item(), global_iter)\n",
    "        if 's_loss' in locals():\n",
    "            writer.add_scalar(\"Loss/Structure\", s_loss.item(), global_iter)\n",
    "\n",
    "        global_iter += 1\n",
    "\n",
    "    train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "    # Save model every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(teacher_net.state_dict(), f\"teacher_net_sr_iter_{epoch}.pth\")\n",
    "        print(f\"Model - teacher_net saved at epoch {epoch}.\")\n",
    "        torch.save(net.state_dict(), f\"net_sr_iter_{epoch}.pth\")\n",
    "        print(f\"Model - net saved at epoch {epoch}.\")\n",
    "\n",
    "    # Validation on SR\n",
    "    teacher_net.eval()\n",
    "    sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "    print(f\"[SR Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "    # TensorBoard logging (per epoch)\n",
    "    writer.add_scalar(\"Metrics/Train_PSNR\", train_psnr, epoch)\n",
    "    writer.add_scalar(\"Metrics/Val_PSNR\", sr_val_psnr, epoch)\n",
    "    writer.add_scalar(\"Metrics/Val_SSIM\", sr_val_ssim, epoch)\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    model_path = f\"teacher_net_sr_{version}.pth\"\n",
    "    print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, sr_val_psnr, sr_val_ssim, model_path)\n",
    "\n",
    "    # Learning rate scheduler based on SR validation PSNR\n",
    "    scheduler.step(sr_val_psnr)\n",
    "\n",
    "    # Save best model\n",
    "    if sr_val_psnr >= best_psnr:\n",
    "        best_model_path = f\"teacher_net_sr_best_{version}.pth\"\n",
    "        torch.save(teacher_net.state_dict(), best_model_path)\n",
    "        best_psnr = sr_val_psnr\n",
    "\n",
    "    train_psnr_prev = train_psnr\n",
    "\n",
    "# Final save\n",
    "torch.save(teacher_net.state_dict(), f\"teacher_net_sr_final_{epoch}.pth\")\n",
    "torch.save(net.state_dict(), f\"net_sr_final_{epoch}.pth\")\n",
    "print(f\"🏁 SR Training completed in {(time.time() - total_start_time)/60:.2f} minutes.\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.205467Z",
     "iopub.status.idle": "2025-04-21T16:01:55.205773Z",
     "shell.execute_reply": "2025-04-21T16:01:55.205660Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.205648Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-04-12T13:10:14.625824",
     "status": "running"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# global_iter = 0\n",
    "# total_start_time = time.time()\n",
    "\n",
    "# # Joint optimizer for net and teacher_net\n",
    "# optimizer = torch.optim.Adam(list(net.parameters()) + list(teacher_net.parameters()), lr=learning_rate)\n",
    "\n",
    "# # Initialize ReduceLROnPlateau scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_start_time = time.time()\n",
    "#     psnr_list = []\n",
    "\n",
    "#     net.train()\n",
    "#     if sr_enabled:\n",
    "#         teacher_net.train()\n",
    "\n",
    "#     train_loader_tqdm = tqdm(train_data_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "#     for batch_id, (haze, gt) in enumerate(train_loader_tqdm):\n",
    "#         haze, gt = haze.to(device), gt.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass - student\n",
    "#         dehaze, base, s = net(haze)\n",
    "#         s1, s2, s3, s4 = s\n",
    "\n",
    "#         # Student losses\n",
    "#         base_loss = F.smooth_l1_loss(base, gt)\n",
    "#         smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "#         perceptual_loss = loss_network(dehaze, gt)\n",
    "#         total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "\n",
    "#         # Teacher SR logic\n",
    "#         if sr_enabled:\n",
    "#             try:\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             except StopIteration:\n",
    "#                 sr_iter = iter(sr_train_loader)\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n",
    "\n",
    "#             # Teacher forward\n",
    "#             sr_out, _, t = teacher_net(sr_lr)\n",
    "\n",
    "#             sr_loss = F.l1_loss(sr_out, sr_hr)\n",
    "#             total_loss += sr_loss\n",
    "\n",
    "#             s_loss = ssfm_loss(s, t)\n",
    "#             total_loss += s_loss\n",
    "\n",
    "#             distillation_loss = fam(dehaze, sr_out)\n",
    "#             total_loss += distillation_loss\n",
    "\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         psnr_list.extend(to_psnr(dehaze, gt))\n",
    "#         train_loader_tqdm.set_postfix(loss=total_loss.item(), iter=global_iter)\n",
    "#         global_iter += 1\n",
    "\n",
    "#     train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "#     # Save model every 5 epochs\n",
    "#     if epoch % 5 == 0:\n",
    "#         iter_model_path = f\"net_haze_iter_{epoch}.pth\"\n",
    "#         torch.save(net.state_dict(), iter_model_path)\n",
    "#         print(f\"Model - net saved in epoch {epoch}.\")\n",
    "#         if sr_enabled:\n",
    "#             iter_model_path = f\"teacher_net_haze_iter_{epoch}.pth\"\n",
    "#             torch.save(teacher_net.state_dict(), iter_model_path)\n",
    "#             print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "#     # Validation\n",
    "#     net.eval()\n",
    "#     val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "#     if sr_enabled:\n",
    "#         sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#         print(f\"[SR Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "#     epoch_duration = time.time() - epoch_start_time\n",
    "#     model_path = f\"net_haze_{version}.pth\"\n",
    "#     print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "#     elapsed = time.time() - total_start_time\n",
    "#     epochs_left = num_epochs - (epoch + 1)\n",
    "#     avg_epoch_time = elapsed / (epoch + 1)\n",
    "#     eta = avg_epoch_time * epochs_left\n",
    "#     print(f\"✅ Epoch [{epoch+1}/{num_epochs}] completed in {epoch_duration:.2f}s — ETA for {epochs_left} more: ~{eta/60:.2f} min\")\n",
    "\n",
    "#     # Adjust the learning rate based on validation PSNR\n",
    "#     scheduler.step(val_psnr)\n",
    "\n",
    "#     # Best model save\n",
    "#     if val_psnr >= best_psnr:\n",
    "#         best_model_path = f\"net_haze_best_{version}.pth\"\n",
    "#         torch.save(net.state_dict(), best_model_path)\n",
    "#         best_psnr = val_psnr\n",
    "\n",
    "#     train_psnr_prev = train_psnr\n",
    "\n",
    "# # Final save\n",
    "# final_path = f\"net_final_{epoch}.pth\"\n",
    "# torch.save(net.state_dict(), final_path)\n",
    "# print(f\"Model - net saved in epoch {epoch}.\")\n",
    "# if sr_enabled:\n",
    "#     final_path = f\"teacher_net_final_{epoch}.pth\"\n",
    "#     torch.save(teacher_net.state_dict(), final_path)\n",
    "#     print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "# total_time = time.time() - total_start_time\n",
    "# print(f\"🏁 Training completed in {total_time/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Final Validation --- #\n",
    "# net.eval()\n",
    "# val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "# print(f\"[Dehazing Final Val] PSNR: {val_psnr:.2f}, SSIM: {val_ssim:.4f}\")\n",
    "# if sr_enabled:\n",
    "#     sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#     print(f\"[SR Final Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.206947Z",
     "iopub.status.idle": "2025-04-21T16:01:55.207230Z",
     "shell.execute_reply": "2025-04-21T16:01:55.207122Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.207107Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Initialize model\n",
    "# # model_path = \"/kaggle/input/rdb-and-transformer/pytorch/default/1/formernewnh_final_49.pth\"\n",
    "# # model_path = \"/kaggle/input/reside-dehaze/pytorch/default/3/formernewreside_haze_best_0.pth\"\n",
    "# # model = DehazingNet().to(device)\n",
    "# # model = SR_model(upscale_factor=1).to(device)\n",
    "# # net.load_state_dict(torch.load(model_path, map_location=device))\n",
    "# net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.208619Z",
     "iopub.status.idle": "2025-04-21T16:01:55.209247Z",
     "shell.execute_reply": "2025-04-21T16:01:55.209064Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.209046Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # LOAD TEST DATA\n",
    "# # -----------------------------\n",
    "# test_hazy_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/hazy\"\n",
    "# test_gt_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/GT\"\n",
    "# # test_hazy_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN\"\n",
    "# # test_gt_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT\"\n",
    "\n",
    "# hazy_images = sorted(glob.glob(os.path.join(test_hazy_dir, \"*.*\")))\n",
    "# gt_images = sorted(glob.glob(os.path.join(test_gt_dir, \"*.*\")))\n",
    "\n",
    "# transform = Compose([\n",
    "#     ToTensor(),\n",
    "#     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# to_pil = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.210801Z",
     "iopub.status.idle": "2025-04-21T16:01:55.211425Z",
     "shell.execute_reply": "2025-04-21T16:01:55.211229Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.211212Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [11, 12, 13,14]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.212728Z",
     "iopub.status.idle": "2025-04-21T16:01:55.213012Z",
     "shell.execute_reply": "2025-04-21T16:01:55.212903Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.212889Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [70, 75, 90, 100]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.214998Z",
     "iopub.status.idle": "2025-04-21T16:01:55.215653Z",
     "shell.execute_reply": "2025-04-21T16:01:55.215495Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.215481Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [1, 3, 5]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 937211,
     "sourceId": 1587463,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2813430,
     "sourceId": 4853613,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6456606,
     "sourceId": 10417877,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6464114,
     "sourceId": 10443410,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 222875724,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 268224,
     "modelInstanceId": 246650,
     "sourceId": 287852,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 288973,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 297672,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 299643,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-12T13:09:18.306161",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
