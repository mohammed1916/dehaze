{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cceec4f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-16T17:01:13.302286Z",
     "iopub.status.busy": "2025-02-16T17:01:13.302003Z",
     "iopub.status.idle": "2025-02-16T17:01:24.035320Z",
     "shell.execute_reply": "2025-02-16T17:01:24.034512Z"
    },
    "papermill": {
     "duration": 10.74105,
     "end_time": "2025-02-16T17:01:24.036816",
     "exception": false,
     "start_time": "2025-02-16T17:01:13.295766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "from timm.models.layers import to_2tuple, trunc_normal_\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "class RLN(nn.Module):\n",
    "\tr\"\"\"Revised LayerNorm\"\"\"\n",
    "\tdef __init__(self, dim, eps=1e-5, detach_grad=False):\n",
    "\t\tsuper(RLN, self).__init__()\n",
    "\t\tself.eps = eps\n",
    "\t\tself.detach_grad = detach_grad\n",
    "\n",
    "\t\tself.weight = nn.Parameter(torch.ones((1, dim, 1, 1)))\n",
    "\t\tself.bias = nn.Parameter(torch.zeros((1, dim, 1, 1)))\n",
    "\n",
    "\t\tself.meta1 = nn.Conv2d(1, dim, 1)\n",
    "\t\tself.meta2 = nn.Conv2d(1, dim, 1)\n",
    "\n",
    "\t\ttrunc_normal_(self.meta1.weight, std=.02)\n",
    "\t\tnn.init.constant_(self.meta1.bias, 1)\n",
    "\n",
    "\t\ttrunc_normal_(self.meta2.weight, std=.02)\n",
    "\t\tnn.init.constant_(self.meta2.bias, 0)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tmean = torch.mean(input, dim=(1, 2, 3), keepdim=True)\n",
    "\t\tstd = torch.sqrt((input - mean).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
    "\n",
    "\t\tnormalized_input = (input - mean) / std\n",
    "\n",
    "\t\tif self.detach_grad:\n",
    "\t\t\trescale, rebias = self.meta1(std.detach()), self.meta2(mean.detach())\n",
    "\t\telse:\n",
    "\t\t\trescale, rebias = self.meta1(std), self.meta2(mean)\n",
    "\n",
    "\t\tout = normalized_input * self.weight + self.bias\n",
    "\t\treturn out, rescale, rebias\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "\tdef __init__(self, network_depth, in_features, hidden_features=None, out_features=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tout_features = out_features or in_features\n",
    "\t\thidden_features = hidden_features or in_features\n",
    "\n",
    "\t\tself.network_depth = network_depth\n",
    "\n",
    "\t\tself.mlp = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_features, hidden_features, 1),\n",
    "\t\t\tnn.ReLU(True),\n",
    "\t\t\tnn.Conv2d(hidden_features, out_features, 1)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.apply(self._init_weights)\n",
    "\n",
    "\tdef _init_weights(self, m):\n",
    "\t\tif isinstance(m, nn.Conv2d):\n",
    "\t\t\tgain = (8 * self.network_depth) ** (-1/4)\n",
    "\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n",
    "\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\t\t\ttrunc_normal_(m.weight, std=std)\n",
    "\t\t\tif m.bias !=   None:\n",
    "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.mlp(x)\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "\tB, H, W, C = x.shape\n",
    "\tx = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "\twindows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, C)\n",
    "\treturn windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "\tB = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "\tx = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "\tx = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "\treturn x\n",
    "\n",
    "\n",
    "def get_relative_positions(window_size):\n",
    "\tcoords_h = torch.arange(window_size)\n",
    "\tcoords_w = torch.arange(window_size)\n",
    "\n",
    "\tcoords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "\tcoords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "\trelative_positions = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "\n",
    "\trelative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "\trelative_positions_log  = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "\treturn relative_positions_log\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "\tdef __init__(self, dim, window_size, num_heads):\n",
    "\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.window_size = window_size  # Wh, Ww\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\thead_dim = dim // num_heads\n",
    "\t\tself.scale = head_dim ** -0.5\n",
    "\n",
    "\t\trelative_positions = get_relative_positions(self.window_size)\n",
    "\t\tself.register_buffer(\"relative_positions\", relative_positions)\n",
    "\t\tself.meta = nn.Sequential(\n",
    "\t\t\tnn.Linear(2, 256, bias=True),\n",
    "\t\t\tnn.ReLU(True),\n",
    "\t\t\tnn.Linear(256, num_heads, bias=True)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\tdef forward(self, qkv):\n",
    "\t\tB_, N, _ = qkv.shape\n",
    "\n",
    "\t\tqkv = qkv.reshape(B_, N, 3, self.num_heads, self.dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "\t\tq, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "\t\tq = q * self.scale\n",
    "\t\tattn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "\t\trelative_position_bias = self.meta(self.relative_positions)\n",
    "\t\trelative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "\t\tattn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "\t\tattn = self.softmax(attn)\n",
    "\n",
    "\t\tx = (attn @ v).transpose(1, 2).reshape(B_, N, self.dim)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, num_heads, window_size, shift_size, use_attn=False, conv_type=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.head_dim = int(dim // num_heads)\n",
    "\t\tself.num_heads = num_heads\n",
    "\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.shift_size = shift_size\n",
    "\n",
    "\t\tself.network_depth = network_depth\n",
    "\t\tself.use_attn = use_attn\n",
    "\t\tself.conv_type = conv_type\n",
    "\n",
    "\t\tif self.conv_type == 'Conv':\n",
    "\t\t\tself.conv = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "\t\t\t)\n",
    "\n",
    "\t\tif self.conv_type == 'DWConv':\n",
    "\t\t\tself.conv = nn.Conv2d(dim, dim, kernel_size=5, padding=2, groups=dim, padding_mode='reflect')\n",
    "\n",
    "\t\tif self.conv_type == 'DWConv' or self.use_attn:\n",
    "\t\t\tself.V = nn.Conv2d(dim, dim, 1)\n",
    "\t\t\tself.proj = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "\t\tif self.use_attn:\n",
    "\t\t\tself.QK = nn.Conv2d(dim, dim * 2, 1)\n",
    "\t\t\tself.attn = WindowAttention(dim, window_size, num_heads)\n",
    "\n",
    "\t\tself.apply(self._init_weights)\n",
    "\n",
    "\tdef _init_weights(self, m):\n",
    "\t\tif isinstance(m, nn.Conv2d):\n",
    "\t\t\tw_shape = m.weight.shape\n",
    "\t\t\t\n",
    "\t\t\tif w_shape[0] == self.dim * 2:\t# QK\n",
    "\t\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n",
    "\t\t\t\tstd = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\t\t\t\ttrunc_normal_(m.weight, std=std)\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t\tgain = (8 * self.network_depth) ** (-1/4)\n",
    "\t\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n",
    "\t\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\t\t\t\ttrunc_normal_(m.weight, std=std)\n",
    "\n",
    "\t\t\tif m.bias !=  None:\n",
    "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
    "\n",
    "\tdef check_size(self, x, shift=False):\n",
    "\t\t_, _, h, w = x.size()\n",
    "\t\tmod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
    "\t\tmod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
    "\n",
    "\t\tif shift:\n",
    "\t\t\tx = F.pad(x, (self.shift_size, (self.window_size-self.shift_size+mod_pad_w) % self.window_size,\n",
    "\t\t\t\t\t\t  self.shift_size, (self.window_size-self.shift_size+mod_pad_h) % self.window_size), mode='reflect')\n",
    "\t\telse:\n",
    "\t\t\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tB, C, H, W = X.shape\n",
    "\n",
    "\t\tif self.conv_type == 'DWConv' or self.use_attn:\n",
    "\t\t\tV = self.V(X)\n",
    "\t\t#print(self.use_attn)\n",
    "\t\tif self.use_attn:\n",
    "\t\t\t#print('attention')      \n",
    "\t\t\tQK = self.QK(X)\n",
    "\t\t\tQKV = torch.cat([QK, V], dim=1)\n",
    "\n",
    "\t\t\t# shift\n",
    "\t\t\tshifted_QKV = self.check_size(QKV, self.shift_size > 0)\n",
    "\t\t\tHt, Wt = shifted_QKV.shape[2:]\n",
    "\n",
    "\t\t\t# partition windows\n",
    "\t\t\tshifted_QKV = shifted_QKV.permute(0, 2, 3, 1)\n",
    "\t\t\tqkv = window_partition(shifted_QKV, self.window_size)  # nW*B, window_size**2, C\n",
    "\n",
    "\t\t\tattn_windows = self.attn(qkv)\n",
    "\n",
    "\t\t\t# merge windows\n",
    "\t\t\tshifted_out = window_reverse(attn_windows, self.window_size, Ht, Wt)  # B H' W' C\n",
    "\n",
    "\t\t\t# reverse cyclic shift\n",
    "\t\t\tout = shifted_out[:, self.shift_size:(self.shift_size+H), self.shift_size:(self.shift_size+W), :]\n",
    "\t\t\tattn_out = out.permute(0, 3, 1, 2)\n",
    "\n",
    "\t\t\tif self.conv_type in ['Conv', 'DWConv']:\n",
    "\t\t\t\tconv_out = self.conv(V)\n",
    "\t\t\t\tout = self.proj(conv_out + attn_out)\n",
    "\t\t\telse:\n",
    "\t\t\t\tout = self.proj(attn_out)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tif self.conv_type == 'Conv':\n",
    "\t\t\t\tout = self.conv(X)\t\t\t\t# no attention and use conv, no projection\n",
    "\t\t\telif self.conv_type == 'DWConv':\n",
    "\t\t\t\tout = self.proj(self.conv(V))\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, num_heads, mlp_ratio=4.,\n",
    "\t\t\t\t norm_layer=nn.LayerNorm, mlp_norm=False,\n",
    "\t\t\t\t window_size=8, shift_size=0, use_attn=True, conv_type=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.use_attn = use_attn\n",
    "\t\tself.mlp_norm = mlp_norm\n",
    "\n",
    "\t\tself.norm1 = norm_layer(dim) if use_attn else nn.Identity()\n",
    "\t\tself.attn = Attention(network_depth, dim, num_heads=num_heads, window_size=window_size,\n",
    "\t\t\t\t\t\t\t  shift_size=shift_size, use_attn=use_attn, conv_type=conv_type)\n",
    "\n",
    "\t\tself.norm2 = norm_layer(dim) if use_attn and mlp_norm else nn.Identity()\n",
    "\t\tself.mlp = Mlp(network_depth, dim, hidden_features=int(dim * mlp_ratio))\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tidentity = x\n",
    "\t\tif self.use_attn: x, rescale, rebias = self.norm1(x)\n",
    "\t\tx = self.attn(x)\n",
    "\t\tif self.use_attn: x = x * rescale + rebias\n",
    "\t\tx = identity + x\n",
    "\n",
    "\t\tidentity = x\n",
    "\t\tif self.use_attn and self.mlp_norm: x, rescale, rebias = self.norm2(x)\n",
    "\t\tx = self.mlp(x)\n",
    "\t\tif self.use_attn and self.mlp_norm: x = x * rescale + rebias\n",
    "\t\tx = identity + x\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, depth, num_heads, mlp_ratio=4.,\n",
    "\t\t\t\t norm_layer=nn.LayerNorm, window_size=8,\n",
    "\t\t\t\t attn_ratio=0., attn_loc='last', conv_type=None):\n",
    "\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.depth = depth\n",
    "\n",
    "\t\tattn_depth = attn_ratio * depth\n",
    "\n",
    "\t\tif attn_loc == 'last':\n",
    "\t\t\tuse_attns = [i >= depth-attn_depth for i in range(depth)]\n",
    "\t\telif attn_loc == 'first':\n",
    "\t\t\tuse_attns = [i < attn_depth for i in range(depth)]\n",
    "\t\telif attn_loc == 'middle':\n",
    "\t\t\tuse_attns = [i >= (depth-attn_depth)//2 and i < (depth+attn_depth)//2 for i in range(depth)]\n",
    "\n",
    "\t\t# build blocks\n",
    "\t\tself.blocks = nn.ModuleList([\n",
    "\t\t\tTransformerBlock(network_depth=network_depth,\n",
    "\t\t\t\t\t\t\t dim=dim, \n",
    "\t\t\t\t\t\t\t num_heads=num_heads,\n",
    "\t\t\t\t\t\t\t mlp_ratio=mlp_ratio,\n",
    "\t\t\t\t\t\t\t norm_layer=norm_layer,\n",
    "\t\t\t\t\t\t\t window_size=window_size,\n",
    "\t\t\t\t\t\t\t shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "\t\t\t\t\t\t\t use_attn=use_attns[i], conv_type=conv_type)\n",
    "\t\t\tfor i in range(depth)])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor blk in self.blocks:\n",
    "\t\t\tx = blk(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\tdef __init__(self, patch_size=4, in_chans=3, embed_dim=96, kernel_size=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.in_chans = in_chans\n",
    "\t\tself.embed_dim = embed_dim\n",
    "\n",
    "\t\tif kernel_size is None:\n",
    "\t\t\tkernel_size = patch_size\n",
    "\n",
    "\t\tself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "\t\t\t\t\t\t\t  padding=(kernel_size-patch_size+1)//2, padding_mode='reflect')\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.proj(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class PatchUnEmbed(nn.Module):\n",
    "\tdef __init__(self, patch_size=4, out_chans=3, embed_dim=96, kernel_size=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.out_chans = out_chans\n",
    "\t\tself.embed_dim = embed_dim\n",
    "\n",
    "\t\tif kernel_size is None:\n",
    "\t\t\tkernel_size = 1\n",
    "\n",
    "\t\tself.proj = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(embed_dim, out_chans*patch_size**2, kernel_size=kernel_size,\n",
    "\t\t\t\t\t  padding=kernel_size//2, padding_mode='reflect'),\n",
    "\t\t\tnn.PixelShuffle(patch_size)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.proj(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class SKFusion(nn.Module):\n",
    "\tdef __init__(self, dim, height=2, reduction=8):\n",
    "\t\tsuper(SKFusion, self).__init__()\n",
    "\t\t\n",
    "\t\tself.height = height\n",
    "\t\td = max(int(dim/reduction), 4)\n",
    "\t\t\n",
    "\t\tself.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\t\tself.mlp = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(dim, d, 1, bias=False), \n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(d, dim*height, 1, bias=False)\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\tdef forward(self, in_feats):\n",
    "\t\tB, C, H, W = in_feats[0].shape\n",
    "\t\t\n",
    "\t\tin_feats = torch.cat(in_feats, dim=1)\n",
    "\t\tin_feats = in_feats.view(B, self.height, C, H, W)\n",
    "\t\t\n",
    "\t\tfeats_sum = torch.sum(in_feats, dim=1)\n",
    "\t\tattn = self.mlp(self.avg_pool(feats_sum))\n",
    "\t\tattn = self.softmax(attn.view(B, self.height, C, 1, 1))\n",
    "\n",
    "\t\tout = torch.sum(in_feats*attn, dim=1)\n",
    "\t\treturn out      \n",
    "\n",
    "\n",
    "class DehazeFormer(nn.Module):\n",
    "\tdef __init__(self, in_chans=3, out_chans=4, window_size=8,\n",
    "\t\t\t\t embed_dims=[24, 48, 96, 48, 24],\n",
    "\t\t\t\t mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\t\t\t depths=[16, 16, 16, 8, 8],\n",
    "\t\t\t\t num_heads=[2, 4, 6, 1, 1],\n",
    "\t\t\t\t attn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\t\t\t conv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "\t\t\t\t norm_layer=[RLN, RLN, RLN, RLN, RLN]):\n",
    "\t\tsuper(DehazeFormer, self).__init__()\n",
    "\n",
    "\t\t# setting\n",
    "\t\tself.patch_size = 4\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.mlp_ratios = mlp_ratios\n",
    "\n",
    "\t\t# split image into non-overlapping patches\n",
    "\t\tself.patch_embed = PatchEmbed(\n",
    "\t\t\tpatch_size=1, in_chans=in_chans, embed_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "\t\t# backbone\n",
    "\t\tself.layer1 = BasicLayer(network_depth=sum(depths), dim=embed_dims[0], depth=depths[0],\n",
    "\t\t\t\t\t   \t\t\t num_heads=num_heads[0], mlp_ratio=mlp_ratios[0],\n",
    "\t\t\t\t\t   \t\t\t norm_layer=norm_layer[0], window_size=window_size,\n",
    "\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio[0], attn_loc='last', conv_type=conv_type[0])\n",
    "\n",
    "\t\tself.patch_merge1 = PatchEmbed(\n",
    "\t\t\tpatch_size=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])\n",
    "\n",
    "\t\tself.skip1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "\n",
    "\t\tself.layer2 = BasicLayer(network_depth=sum(depths), dim=embed_dims[1], depth=depths[1],\n",
    "\t\t\t\t\t\t\t\t num_heads=num_heads[1], mlp_ratio=mlp_ratios[1],\n",
    "\t\t\t\t\t\t\t\t norm_layer=norm_layer[1], window_size=window_size,\n",
    "\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[1], attn_loc='last', conv_type=conv_type[1])\n",
    "\n",
    "\t\tself.patch_merge2 = PatchEmbed(\n",
    "\t\t\tpatch_size=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])\n",
    "\n",
    "\t\tself.skip2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "\n",
    "\t\tself.layer3 = BasicLayer(network_depth=sum(depths), dim=embed_dims[2], depth=depths[2],\n",
    "\t\t\t\t\t\t\t\t num_heads=num_heads[2], mlp_ratio=mlp_ratios[2],\n",
    "\t\t\t\t\t\t\t\t norm_layer=norm_layer[2], window_size=window_size,\n",
    "\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[2], attn_loc='last', conv_type=conv_type[2])\n",
    "\n",
    "\t\tself.patch_split1 = PatchUnEmbed(\n",
    "\t\t\tpatch_size=2, out_chans=embed_dims[3], embed_dim=embed_dims[2])\n",
    "\n",
    "\t\tassert embed_dims[1] == embed_dims[3]\n",
    "\t\tself.fusion1 = SKFusion(embed_dims[3])\n",
    "\n",
    "\t\tself.layer4 = BasicLayer(network_depth=sum(depths), dim=embed_dims[3], depth=depths[3],\n",
    "\t\t\t\t\t\t\t\t num_heads=num_heads[3], mlp_ratio=mlp_ratios[3],\n",
    "\t\t\t\t\t\t\t\t norm_layer=norm_layer[3], window_size=window_size,\n",
    "\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[3], attn_loc='last', conv_type=conv_type[3])\n",
    "\n",
    "\t\tself.patch_split2 = PatchUnEmbed(\n",
    "\t\t\tpatch_size=2, out_chans=embed_dims[4], embed_dim=embed_dims[3])\n",
    "\n",
    "\t\tassert embed_dims[0] == embed_dims[4]\n",
    "\t\tself.fusion2 = SKFusion(embed_dims[4])\t\t\t\n",
    "\n",
    "\t\tself.layer5 = BasicLayer(network_depth=sum(depths), dim=embed_dims[4], depth=depths[4],\n",
    "\t\t\t\t\t   \t\t\t num_heads=num_heads[4], mlp_ratio=mlp_ratios[4],\n",
    "\t\t\t\t\t   \t\t\t norm_layer=norm_layer[4], window_size=window_size,\n",
    "\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio[4], attn_loc='last', conv_type=conv_type[4])\n",
    "\n",
    "\t\t# merge non-overlapping patches into image\n",
    "\t\tself.patch_unembed = PatchUnEmbed(\n",
    "\t\t\tpatch_size=1, out_chans=out_chans, embed_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "\n",
    "\tdef check_image_size(self, x):\n",
    "\t\t# NOTE: for I2I test\n",
    "\t\t_, _, h, w = x.size()\n",
    "\t\tmod_pad_h = (self.patch_size - h % self.patch_size) % self.patch_size\n",
    "\t\tmod_pad_w = (self.patch_size - w % self.patch_size) % self.patch_size\n",
    "\t\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward_features(self, x):\n",
    "\t\tx = self.patch_embed(x)\n",
    "\t\tx = self.layer1(x)\n",
    "\t\tskip1 = x\n",
    "\n",
    "\t\tx = self.patch_merge1(x)\n",
    "\t\tx = self.layer2(x)\n",
    "\t\tskip2 = x\n",
    "\n",
    "\t\tx = self.patch_merge2(x)\n",
    "\t\tx = self.layer3(x)\n",
    "\t\tx = self.patch_split1(x)\n",
    "\n",
    "\t\tx = self.fusion1([x, self.skip2(skip2)]) + x\n",
    "\t\tx = self.layer4(x)\n",
    "\t\tx = self.patch_split2(x)\n",
    "\n",
    "\t\tx = self.fusion2([x, self.skip1(skip1)]) + x\n",
    "\t\tx = self.layer5(x)\n",
    "\t\tx = self.patch_unembed(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tH, W = x.shape[2:]\n",
    "\t\tx = self.check_image_size(x)\n",
    "\n",
    "\t\tfeat = self.forward_features(x)\n",
    "\t\tK, B = torch.split(feat, (1, 3), dim=1)\n",
    "\n",
    "\t\tx = K * x - B + x\n",
    "\t\tx = x[:, :, :H, :W]\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def dehazeformer_t():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[4, 4, 4, 2, 2],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[0, 1/2, 1, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_s():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[8, 8, 8, 4, 4],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_b():\n",
    "    return DehazeFormer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[16, 16, 16, 8, 8],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_d():\n",
    "    return DehazeFormer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[32, 32, 32, 16, 16],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_w():\n",
    "    return DehazeFormer(\n",
    "        embed_dims=[48, 96, 192, 96, 48],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[16, 16, 16, 8, 8],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_m():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[12, 12, 12, 6, 6],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['Conv', 'Conv', 'Conv', 'Conv', 'Conv'])\n",
    "\n",
    "\n",
    "def dehazeformer_l():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[48, 96, 192, 96, 48],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[16, 16, 16, 12, 12],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['Conv', 'Conv', 'Conv', 'Conv', 'Conv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef98f1d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:01:24.046447Z",
     "iopub.status.busy": "2025-02-16T17:01:24.046214Z",
     "iopub.status.idle": "2025-02-16T17:01:24.074261Z",
     "shell.execute_reply": "2025-02-16T17:01:24.073627Z"
    },
    "papermill": {
     "duration": 0.034147,
     "end_time": "2025-02-16T17:01:24.075483",
     "exception": false,
     "start_time": "2025-02-16T17:01:24.041336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, depth, num_heads, mlp_ratio=4.,\n",
    "\t\t\t\t norm_layer=nn.LayerNorm, window_size=8,\n",
    "\t\t\t\t attn_ratio=0., attn_loc='last', conv_type=None):\n",
    "\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.depth = depth\n",
    "\t\tself.gf=FastGuidedFilter(r=1)\n",
    "\t\tself.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "    \n",
    "\t\tdepth_rate=24\n",
    "\t\tkernel_size=3\n",
    "\t\tin_channels=3\n",
    "\t\tself.conv_out = nn.Conv2d(depth_rate*2, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "\t\tself.relu1=nn.ReLU(inplace=True)\n",
    "\t\tself.relu2=nn.ReLU(inplace=True)\n",
    "\t\tself.norm1=AdaptiveInstanceNorm(depth_rate)\n",
    "\t\tself.norm2=AdaptiveInstanceNorm(depth_rate) \n",
    "\t\tattn_depth = attn_ratio * depth\n",
    "\t\t#print(attn_depth,attn_ratio,depth)\n",
    "\t\tif attn_loc == 'last':\n",
    "\t\t\tuse_attns = [i >= depth-attn_depth for i in range(depth)]\n",
    "\t\telif attn_loc == 'first':\n",
    "\t\t\tuse_attns = [i < attn_depth for i in range(depth)]\n",
    "\t\telif attn_loc == 'middle':\n",
    "\t\t\tuse_attns = [i >= (depth-attn_depth)//2 and i < (depth+attn_depth)//2 for i in range(depth)]\n",
    "\n",
    "\t\t# build blocks\n",
    "\t\tself.blocks = nn.ModuleList([\n",
    "\t\t\tTransformerBlock(network_depth=network_depth,\n",
    "\t\t\t\t\t\t\t dim=dim, \n",
    "\t\t\t\t\t\t\t num_heads=num_heads,\n",
    "\t\t\t\t\t\t\t mlp_ratio=mlp_ratio,\n",
    "\t\t\t\t\t\t\t norm_layer=norm_layer,\n",
    "\t\t\t\t\t\t\t window_size=window_size,\n",
    "\t\t\t\t\t\t\t shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "\t\t\t\t\t\t\t use_attn=use_attns[i], conv_type=conv_type)\n",
    "\t\t\tfor i in range(depth)])\n",
    "\n",
    "\tdef forward(self, x_hr):\n",
    "\t\tx_lr = self.downsample(x_hr)\n",
    "   \n",
    "\t\tx_lr_new=self.norm1(x_lr)\n",
    "\t\tx_lr_new=self.relu1( x_lr_new)\n",
    "\t\tfor blc in self.blocks:\n",
    "   \n",
    "\t\t    x_lr_new = blc(x_lr_new)\n",
    "    \n",
    "\t\tg_hr= self.gf(x_lr, x_lr_new, x_hr)\n",
    "\t\tgx_cat=torch.cat([g_hr,x_hr],1)\n",
    "\t\tg_hr=self.conv_out(gx_cat)\n",
    "\t\tg_hr=self.norm2(g_hr)\n",
    "\t\tg_hr=self.relu2( g_hr)\n",
    "\t\tx=g_hr+x_hr\n",
    "   \n",
    "\t\treturn g_hr\n",
    "   \n",
    "\n",
    "\n",
    "class DeepGuidedFilterFormer(nn.Module):\n",
    "    def __init__(self,  radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        norm = AdaptiveInstanceNorm\n",
    "        depth_rate=24\n",
    "        kernel_size=3\n",
    "        in_channels=3\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.relu1=nn.ReLU(inplace=True)\n",
    "        self.block_num=3\n",
    "        network_depth=50\n",
    "        dim=depth_rate\n",
    "        mlp_ratio=2.0\n",
    "        norm_layer=RLN\n",
    "        window_size=16\n",
    "        conv_type='Conv'\n",
    "        depth=4\n",
    "        num_heads=4\n",
    "        attn_ratio=1/4\n",
    "        \n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "                   BasicBlock(network_depth=network_depth, dim=dim, depth=depth,\n",
    "\t\t\t\t\t   \t\t\t num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "\t\t\t\t\t   \t\t\t norm_layer=norm_layer, window_size=window_size,\n",
    "\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio, attn_loc='last', conv_type=conv_type)\n",
    "\t\t\t             for i in range(self.block_num)])\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x_hr):\n",
    "        x_hr=self.conv_in(x_hr)\n",
    "        #x_hr=self.relu1(x_hr)\n",
    "        #pixelshuffle_ratio=2\n",
    "        # Unpixelshuffle\n",
    "        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n",
    "        \n",
    "        for blc in self.blocks:\n",
    "            x_hr=blc(x_hr)\n",
    "            \n",
    "        x_hr=self.conv_out(x_hr)\n",
    "        # Pixelshuffle\n",
    "        #y_lr = F.pixel_shuffle(\n",
    "           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n",
    "        #)\n",
    "\n",
    "        return x_hr\n",
    "           \n",
    "   \n",
    "   \n",
    "class ConvGuidedFilter(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/wuhuikai/DeepGuidedFilter\n",
    "    \"\"\"\n",
    "    def __init__(self, radius=1, norm=nn.BatchNorm2d, conv_a_kernel_size: int = 1):\n",
    "        super(ConvGuidedFilter, self).__init__()\n",
    "\n",
    "        self.box_filter = nn.Conv2d(\n",
    "            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n",
    "        )\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                6,\n",
    "                32,\n",
    "                kernel_size=conv_a_kernel_size,\n",
    "                padding=conv_a_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                32,\n",
    "                kernel_size=conv_a_kernel_size,\n",
    "                padding=conv_a_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                3,\n",
    "                kernel_size=conv_a_kernel_size,\n",
    "                padding=conv_a_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.box_filter.weight.data[...] = 1.0\n",
    "\n",
    "    def forward(self, x_lr, y_lr, x_hr):\n",
    "        _, _, h_lrx, w_lrx = x_lr.size()\n",
    "        _, _, h_hrx, w_hrx = x_hr.size()\n",
    "\n",
    "        N = self.box_filter(x_lr.data.new().resize_((1, 3, h_lrx, w_lrx)).fill_(1.0))\n",
    "        ## mean_x\n",
    "        mean_x = self.box_filter(x_lr) / N\n",
    "        ## mean_y\n",
    "        mean_y = self.box_filter(y_lr) / N\n",
    "        ## cov_xy\n",
    "        cov_xy = self.box_filter(x_lr * y_lr) / N - mean_x * mean_y\n",
    "        ## var_x\n",
    "        var_x = self.box_filter(x_lr * x_lr) / N - mean_x * mean_x\n",
    "\n",
    "        ## A\n",
    "        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n",
    "        ## b\n",
    "        b = mean_y - A * mean_x\n",
    "\n",
    "        ## mean_A; mean_b\n",
    "        mean_A = F.interpolate(A, (h_hrx, w_hrx), mode=\"bilinear\", align_corners=True)\n",
    "        mean_b = F.interpolate(b, (h_hrx, w_hrx), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return mean_A * x_hr + mean_b\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class DeepGuideddetail(nn.Module):\n",
    "    def __init__(self,  radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        norm = AdaptiveInstanceNorm\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        #self.lr = dehazeformer_m()\n",
    "        kernel_size=3\n",
    "        depth_rate=16\n",
    "        in_channels=3\n",
    "        num_dense_layer=4\n",
    "        growth_rate=16\n",
    "        growth_rate=16\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.rdb1 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb2 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb3 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb4 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "\n",
    "        self.gf = ConvGuidedFilter(radius, norm=norm)\n",
    "\n",
    "        self.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x_hr):\n",
    "        x_lr = self.downsample(x_hr)\n",
    "        y_lr=self.conv_in(x_lr)\n",
    "        y_lr=self.rdb1(y_lr)\n",
    "        y_lr=self.rdb2(y_lr)\n",
    "        y_lr=self.rdb3(y_lr)\n",
    "        y_lr=self.rdb4(y_lr)\n",
    "        y_lr=self.conv_out(y_lr)\n",
    "        \n",
    "        #pixelshuffle_ratio=2\n",
    "        # Unpixelshuffle\n",
    "        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n",
    "        \n",
    "        #y_lr=self.lr(x_lr)\n",
    "        # Pixelshuffle\n",
    "        #y_lr = F.pixel_shuffle(\n",
    "           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n",
    "        #)\n",
    "\n",
    "        return F.tanh( self.gf(x_lr, y_lr, x_hr))\n",
    "                \n",
    "        \n",
    "\n",
    "class DeepGuidedall(nn.Module):\n",
    "    def __init__(self,  radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        norm = AdaptiveInstanceNorm\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        #self.lr = dehazeformer_m()\n",
    "        kernel_size=3\n",
    "        depth_rate=16\n",
    "        in_channels=3\n",
    "        num_dense_layer=4\n",
    "        growth_rate=16\n",
    "        growth_rate=16\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.rdb1 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb2 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb3 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb4 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "\n",
    "        self.gf = ConvGuidedFilter(radius, norm=norm)\n",
    "        self.lr = dehazeformer_m()\n",
    "\n",
    "        self.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "        self.upsample = nn.Upsample(\n",
    "            scale_factor=2, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x_hr):\n",
    "        x_lr = self.downsample(x_hr)\n",
    "        y_lr=self.conv_in(x_lr)\n",
    "        y_lr=self.rdb1(y_lr)\n",
    "        y_lr=self.rdb2(y_lr)\n",
    "        y_lr=self.rdb3(y_lr)\n",
    "        y_lr=self.rdb4(y_lr)\n",
    "        y_detail=self.conv_out(y_lr)\n",
    "        y_base=self.lr(x_lr)\n",
    "        y_lr=y_base+y_detail\n",
    "        y_base=self.upsample(y_base)\n",
    "        \n",
    "        #pixelshuffle_ratio=2\n",
    "        # Unpixelshuffle\n",
    "        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n",
    "        \n",
    "        #y_lr=self.lr(x_lr)\n",
    "        # Pixelshuffle\n",
    "        #y_lr = F.pixel_shuffle(\n",
    "           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n",
    "        #)\n",
    "\n",
    "        return F.tanh( self.gf(x_lr, y_lr, x_hr)), y_base   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepGuidednew(nn.Module):\n",
    "    def __init__(self,  radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        norm = AdaptiveInstanceNorm\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        #self.lr = dehazeformer_m()\n",
    "        kernel_size=3\n",
    "        depth_rate=16\n",
    "        in_channels=3\n",
    "        num_dense_layer=4\n",
    "        growth_rate=16\n",
    "        growth_rate=16\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.rdb1 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb2 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb3 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb4 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "\n",
    "        self.gf = ConvGuidedFilter(radius, norm=norm)\n",
    "        self.lr = dehazeformer_m()\n",
    "\n",
    "        self.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "        self.upsample = nn.Upsample(\n",
    "            scale_factor=2, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x_hr):\n",
    "        x_lr = self.downsample(x_hr)\n",
    "        y_lr=self.conv_in(x_lr)\n",
    "        y_lr=self.rdb1(y_lr)\n",
    "        y_lr=self.rdb2(y_lr)\n",
    "        y_lr=self.rdb3(y_lr)\n",
    "        y_lr=self.rdb4(y_lr)\n",
    "        y_detail=self.conv_out(y_lr)\n",
    "        y_base=self.lr(x_lr)\n",
    "        y_lr=y_base+y_detail\n",
    "        y_base=self.upsample(y_base)\n",
    "        \n",
    "        #pixelshuffle_ratio=2\n",
    "        # Unpixelshuffle\n",
    "        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n",
    "        \n",
    "        #y_lr=self.lr(x_lr)\n",
    "        # Pixelshuffle\n",
    "        #y_lr = F.pixel_shuffle(\n",
    "           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n",
    "        #)\n",
    "\n",
    "        return  self.gf(x_lr, y_lr, x_hr), y_base               \n",
    "        \n",
    "        \n",
    "\n",
    "class DeepAtrousGuidedFilter(nn.Module):\n",
    "    def __init__(self,  radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        norm = AdaptiveInstanceNorm\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        self.lr = dehazeformer_m()\n",
    "\n",
    "        self.gf = ConvGuidedFilter(radius, norm=norm)\n",
    "\n",
    "        self.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x_hr):\n",
    "        x_lr = self.downsample(x_hr)\n",
    "        #pixelshuffle_ratio=2\n",
    "        # Unpixelshuffle\n",
    "        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n",
    "        y_lr=self.lr(x_lr)\n",
    "        # Pixelshuffle\n",
    "        #y_lr = F.pixel_shuffle(\n",
    "           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n",
    "        #)\n",
    "\n",
    "        return F.tanh( self.gf(x_lr, y_lr, x_hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "763d4c13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:01:24.084839Z",
     "iopub.status.busy": "2025-02-16T17:01:24.084555Z",
     "iopub.status.idle": "2025-02-16T17:01:24.091733Z",
     "shell.execute_reply": "2025-02-16T17:01:24.090906Z"
    },
    "papermill": {
     "duration": 0.013147,
     "end_time": "2025-02-16T17:01:24.093020",
     "exception": false,
     "start_time": "2025-02-16T17:01:24.079873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AdaptiveInstanceNorm(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(AdaptiveInstanceNorm, self).__init__()\n",
    "\n",
    "        self.w_0 = nn.Parameter(torch.Tensor([1.0]))\n",
    "        self.w_1 = nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "        self.ins_norm = nn.InstanceNorm2d(n, momentum=0.999, eps=0.001, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_0 * x + self.w_1 * self.ins_norm(x)\n",
    "\n",
    "\n",
    "class PALayer(nn.Module):\n",
    "    def __init__(self, channel: int):\n",
    "        super(PALayer, self).__init__()\n",
    "        self.pa = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.pa(x)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel: int):\n",
    "        super(CALayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.ca = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.ca(y)\n",
    "        return x * y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc49f88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:01:24.101995Z",
     "iopub.status.busy": "2025-02-16T17:01:24.101746Z",
     "iopub.status.idle": "2025-02-16T17:01:24.111079Z",
     "shell.execute_reply": "2025-02-16T17:01:24.110247Z"
    },
    "papermill": {
     "duration": 0.015195,
     "end_time": "2025-02-16T17:01:24.112279",
     "exception": false,
     "start_time": "2025-02-16T17:01:24.097084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def unpixel_shuffle(feature, r: int = 1):\n",
    "    b, c, h, w = feature.shape\n",
    "    out_channel = c * (r ** 2)\n",
    "    out_h = h // r\n",
    "    out_w = w // r\n",
    "    feature_view = feature.contiguous().view(b, c, out_h, r, out_w, r)\n",
    "    feature_prime = (\n",
    "        feature_view.permute(0, 1, 3, 5, 2, 4)\n",
    "        .contiguous()\n",
    "        .view(b, out_channel, out_h, out_w)\n",
    "    )\n",
    "    return feature_prime\n",
    "\n",
    "\n",
    "def sample_patches(\n",
    "    inputs: torch.Tensor, patch_size: int = 3, stride: int = 2\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    :param inputs: the input feature maps, shape: (n, c, h, w).\n",
    "    :param patch_size: the spatial size of sampled patches\n",
    "    :param stride: the stride of sampling.\n",
    "    :return: extracted patches, shape: (n, c, patch_size, patch_size, n_patches).\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Patch sampler for feature maps.\n",
    "    Parameters\n",
    "    ---\n",
    "    inputs : torch.Tensor\n",
    "        \n",
    "    patch_size : int, optional\n",
    "       \n",
    "    stride : int, optional\n",
    "        \n",
    "    Returns\n",
    "    ---\n",
    "    patches : torch.Tensor\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    n, c, h, w = inputs.shape\n",
    "    patches = (\n",
    "        inputs.unfold(2, patch_size, stride)\n",
    "        .unfold(3, patch_size, stride)\n",
    "        .reshape(n, c, -1, patch_size, patch_size)\n",
    "        .permute(0, 1, 3, 4, 2)\n",
    "    )\n",
    "    return patches\n",
    "\n",
    "\n",
    "def chop_patches(\n",
    "    img: torch.Tensor, patch_size_h: int = 256, patch_size_w: int = 512\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    :param inputs: the input feature maps, shape: (n, c, h, w).\n",
    "    :param patch_size: the spatial size of sampled patches\n",
    "    :param stride: the stride of sampling.\n",
    "    :return: extracted patches, shape: (n, c, patch_size, patch_size, n_patches).\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Patch sampler for feature maps.\n",
    "    Parameters\n",
    "    ---\n",
    "    inputs : torch.Tensor\n",
    "\n",
    "    patch_size : int, optional\n",
    "\n",
    "    stride : int, optional\n",
    "\n",
    "    Returns\n",
    "    ---\n",
    "    patches : torch.Tensor\n",
    "\n",
    "    \"\"\"\n",
    "    patches = (\n",
    "        img.unfold(2, patch_size_h, patch_size_h)\n",
    "        .unfold(3, patch_size_w, patch_size_w)\n",
    "        .contiguous()\n",
    "        .permute(2, 3, 0, 1, 4, 5)\n",
    "        .flatten(start_dim=0, end_dim=2)\n",
    "        # .reshape(-1, c, patch_size_h, patch_size_w)\n",
    "    )\n",
    "    return patches\n",
    "\n",
    "\n",
    "def unchop_patches(\n",
    "    patches: torch.Tensor, img_h: int = 1024, img_w: int = 2048, n: int = 1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Assumes non-overlapping patches\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/reshaping-windows-into-image/19805\n",
    "    \"\"\"\n",
    "    _, c, patch_size_h, patch_size_w = patches.shape\n",
    "    num_h = img_h // patch_size_h\n",
    "    num_w = img_w // patch_size_w\n",
    "\n",
    "    img = patches.reshape(n, num_h * num_w, patch_size_h * patch_size_w * c).permute(\n",
    "        0, 2, 1\n",
    "    )\n",
    "    img = F.fold(\n",
    "        img,\n",
    "        (img_h, img_w),\n",
    "        (patch_size_h, patch_size_w),\n",
    "        1,\n",
    "        0,\n",
    "        (patch_size_h, patch_size_w),\n",
    "    )\n",
    "    return img.reshape(n, c, img_h, img_w)\n",
    "\n",
    "def roll_n(X, axis, n):\n",
    "    f_idx = tuple(\n",
    "        slice(None, None, None) if i != axis else slice(0, n, None)\n",
    "        for i in range(X.dim())\n",
    "    )\n",
    "    b_idx = tuple(\n",
    "        slice(None, None, None) if i != axis else slice(n, None, None)\n",
    "        for i in range(X.dim())\n",
    "    )\n",
    "    front = X[f_idx]\n",
    "    back = X[b_idx]\n",
    "    return torch.cat([back, front], axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f50f71ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:01:24.120881Z",
     "iopub.status.busy": "2025-02-16T17:01:24.120626Z",
     "iopub.status.idle": "2025-02-16T17:01:24.125623Z",
     "shell.execute_reply": "2025-02-16T17:01:24.125038Z"
    },
    "papermill": {
     "duration": 0.01061,
     "end_time": "2025-02-16T17:01:24.126815",
     "exception": false,
     "start_time": "2025-02-16T17:01:24.116205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Imports --- #\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# --- Perceptual loss network  --- #\n",
    "class LossNetwork(torch.nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        super(LossNetwork, self).__init__()\n",
    "        self.vgg_layers = vgg_model\n",
    "        self.layer_name_mapping = {\n",
    "            '3': \"relu1_2\",\n",
    "            '8': \"relu2_2\",\n",
    "            '15': \"relu3_3\"\n",
    "        }\n",
    "\n",
    "    def output_features(self, x):\n",
    "        output = {}\n",
    "        for name, module in self.vgg_layers._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layer_name_mapping:\n",
    "                output[self.layer_name_mapping[name]] = x\n",
    "        return list(output.values())\n",
    "\n",
    "    def forward(self, dehaze, gt):\n",
    "        loss = []\n",
    "        dehaze_features = self.output_features(dehaze)\n",
    "        gt_features = self.output_features(gt)\n",
    "        for dehaze_feature, gt_feature in zip(dehaze_features, gt_features):\n",
    "            loss.append(F.mse_loss(dehaze_feature, gt_feature))\n",
    "\n",
    "        return sum(loss)/len(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8cabd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:01:24.135776Z",
     "iopub.status.busy": "2025-02-16T17:01:24.135525Z",
     "iopub.status.idle": "2025-02-16T17:01:24.168008Z",
     "shell.execute_reply": "2025-02-16T17:01:24.167318Z"
    },
    "papermill": {
     "duration": 0.038249,
     "end_time": "2025-02-16T17:01:24.169146",
     "exception": false,
     "start_time": "2025-02-16T17:01:24.130897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Imports --- #\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 =nn.Conv2d(in_channel, in_channel, kernel_size=kernel_size, padding=(kernel_size - 1) // 2) #ConvLayer(in_channel, in_channel, 3)\n",
    "        self.conv2 = ConvLayer(in_channel, out_channel, 3, downsample=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv1(input)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        return out    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class CAB(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(CAB, self).__init__()\n",
    "        #new_features=features//2\n",
    "        features=features//2\n",
    "        self.reduce_fature=nn.Conv2d(features*2, features, kernel_size=1, bias=False)\n",
    "        self.delta_gen1 = nn.Sequential(\n",
    "                        nn.Conv2d(features*2, features, kernel_size=1, bias=False),\n",
    "                        nn.Conv2d(features, 2, kernel_size=3, padding=1, bias=False)\n",
    "                        )\n",
    "\n",
    "        self.delta_gen2 = nn.Sequential(\n",
    "                        nn.Conv2d(features*2, features, kernel_size=1, bias=False),\n",
    "                        nn.Conv2d(features, 2, kernel_size=3, padding=1, bias=False)\n",
    "                        )\n",
    "\n",
    "\n",
    "        #self.delta_gen1.weight.data.zero_()\n",
    "        #self.delta_gen2.weight.data.zero_()\n",
    "\n",
    "    # https://github.com/speedinghzl/AlignSeg/issues/7\n",
    "    # the normlization item is set to [w/s, h/s] rather than [h/s, w/s]\n",
    "    # the function bilinear_interpolate_torch_gridsample2 is standard implementation, please use bilinear_interpolate_torch_gridsample2 for training.\n",
    "    def bilinear_interpolate_torch_gridsample(self, input, size, delta=0):\n",
    "        out_h, out_w = size\n",
    "        n, c, h, w = input.shape\n",
    "        s = 1.0\n",
    "        norm = torch.tensor([[[[w/s, h/s]]]]).type_as(input).to(input.device)\n",
    "        w_list = torch.linspace(-1.0, 1.0, out_h).view(-1, 1).repeat(1, out_w)\n",
    "        h_list = torch.linspace(-1.0, 1.0, out_w).repeat(out_h, 1)\n",
    "        grid = torch.cat((h_list.unsqueeze(2), w_list.unsqueeze(2)), 2)\n",
    "        grid = grid.repeat(n, 1, 1, 1).type_as(input).to(input.device)\n",
    "        grid = grid + delta.permute(0, 2, 3, 1) / norm\n",
    "\n",
    "        output = F.grid_sample(input, grid)\n",
    "        return output\n",
    "\n",
    "    def bilinear_interpolate_torch_gridsample2(self, input, size, delta=0):\n",
    "        out_h, out_w = size\n",
    "        n, c, h, w = input.shape\n",
    "        s = 2.0\n",
    "        norm = torch.tensor([[[[(out_w-1)/s, (out_h-1)/s]]]]).type_as(input).to(input.device) # not [h/s, w/s]\n",
    "        w_list = torch.linspace(-1.0, 1.0, out_h).view(-1, 1).repeat(1, out_w)\n",
    "        h_list = torch.linspace(-1.0, 1.0, out_w).repeat(out_h, 1)\n",
    "        grid = torch.cat((h_list.unsqueeze(2), w_list.unsqueeze(2)), 2)\n",
    "        grid = grid.repeat(n, 1, 1, 1).type_as(input).to(input.device)\n",
    "        grid = grid + delta.permute(0, 2, 3, 1) / norm\n",
    "\n",
    "        output = F.grid_sample(input, grid, align_corners=True)\n",
    "        return output\n",
    "\n",
    "    def forward(self, low_stage, high_stage):\n",
    "        h, w = low_stage.size(2), low_stage.size(3)\n",
    "        high_stage=self.reduce_fature(high_stage)\n",
    "        high_stage = F.interpolate(input=high_stage, size=(h, w), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        concat = torch.cat((low_stage, high_stage), 1)\n",
    "        delta1 = self.delta_gen1(concat)\n",
    "        delta2 = self.delta_gen2(concat)\n",
    "        high_stage = self.bilinear_interpolate_torch_gridsample2(high_stage, (h, w), delta1)\n",
    "        low_stage = self.bilinear_interpolate_torch_gridsample2(low_stage, (h, w), delta2)\n",
    "\n",
    "        high_stage += low_stage\n",
    "        return high_stage\n",
    "\n",
    "class MakeDense(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, kernel_size=3):\n",
    "        super(MakeDense, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=kernel_size, padding=(kernel_size-1)//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv(x))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class PALayer(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(PALayer, self).__init__()\n",
    "        self.pa = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.pa(x)\n",
    "        return x * y\n",
    "\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(CALayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.ca = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.ca(y)\n",
    "        return x * y\n",
    "\n",
    "class SRDBDK(nn.Module):\n",
    "    def __init__(self, in_channels, num_dense_layer, growth_rate):\n",
    "        super(SRDBDK, self).__init__()\n",
    "        \n",
    "        modules = []\n",
    "        self.split_channel=in_channels//8\n",
    "        kernel_size=3\n",
    "        dilation=1\n",
    "        self.conv1 = nn.Conv2d(self.split_channel*1, self.split_channel, kernel_size=9, padding=4, dilation=1)\n",
    "        dilation=2\n",
    "        self.conv2 = nn.Conv2d(self.split_channel*2, self.split_channel*1, kernel_size=7, padding=3, dilation=1)\n",
    "        dilation=4\n",
    "        self.conv3 = nn.Conv2d(self.split_channel*4, self.split_channel*2, kernel_size=5,  padding=2, dilation=1)\n",
    "        dilation=8\n",
    "        self.conv4 = nn.Conv2d(self.split_channel*8, self.split_channel*4, kernel_size=3, padding=1, dilation=1)\n",
    "\n",
    "            \n",
    "        #self.residual_dense_layers = nn.Sequential(*modules)\n",
    "        _in_channels=in_channels\n",
    "        self.calayer=CALayer(in_channels)\n",
    "        self.palayer=PALayer(in_channels)\n",
    "        self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        splited = torch.split(x, [self.split_channel,self.split_channel*1,self.split_channel*2,self.split_channel*4], dim=1)\n",
    "        x0=F.relu(self.conv1(splited[0]))\n",
    "        tmp= torch.cat((splited[1], x0), 1)\n",
    "        x1=F.relu(self.conv2(tmp))\n",
    "        tmp= torch.cat((splited[2], x0, x1), 1)\n",
    "        x2=F.relu(self.conv3(tmp))\n",
    "        tmp= torch.cat((splited[3], x0, x1, x2), 1)\n",
    "        x3=F.relu(self.conv4(tmp))\n",
    "        tmp= torch.cat(( x0, x1, x2, x3), 1)\n",
    "        \n",
    "        out = self.conv_1x1(tmp)\n",
    "        out=self.calayer(out)\n",
    "        out=self.palayer(out)\n",
    "        out=out+x\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class SRDB(nn.Module):\n",
    "    def __init__(self, in_channels, num_dense_layer, growth_rate):\n",
    "        super(SRDB, self).__init__()\n",
    "        \n",
    "        modules = []\n",
    "        self.split_channel=in_channels//4\n",
    "        kernel_size=3\n",
    "        dilation=1\n",
    "        self.conv1 = nn.Conv2d(self.split_channel*1, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        dilation=2\n",
    "        self.conv2 = nn.Conv2d(self.split_channel*2, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        dilation=4\n",
    "        self.conv3 = nn.Conv2d(self.split_channel*3, self.split_channel, kernel_size=kernel_size,  padding=dilation, dilation=dilation)\n",
    "        dilation=8\n",
    "        self.conv4 = nn.Conv2d(self.split_channel*4, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "\n",
    "            \n",
    "        #self.residual_dense_layers = nn.Sequential(*modules)\n",
    "        _in_channels=in_channels\n",
    "        self.calayer=CALayer(in_channels)\n",
    "        self.palayer=PALayer(in_channels)\n",
    "        self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        splited = torch.split(x, self.split_channel, dim=1)\n",
    "        x0=F.relu(self.conv1(splited[0]))\n",
    "        tmp= torch.cat((splited[1], x0), 1)\n",
    "        x1=F.relu(self.conv2(tmp))\n",
    "        tmp= torch.cat((splited[2], x0, x1), 1)\n",
    "        x2=F.relu(self.conv3(tmp))\n",
    "        tmp= torch.cat((splited[3], x0, x1, x2), 1)\n",
    "        x3=F.relu(self.conv4(tmp))\n",
    "        tmp= torch.cat(( x0, x1, x2, x3), 1)\n",
    "        \n",
    "        out = self.conv_1x1(tmp)\n",
    "        out=self.calayer(out)\n",
    "        out=self.palayer(out)\n",
    "        #print(out.shape, x.shape)\n",
    "        out=out+x\n",
    "        return out\n",
    "\n",
    "\n",
    "class SRDBN(nn.Module):\n",
    "    def __init__(self, in_channels, num_dense_layer, growth_rate):\n",
    "        super(SRDBN, self).__init__()\n",
    "        modules = []\n",
    "        self.split_channel=in_channels//8\n",
    "        kernel_size=3\n",
    "        dilation=1\n",
    "        self.conv1 = nn.Conv2d(self.split_channel*1, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        dilation=2\n",
    "        self.conv2 = nn.Conv2d(self.split_channel*2, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        dilation=4\n",
    "        self.conv3 = nn.Conv2d(self.split_channel*3, self.split_channel, kernel_size=kernel_size,  padding=dilation, dilation=dilation)\n",
    "        dilation=8\n",
    "        self.conv4 = nn.Conv2d(self.split_channel*4, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        \n",
    "        dilation=8\n",
    "        self.conv5 = nn.Conv2d(self.split_channel*5, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        \n",
    "        dilation=4\n",
    "        self.conv6 = nn.Conv2d(self.split_channel*6, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        \n",
    "        dilation=2\n",
    "        self.conv7 = nn.Conv2d(self.split_channel*7, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        \n",
    "        dilation=1\n",
    "        self.conv8 = nn.Conv2d(self.split_channel*8, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "\n",
    "            \n",
    "        #self.residual_dense_layers = nn.Sequential(*modules)\n",
    "        _in_channels=in_channels\n",
    "        self.calayer=CALayer(in_channels)\n",
    "        self.palayer=PALayer(in_channels)\n",
    "        self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        splited = torch.split(x, self.split_channel, dim=1)\n",
    "        x0=F.relu(self.conv1(splited[0]))\n",
    "        tmp= torch.cat((splited[1], x0), 1)\n",
    "        x1=F.relu(self.conv2(tmp))\n",
    "        tmp= torch.cat((splited[2], x0, x1), 1)\n",
    "        x2=F.relu(self.conv3(tmp))\n",
    "        tmp= torch.cat((splited[3], x0, x1, x2), 1)\n",
    "        x3=F.relu(self.conv4(tmp))\n",
    "        tmp= torch.cat(( splited[4],x0, x1, x2, x3), 1)\n",
    "        x4=F.relu(self.conv5(tmp))\n",
    "        \n",
    "        tmp= torch.cat(( splited[5],x0, x1, x2, x3,x4), 1)\n",
    "        x5=F.relu(self.conv6(tmp))\n",
    "        \n",
    "        tmp= torch.cat(( splited[6],x0, x1, x2, x3,x4,x5), 1)\n",
    "        x6=F.relu(self.conv7(tmp))\n",
    "        \n",
    "        tmp= torch.cat(( splited[7],x0, x1, x2, x3,x4,x5,x6), 1)\n",
    "        x7=F.relu(self.conv8(tmp))\n",
    "        \n",
    "       \n",
    "        tmp= torch.cat(( x0, x1, x2, x3,x4,x5,x6,x7), 1)\n",
    "        out = self.conv_1x1(tmp)\n",
    "        out=self.calayer(out)\n",
    "        out=self.palayer(out)\n",
    "        out=out+x\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n",
    "                \n",
    "\n",
    "class RDB(nn.Module):\n",
    "    def __init__(self, in_channels, num_dense_layer, growth_rate):\n",
    "        super(RDB, self).__init__()\n",
    "        _in_channels = in_channels\n",
    "        modules = []\n",
    "        for i in range(num_dense_layer):\n",
    "            modules.append(MakeDense(_in_channels, growth_rate))\n",
    "            _in_channels += growth_rate\n",
    "        self.residual_dense_layers = nn.Sequential(*modules)\n",
    "        self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.residual_dense_layers(x)\n",
    "        out = self.conv_1x1(out)\n",
    "        out = out + x\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe9105a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:01:24.178298Z",
     "iopub.status.busy": "2025-02-16T17:01:24.178086Z",
     "iopub.status.idle": "2025-02-16T17:01:24.194648Z",
     "shell.execute_reply": "2025-02-16T17:01:24.193998Z"
    },
    "papermill": {
     "duration": 0.022521,
     "end_time": "2025-02-16T17:01:24.195898",
     "exception": false,
     "start_time": "2025-02-16T17:01:24.173377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports --- #\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from random import randrange\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "import glob\n",
    "# --- Training dataset --- #\n",
    "\n",
    "class TrainData512(data.Dataset):\n",
    "    def __init__(self, crop_size, train_data_dir):\n",
    "        super().__init__()\n",
    "        hazeeffected_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/reside/hazy/'\n",
    "        \n",
    "        hazy_data = glob.glob(hazeeffected_images_dir + \"*.png\")\n",
    "        hazefree_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/reside/clear/'\n",
    "        haze_names=[]\n",
    "        gt_names=[]\n",
    "        for h_image in hazy_data:\n",
    "\t\t        h_image = h_image.split(\"/\")[-1]\n",
    "\t\t        id_ = h_image.split(\"_\")[0]  + \".png\"\n",
    "\t\t        haze_names.append(hazeeffected_images_dir+h_image)\n",
    "\t\t        gt_names.append(hazefree_images_dir+id_)\n",
    "        self.haze_names = haze_names\n",
    "        self.gt_names = gt_names\n",
    "        self.crop_size = crop_size\n",
    "        self.train_data_dir = train_data_dir\n",
    "\n",
    "    def get_images(self, index):\n",
    "        crop_width, crop_height = self.crop_size\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        haze_img = Image.open(haze_name)\n",
    "\n",
    "        try:\n",
    "            gt_img = Image.open(gt_name)\n",
    "        except:\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # --- x,y coordinate of left-top corner --- #\n",
    "        \n",
    "        haze_crop_img = haze_img.resize((512, 512),Image.ANTIALIAS)\n",
    "        gt_crop_img = gt_img.resize((512, 512),Image.ANTIALIAS)\n",
    "\n",
    "        # --- Transform to tensor --- #\n",
    "        transform_all = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "       \n",
    "        haze = transform_all(haze_crop_img)\n",
    "        gt = transform_all(gt_crop_img)\n",
    "\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        res = self.get_images(index)\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)\n",
    "        \n",
    "        \n",
    "\n",
    "class TrainDataNew(data.Dataset):\n",
    "    def __init__(self, crop_size, train_data_dir):\n",
    "        super().__init__()\n",
    "        # hazeeffected_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/light_dehazenet/data/'\n",
    "        hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "        \n",
    "        \n",
    "        hazy_data = glob.glob(hazeeffected_images_dir + \"*.jpg\")\n",
    "        # hazefree_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/light_dehazenet/image/'\n",
    "        hazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "        haze_names=[]\n",
    "        gt_names=[]\n",
    "        for h_image in hazy_data:\n",
    "\t\t        h_image = h_image.split(\"/\")[-1]\n",
    "\t\t        id_ = h_image.split(\"_\")[0] + \"_\" + h_image.split(\"_\")[1] + \".jpg\"\n",
    "\t\t        haze_names.append(hazeeffected_images_dir+h_image)\n",
    "\t\t        gt_names.append(hazefree_images_dir+id_)\n",
    "        self.haze_names = haze_names\n",
    "        self.gt_names = gt_names\n",
    "        self.crop_size = crop_size\n",
    "        self.train_data_dir = train_data_dir\n",
    "\n",
    "    def get_images(self, index):\n",
    "        crop_width, crop_height = self.crop_size\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        haze_img = Image.open(haze_name)\n",
    "\n",
    "        try:\n",
    "            gt_img = Image.open(gt_name)\n",
    "        except:\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "\n",
    "        width, height = haze_img.size\n",
    "\n",
    "        if width < crop_width or height < crop_height:\n",
    "            raise Exception('Bad image size: {}'.format(gt_name))\n",
    "\n",
    "        # --- x,y coordinate of left-top corner --- #\n",
    "        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n",
    "        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "\n",
    "        # --- Transform to tensor --- #\n",
    "         #transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        transform_gt = Compose([ToTensor()])\n",
    "        haze = transform_gt(haze_crop_img)\n",
    "        gt = transform_gt(gt_crop_img)\n",
    "\n",
    "        # --- Check the channel is 3 or not --- #\n",
    "        if list(haze.shape)[0] !=  3 or list(gt.shape)[0] !=  3:\n",
    "            raise Exception('Bad image channel: {}'.format(gt_name))\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        res = self.get_images(index)\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)\n",
    "        \n",
    "        \n",
    "        \n",
    "class TrainData(data.Dataset):\n",
    "    def __init__(self, crop_size, train_data_dir,istrain = True):\n",
    "        super().__init__()\n",
    "        if istrain:\n",
    "            # hazeeffected_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "            # hazefree_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "            # hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "            # hazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/GT'\n",
    "            hazeeffected_images_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN'\n",
    "            hazefree_images_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT'\n",
    "        else:\n",
    "            hazeeffected_images_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN'\n",
    "            hazefree_images_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/GT'\n",
    "\n",
    "        hazy_data = glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n",
    "        \n",
    "        haze_names=[]\n",
    "        gt_names=[]\n",
    "        print(hazy_data)\n",
    "        print(len(hazy_data))\n",
    "        for h_image in hazy_data:\n",
    "            # h_image = h_image.split(\"/\")[-1]\n",
    "            # id_ = h_image.split(\"_\")[0] + \"_\" + h_image.split(\"_\")[1] + \".jpg\"\n",
    "            # print(\"id\",id_)\n",
    "            # haze_names.append(hazeeffected_images_dir+h_image)\n",
    "            # gt_names.append(hazefree_images_dir+id_)\n",
    "            h_image = h_image.split(\"/\")[-1]  # Extract filename\n",
    "            haze_names.append(os.path.join(hazeeffected_images_dir, h_image))\n",
    "            gt_names.append(os.path.join(hazefree_images_dir, h_image)) \n",
    "\n",
    "        self.haze_names = haze_names\n",
    "        self.gt_names = gt_names\n",
    "        self.crop_size = crop_size\n",
    "        self.train_data_dir = train_data_dir\n",
    "\n",
    "    def get_images(self, index):\n",
    "        crop_width, crop_height = self.crop_size\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        haze_img = Image.open(haze_name)\n",
    "\n",
    "        try:\n",
    "            gt_img = Image.open(gt_name)\n",
    "        except:\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "\n",
    "        width, height = haze_img.size\n",
    "\n",
    "        if width < crop_width or height < crop_height:\n",
    "            raise Exception('Bad image size: {}'.format(gt_name))\n",
    "\n",
    "        # --- x,y coordinate of left-top corner --- #\n",
    "        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n",
    "        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "\n",
    "        # --- Transform to tensor --- #\n",
    "        transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        transform_gt = Compose([ToTensor()])\n",
    "        haze = transform_haze(haze_crop_img)\n",
    "        gt = transform_gt(gt_crop_img)\n",
    "\n",
    "        # --- Check the channel is 3 or not --- #\n",
    "        if list(haze.shape)[0] !=  3 or list(gt.shape)[0] !=  3:\n",
    "            raise Exception('Bad image channel: {}'.format(gt_name))\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        res = self.get_images(index)\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af93ad9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:01:24.205030Z",
     "iopub.status.busy": "2025-02-16T17:01:24.204825Z",
     "iopub.status.idle": "2025-02-16T17:01:24.571053Z",
     "shell.execute_reply": "2025-02-16T17:01:24.570351Z"
    },
    "papermill": {
     "duration": 0.372519,
     "end_time": "2025-02-16T17:01:24.572514",
     "exception": false,
     "start_time": "2025-02-16T17:01:24.199995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "paper: GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing\n",
    "file: utils.py\n",
    "about: all utilities\n",
    "author: Xiaohong Liu\n",
    "date: 01/08/19\n",
    "\"\"\"\n",
    "\n",
    "# --- Imports --- #\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as utils\n",
    "from math import log10\n",
    "from skimage import measure\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "\n",
    "def to_psnr(dehaze, gt):\n",
    "    mse = F.mse_loss(dehaze, gt, reduction='none')\n",
    "    #print (mse)\n",
    "    mse_split = torch.split(mse, 1, dim=0)\n",
    "    mse_list = [torch.mean(torch.squeeze(mse_split[ind])).item() for ind in range(len(mse_split))]\n",
    "\n",
    "    intensity_max = 1.0\n",
    "    psnr_list = [10.0 * log10(intensity_max / min(max(mse,0.000001),1000)) for mse in mse_list]\n",
    "    return psnr_list\n",
    "\n",
    "\n",
    "def to_ssim_skimage(dehaze, gt):\n",
    "    dehaze_list = torch.split(dehaze, 1, dim=0)\n",
    "    gt_list = torch.split(gt, 1, dim=0)\n",
    "\n",
    "    dehaze_list_np = [dehaze_list[ind].permute(0, 2, 3, 1).data.cpu().numpy().squeeze() for ind in range(len(dehaze_list))]\n",
    "    gt_list_np = [gt_list[ind].permute(0, 2, 3, 1).data.cpu().numpy().squeeze() for ind in range(len(dehaze_list))]\n",
    "    # ssim_list = [measure.compare_ssim(dehaze_list_np[ind],  gt_list_np[ind], data_range=1, multichannel=True) for ind in range(len(dehaze_list))]\n",
    "    ssim_list = [ssim(dehaze_list_np[ind], gt_list_np[ind], data_range=1, channel_axis=-1) for ind in range(len(dehaze_list))]\n",
    "\n",
    "    return ssim_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validationStlyle(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: GateDehazeNet\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: The GPU that loads the network\n",
    "    :param category: indoor or outdoor test dataset\n",
    "    :param save_tag: tag of saving image or not\n",
    "    :return: average PSNR value\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "\n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            haze, gt, image_name = val_data\n",
    "            haze = haze.to(device)\n",
    "            gt = gt.to(device)\n",
    "            hazing = net(gt,haze)\n",
    "\n",
    "        # --- Calculate the average PSNR --- #\n",
    "        psnr_list.extend(to_psnr(hazing, haze))\n",
    "\n",
    "        # --- Calculate the average SSIM --- #\n",
    "        ssim_list.extend(to_ssim_skimage(hazing, haze))\n",
    "\n",
    "        # --- Save image --- #\n",
    "        if save_tag:\n",
    "            save_image(dehaze, image_name, category)\n",
    "  \n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list)\n",
    "    \n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list)\n",
    "    return avr_psnr, avr_ssim\n",
    "    \n",
    "    \n",
    "def validationB(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: GateDehazeNet\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: The GPU that loads the network\n",
    "    :param category: indoor or outdoor test dataset\n",
    "    :param save_tag: tag of saving image or not\n",
    "    :return: average PSNR value\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    i=0\n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # haze, gt, image_name = val_data\n",
    "            haze, gt = val_data\n",
    "            haze = haze.to(device)\n",
    "            gt = gt.to(device)\n",
    "            dehaze, _ = net(haze)\n",
    "\n",
    "        # --- Calculate the average PSNR --- #\n",
    "        psnr_list.extend(to_psnr(dehaze, gt))\n",
    "\n",
    "        # --- Calculate the average SSIM --- #\n",
    "        ssim_list.extend(to_ssim_skimage(dehaze, gt))\n",
    "\n",
    "        # --- Save image --- #\n",
    "        if save_tag:\n",
    "            i+=1\n",
    "            save_image(dehaze, i, category)\n",
    "  \n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list)\n",
    "    \n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list)\n",
    "    return avr_psnr, avr_ssim\n",
    "        \n",
    "    \n",
    "\n",
    "def validation(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: GateDehazeNet\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: The GPU that loads the network\n",
    "    :param category: indoor or outdoor test dataset\n",
    "    :param save_tag: tag of saving image or not\n",
    "    :return: average PSNR value\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "\n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            haze, gt, image_name = val_data\n",
    "            haze = haze.to(device)\n",
    "            gt = gt.to(device)\n",
    "            dehaze = net(haze)\n",
    "\n",
    "        # --- Calculate the average PSNR --- #\n",
    "        psnr_list.extend(to_psnr(dehaze, gt))\n",
    "\n",
    "        # --- Calculate the average SSIM --- #\n",
    "        ssim_list.extend(to_ssim_skimage(dehaze, gt))\n",
    "\n",
    "        # --- Save image --- #\n",
    "        if save_tag:\n",
    "            save_image(dehaze, image_name, category)\n",
    "  \n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list)\n",
    "    \n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list)\n",
    "    return avr_psnr, avr_ssim\n",
    "def validationN(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: GateDehazeNet\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: The GPU that loads the network\n",
    "    :param category: indoor or outdoor test dataset\n",
    "    :param save_tag: tag of saving image or not\n",
    "    :return: average PSNR value\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "\n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            haze, gt, image_name = val_data\n",
    "            haze = haze.to(device)\n",
    "            gt = gt.to(device)\n",
    "            dehaze,_,_ = net(haze)\n",
    "\n",
    "        # --- Calculate the average PSNR --- #\n",
    "        psnr_list.extend(to_psnr(dehaze, gt))\n",
    "\n",
    "        # --- Calculate the average SSIM --- #\n",
    "        ssim_list.extend(to_ssim_skimage(dehaze, gt))\n",
    "\n",
    "        # --- Save image --- #\n",
    "        if save_tag:\n",
    "            save_image(dehaze, image_name, category)\n",
    "\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list)\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list)\n",
    "    \n",
    "    return avr_psnr, avr_ssim\n",
    "\n",
    "\n",
    "def save_image(dehaze, image_name, category):\n",
    "    dehaze_images = torch.split(dehaze, 1, dim=0)\n",
    "    batch_num = len(dehaze_images)\n",
    "\n",
    "    for ind in range(batch_num):\n",
    "        utils.save_image(dehaze_images[ind], './{}_results/{}'.format(category, image_name[ind][:-3] + 'png'))\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "def print_log(epoch, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, category):\n",
    "    log_dir = \"./training_log\"\n",
    "    os.makedirs(log_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    log_path = os.path.join(log_dir, f\"{category}_log.txt\")\n",
    "\n",
    "    print('({0:.0f}s) Epoch [{1}/{2}], Train_PSNR:{3:.2f}, Val_PSNR:{4:.2f}, Val_SSIM:{5:.4f}'\n",
    "          .format(one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim))\n",
    "\n",
    "    # --- Write the training log --- #\n",
    "    with open(log_path, 'a') as f:\n",
    "        print('Date: {0}, Time_Cost: {1:.0f}s, Epoch: [{2}/{3}], Train_PSNR: {4:.2f}, Val_PSNR: {5:.2f}, Val_SSIM: {6:.4f}'\n",
    "              .format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "                      one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim), file=f)\n",
    "\n",
    "\n",
    "\n",
    "def adjust_learning_rate_step(optimizer, category, lr_decay=0.95):\n",
    "\n",
    "    # --- Decay learning rate --- #\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "       param_group['lr'] *= lr_decay\n",
    "       print('Learning rate sets to {}.'.format(param_group['lr']))\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "def adjust_learning_rate(optimizer, epoch, category, lr_decay=0.90):\n",
    "\n",
    "    # --- Decay learning rate --- #\n",
    "    step = 18 if category == 'indoor' else 3\n",
    "    if category == 'NH':\n",
    "       step = 20\n",
    "    #if not category == 'indoor':\n",
    "       #for param_group in optimizer.param_groups:\n",
    "            #param_group['lr'] *= 0.99\n",
    "            #print('Learning rate sets to {}.'.format(param_group['lr']))\n",
    "    if not epoch % step and epoch > 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= lr_decay\n",
    "            print('Learning rate sets to {}.'.format(param_group['lr']))\n",
    "    else:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('Learning rate sets to {}.'.format(param_group['lr']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f09045d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:01:24.582145Z",
     "iopub.status.busy": "2025-02-16T17:01:24.581766Z",
     "iopub.status.idle": "2025-02-16T17:01:24.585278Z",
     "shell.execute_reply": "2025-02-16T17:01:24.584650Z"
    },
    "papermill": {
     "duration": 0.00953,
     "end_time": "2025-02-16T17:01:24.586465",
     "exception": false,
     "start_time": "2025-02-16T17:01:24.576935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "\n",
    "# # Create widgets for each hyper-parameter\n",
    "# learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n",
    "# crop_size_widget = widgets.Text(value='360,360', description='Crop Size:')\n",
    "# train_batch_size_widget = widgets.IntText(value=6, description='Train Batch Size:')\n",
    "# network_height_widget = widgets.IntText(value=3, description='Network Height:')\n",
    "# network_width_widget = widgets.IntText(value=6, description='Network Width:')\n",
    "# num_dense_layer_widget = widgets.IntText(value=4, description='Num Dense Layer:')\n",
    "# growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n",
    "# lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n",
    "# val_batch_size_widget = widgets.IntText(value=1, description='Val Batch Size:')\n",
    "# category_widget = widgets.Dropdown(options=['indoor', 'outdoor'], value='indoor', description='Category:')\n",
    "\n",
    "# # Display the widgets\n",
    "# display(learning_rate_widget, crop_size_widget, train_batch_size_widget, network_height_widget, network_width_widget, num_dense_layer_widget, growth_rate_widget, lambda_loss_widget, val_batch_size_widget, category_widget)\n",
    "\n",
    "# # Function to parse the crop size\n",
    "# def parse_crop_size(crop_size_str):\n",
    "#     return [int(x) for x in crop_size_str.split(',')]\n",
    "\n",
    "# # Assign the widget values to variables\n",
    "# learning_rate = learning_rate_widget.value\n",
    "# crop_size = parse_crop_size(crop_size_widget.value)\n",
    "# train_batch_size = train_batch_size_widget.value\n",
    "# network_height = network_height_widget.value\n",
    "# network_width = network_width_widget.value\n",
    "# num_dense_layer = num_dense_layer_widget.value\n",
    "# growth_rate = growth_rate_widget.value\n",
    "# lambda_loss = lambda_loss_widget.value\n",
    "# val_batch_size = val_batch_size_widget.value\n",
    "# category = category_widget.value\n",
    "\n",
    "# print('Hyper-parameters set:')\n",
    "# print(f'learning_rate: {learning_rate}')\n",
    "# print(f'crop_size: {crop_size}')\n",
    "# print(f'train_batch_size: {train_batch_size}')\n",
    "# print(f'network_height: {network_height}')\n",
    "# print(f'network_width: {network_width}')\n",
    "# print(f'num_dense_layer: {num_dense_layer}')\n",
    "# print(f'growth_rate: {growth_rate}')\n",
    "# print(f'lambda_loss: {lambda_loss}')\n",
    "# print(f'val_batch_size: {val_batch_size}')\n",
    "# print(f'category: {category}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e362676",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:01:24.595249Z",
     "iopub.status.busy": "2025-02-16T17:01:24.595038Z",
     "iopub.status.idle": "2025-02-16T17:01:24.643624Z",
     "shell.execute_reply": "2025-02-16T17:01:24.642843Z"
    },
    "papermill": {
     "duration": 0.056237,
     "end_time": "2025-02-16T17:01:24.646732",
     "exception": false,
     "start_time": "2025-02-16T17:01:24.590495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca9ac5dfbc94f7f9b1b2e0a8cdd4ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Learning Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a1b8cf72364d3489e00447f2549b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='360,360', description='Crop Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8261977192b04eb9aaff73d8472affb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=6, description='Train Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53aa9ee9f9694f53955d6962a45bcc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=3, description='Network Height:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67bd4932a394d4e853ca408c909ad21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=6, description='Network Width:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2534c7426bc4aafac638dc620df29bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='Num Dense Layer:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b40e9e6cb941468a694a322c7cefac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='Growth Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8031d363274751aa7204d097979fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.04, description='Lambda Loss:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9383e3faa3dc4cb290237c265d93fb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='Val Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d08888ce2c4b239df9730f1e2b4676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', index=2, options=('indoor', 'outdoor', 'nh'), value='nh')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cd3239b2f545a786ce436d963f64f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Execution Env:', options=('local', 'kaggle'), value='local')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters set:\n",
      "learning_rate: 0.0001\n",
      "crop_size: [360, 360]\n",
      "train_batch_size: 6\n",
      "network_height: 3\n",
      "network_width: 6\n",
      "num_dense_layer: 4\n",
      "growth_rate: 16\n",
      "lambda_loss: 0.04\n",
      "val_batch_size: 1\n",
      "category: nh\n",
      "execution_env: local\n",
      "\n",
      "Final dataset paths:\n",
      "Training directory: /Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy\n",
      "Validation directory: /Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT\n",
      "Number of epochs: 10\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Create widgets for each hyper-parameter ---\n",
    "learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n",
    "crop_size_widget = widgets.Text(value='360,360', description='Crop Size:')\n",
    "train_batch_size_widget = widgets.IntText(value=6, description='Train Batch Size:')\n",
    "network_height_widget = widgets.IntText(value=3, description='Network Height:')\n",
    "network_width_widget = widgets.IntText(value=6, description='Network Width:')\n",
    "num_dense_layer_widget = widgets.IntText(value=4, description='Num Dense Layer:')\n",
    "growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n",
    "lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n",
    "val_batch_size_widget = widgets.IntText(value=1, description='Val Batch Size:')\n",
    "category_widget = widgets.Dropdown(options=['indoor', 'outdoor', 'nh'], value='nh', description='Category:')\n",
    "execution_env_widget = widgets.Dropdown(options=['local', 'kaggle'], value='local', description='Execution Env:')\n",
    "\n",
    "# --- Display the widgets ---\n",
    "display(\n",
    "    learning_rate_widget, crop_size_widget, train_batch_size_widget, network_height_widget, \n",
    "    network_width_widget, num_dense_layer_widget, growth_rate_widget, lambda_loss_widget, \n",
    "    val_batch_size_widget, category_widget, execution_env_widget\n",
    ")\n",
    "\n",
    "# --- Function to parse crop size ---\n",
    "def parse_crop_size(crop_size_str):\n",
    "    return [int(x) for x in crop_size_str.split(',')]\n",
    "\n",
    "# --- Assign the widget values to variables ---\n",
    "learning_rate = learning_rate_widget.value\n",
    "crop_size = parse_crop_size(crop_size_widget.value)\n",
    "train_batch_size = train_batch_size_widget.value\n",
    "network_height = network_height_widget.value\n",
    "network_width = network_width_widget.value\n",
    "num_dense_layer = num_dense_layer_widget.value\n",
    "growth_rate = growth_rate_widget.value\n",
    "lambda_loss = lambda_loss_widget.value\n",
    "val_batch_size = val_batch_size_widget.value\n",
    "category = category_widget.value\n",
    "execution_env = execution_env_widget.value  # Local or Kaggle\n",
    "\n",
    "print('\\nHyper-parameters set:')\n",
    "print(f'learning_rate: {learning_rate}')\n",
    "print(f'crop_size: {crop_size}')\n",
    "print(f'train_batch_size: {train_batch_size}')\n",
    "print(f'network_height: {network_height}')\n",
    "print(f'network_width: {network_width}')\n",
    "print(f'num_dense_layer: {num_dense_layer}')\n",
    "print(f'growth_rate: {growth_rate}')\n",
    "print(f'lambda_loss: {lambda_loss}')\n",
    "print(f'val_batch_size: {val_batch_size}')\n",
    "print(f'category: {category}')\n",
    "print(f'execution_env: {execution_env}')\n",
    "\n",
    "# --- Set category-specific hyper-parameters ---\n",
    "if category == 'indoor':\n",
    "    num_epochs = 1500\n",
    "    train_data_dir = './data/train/indoor/'\n",
    "    val_data_dir = './data/test/SOTS/indoor/'\n",
    "elif category == 'outdoor':\n",
    "    num_epochs = 10\n",
    "    train_data_dir = './data/train/outdoor/'\n",
    "    val_data_dir = './data/test/SOTS/outdoor/'\n",
    "elif category == 'nh':\n",
    "    num_epochs = 10\n",
    "    train_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "    val_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "else:\n",
    "    raise Exception('Wrong image category. Set it to indoor or outdoor for RESIDE dataset.')\n",
    "\n",
    "# --- Adjust paths based on execution environment ---\n",
    "if execution_env == 'kaggle':\n",
    "    # train_data_dir = '/kaggle/input/reside-dataset/' + train_data_dir.strip('./')\n",
    "    # val_data_dir = '/kaggle/input/reside-dataset/' + val_data_dir.strip('./')\n",
    "    train_data_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "    val_data_dir = '/kaggle/input/o-haze/O-HAZY/GT' \n",
    "print('\\nFinal dataset paths:')\n",
    "print(f'Training directory: {train_data_dir}')\n",
    "print(f'Validation directory: {val_data_dir}')\n",
    "print(f'Number of epochs: {num_epochs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "349fe93d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:01:24.658201Z",
     "iopub.status.busy": "2025-02-16T17:01:24.657977Z",
     "iopub.status.idle": "2025-02-16T17:01:29.345886Z",
     "shell.execute_reply": "2025-02-16T17:01:29.344727Z"
    },
    "papermill": {
     "duration": 4.694958,
     "end_time": "2025-02-16T17:01:29.347281",
     "exception": false,
     "start_time": "2025-02-16T17:01:24.652323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|| 528M/528M [00:02<00:00, 231MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- no weight loaded ---\n",
      "Total_params: 4645694\n",
      "['/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/37.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/35.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/11.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/31.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/03.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/40.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/33.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/09.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/02.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/14.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/08.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/39.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/20.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/38.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/10.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/36.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/06.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/18.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/21.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/12.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/28.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/22.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/34.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/01.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/29.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/07.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/17.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/16.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/24.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/32.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/23.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/15.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/30.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/27.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/13.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/25.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/19.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/26.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/04.png', '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN/05.png']\n",
      "40\n",
      "['/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN/43.png', '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN/41.png', '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN/45.png', '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN/44.png', '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN/42.png']\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-a0f2cea92eac>:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(models+'{}_haze_best_{}_{}'.format(category, network_height, network_width)))\n"
     ]
    }
   ],
   "source": [
    "# --- Imports --- #\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "#from dehazeformer import *\n",
    "from torchvision.models import vgg16\n",
    "#plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Gpu device --- #\n",
    "device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# --- Define the network --- #\n",
    "net = DeepGuidednew() #GridDehazeNet(height=network_height, width=network_width, num_dense_layer=num_dense_layer, growth_rate=growth_rate)\n",
    "\n",
    "#net =FFA(3,19)\n",
    "# --- Build optimizer --- #\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# --- Multi-GPU --- #\n",
    "net = net.to(device)\n",
    "net = nn.DataParallel(net, device_ids=device_ids)\n",
    "\n",
    "\n",
    "# --- Define the perceptual loss network --- #\n",
    "vgg_model = vgg16(pretrained=True).features[:16]\n",
    "vgg_model = vgg_model.to(device)\n",
    "for param in vgg_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "loss_network = LossNetwork(vgg_model)\n",
    "loss_network.eval()\n",
    "models='formernew'\n",
    "\n",
    "# --- Load the network weight --- #\n",
    "try:\n",
    "    net.load_state_dict(torch.load(models+'{}_haze_best_{}_{}'.format(category, network_height, network_width)))\n",
    "    print('--- weight loaded ---')\n",
    "except:\n",
    "    print('--- no weight loaded ---')\n",
    "\n",
    "\n",
    "# --- Calculate all trainable parameters in network --- #\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total_params: {}\".format(pytorch_total_params))\n",
    "\n",
    "\n",
    "# --- Load training data and validation/test data --- #\n",
    "train_data_loader = DataLoader(TrainData(crop_size, train_data_dir), batch_size=train_batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(TrainData(crop_size, train_data_dir, istrain=False), batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "165f3c92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T17:01:29.362992Z",
     "iopub.status.busy": "2025-02-16T17:01:29.362722Z",
     "iopub.status.idle": "2025-02-16T17:03:17.979582Z",
     "shell.execute_reply": "2025-02-16T17:03:17.978417Z"
    },
    "papermill": {
     "duration": 108.626313,
     "end_time": "2025-02-16T17:03:17.981047",
     "exception": false,
     "start_time": "2025-02-16T17:01:29.354734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_val_psnr: 7.84, old_val_ssim: 0.1986\n",
      "Learning rate sets to 0.0001.\n",
      "Epoch: 0, Iteration: 0\n",
      "(14s) Epoch [1/10], Train_PSNR:10.89, Val_PSNR:10.72, Val_SSIM:0.1983\n",
      "Learning rate sets to 0.0001.\n",
      "Epoch: 1, Iteration: 0\n",
      "(10s) Epoch [2/10], Train_PSNR:12.42, Val_PSNR:12.80, Val_SSIM:0.2487\n",
      "Learning rate sets to 0.0001.\n",
      "Epoch: 2, Iteration: 0\n",
      "(10s) Epoch [3/10], Train_PSNR:12.30, Val_PSNR:11.61, Val_SSIM:0.2971\n",
      "Learning rate sets to 9e-05.\n",
      "Epoch: 3, Iteration: 0\n",
      "(10s) Epoch [4/10], Train_PSNR:13.34, Val_PSNR:11.56, Val_SSIM:0.2793\n",
      "Learning rate sets to 9e-05.\n",
      "Epoch: 4, Iteration: 0\n",
      "(10s) Epoch [5/10], Train_PSNR:14.38, Val_PSNR:13.24, Val_SSIM:0.3181\n",
      "Learning rate sets to 9e-05.\n",
      "Epoch: 5, Iteration: 0\n",
      "(10s) Epoch [6/10], Train_PSNR:13.46, Val_PSNR:13.17, Val_SSIM:0.3146\n",
      "Learning rate sets to 8.1e-05.\n",
      "Epoch: 6, Iteration: 0\n",
      "(10s) Epoch [7/10], Train_PSNR:13.91, Val_PSNR:15.12, Val_SSIM:0.3857\n",
      "Learning rate sets to 8.1e-05.\n",
      "Epoch: 7, Iteration: 0\n",
      "(10s) Epoch [8/10], Train_PSNR:13.58, Val_PSNR:12.66, Val_SSIM:0.3297\n",
      "Learning rate sets to 8.1e-05.\n",
      "Epoch: 8, Iteration: 0\n",
      "(10s) Epoch [9/10], Train_PSNR:14.33, Val_PSNR:14.92, Val_SSIM:0.3471\n",
      "Learning rate sets to 7.290000000000001e-05.\n",
      "Epoch: 9, Iteration: 0\n",
      "(10s) Epoch [10/10], Train_PSNR:14.72, Val_PSNR:15.71, Val_SSIM:0.3718\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- Previous PSNR and SSIM in testing --- #\n",
    "old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)\n",
    "print('old_val_psnr: {0:.2f}, old_val_ssim: {1:.4f}'.format(old_val_psnr, old_val_ssim))\n",
    "train_psnrold=0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    psnr_list = []\n",
    "    start_time = time.time()\n",
    "    adjust_learning_rate(optimizer, epoch, category=category)\n",
    "\n",
    "    for batch_id, train_data in enumerate(train_data_loader):\n",
    "\n",
    "        haze, gt = train_data\n",
    "        haze = haze.to(device)\n",
    "        gt = gt.to(device)\n",
    "\n",
    "        # --- Zero the parameter gradients --- #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # --- Forward + Backward + Optimize --- #\n",
    "        net.train()\n",
    "        dehaze,base = net(haze)\n",
    "        base_loss = F.smooth_l1_loss(base, gt)\n",
    "\n",
    "        smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "        perceptual_loss = loss_network(dehaze, gt)\n",
    "        loss = smooth_loss + lambda_loss*perceptual_loss+base_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- To calculate average PSNR --- #\n",
    "        psnr_list.extend(to_psnr(dehaze, gt))\n",
    "\n",
    "        if not (batch_id % 100):\n",
    "            print('Epoch: {0}, Iteration: {1}'.format(epoch, batch_id))\n",
    "\n",
    "    # --- Calculate the average training PSNR in one epoch --- #\n",
    "    train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "    # --- Save the network parameters --- #\n",
    "    torch.save(net.state_dict(), models+'{}_haze_{}_{}'.format(category, network_height, network_width))\n",
    "\n",
    "    # --- Use the evaluation model in testing --- #\n",
    "    net.eval()\n",
    "\n",
    "    val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "    one_epoch_time = time.time() - start_time\n",
    "    print_log(epoch+1, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, models+category)\n",
    "    \n",
    "    \n",
    "    if train_psnr< train_psnrold:\n",
    "        adjust_learning_rate_step(optimizer, category=category)            \n",
    "\n",
    "    # --- update the network weight --- #\n",
    "    if val_psnr >= old_val_psnr:\n",
    "        torch.save(net.state_dict(), models+'{}_haze_best_{}_{}'.format(category, network_height, network_width))\n",
    "        old_val_psnr = val_psnr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7a7f8",
   "metadata": {
    "papermill": {
     "duration": 0.007331,
     "end_time": "2025-02-16T17:03:17.996398",
     "exception": false,
     "start_time": "2025-02-16T17:03:17.989067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3551148,
     "sourceId": 6187317,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6456606,
     "sourceId": 10417877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 129.826008,
   "end_time": "2025-02-16T17:03:20.534112",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-16T17:01:10.708104",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "028d62bf51684e79b1386adae10608b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "02b40e9e6cb941468a694a322c7cefac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Growth Rate:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_71cf516a015549a893b790f1056f3510",
       "step": 1,
       "style": "IPY_MODEL_a736e2fecf5845f9b200ea97fa5f6149",
       "tabbable": null,
       "tooltip": null,
       "value": 16
      }
     },
     "19a1b8cf72364d3489e00447f2549b21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "TextView",
       "continuous_update": true,
       "description": "Crop Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_e52ef7a6792c48a98f977cda5e2bfe82",
       "placeholder": "",
       "style": "IPY_MODEL_cf8783677ef14437bacc8e61dd612e4d",
       "tabbable": null,
       "tooltip": null,
       "value": "360,360"
      }
     },
     "2d8761f8cc7942568085cc442510cb90": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2f1579b3b8cb49ad82d859eff955146f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4068f8b211484a3f89e171c8c4e52dab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53aa9ee9f9694f53955d6962a45bcc10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Network Height:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_c29986e6af6f432b9e53832a6e82ac64",
       "step": 1,
       "style": "IPY_MODEL_2d8761f8cc7942568085cc442510cb90",
       "tabbable": null,
       "tooltip": null,
       "value": 3
      }
     },
     "71cd3239b2f545a786ce436d963f64f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DropdownModel",
       "_options_labels": [
        "local",
        "kaggle"
       ],
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "DropdownView",
       "description": "Execution Env:",
       "description_allow_html": false,
       "disabled": false,
       "index": 0,
       "layout": "IPY_MODEL_f41f746e66e947eda7a2877dda863706",
       "style": "IPY_MODEL_7c62ef5e48c74784b608cde0a10f63dd",
       "tabbable": null,
       "tooltip": null
      }
     },
     "71cf516a015549a893b790f1056f3510": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7549f856064c40489f7575c0e99cd7a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "78a5394945074279b323a44222271d62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "796125a05b3f44088012528634c13906": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7b7b58af7e1343859b7c893b51b39372": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7c62ef5e48c74784b608cde0a10f63dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8261977192b04eb9aaff73d8472affb4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Train Batch Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_b21351ee883547ca9a80f80e9483e25e",
       "step": 1,
       "style": "IPY_MODEL_9535d65d316540ea92df358cff8c3922",
       "tabbable": null,
       "tooltip": null,
       "value": 6
      }
     },
     "863f55f1abec4dcf86e60720ccafb88f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9383e3faa3dc4cb290237c265d93fb4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Val Batch Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_f03b4eeaa93044f4b466175adaab2f5e",
       "step": 1,
       "style": "IPY_MODEL_c1c06c494e3b49429a18174cbe0f9ceb",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "94b8e425e7304b7baf6bf8f80c7b8f13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9535d65d316540ea92df358cff8c3922": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a736e2fecf5845f9b200ea97fa5f6149": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b21351ee883547ca9a80f80e9483e25e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b67bd4932a394d4e853ca408c909ad21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Network Width:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_863f55f1abec4dcf86e60720ccafb88f",
       "step": 1,
       "style": "IPY_MODEL_028d62bf51684e79b1386adae10608b6",
       "tabbable": null,
       "tooltip": null,
       "value": 6
      }
     },
     "bca9ac5dfbc94f7f9b1b2e0a8cdd4ac1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "FloatTextView",
       "continuous_update": false,
       "description": "Learning Rate:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_c823ee52121e4b1897440408953cc5dc",
       "step": null,
       "style": "IPY_MODEL_7549f856064c40489f7575c0e99cd7a3",
       "tabbable": null,
       "tooltip": null,
       "value": 0.0001
      }
     },
     "c1c06c494e3b49429a18174cbe0f9ceb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c29986e6af6f432b9e53832a6e82ac64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c823ee52121e4b1897440408953cc5dc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cf8783677ef14437bacc8e61dd612e4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2534c7426bc4aafac638dc620df29bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Num Dense Layer:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_78a5394945074279b323a44222271d62",
       "step": 1,
       "style": "IPY_MODEL_796125a05b3f44088012528634c13906",
       "tabbable": null,
       "tooltip": null,
       "value": 4
      }
     },
     "e52ef7a6792c48a98f977cda5e2bfe82": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f03b4eeaa93044f4b466175adaab2f5e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0d08888ce2c4b239df9730f1e2b4676": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DropdownModel",
       "_options_labels": [
        "indoor",
        "outdoor",
        "nh"
       ],
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "DropdownView",
       "description": "Category:",
       "description_allow_html": false,
       "disabled": false,
       "index": 2,
       "layout": "IPY_MODEL_4068f8b211484a3f89e171c8c4e52dab",
       "style": "IPY_MODEL_2f1579b3b8cb49ad82d859eff955146f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f41f746e66e947eda7a2877dda863706": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd8031d363274751aa7204d097979fcc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "FloatTextView",
       "continuous_update": false,
       "description": "Lambda Loss:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_94b8e425e7304b7baf6bf8f80c7b8f13",
       "step": null,
       "style": "IPY_MODEL_7b7b58af7e1343859b7c893b51b39372",
       "tabbable": null,
       "tooltip": null,
       "value": 0.04
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
