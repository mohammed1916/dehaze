{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6187317,"sourceType":"datasetVersion","datasetId":3551148}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport numpy as np\nfrom torch.nn.init import _calculate_fan_in_and_fan_out\nfrom timm.models.layers import to_2tuple, trunc_normal_\nimport os\n\n\nclass RLN(nn.Module):\n\tr\"\"\"Revised LayerNorm\"\"\"\n\tdef __init__(self, dim, eps=1e-5, detach_grad=False):\n\t\tsuper(RLN, self).__init__()\n\t\tself.eps = eps\n\t\tself.detach_grad = detach_grad\n\n\t\tself.weight = nn.Parameter(torch.ones((1, dim, 1, 1)))\n\t\tself.bias = nn.Parameter(torch.zeros((1, dim, 1, 1)))\n\n\t\tself.meta1 = nn.Conv2d(1, dim, 1)\n\t\tself.meta2 = nn.Conv2d(1, dim, 1)\n\n\t\ttrunc_normal_(self.meta1.weight, std=.02)\n\t\tnn.init.constant_(self.meta1.bias, 1)\n\n\t\ttrunc_normal_(self.meta2.weight, std=.02)\n\t\tnn.init.constant_(self.meta2.bias, 0)\n\n\tdef forward(self, input):\n\t\tmean = torch.mean(input, dim=(1, 2, 3), keepdim=True)\n\t\tstd = torch.sqrt((input - mean).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.eps)\n\n\t\tnormalized_input = (input - mean) / std\n\n\t\tif self.detach_grad:\n\t\t\trescale, rebias = self.meta1(std.detach()), self.meta2(mean.detach())\n\t\telse:\n\t\t\trescale, rebias = self.meta1(std), self.meta2(mean)\n\n\t\tout = normalized_input * self.weight + self.bias\n\t\treturn out, rescale, rebias\n\n\nclass Mlp(nn.Module):\n\tdef __init__(self, network_depth, in_features, hidden_features=None, out_features=None):\n\t\tsuper().__init__()\n\t\tout_features = out_features or in_features\n\t\thidden_features = hidden_features or in_features\n\n\t\tself.network_depth = network_depth\n\n\t\tself.mlp = nn.Sequential(\n\t\t\tnn.Conv2d(in_features, hidden_features, 1),\n\t\t\tnn.ReLU(True),\n\t\t\tnn.Conv2d(hidden_features, out_features, 1)\n\t\t)\n\n\t\tself.apply(self._init_weights)\n\n\tdef _init_weights(self, m):\n\t\tif isinstance(m, nn.Conv2d):\n\t\t\tgain = (8 * self.network_depth) ** (-1/4)\n\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n\t\t\ttrunc_normal_(m.weight, std=std)\n\t\t\tif m.bias !=   None:\n\t\t\t\tnn.init.constant_(m.bias, 0)\n\n\tdef forward(self, x):\n\t\treturn self.mlp(x)\n\n\ndef window_partition(x, window_size):\n\tB, H, W, C = x.shape\n\tx = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n\twindows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, C)\n\treturn windows\n\n\ndef window_reverse(windows, window_size, H, W):\n\tB = int(windows.shape[0] / (H * W / window_size / window_size))\n\tx = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n\tx = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n\treturn x\n\n\ndef get_relative_positions(window_size):\n\tcoords_h = torch.arange(window_size)\n\tcoords_w = torch.arange(window_size)\n\n\tcoords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n\tcoords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n\trelative_positions = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n\n\trelative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n\trelative_positions_log  = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n\n\treturn relative_positions_log\n\n\nclass WindowAttention(nn.Module):\n\tdef __init__(self, dim, window_size, num_heads):\n\n\t\tsuper().__init__()\n\t\tself.dim = dim\n\t\tself.window_size = window_size  # Wh, Ww\n\t\tself.num_heads = num_heads\n\t\thead_dim = dim // num_heads\n\t\tself.scale = head_dim ** -0.5\n\n\t\trelative_positions = get_relative_positions(self.window_size)\n\t\tself.register_buffer(\"relative_positions\", relative_positions)\n\t\tself.meta = nn.Sequential(\n\t\t\tnn.Linear(2, 256, bias=True),\n\t\t\tnn.ReLU(True),\n\t\t\tnn.Linear(256, num_heads, bias=True)\n\t\t)\n\n\t\tself.softmax = nn.Softmax(dim=-1)\n\n\tdef forward(self, qkv):\n\t\tB_, N, _ = qkv.shape\n\n\t\tqkv = qkv.reshape(B_, N, 3, self.num_heads, self.dim // self.num_heads).permute(2, 0, 3, 1, 4)\n\n\t\tq, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n\t\tq = q * self.scale\n\t\tattn = (q @ k.transpose(-2, -1))\n\n\t\trelative_position_bias = self.meta(self.relative_positions)\n\t\trelative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\t\tattn = attn + relative_position_bias.unsqueeze(0)\n\n\t\tattn = self.softmax(attn)\n\n\t\tx = (attn @ v).transpose(1, 2).reshape(B_, N, self.dim)\n\t\treturn x\n\n\nclass Attention(nn.Module):\n\tdef __init__(self, network_depth, dim, num_heads, window_size, shift_size, use_attn=False, conv_type=None):\n\t\tsuper().__init__()\n\t\tself.dim = dim\n\t\tself.head_dim = int(dim // num_heads)\n\t\tself.num_heads = num_heads\n\n\t\tself.window_size = window_size\n\t\tself.shift_size = shift_size\n\n\t\tself.network_depth = network_depth\n\t\tself.use_attn = use_attn\n\t\tself.conv_type = conv_type\n\n\t\tif self.conv_type == 'Conv':\n\t\t\tself.conv = nn.Sequential(\n\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1, padding_mode='reflect'),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1, padding_mode='reflect')\n\t\t\t)\n\n\t\tif self.conv_type == 'DWConv':\n\t\t\tself.conv = nn.Conv2d(dim, dim, kernel_size=5, padding=2, groups=dim, padding_mode='reflect')\n\n\t\tif self.conv_type == 'DWConv' or self.use_attn:\n\t\t\tself.V = nn.Conv2d(dim, dim, 1)\n\t\t\tself.proj = nn.Conv2d(dim, dim, 1)\n\n\t\tif self.use_attn:\n\t\t\tself.QK = nn.Conv2d(dim, dim * 2, 1)\n\t\t\tself.attn = WindowAttention(dim, window_size, num_heads)\n\n\t\tself.apply(self._init_weights)\n\n\tdef _init_weights(self, m):\n\t\tif isinstance(m, nn.Conv2d):\n\t\t\tw_shape = m.weight.shape\n\t\t\t\n\t\t\tif w_shape[0] == self.dim * 2:\t# QK\n\t\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n\t\t\t\tstd = math.sqrt(2.0 / float(fan_in + fan_out))\n\t\t\t\ttrunc_normal_(m.weight, std=std)\t\t\n\t\t\telse:\n\t\t\t\tgain = (8 * self.network_depth) ** (-1/4)\n\t\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n\t\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n\t\t\t\ttrunc_normal_(m.weight, std=std)\n\n\t\t\tif m.bias !=  None:\n\t\t\t\tnn.init.constant_(m.bias, 0)\n\n\tdef check_size(self, x, shift=False):\n\t\t_, _, h, w = x.size()\n\t\tmod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n\t\tmod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n\n\t\tif shift:\n\t\t\tx = F.pad(x, (self.shift_size, (self.window_size-self.shift_size+mod_pad_w) % self.window_size,\n\t\t\t\t\t\t  self.shift_size, (self.window_size-self.shift_size+mod_pad_h) % self.window_size), mode='reflect')\n\t\telse:\n\t\t\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n\t\treturn x\n\n\tdef forward(self, X):\n\t\tB, C, H, W = X.shape\n\n\t\tif self.conv_type == 'DWConv' or self.use_attn:\n\t\t\tV = self.V(X)\n\t\t#print(self.use_attn)\n\t\tif self.use_attn:\n\t\t\t#print('attention')      \n\t\t\tQK = self.QK(X)\n\t\t\tQKV = torch.cat([QK, V], dim=1)\n\n\t\t\t# shift\n\t\t\tshifted_QKV = self.check_size(QKV, self.shift_size > 0)\n\t\t\tHt, Wt = shifted_QKV.shape[2:]\n\n\t\t\t# partition windows\n\t\t\tshifted_QKV = shifted_QKV.permute(0, 2, 3, 1)\n\t\t\tqkv = window_partition(shifted_QKV, self.window_size)  # nW*B, window_size**2, C\n\n\t\t\tattn_windows = self.attn(qkv)\n\n\t\t\t# merge windows\n\t\t\tshifted_out = window_reverse(attn_windows, self.window_size, Ht, Wt)  # B H' W' C\n\n\t\t\t# reverse cyclic shift\n\t\t\tout = shifted_out[:, self.shift_size:(self.shift_size+H), self.shift_size:(self.shift_size+W), :]\n\t\t\tattn_out = out.permute(0, 3, 1, 2)\n\n\t\t\tif self.conv_type in ['Conv', 'DWConv']:\n\t\t\t\tconv_out = self.conv(V)\n\t\t\t\tout = self.proj(conv_out + attn_out)\n\t\t\telse:\n\t\t\t\tout = self.proj(attn_out)\n\n\t\telse:\n\t\t\tif self.conv_type == 'Conv':\n\t\t\t\tout = self.conv(X)\t\t\t\t# no attention and use conv, no projection\n\t\t\telif self.conv_type == 'DWConv':\n\t\t\t\tout = self.proj(self.conv(V))\n\n\t\treturn out\n\n\nclass TransformerBlock(nn.Module):\n\tdef __init__(self, network_depth, dim, num_heads, mlp_ratio=4.,\n\t\t\t\t norm_layer=nn.LayerNorm, mlp_norm=False,\n\t\t\t\t window_size=8, shift_size=0, use_attn=True, conv_type=None):\n\t\tsuper().__init__()\n\t\tself.use_attn = use_attn\n\t\tself.mlp_norm = mlp_norm\n\n\t\tself.norm1 = norm_layer(dim) if use_attn else nn.Identity()\n\t\tself.attn = Attention(network_depth, dim, num_heads=num_heads, window_size=window_size,\n\t\t\t\t\t\t\t  shift_size=shift_size, use_attn=use_attn, conv_type=conv_type)\n\n\t\tself.norm2 = norm_layer(dim) if use_attn and mlp_norm else nn.Identity()\n\t\tself.mlp = Mlp(network_depth, dim, hidden_features=int(dim * mlp_ratio))\n\n\tdef forward(self, x):\n\t\tidentity = x\n\t\tif self.use_attn: x, rescale, rebias = self.norm1(x)\n\t\tx = self.attn(x)\n\t\tif self.use_attn: x = x * rescale + rebias\n\t\tx = identity + x\n\n\t\tidentity = x\n\t\tif self.use_attn and self.mlp_norm: x, rescale, rebias = self.norm2(x)\n\t\tx = self.mlp(x)\n\t\tif self.use_attn and self.mlp_norm: x = x * rescale + rebias\n\t\tx = identity + x\n\t\treturn x\n\n\nclass BasicLayer(nn.Module):\n\tdef __init__(self, network_depth, dim, depth, num_heads, mlp_ratio=4.,\n\t\t\t\t norm_layer=nn.LayerNorm, window_size=8,\n\t\t\t\t attn_ratio=0., attn_loc='last', conv_type=None):\n\n\t\tsuper().__init__()\n\t\tself.dim = dim\n\t\tself.depth = depth\n\n\t\tattn_depth = attn_ratio * depth\n\n\t\tif attn_loc == 'last':\n\t\t\tuse_attns = [i >= depth-attn_depth for i in range(depth)]\n\t\telif attn_loc == 'first':\n\t\t\tuse_attns = [i < attn_depth for i in range(depth)]\n\t\telif attn_loc == 'middle':\n\t\t\tuse_attns = [i >= (depth-attn_depth)//2 and i < (depth+attn_depth)//2 for i in range(depth)]\n\n\t\t# build blocks\n\t\tself.blocks = nn.ModuleList([\n\t\t\tTransformerBlock(network_depth=network_depth,\n\t\t\t\t\t\t\t dim=dim, \n\t\t\t\t\t\t\t num_heads=num_heads,\n\t\t\t\t\t\t\t mlp_ratio=mlp_ratio,\n\t\t\t\t\t\t\t norm_layer=norm_layer,\n\t\t\t\t\t\t\t window_size=window_size,\n\t\t\t\t\t\t\t shift_size=0 if (i % 2 == 0) else window_size // 2,\n\t\t\t\t\t\t\t use_attn=use_attns[i], conv_type=conv_type)\n\t\t\tfor i in range(depth)])\n\n\tdef forward(self, x):\n\t\tfor blk in self.blocks:\n\t\t\tx = blk(x)\n\t\treturn x\n\n\nclass PatchEmbed(nn.Module):\n\tdef __init__(self, patch_size=4, in_chans=3, embed_dim=96, kernel_size=None):\n\t\tsuper().__init__()\n\t\tself.in_chans = in_chans\n\t\tself.embed_dim = embed_dim\n\n\t\tif kernel_size is None:\n\t\t\tkernel_size = patch_size\n\n\t\tself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=kernel_size, stride=patch_size,\n\t\t\t\t\t\t\t  padding=(kernel_size-patch_size+1)//2, padding_mode='reflect')\n\n\tdef forward(self, x):\n\t\tx = self.proj(x)\n\t\treturn x\n\n\nclass PatchUnEmbed(nn.Module):\n\tdef __init__(self, patch_size=4, out_chans=3, embed_dim=96, kernel_size=None):\n\t\tsuper().__init__()\n\t\tself.out_chans = out_chans\n\t\tself.embed_dim = embed_dim\n\n\t\tif kernel_size is None:\n\t\t\tkernel_size = 1\n\n\t\tself.proj = nn.Sequential(\n\t\t\tnn.Conv2d(embed_dim, out_chans*patch_size**2, kernel_size=kernel_size,\n\t\t\t\t\t  padding=kernel_size//2, padding_mode='reflect'),\n\t\t\tnn.PixelShuffle(patch_size)\n\t\t)\n\n\tdef forward(self, x):\n\t\tx = self.proj(x)\n\t\treturn x\n\n\nclass SKFusion(nn.Module):\n\tdef __init__(self, dim, height=2, reduction=8):\n\t\tsuper(SKFusion, self).__init__()\n\t\t\n\t\tself.height = height\n\t\td = max(int(dim/reduction), 4)\n\t\t\n\t\tself.avg_pool = nn.AdaptiveAvgPool2d(1)\n\t\tself.mlp = nn.Sequential(\n\t\t\tnn.Conv2d(dim, d, 1, bias=False), \n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(d, dim*height, 1, bias=False)\n\t\t)\n\t\t\n\t\tself.softmax = nn.Softmax(dim=1)\n\n\tdef forward(self, in_feats):\n\t\tB, C, H, W = in_feats[0].shape\n\t\t\n\t\tin_feats = torch.cat(in_feats, dim=1)\n\t\tin_feats = in_feats.view(B, self.height, C, H, W)\n\t\t\n\t\tfeats_sum = torch.sum(in_feats, dim=1)\n\t\tattn = self.mlp(self.avg_pool(feats_sum))\n\t\tattn = self.softmax(attn.view(B, self.height, C, 1, 1))\n\n\t\tout = torch.sum(in_feats*attn, dim=1)\n\t\treturn out      \n\n\nclass DehazeFormer(nn.Module):\n\tdef __init__(self, in_chans=3, out_chans=4, window_size=8,\n\t\t\t\t embed_dims=[24, 48, 96, 48, 24],\n\t\t\t\t mlp_ratios=[2., 4., 4., 2., 2.],\n\t\t\t\t depths=[16, 16, 16, 8, 8],\n\t\t\t\t num_heads=[2, 4, 6, 1, 1],\n\t\t\t\t attn_ratio=[1/4, 1/2, 3/4, 0, 0],\n\t\t\t\t conv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n\t\t\t\t norm_layer=[RLN, RLN, RLN, RLN, RLN]):\n\t\tsuper(DehazeFormer, self).__init__()\n\n\t\t# setting\n\t\tself.patch_size = 4\n\t\tself.window_size = window_size\n\t\tself.mlp_ratios = mlp_ratios\n\n\t\t# split image into non-overlapping patches\n\t\tself.patch_embed = PatchEmbed(\n\t\t\tpatch_size=1, in_chans=in_chans, embed_dim=embed_dims[0], kernel_size=3)\n\n\t\t# backbone\n\t\tself.layer1 = BasicLayer(network_depth=sum(depths), dim=embed_dims[0], depth=depths[0],\n\t\t\t\t\t   \t\t\t num_heads=num_heads[0], mlp_ratio=mlp_ratios[0],\n\t\t\t\t\t   \t\t\t norm_layer=norm_layer[0], window_size=window_size,\n\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio[0], attn_loc='last', conv_type=conv_type[0])\n\n\t\tself.patch_merge1 = PatchEmbed(\n\t\t\tpatch_size=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])\n\n\t\tself.skip1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n\n\t\tself.layer2 = BasicLayer(network_depth=sum(depths), dim=embed_dims[1], depth=depths[1],\n\t\t\t\t\t\t\t\t num_heads=num_heads[1], mlp_ratio=mlp_ratios[1],\n\t\t\t\t\t\t\t\t norm_layer=norm_layer[1], window_size=window_size,\n\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[1], attn_loc='last', conv_type=conv_type[1])\n\n\t\tself.patch_merge2 = PatchEmbed(\n\t\t\tpatch_size=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])\n\n\t\tself.skip2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n\n\t\tself.layer3 = BasicLayer(network_depth=sum(depths), dim=embed_dims[2], depth=depths[2],\n\t\t\t\t\t\t\t\t num_heads=num_heads[2], mlp_ratio=mlp_ratios[2],\n\t\t\t\t\t\t\t\t norm_layer=norm_layer[2], window_size=window_size,\n\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[2], attn_loc='last', conv_type=conv_type[2])\n\n\t\tself.patch_split1 = PatchUnEmbed(\n\t\t\tpatch_size=2, out_chans=embed_dims[3], embed_dim=embed_dims[2])\n\n\t\tassert embed_dims[1] == embed_dims[3]\n\t\tself.fusion1 = SKFusion(embed_dims[3])\n\n\t\tself.layer4 = BasicLayer(network_depth=sum(depths), dim=embed_dims[3], depth=depths[3],\n\t\t\t\t\t\t\t\t num_heads=num_heads[3], mlp_ratio=mlp_ratios[3],\n\t\t\t\t\t\t\t\t norm_layer=norm_layer[3], window_size=window_size,\n\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[3], attn_loc='last', conv_type=conv_type[3])\n\n\t\tself.patch_split2 = PatchUnEmbed(\n\t\t\tpatch_size=2, out_chans=embed_dims[4], embed_dim=embed_dims[3])\n\n\t\tassert embed_dims[0] == embed_dims[4]\n\t\tself.fusion2 = SKFusion(embed_dims[4])\t\t\t\n\n\t\tself.layer5 = BasicLayer(network_depth=sum(depths), dim=embed_dims[4], depth=depths[4],\n\t\t\t\t\t   \t\t\t num_heads=num_heads[4], mlp_ratio=mlp_ratios[4],\n\t\t\t\t\t   \t\t\t norm_layer=norm_layer[4], window_size=window_size,\n\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio[4], attn_loc='last', conv_type=conv_type[4])\n\n\t\t# merge non-overlapping patches into image\n\t\tself.patch_unembed = PatchUnEmbed(\n\t\t\tpatch_size=1, out_chans=out_chans, embed_dim=embed_dims[4], kernel_size=3)\n\n\n\tdef check_image_size(self, x):\n\t\t# NOTE: for I2I test\n\t\t_, _, h, w = x.size()\n\t\tmod_pad_h = (self.patch_size - h % self.patch_size) % self.patch_size\n\t\tmod_pad_w = (self.patch_size - w % self.patch_size) % self.patch_size\n\t\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n\t\treturn x\n\n\tdef forward_features(self, x):\n\t\tx = self.patch_embed(x)\n\t\tx = self.layer1(x)\n\t\tskip1 = x\n\n\t\tx = self.patch_merge1(x)\n\t\tx = self.layer2(x)\n\t\tskip2 = x\n\n\t\tx = self.patch_merge2(x)\n\t\tx = self.layer3(x)\n\t\tx = self.patch_split1(x)\n\n\t\tx = self.fusion1([x, self.skip2(skip2)]) + x\n\t\tx = self.layer4(x)\n\t\tx = self.patch_split2(x)\n\n\t\tx = self.fusion2([x, self.skip1(skip1)]) + x\n\t\tx = self.layer5(x)\n\t\tx = self.patch_unembed(x)\n\t\treturn x\n\n\tdef forward(self, x):\n\t\tH, W = x.shape[2:]\n\t\tx = self.check_image_size(x)\n\n\t\tfeat = self.forward_features(x)\n\t\tK, B = torch.split(feat, (1, 3), dim=1)\n\n\t\tx = K * x - B + x\n\t\tx = x[:, :, :H, :W]\n\t\treturn x\n\n\ndef dehazeformer_t():\n    return DehazeFormer(\n\t\tembed_dims=[24, 48, 96, 48, 24],\n\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n\t\tdepths=[4, 4, 4, 2, 2],\n\t\tnum_heads=[2, 4, 6, 1, 1],\n\t\tattn_ratio=[0, 1/2, 1, 0, 0],\n\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n\n\ndef dehazeformer_s():\n    return DehazeFormer(\n\t\tembed_dims=[24, 48, 96, 48, 24],\n\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n\t\tdepths=[8, 8, 8, 4, 4],\n\t\tnum_heads=[2, 4, 6, 1, 1],\n\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n\n\ndef dehazeformer_b():\n    return DehazeFormer(\n        embed_dims=[24, 48, 96, 48, 24],\n\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n\t\tdepths=[16, 16, 16, 8, 8],\n\t\tnum_heads=[2, 4, 6, 1, 1],\n\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n\n\ndef dehazeformer_d():\n    return DehazeFormer(\n        embed_dims=[24, 48, 96, 48, 24],\n\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n\t\tdepths=[32, 32, 32, 16, 16],\n\t\tnum_heads=[2, 4, 6, 1, 1],\n\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n\n\ndef dehazeformer_w():\n    return DehazeFormer(\n        embed_dims=[48, 96, 192, 96, 48],\n\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n\t\tdepths=[16, 16, 16, 8, 8],\n\t\tnum_heads=[2, 4, 6, 1, 1],\n\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n\n\ndef dehazeformer_m():\n    return DehazeFormer(\n\t\tembed_dims=[24, 48, 96, 48, 24],\n\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n\t\tdepths=[12, 12, 12, 6, 6],\n\t\tnum_heads=[2, 4, 6, 1, 1],\n\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n\t\tconv_type=['Conv', 'Conv', 'Conv', 'Conv', 'Conv'])\n\n\ndef dehazeformer_l():\n    return DehazeFormer(\n\t\tembed_dims=[48, 96, 192, 96, 48],\n\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n\t\tdepths=[16, 16, 16, 12, 12],\n\t\tnum_heads=[2, 4, 6, 1, 1],\n\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n\t\tconv_type=['Conv', 'Conv', 'Conv', 'Conv', 'Conv'])","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-02-16T15:38:17.859372Z","iopub.execute_input":"2025-02-16T15:38:17.859714Z","iopub.status.idle":"2025-02-16T15:38:18.144678Z","shell.execute_reply.started":"2025-02-16T15:38:17.859689Z","shell.execute_reply":"2025-02-16T15:38:18.143590Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"\nfrom torch.nn import functional as F\n\nimport torch.nn as nn\n\nclass BasicBlock(nn.Module):\n\tdef __init__(self, network_depth, dim, depth, num_heads, mlp_ratio=4.,\n\t\t\t\t norm_layer=nn.LayerNorm, window_size=8,\n\t\t\t\t attn_ratio=0., attn_loc='last', conv_type=None):\n\n\t\tsuper().__init__()\n\t\tself.dim = dim\n\t\tself.depth = depth\n\t\tself.gf=FastGuidedFilter(r=1)\n\t\tself.downsample = nn.Upsample(\n            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n        )\n    \n\t\tdepth_rate=24\n\t\tkernel_size=3\n\t\tin_channels=3\n\t\tself.conv_out = nn.Conv2d(depth_rate*2, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n\t\tself.relu1=nn.ReLU(inplace=True)\n\t\tself.relu2=nn.ReLU(inplace=True)\n\t\tself.norm1=AdaptiveInstanceNorm(depth_rate)\n\t\tself.norm2=AdaptiveInstanceNorm(depth_rate) \n\t\tattn_depth = attn_ratio * depth\n\t\t#print(attn_depth,attn_ratio,depth)\n\t\tif attn_loc == 'last':\n\t\t\tuse_attns = [i >= depth-attn_depth for i in range(depth)]\n\t\telif attn_loc == 'first':\n\t\t\tuse_attns = [i < attn_depth for i in range(depth)]\n\t\telif attn_loc == 'middle':\n\t\t\tuse_attns = [i >= (depth-attn_depth)//2 and i < (depth+attn_depth)//2 for i in range(depth)]\n\n\t\t# build blocks\n\t\tself.blocks = nn.ModuleList([\n\t\t\tTransformerBlock(network_depth=network_depth,\n\t\t\t\t\t\t\t dim=dim, \n\t\t\t\t\t\t\t num_heads=num_heads,\n\t\t\t\t\t\t\t mlp_ratio=mlp_ratio,\n\t\t\t\t\t\t\t norm_layer=norm_layer,\n\t\t\t\t\t\t\t window_size=window_size,\n\t\t\t\t\t\t\t shift_size=0 if (i % 2 == 0) else window_size // 2,\n\t\t\t\t\t\t\t use_attn=use_attns[i], conv_type=conv_type)\n\t\t\tfor i in range(depth)])\n\n\tdef forward(self, x_hr):\n\t\tx_lr = self.downsample(x_hr)\n   \n\t\tx_lr_new=self.norm1(x_lr)\n\t\tx_lr_new=self.relu1( x_lr_new)\n\t\tfor blc in self.blocks:\n   \n\t\t    x_lr_new = blc(x_lr_new)\n    \n\t\tg_hr= self.gf(x_lr, x_lr_new, x_hr)\n\t\tgx_cat=torch.cat([g_hr,x_hr],1)\n\t\tg_hr=self.conv_out(gx_cat)\n\t\tg_hr=self.norm2(g_hr)\n\t\tg_hr=self.relu2( g_hr)\n\t\tx=g_hr+x_hr\n   \n\t\treturn g_hr\n   \n\n\nclass DeepGuidedFilterFormer(nn.Module):\n    def __init__(self,  radius=1):\n        super().__init__()\n\n        \n        norm = AdaptiveInstanceNorm\n        depth_rate=24\n        kernel_size=3\n        in_channels=3\n        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n        self.relu1=nn.ReLU(inplace=True)\n        self.block_num=3\n        network_depth=50\n        dim=depth_rate\n        mlp_ratio=2.0\n        norm_layer=RLN\n        window_size=16\n        conv_type='Conv'\n        depth=4\n        num_heads=4\n        attn_ratio=1/4\n        \n        \n        self.blocks = nn.ModuleList([\n                   BasicBlock(network_depth=network_depth, dim=dim, depth=depth,\n\t\t\t\t\t   \t\t\t num_heads=num_heads, mlp_ratio=mlp_ratio,\n\t\t\t\t\t   \t\t\t norm_layer=norm_layer, window_size=window_size,\n\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio, attn_loc='last', conv_type=conv_type)\n\t\t\t             for i in range(self.block_num)])\n\n        \n\n    def forward(self, x_hr):\n        x_hr=self.conv_in(x_hr)\n        #x_hr=self.relu1(x_hr)\n        #pixelshuffle_ratio=2\n        # Unpixelshuffle\n        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n        \n        for blc in self.blocks:\n            x_hr=blc(x_hr)\n            \n        x_hr=self.conv_out(x_hr)\n        # Pixelshuffle\n        #y_lr = F.pixel_shuffle(\n           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n        #)\n\n        return x_hr\n           \n   \n   \nclass ConvGuidedFilter(nn.Module):\n    \"\"\"\n    Adapted from https://github.com/wuhuikai/DeepGuidedFilter\n    \"\"\"\n    def __init__(self, radius=1, norm=nn.BatchNorm2d, conv_a_kernel_size: int = 1):\n        super(ConvGuidedFilter, self).__init__()\n\n        self.box_filter = nn.Conv2d(\n            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n        )\n        self.conv_a = nn.Sequential(\n            nn.Conv2d(\n                6,\n                32,\n                kernel_size=conv_a_kernel_size,\n                padding=conv_a_kernel_size // 2,\n                bias=False,\n            ),\n            norm(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                32,\n                32,\n                kernel_size=conv_a_kernel_size,\n                padding=conv_a_kernel_size // 2,\n                bias=False,\n            ),\n            norm(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                32,\n                3,\n                kernel_size=conv_a_kernel_size,\n                padding=conv_a_kernel_size // 2,\n                bias=False,\n            ),\n        )\n        self.box_filter.weight.data[...] = 1.0\n\n    def forward(self, x_lr, y_lr, x_hr):\n        _, _, h_lrx, w_lrx = x_lr.size()\n        _, _, h_hrx, w_hrx = x_hr.size()\n\n        N = self.box_filter(x_lr.data.new().resize_((1, 3, h_lrx, w_lrx)).fill_(1.0))\n        ## mean_x\n        mean_x = self.box_filter(x_lr) / N\n        ## mean_y\n        mean_y = self.box_filter(y_lr) / N\n        ## cov_xy\n        cov_xy = self.box_filter(x_lr * y_lr) / N - mean_x * mean_y\n        ## var_x\n        var_x = self.box_filter(x_lr * x_lr) / N - mean_x * mean_x\n\n        ## A\n        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n        ## b\n        b = mean_y - A * mean_x\n\n        ## mean_A; mean_b\n        mean_A = F.interpolate(A, (h_hrx, w_hrx), mode=\"bilinear\", align_corners=True)\n        mean_b = F.interpolate(b, (h_hrx, w_hrx), mode=\"bilinear\", align_corners=True)\n\n        return mean_A * x_hr + mean_b\n        \n\n\n        \n\nclass DeepGuideddetail(nn.Module):\n    def __init__(self,  radius=1):\n        super().__init__()\n\n        \n        norm = AdaptiveInstanceNorm\n\n        \n        \n\n        #self.lr = dehazeformer_m()\n        kernel_size=3\n        depth_rate=16\n        in_channels=3\n        num_dense_layer=4\n        growth_rate=16\n        growth_rate=16\n        \n        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n        self.rdb1 = SRDB(depth_rate, num_dense_layer, growth_rate)\n        self.rdb2 = SRDB(depth_rate, num_dense_layer, growth_rate)\n        self.rdb3 = SRDB(depth_rate, num_dense_layer, growth_rate)\n        self.rdb4 = SRDB(depth_rate, num_dense_layer, growth_rate)\n\n        self.gf = ConvGuidedFilter(radius, norm=norm)\n\n        self.downsample = nn.Upsample(\n            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n        )\n\n    def forward(self, x_hr):\n        x_lr = self.downsample(x_hr)\n        y_lr=self.conv_in(x_lr)\n        y_lr=self.rdb1(y_lr)\n        y_lr=self.rdb2(y_lr)\n        y_lr=self.rdb3(y_lr)\n        y_lr=self.rdb4(y_lr)\n        y_lr=self.conv_out(y_lr)\n        \n        #pixelshuffle_ratio=2\n        # Unpixelshuffle\n        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n        \n        #y_lr=self.lr(x_lr)\n        # Pixelshuffle\n        #y_lr = F.pixel_shuffle(\n           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n        #)\n\n        return F.tanh( self.gf(x_lr, y_lr, x_hr))\n                \n        \n\nclass DeepGuidedall(nn.Module):\n    def __init__(self,  radius=1):\n        super().__init__()\n\n        \n        norm = AdaptiveInstanceNorm\n\n        \n        \n\n        #self.lr = dehazeformer_m()\n        kernel_size=3\n        depth_rate=16\n        in_channels=3\n        num_dense_layer=4\n        growth_rate=16\n        growth_rate=16\n        \n        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n        self.rdb1 = SRDB(depth_rate, num_dense_layer, growth_rate)\n        self.rdb2 = SRDB(depth_rate, num_dense_layer, growth_rate)\n        self.rdb3 = SRDB(depth_rate, num_dense_layer, growth_rate)\n        self.rdb4 = SRDB(depth_rate, num_dense_layer, growth_rate)\n\n        self.gf = ConvGuidedFilter(radius, norm=norm)\n        self.lr = dehazeformer_m()\n\n        self.downsample = nn.Upsample(\n            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n        )\n        self.upsample = nn.Upsample(\n            scale_factor=2, mode=\"bilinear\", align_corners=True\n        )\n\n    def forward(self, x_hr):\n        x_lr = self.downsample(x_hr)\n        y_lr=self.conv_in(x_lr)\n        y_lr=self.rdb1(y_lr)\n        y_lr=self.rdb2(y_lr)\n        y_lr=self.rdb3(y_lr)\n        y_lr=self.rdb4(y_lr)\n        y_detail=self.conv_out(y_lr)\n        y_base=self.lr(x_lr)\n        y_lr=y_base+y_detail\n        y_base=self.upsample(y_base)\n        \n        #pixelshuffle_ratio=2\n        # Unpixelshuffle\n        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n        \n        #y_lr=self.lr(x_lr)\n        # Pixelshuffle\n        #y_lr = F.pixel_shuffle(\n           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n        #)\n\n        return F.tanh( self.gf(x_lr, y_lr, x_hr)), y_base   \n\n\n\n\nclass DeepGuidednew(nn.Module):\n    def __init__(self,  radius=1):\n        super().__init__()\n\n        \n        norm = AdaptiveInstanceNorm\n\n        \n        \n\n        #self.lr = dehazeformer_m()\n        kernel_size=3\n        depth_rate=16\n        in_channels=3\n        num_dense_layer=4\n        growth_rate=16\n        growth_rate=16\n        \n        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n        self.rdb1 = SRDB(depth_rate, num_dense_layer, growth_rate)\n        self.rdb2 = SRDB(depth_rate, num_dense_layer, growth_rate)\n        self.rdb3 = SRDB(depth_rate, num_dense_layer, growth_rate)\n        self.rdb4 = SRDB(depth_rate, num_dense_layer, growth_rate)\n\n        self.gf = ConvGuidedFilter(radius, norm=norm)\n        self.lr = dehazeformer_m()\n\n        self.downsample = nn.Upsample(\n            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n        )\n        self.upsample = nn.Upsample(\n            scale_factor=2, mode=\"bilinear\", align_corners=True\n        )\n\n    def forward(self, x_hr):\n        x_lr = self.downsample(x_hr)\n        y_lr=self.conv_in(x_lr)\n        y_lr=self.rdb1(y_lr)\n        y_lr=self.rdb2(y_lr)\n        y_lr=self.rdb3(y_lr)\n        y_lr=self.rdb4(y_lr)\n        y_detail=self.conv_out(y_lr)\n        y_base=self.lr(x_lr)\n        y_lr=y_base+y_detail\n        y_base=self.upsample(y_base)\n        \n        #pixelshuffle_ratio=2\n        # Unpixelshuffle\n        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n        \n        #y_lr=self.lr(x_lr)\n        # Pixelshuffle\n        #y_lr = F.pixel_shuffle(\n           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n        #)\n\n        return  self.gf(x_lr, y_lr, x_hr), y_base               \n        \n        \n\nclass DeepAtrousGuidedFilter(nn.Module):\n    def __init__(self,  radius=1):\n        super().__init__()\n\n        \n        norm = AdaptiveInstanceNorm\n\n        \n        \n\n        self.lr = dehazeformer_m()\n\n        self.gf = ConvGuidedFilter(radius, norm=norm)\n\n        self.downsample = nn.Upsample(\n            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n        )\n\n    def forward(self, x_hr):\n        x_lr = self.downsample(x_hr)\n        #pixelshuffle_ratio=2\n        # Unpixelshuffle\n        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n        y_lr=self.lr(x_lr)\n        # Pixelshuffle\n        #y_lr = F.pixel_shuffle(\n           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n        #)\n\n        return F.tanh( self.gf(x_lr, y_lr, x_hr))","metadata":{"execution":{"iopub.status.busy":"2025-02-16T15:33:34.159901Z","iopub.execute_input":"2025-02-16T15:33:34.160294Z","iopub.status.idle":"2025-02-16T15:33:34.199654Z","shell.execute_reply.started":"2025-02-16T15:33:34.160253Z","shell.execute_reply":"2025-02-16T15:33:34.198506Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass AdaptiveInstanceNorm(nn.Module):\n    def __init__(self, n):\n        super(AdaptiveInstanceNorm, self).__init__()\n\n        self.w_0 = nn.Parameter(torch.Tensor([1.0]))\n        self.w_1 = nn.Parameter(torch.Tensor([0.0]))\n\n        self.ins_norm = nn.InstanceNorm2d(n, momentum=0.999, eps=0.001, affine=True)\n\n    def forward(self, x):\n        return self.w_0 * x + self.w_1 * self.ins_norm(x)\n\n\nclass PALayer(nn.Module):\n    def __init__(self, channel: int):\n        super(PALayer, self).__init__()\n        self.pa = nn.Sequential(\n            nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        y = self.pa(x)\n        return x * y\n\n\nclass CALayer(nn.Module):\n    def __init__(self, channel: int):\n        super(CALayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.ca = nn.Sequential(\n            nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.ca(y)\n        return x * y\n","metadata":{"execution":{"iopub.status.busy":"2025-02-16T15:33:34.201243Z","iopub.execute_input":"2025-02-16T15:33:34.201693Z","iopub.status.idle":"2025-02-16T15:33:34.230864Z","shell.execute_reply.started":"2025-02-16T15:33:34.201651Z","shell.execute_reply":"2025-02-16T15:33:34.228905Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom torch.nn import functional as F\n\n\ndef unpixel_shuffle(feature, r: int = 1):\n    b, c, h, w = feature.shape\n    out_channel = c * (r ** 2)\n    out_h = h // r\n    out_w = w // r\n    feature_view = feature.contiguous().view(b, c, out_h, r, out_w, r)\n    feature_prime = (\n        feature_view.permute(0, 1, 3, 5, 2, 4)\n        .contiguous()\n        .view(b, out_channel, out_h, out_w)\n    )\n    return feature_prime\n\n\ndef sample_patches(\n    inputs: torch.Tensor, patch_size: int = 3, stride: int = 2\n) -> torch.Tensor:\n    \"\"\"\n\n    :param inputs: the input feature maps, shape: (n, c, h, w).\n    :param patch_size: the spatial size of sampled patches\n    :param stride: the stride of sampling.\n    :return: extracted patches, shape: (n, c, patch_size, patch_size, n_patches).\n    \"\"\"\n    \"\"\"\n    Patch sampler for feature maps.\n    Parameters\n    ---\n    inputs : torch.Tensor\n        \n    patch_size : int, optional\n       \n    stride : int, optional\n        \n    Returns\n    ---\n    patches : torch.Tensor\n        \n    \"\"\"\n\n    n, c, h, w = inputs.shape\n    patches = (\n        inputs.unfold(2, patch_size, stride)\n        .unfold(3, patch_size, stride)\n        .reshape(n, c, -1, patch_size, patch_size)\n        .permute(0, 1, 3, 4, 2)\n    )\n    return patches\n\n\ndef chop_patches(\n    img: torch.Tensor, patch_size_h: int = 256, patch_size_w: int = 512\n) -> torch.Tensor:\n    \"\"\"\n\n    :param inputs: the input feature maps, shape: (n, c, h, w).\n    :param patch_size: the spatial size of sampled patches\n    :param stride: the stride of sampling.\n    :return: extracted patches, shape: (n, c, patch_size, patch_size, n_patches).\n    \"\"\"\n    \"\"\"\n    Patch sampler for feature maps.\n    Parameters\n    ---\n    inputs : torch.Tensor\n\n    patch_size : int, optional\n\n    stride : int, optional\n\n    Returns\n    ---\n    patches : torch.Tensor\n\n    \"\"\"\n    patches = (\n        img.unfold(2, patch_size_h, patch_size_h)\n        .unfold(3, patch_size_w, patch_size_w)\n        .contiguous()\n        .permute(2, 3, 0, 1, 4, 5)\n        .flatten(start_dim=0, end_dim=2)\n        # .reshape(-1, c, patch_size_h, patch_size_w)\n    )\n    return patches\n\n\ndef unchop_patches(\n    patches: torch.Tensor, img_h: int = 1024, img_w: int = 2048, n: int = 1\n) -> torch.Tensor:\n    \"\"\"\n    Assumes non-overlapping patches\n\n    See: https://discuss.pytorch.org/t/reshaping-windows-into-image/19805\n    \"\"\"\n    _, c, patch_size_h, patch_size_w = patches.shape\n    num_h = img_h // patch_size_h\n    num_w = img_w // patch_size_w\n\n    img = patches.reshape(n, num_h * num_w, patch_size_h * patch_size_w * c).permute(\n        0, 2, 1\n    )\n    img = F.fold(\n        img,\n        (img_h, img_w),\n        (patch_size_h, patch_size_w),\n        1,\n        0,\n        (patch_size_h, patch_size_w),\n    )\n    return img.reshape(n, c, img_h, img_w)\n\ndef roll_n(X, axis, n):\n    f_idx = tuple(\n        slice(None, None, None) if i != axis else slice(0, n, None)\n        for i in range(X.dim())\n    )\n    b_idx = tuple(\n        slice(None, None, None) if i != axis else slice(n, None, None)\n        for i in range(X.dim())\n    )\n    front = X[f_idx]\n    back = X[b_idx]\n    return torch.cat([back, front], axis)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:33:34.280558Z","iopub.execute_input":"2025-02-16T15:33:34.280935Z","iopub.status.idle":"2025-02-16T15:33:34.295904Z","shell.execute_reply.started":"2025-02-16T15:33:34.280897Z","shell.execute_reply":"2025-02-16T15:33:34.294492Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\n\n# --- Imports --- #\nimport torch\nimport torch.nn.functional as F\n\n\n# --- Perceptual loss network  --- #\nclass LossNetwork(torch.nn.Module):\n    def __init__(self, vgg_model):\n        super(LossNetwork, self).__init__()\n        self.vgg_layers = vgg_model\n        self.layer_name_mapping = {\n            '3': \"relu1_2\",\n            '8': \"relu2_2\",\n            '15': \"relu3_3\"\n        }\n\n    def output_features(self, x):\n        output = {}\n        for name, module in self.vgg_layers._modules.items():\n            x = module(x)\n            if name in self.layer_name_mapping:\n                output[self.layer_name_mapping[name]] = x\n        return list(output.values())\n\n    def forward(self, dehaze, gt):\n        loss = []\n        dehaze_features = self.output_features(dehaze)\n        gt_features = self.output_features(gt)\n        for dehaze_feature, gt_feature in zip(dehaze_features, gt_features):\n            loss.append(F.mse_loss(dehaze_feature, gt_feature))\n\n        return sum(loss)/len(loss)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:33:34.297293Z","iopub.execute_input":"2025-02-16T15:33:34.297769Z","iopub.status.idle":"2025-02-16T15:33:34.324067Z","shell.execute_reply.started":"2025-02-16T15:33:34.297727Z","shell.execute_reply":"2025-02-16T15:33:34.322623Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n\n# --- Imports --- #\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\n    \nclass ConvBlock(nn.Module):\n    def __init__(self, in_channel, out_channel, kernel_size=3):\n        super().__init__()\n\n        self.conv1 =nn.Conv2d(in_channel, in_channel, kernel_size=kernel_size, padding=(kernel_size - 1) // 2) #ConvLayer(in_channel, in_channel, 3)\n        self.conv2 = ConvLayer(in_channel, out_channel, 3, downsample=True)\n\n    def forward(self, input):\n        out = self.conv1(input)\n        out = self.conv2(out)\n\n        return out    \n    \n\n\n\n\n        \n\n\n\n\n        \nclass CAB(nn.Module):\n    def __init__(self, features):\n        super(CAB, self).__init__()\n        #new_features=features//2\n        features=features//2\n        self.reduce_fature=nn.Conv2d(features*2, features, kernel_size=1, bias=False)\n        self.delta_gen1 = nn.Sequential(\n                        nn.Conv2d(features*2, features, kernel_size=1, bias=False),\n                        nn.Conv2d(features, 2, kernel_size=3, padding=1, bias=False)\n                        )\n\n        self.delta_gen2 = nn.Sequential(\n                        nn.Conv2d(features*2, features, kernel_size=1, bias=False),\n                        nn.Conv2d(features, 2, kernel_size=3, padding=1, bias=False)\n                        )\n\n\n        #self.delta_gen1.weight.data.zero_()\n        #self.delta_gen2.weight.data.zero_()\n\n    # https://github.com/speedinghzl/AlignSeg/issues/7\n    # the normlization item is set to [w/s, h/s] rather than [h/s, w/s]\n    # the function bilinear_interpolate_torch_gridsample2 is standard implementation, please use bilinear_interpolate_torch_gridsample2 for training.\n    def bilinear_interpolate_torch_gridsample(self, input, size, delta=0):\n        out_h, out_w = size\n        n, c, h, w = input.shape\n        s = 1.0\n        norm = torch.tensor([[[[w/s, h/s]]]]).type_as(input).to(input.device)\n        w_list = torch.linspace(-1.0, 1.0, out_h).view(-1, 1).repeat(1, out_w)\n        h_list = torch.linspace(-1.0, 1.0, out_w).repeat(out_h, 1)\n        grid = torch.cat((h_list.unsqueeze(2), w_list.unsqueeze(2)), 2)\n        grid = grid.repeat(n, 1, 1, 1).type_as(input).to(input.device)\n        grid = grid + delta.permute(0, 2, 3, 1) / norm\n\n        output = F.grid_sample(input, grid)\n        return output\n\n    def bilinear_interpolate_torch_gridsample2(self, input, size, delta=0):\n        out_h, out_w = size\n        n, c, h, w = input.shape\n        s = 2.0\n        norm = torch.tensor([[[[(out_w-1)/s, (out_h-1)/s]]]]).type_as(input).to(input.device) # not [h/s, w/s]\n        w_list = torch.linspace(-1.0, 1.0, out_h).view(-1, 1).repeat(1, out_w)\n        h_list = torch.linspace(-1.0, 1.0, out_w).repeat(out_h, 1)\n        grid = torch.cat((h_list.unsqueeze(2), w_list.unsqueeze(2)), 2)\n        grid = grid.repeat(n, 1, 1, 1).type_as(input).to(input.device)\n        grid = grid + delta.permute(0, 2, 3, 1) / norm\n\n        output = F.grid_sample(input, grid, align_corners=True)\n        return output\n\n    def forward(self, low_stage, high_stage):\n        h, w = low_stage.size(2), low_stage.size(3)\n        high_stage=self.reduce_fature(high_stage)\n        high_stage = F.interpolate(input=high_stage, size=(h, w), mode='bilinear', align_corners=True)\n        \n        concat = torch.cat((low_stage, high_stage), 1)\n        delta1 = self.delta_gen1(concat)\n        delta2 = self.delta_gen2(concat)\n        high_stage = self.bilinear_interpolate_torch_gridsample2(high_stage, (h, w), delta1)\n        low_stage = self.bilinear_interpolate_torch_gridsample2(low_stage, (h, w), delta2)\n\n        high_stage += low_stage\n        return high_stage\n\nclass MakeDense(nn.Module):\n    def __init__(self, in_channels, growth_rate, kernel_size=3):\n        super(MakeDense, self).__init__()\n        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=kernel_size, padding=(kernel_size-1)//2)\n\n    def forward(self, x):\n        out = F.relu(self.conv(x))\n        out = torch.cat((x, out), 1)\n        return out\n\nclass PALayer(nn.Module):\n    def __init__(self, channel):\n        super(PALayer, self).__init__()\n        self.pa = nn.Sequential(\n                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n                nn.Sigmoid()\n        )\n    def forward(self, x):\n        y = self.pa(x)\n        return x * y\n\nclass CALayer(nn.Module):\n    def __init__(self, channel):\n        super(CALayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.ca = nn.Sequential(\n                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.ca(y)\n        return x * y\n\nclass SRDBDK(nn.Module):\n    def __init__(self, in_channels, num_dense_layer, growth_rate):\n        super(SRDBDK, self).__init__()\n        \n        modules = []\n        self.split_channel=in_channels//8\n        kernel_size=3\n        dilation=1\n        self.conv1 = nn.Conv2d(self.split_channel*1, self.split_channel, kernel_size=9, padding=4, dilation=1)\n        dilation=2\n        self.conv2 = nn.Conv2d(self.split_channel*2, self.split_channel*1, kernel_size=7, padding=3, dilation=1)\n        dilation=4\n        self.conv3 = nn.Conv2d(self.split_channel*4, self.split_channel*2, kernel_size=5,  padding=2, dilation=1)\n        dilation=8\n        self.conv4 = nn.Conv2d(self.split_channel*8, self.split_channel*4, kernel_size=3, padding=1, dilation=1)\n\n            \n        #self.residual_dense_layers = nn.Sequential(*modules)\n        _in_channels=in_channels\n        self.calayer=CALayer(in_channels)\n        self.palayer=PALayer(in_channels)\n        self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n\n    def forward(self, x):\n        splited = torch.split(x, [self.split_channel,self.split_channel*1,self.split_channel*2,self.split_channel*4], dim=1)\n        x0=F.relu(self.conv1(splited[0]))\n        tmp= torch.cat((splited[1], x0), 1)\n        x1=F.relu(self.conv2(tmp))\n        tmp= torch.cat((splited[2], x0, x1), 1)\n        x2=F.relu(self.conv3(tmp))\n        tmp= torch.cat((splited[3], x0, x1, x2), 1)\n        x3=F.relu(self.conv4(tmp))\n        tmp= torch.cat(( x0, x1, x2, x3), 1)\n        \n        out = self.conv_1x1(tmp)\n        out=self.calayer(out)\n        out=self.palayer(out)\n        out=out+x\n        return out\n        \n        \nclass SRDB(nn.Module):\n    def __init__(self, in_channels, num_dense_layer, growth_rate):\n        super(SRDB, self).__init__()\n        \n        modules = []\n        self.split_channel=in_channels//4\n        kernel_size=3\n        dilation=1\n        self.conv1 = nn.Conv2d(self.split_channel*1, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n        dilation=2\n        self.conv2 = nn.Conv2d(self.split_channel*2, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n        dilation=4\n        self.conv3 = nn.Conv2d(self.split_channel*3, self.split_channel, kernel_size=kernel_size,  padding=dilation, dilation=dilation)\n        dilation=8\n        self.conv4 = nn.Conv2d(self.split_channel*4, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n\n            \n        #self.residual_dense_layers = nn.Sequential(*modules)\n        _in_channels=in_channels\n        self.calayer=CALayer(in_channels)\n        self.palayer=PALayer(in_channels)\n        self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n\n    def forward(self, x):\n        splited = torch.split(x, self.split_channel, dim=1)\n        x0=F.relu(self.conv1(splited[0]))\n        tmp= torch.cat((splited[1], x0), 1)\n        x1=F.relu(self.conv2(tmp))\n        tmp= torch.cat((splited[2], x0, x1), 1)\n        x2=F.relu(self.conv3(tmp))\n        tmp= torch.cat((splited[3], x0, x1, x2), 1)\n        x3=F.relu(self.conv4(tmp))\n        tmp= torch.cat(( x0, x1, x2, x3), 1)\n        \n        out = self.conv_1x1(tmp)\n        out=self.calayer(out)\n        out=self.palayer(out)\n        #print(out.shape, x.shape)\n        out=out+x\n        return out\n\n\nclass SRDBN(nn.Module):\n    def __init__(self, in_channels, num_dense_layer, growth_rate):\n        super(SRDBN, self).__init__()\n        modules = []\n        self.split_channel=in_channels//8\n        kernel_size=3\n        dilation=1\n        self.conv1 = nn.Conv2d(self.split_channel*1, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n        dilation=2\n        self.conv2 = nn.Conv2d(self.split_channel*2, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n        dilation=4\n        self.conv3 = nn.Conv2d(self.split_channel*3, self.split_channel, kernel_size=kernel_size,  padding=dilation, dilation=dilation)\n        dilation=8\n        self.conv4 = nn.Conv2d(self.split_channel*4, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n        \n        dilation=8\n        self.conv5 = nn.Conv2d(self.split_channel*5, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n        \n        dilation=4\n        self.conv6 = nn.Conv2d(self.split_channel*6, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n        \n        dilation=2\n        self.conv7 = nn.Conv2d(self.split_channel*7, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n        \n        dilation=1\n        self.conv8 = nn.Conv2d(self.split_channel*8, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n\n            \n        #self.residual_dense_layers = nn.Sequential(*modules)\n        _in_channels=in_channels\n        self.calayer=CALayer(in_channels)\n        self.palayer=PALayer(in_channels)\n        self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n\n    def forward(self, x):\n        splited = torch.split(x, self.split_channel, dim=1)\n        x0=F.relu(self.conv1(splited[0]))\n        tmp= torch.cat((splited[1], x0), 1)\n        x1=F.relu(self.conv2(tmp))\n        tmp= torch.cat((splited[2], x0, x1), 1)\n        x2=F.relu(self.conv3(tmp))\n        tmp= torch.cat((splited[3], x0, x1, x2), 1)\n        x3=F.relu(self.conv4(tmp))\n        tmp= torch.cat(( splited[4],x0, x1, x2, x3), 1)\n        x4=F.relu(self.conv5(tmp))\n        \n        tmp= torch.cat(( splited[5],x0, x1, x2, x3,x4), 1)\n        x5=F.relu(self.conv6(tmp))\n        \n        tmp= torch.cat(( splited[6],x0, x1, x2, x3,x4,x5), 1)\n        x6=F.relu(self.conv7(tmp))\n        \n        tmp= torch.cat(( splited[7],x0, x1, x2, x3,x4,x5,x6), 1)\n        x7=F.relu(self.conv8(tmp))\n        \n       \n        tmp= torch.cat(( x0, x1, x2, x3,x4,x5,x6,x7), 1)\n        out = self.conv_1x1(tmp)\n        out=self.calayer(out)\n        out=self.palayer(out)\n        out=out+x\n        return out\n\n\n        \n                \n\nclass RDB(nn.Module):\n    def __init__(self, in_channels, num_dense_layer, growth_rate):\n        super(RDB, self).__init__()\n        _in_channels = in_channels\n        modules = []\n        for i in range(num_dense_layer):\n            modules.append(MakeDense(_in_channels, growth_rate))\n            _in_channels += growth_rate\n        self.residual_dense_layers = nn.Sequential(*modules)\n        self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n\n    def forward(self, x):\n        out = self.residual_dense_layers(x)\n        out = self.conv_1x1(out)\n        out = out + x\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:33:34.326097Z","iopub.execute_input":"2025-02-16T15:33:34.326495Z","iopub.status.idle":"2025-02-16T15:33:34.374273Z","shell.execute_reply.started":"2025-02-16T15:33:34.326456Z","shell.execute_reply":"2025-02-16T15:33:34.373004Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n# --- Imports --- #\nimport torch.utils.data as data\nfrom PIL import Image\nfrom random import randrange\nfrom torchvision.transforms import Compose, ToTensor, Normalize\n\nimport glob\n# --- Training dataset --- #\n\nclass TrainData512(data.Dataset):\n    def __init__(self, crop_size, train_data_dir):\n        super().__init__()\n        hazeeffected_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/reside/hazy/'\n        \n        hazy_data = glob.glob(hazeeffected_images_dir + \"*.png\")\n        hazefree_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/reside/clear/'\n        haze_names=[]\n        gt_names=[]\n        for h_image in hazy_data:\n\t\t        h_image = h_image.split(\"/\")[-1]\n\t\t        id_ = h_image.split(\"_\")[0]  + \".png\"\n\t\t        haze_names.append(hazeeffected_images_dir+h_image)\n\t\t        gt_names.append(hazefree_images_dir+id_)\n        self.haze_names = haze_names\n        self.gt_names = gt_names\n        self.crop_size = crop_size\n        self.train_data_dir = train_data_dir\n\n    def get_images(self, index):\n        crop_width, crop_height = self.crop_size\n        haze_name = self.haze_names[index]\n        gt_name = self.gt_names[index]\n\n        haze_img = Image.open(haze_name)\n\n        try:\n            gt_img = Image.open(gt_name)\n        except:\n            gt_img = Image.open(gt_name).convert('RGB')\n\n        \n\n        \n\n        # --- x,y coordinate of left-top corner --- #\n        \n        haze_crop_img = haze_img.resize((512, 512),Image.ANTIALIAS)\n        gt_crop_img = gt_img.resize((512, 512),Image.ANTIALIAS)\n\n        # --- Transform to tensor --- #\n        transform_all = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n       \n        haze = transform_all(haze_crop_img)\n        gt = transform_all(gt_crop_img)\n\n\n        return haze, gt\n\n    def __getitem__(self, index):\n        res = self.get_images(index)\n        return res\n\n    def __len__(self):\n        return len(self.haze_names)\n        \n        \n\nclass TrainDataNew(data.Dataset):\n    def __init__(self, crop_size, train_data_dir):\n        super().__init__()\n        # hazeeffected_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/light_dehazenet/data/'\n        hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n        \n        \n        hazy_data = glob.glob(hazeeffected_images_dir + \"*.jpg\")\n        # hazefree_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/light_dehazenet/image/'\n        hazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n        haze_names=[]\n        gt_names=[]\n        for h_image in hazy_data:\n\t\t        h_image = h_image.split(\"/\")[-1]\n\t\t        id_ = h_image.split(\"_\")[0] + \"_\" + h_image.split(\"_\")[1] + \".jpg\"\n\t\t        haze_names.append(hazeeffected_images_dir+h_image)\n\t\t        gt_names.append(hazefree_images_dir+id_)\n        self.haze_names = haze_names\n        self.gt_names = gt_names\n        self.crop_size = crop_size\n        self.train_data_dir = train_data_dir\n\n    def get_images(self, index):\n        crop_width, crop_height = self.crop_size\n        haze_name = self.haze_names[index]\n        gt_name = self.gt_names[index]\n\n        haze_img = Image.open(haze_name)\n\n        try:\n            gt_img = Image.open(gt_name)\n        except:\n            gt_img = Image.open(gt_name).convert('RGB')\n\n        width, height = haze_img.size\n\n        if width < crop_width or height < crop_height:\n            raise Exception('Bad image size: {}'.format(gt_name))\n\n        # --- x,y coordinate of left-top corner --- #\n        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n\n        # --- Transform to tensor --- #\n         #transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        transform_gt = Compose([ToTensor()])\n        haze = transform_gt(haze_crop_img)\n        gt = transform_gt(gt_crop_img)\n\n        # --- Check the channel is 3 or not --- #\n        if list(haze.shape)[0] !=  3 or list(gt.shape)[0] !=  3:\n            raise Exception('Bad image channel: {}'.format(gt_name))\n\n        return haze, gt\n\n    def __getitem__(self, index):\n        res = self.get_images(index)\n        return res\n\n    def __len__(self):\n        return len(self.haze_names)\n        \n        \n        \nclass TrainData(data.Dataset):\n    def __init__(self, crop_size, train_data_dir):\n        super().__init__()\n        # hazeeffected_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/light_dehazenet/data/'\n        # hazeeffected_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n        hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n        \n        hazy_data = glob.glob(hazeeffected_images_dir + \"*.*\")\n        # hazefree_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/light_dehazenet/image/'\n        # hazefree_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n        hazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n\n        haze_names=[]\n        gt_names=[]\n        for h_image in hazy_data:\n\t\t        h_image = h_image.split(\"/\")[-1]\n\t\t        id_ = h_image.split(\"_\")[0] + \"_\" + h_image.split(\"_\")[1] + \".jpg\"\n\t\t        haze_names.append(hazeeffected_images_dir+h_image)\n\t\t        gt_names.append(hazefree_images_dir+id_)\n        self.haze_names = haze_names\n        self.gt_names = gt_names\n        self.crop_size = crop_size\n        self.train_data_dir = train_data_dir\n\n    def get_images(self, index):\n        crop_width, crop_height = self.crop_size\n        haze_name = self.haze_names[index]\n        gt_name = self.gt_names[index]\n\n        haze_img = Image.open(haze_name)\n\n        try:\n            gt_img = Image.open(gt_name)\n        except:\n            gt_img = Image.open(gt_name).convert('RGB')\n\n        width, height = haze_img.size\n\n        if width < crop_width or height < crop_height:\n            raise Exception('Bad image size: {}'.format(gt_name))\n\n        # --- x,y coordinate of left-top corner --- #\n        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n\n        # --- Transform to tensor --- #\n        transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        transform_gt = Compose([ToTensor()])\n        haze = transform_haze(haze_crop_img)\n        gt = transform_gt(gt_crop_img)\n\n        # --- Check the channel is 3 or not --- #\n        if list(haze.shape)[0] !=  3 or list(gt.shape)[0] !=  3:\n            raise Exception('Bad image channel: {}'.format(gt_name))\n\n        return haze, gt\n\n    def __getitem__(self, index):\n        res = self.get_images(index)\n        return res\n\n    def __len__(self):\n        return len(self.haze_names)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:33:34.376543Z","iopub.execute_input":"2025-02-16T15:33:34.376898Z","iopub.status.idle":"2025-02-16T15:33:34.414780Z","shell.execute_reply.started":"2025-02-16T15:33:34.376871Z","shell.execute_reply":"2025-02-16T15:33:34.413649Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n\nhazy_data = glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n# hazefree_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/light_dehazenet/image/'\n# hazefree_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\nhazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n\nhaze_names=[]\ngt_names=[]\nprint(hazy_data)\nprint(len(hazy_data))\nfor h_image in hazy_data:\n        h_image = h_image.split(\"/\")[-1]\n        id_ = h_image.split(\"_\")[0] + \"_\" + h_image.split(\"_\")[1] + \".jpg\"\n        print(\"id\",id_)\n        haze_names.append(hazeeffected_images_dir+h_image)\n        gt_names.append(hazefree_images_dir+id_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:38:42.890023Z","iopub.execute_input":"2025-02-16T15:38:42.890391Z","iopub.status.idle":"2025-02-16T15:38:42.916524Z","shell.execute_reply.started":"2025-02-16T15:38:42.890362Z","shell.execute_reply":"2025-02-16T15:38:42.915119Z"}},"outputs":[{"name":"stdout","text":"['/kaggle/input/o-haze/O-HAZY/hazy/06_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/18_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/17_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/37_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/09_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/32_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/42_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/21_outdoor.JPG', '/kaggle/input/o-haze/O-HAZY/hazy/31_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/27_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/02_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/28_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/39_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/24_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/19_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/30_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/16_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/43_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/38_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/08_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/07_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/14_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/34_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/05_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/04_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/33_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/26_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/12_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/41_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/44_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/35_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/10_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/45_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/20_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/11_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/29_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/01_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/23_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/13_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/36_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/03_outdoor.JPG', '/kaggle/input/o-haze/O-HAZY/hazy/15_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/25_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/40_outdoor.jpg', '/kaggle/input/o-haze/O-HAZY/hazy/22_outdoor.jpg']\n45\nid 06_outdoor.jpg.jpg\nid 18_outdoor.jpg.jpg\nid 17_outdoor.jpg.jpg\nid 37_outdoor.jpg.jpg\nid 09_outdoor.jpg.jpg\nid 32_outdoor.jpg.jpg\nid 42_outdoor.jpg.jpg\nid 21_outdoor.JPG.jpg\nid 31_outdoor.jpg.jpg\nid 27_outdoor.jpg.jpg\nid 02_outdoor.jpg.jpg\nid 28_outdoor.jpg.jpg\nid 39_outdoor.jpg.jpg\nid 24_outdoor.jpg.jpg\nid 19_outdoor.jpg.jpg\nid 30_outdoor.jpg.jpg\nid 16_outdoor.jpg.jpg\nid 43_outdoor.jpg.jpg\nid 38_outdoor.jpg.jpg\nid 08_outdoor.jpg.jpg\nid 07_outdoor.jpg.jpg\nid 14_outdoor.jpg.jpg\nid 34_outdoor.jpg.jpg\nid 05_outdoor.jpg.jpg\nid 04_outdoor.jpg.jpg\nid 33_outdoor.jpg.jpg\nid 26_outdoor.jpg.jpg\nid 12_outdoor.jpg.jpg\nid 41_outdoor.jpg.jpg\nid 44_outdoor.jpg.jpg\nid 35_outdoor.jpg.jpg\nid 10_outdoor.jpg.jpg\nid 45_outdoor.jpg.jpg\nid 20_outdoor.jpg.jpg\nid 11_outdoor.jpg.jpg\nid 29_outdoor.jpg.jpg\nid 01_outdoor.jpg.jpg\nid 23_outdoor.jpg.jpg\nid 13_outdoor.jpg.jpg\nid 36_outdoor.jpg.jpg\nid 03_outdoor.JPG.jpg\nid 15_outdoor.jpg.jpg\nid 25_outdoor.jpg.jpg\nid 40_outdoor.jpg.jpg\nid 22_outdoor.jpg.jpg\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# import ipywidgets as widgets\n# from IPython.display import display\n\n# # Create widgets for each hyper-parameter\n# learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n# crop_size_widget = widgets.Text(value='360,360', description='Crop Size:')\n# train_batch_size_widget = widgets.IntText(value=6, description='Train Batch Size:')\n# network_height_widget = widgets.IntText(value=3, description='Network Height:')\n# network_width_widget = widgets.IntText(value=6, description='Network Width:')\n# num_dense_layer_widget = widgets.IntText(value=4, description='Num Dense Layer:')\n# growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n# lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n# val_batch_size_widget = widgets.IntText(value=1, description='Val Batch Size:')\n# category_widget = widgets.Dropdown(options=['indoor', 'outdoor'], value='indoor', description='Category:')\n\n# # Display the widgets\n# display(learning_rate_widget, crop_size_widget, train_batch_size_widget, network_height_widget, network_width_widget, num_dense_layer_widget, growth_rate_widget, lambda_loss_widget, val_batch_size_widget, category_widget)\n\n# # Function to parse the crop size\n# def parse_crop_size(crop_size_str):\n#     return [int(x) for x in crop_size_str.split(',')]\n\n# # Assign the widget values to variables\n# learning_rate = learning_rate_widget.value\n# crop_size = parse_crop_size(crop_size_widget.value)\n# train_batch_size = train_batch_size_widget.value\n# network_height = network_height_widget.value\n# network_width = network_width_widget.value\n# num_dense_layer = num_dense_layer_widget.value\n# growth_rate = growth_rate_widget.value\n# lambda_loss = lambda_loss_widget.value\n# val_batch_size = val_batch_size_widget.value\n# category = category_widget.value\n\n# print('Hyper-parameters set:')\n# print(f'learning_rate: {learning_rate}')\n# print(f'crop_size: {crop_size}')\n# print(f'train_batch_size: {train_batch_size}')\n# print(f'network_height: {network_height}')\n# print(f'network_width: {network_width}')\n# print(f'num_dense_layer: {num_dense_layer}')\n# print(f'growth_rate: {growth_rate}')\n# print(f'lambda_loss: {lambda_loss}')\n# print(f'val_batch_size: {val_batch_size}')\n# print(f'category: {category}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:33:34.415827Z","iopub.execute_input":"2025-02-16T15:33:34.416205Z","iopub.status.idle":"2025-02-16T15:33:34.439484Z","shell.execute_reply.started":"2025-02-16T15:33:34.416166Z","shell.execute_reply":"2025-02-16T15:33:34.438466Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import ipywidgets as widgets\nfrom IPython.display import display\n\n# --- Create widgets for each hyper-parameter ---\nlearning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\ncrop_size_widget = widgets.Text(value='360,360', description='Crop Size:')\ntrain_batch_size_widget = widgets.IntText(value=6, description='Train Batch Size:')\nnetwork_height_widget = widgets.IntText(value=3, description='Network Height:')\nnetwork_width_widget = widgets.IntText(value=6, description='Network Width:')\nnum_dense_layer_widget = widgets.IntText(value=4, description='Num Dense Layer:')\ngrowth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\nlambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\nval_batch_size_widget = widgets.IntText(value=1, description='Val Batch Size:')\ncategory_widget = widgets.Dropdown(options=['indoor', 'outdoor', 'nh'], value='nh', description='Category:')\nexecution_env_widget = widgets.Dropdown(options=['local', 'kaggle'], value='local', description='Execution Env:')\n\n# --- Display the widgets ---\ndisplay(\n    learning_rate_widget, crop_size_widget, train_batch_size_widget, network_height_widget, \n    network_width_widget, num_dense_layer_widget, growth_rate_widget, lambda_loss_widget, \n    val_batch_size_widget, category_widget, execution_env_widget\n)\n\n# --- Function to parse crop size ---\ndef parse_crop_size(crop_size_str):\n    return [int(x) for x in crop_size_str.split(',')]\n\n# --- Assign the widget values to variables ---\nlearning_rate = learning_rate_widget.value\ncrop_size = parse_crop_size(crop_size_widget.value)\ntrain_batch_size = train_batch_size_widget.value\nnetwork_height = network_height_widget.value\nnetwork_width = network_width_widget.value\nnum_dense_layer = num_dense_layer_widget.value\ngrowth_rate = growth_rate_widget.value\nlambda_loss = lambda_loss_widget.value\nval_batch_size = val_batch_size_widget.value\ncategory = category_widget.value\nexecution_env = execution_env_widget.value  # Local or Kaggle\n\nprint('\\nHyper-parameters set:')\nprint(f'learning_rate: {learning_rate}')\nprint(f'crop_size: {crop_size}')\nprint(f'train_batch_size: {train_batch_size}')\nprint(f'network_height: {network_height}')\nprint(f'network_width: {network_width}')\nprint(f'num_dense_layer: {num_dense_layer}')\nprint(f'growth_rate: {growth_rate}')\nprint(f'lambda_loss: {lambda_loss}')\nprint(f'val_batch_size: {val_batch_size}')\nprint(f'category: {category}')\nprint(f'execution_env: {execution_env}')\n\n# --- Set category-specific hyper-parameters ---\nif category == 'indoor':\n    num_epochs = 1500\n    train_data_dir = './data/train/indoor/'\n    val_data_dir = './data/test/SOTS/indoor/'\nelif category == 'outdoor':\n    num_epochs = 10\n    train_data_dir = './data/train/outdoor/'\n    val_data_dir = './data/test/SOTS/outdoor/'\nelif category == 'nh':\n    num_epochs = 10\n    train_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n    val_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\nelse:\n    raise Exception('Wrong image category. Set it to indoor or outdoor for RESIDE dataset.')\n\n# --- Adjust paths based on execution environment ---\nif execution_env == 'kaggle':\n    # train_data_dir = '/kaggle/input/reside-dataset/' + train_data_dir.strip('./')\n    # val_data_dir = '/kaggle/input/reside-dataset/' + val_data_dir.strip('./')\n    train_data_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n    val_data_dir = '/kaggle/input/o-haze/O-HAZY/GT' \nprint('\\nFinal dataset paths:')\nprint(f'Training directory: {train_data_dir}')\nprint(f'Validation directory: {val_data_dir}')\nprint(f'Number of epochs: {num_epochs}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:33:34.440723Z","iopub.execute_input":"2025-02-16T15:33:34.441145Z","iopub.status.idle":"2025-02-16T15:33:34.519631Z","shell.execute_reply.started":"2025-02-16T15:33:34.441108Z","shell.execute_reply":"2025-02-16T15:33:34.517703Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"FloatText(value=0.0001, description='Learning Rate:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f33f97dad93148958ce3092863df7b67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Text(value='360,360', description='Crop Size:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b8420fbb9774554bb91112e4914272b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IntText(value=6, description='Train Batch Size:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"510595975a9842dea93bb2e498c9ad54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IntText(value=3, description='Network Height:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64299565da7847da918636d09a07e0de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IntText(value=6, description='Network Width:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b69c602a7644521bf19250d0badd8c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IntText(value=4, description='Num Dense Layer:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad816bcea61b402ca2a3cf84669390fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IntText(value=16, description='Growth Rate:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ae55daea09f4bfaab044fad3bb8c390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"FloatText(value=0.04, description='Lambda Loss:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20cb5a4259ab44c4ba0a8d1bea7409ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IntText(value=1, description='Val Batch Size:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3b80fe9c6a64c2f99b3bbedb25d8526"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dropdown(description='Category:', index=2, options=('indoor', 'outdoor', 'nh'), value='nh')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24e77057eeef4b81b0cd9d97268ce4dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dropdown(description='Execution Env:', options=('local', 'kaggle'), value='local')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"843d7fe16ded49e0899802b039604150"}},"metadata":{}},{"name":"stdout","text":"\nHyper-parameters set:\nlearning_rate: 0.0001\ncrop_size: [360, 360]\ntrain_batch_size: 6\nnetwork_height: 3\nnetwork_width: 6\nnum_dense_layer: 4\ngrowth_rate: 16\nlambda_loss: 0.04\nval_batch_size: 1\ncategory: nh\nexecution_env: local\n\nFinal dataset paths:\nTraining directory: /Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy\nValidation directory: /Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT\nNumber of epochs: 10\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"train_data_loader = DataLoader(TrainData(crop_size, train_data_dir), batch_size=train_batch_size, shuffle=True)\nval_data_loader = DataLoader(ValData(val_data_dir), batch_size=val_batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:33:34.520530Z","iopub.execute_input":"2025-02-16T15:33:34.520892Z","iopub.status.idle":"2025-02-16T15:33:34.545206Z","shell.execute_reply.started":"2025-02-16T15:33:34.520858Z","shell.execute_reply":"2025-02-16T15:33:34.543136Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-c3600ec03bb5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"],"ename":"NameError","evalue":"name 'DataLoader' is not defined","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"# --- Imports --- #\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n#import matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\n#from dehazeformer import *\nfrom torchvision.models import vgg16\n#plt.switch_backend('agg')\n\n\n\n\n\n\n# --- Gpu device --- #\ndevice_ids = [Id for Id in range(torch.cuda.device_count())]\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n# --- Define the network --- #\nnet = DeepGuidednew() #GridDehazeNet(height=network_height, width=network_width, num_dense_layer=num_dense_layer, growth_rate=growth_rate)\n\n#net =FFA(3,19)\n# --- Build optimizer --- #\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n\n\n# --- Multi-GPU --- #\nnet = net.to(device)\nnet = nn.DataParallel(net, device_ids=device_ids)\n\n\n# --- Define the perceptual loss network --- #\nvgg_model = vgg16(pretrained=True).features[:16]\nvgg_model = vgg_model.to(device)\nfor param in vgg_model.parameters():\n    param.requires_grad = False\n\nloss_network = LossNetwork(vgg_model)\nloss_network.eval()\nmodels='formernew'\n\n# --- Load the network weight --- #\ntry:\n    net.load_state_dict(torch.load(models+'{}_haze_best_{}_{}'.format(category, network_height, network_width)))\n    print('--- weight loaded ---')\nexcept:\n    print('--- no weight loaded ---')\n\n\n# --- Calculate all trainable parameters in network --- #\npytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\nprint(\"Total_params: {}\".format(pytorch_total_params))\n\n\n# --- Load training data and validation/test data --- #\ntrain_data_loader = DataLoader(TrainData(crop_size, train_data_dir), batch_size=train_batch_size, shuffle=True)\nval_data_loader = DataLoader(ValData(val_data_dir), batch_size=val_batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:33:34.546276Z","iopub.status.idle":"2025-02-16T15:33:34.546897Z","shell.execute_reply":"2025-02-16T15:33:34.546688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n# --- Previous PSNR and SSIM in testing --- #\nold_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)\nprint('old_val_psnr: {0:.2f}, old_val_ssim: {1:.4f}'.format(old_val_psnr, old_val_ssim))\ntrain_psnrold=0\n\nfor epoch in range(num_epochs):\n    psnr_list = []\n    start_time = time.time()\n    adjust_learning_rate(optimizer, epoch, category=category)\n\n    for batch_id, train_data in enumerate(train_data_loader):\n\n        haze, gt = train_data\n        haze = haze.to(device)\n        gt = gt.to(device)\n\n        # --- Zero the parameter gradients --- #\n        optimizer.zero_grad()\n\n        # --- Forward + Backward + Optimize --- #\n        net.train()\n        dehaze,base = net(haze)\n        base_loss = F.smooth_l1_loss(base, gt)\n\n        smooth_loss = F.smooth_l1_loss(dehaze, gt)\n        perceptual_loss = loss_network(dehaze, gt)\n        loss = smooth_loss + lambda_loss*perceptual_loss+base_loss\n\n        loss.backward()\n        optimizer.step()\n\n        # --- To calculate average PSNR --- #\n        psnr_list.extend(to_psnr(dehaze, gt))\n\n        if not (batch_id % 100):\n            print('Epoch: {0}, Iteration: {1}'.format(epoch, batch_id))\n\n    # --- Calculate the average training PSNR in one epoch --- #\n    train_psnr = sum(psnr_list) / len(psnr_list)\n\n    # --- Save the network parameters --- #\n    torch.save(net.state_dict(), models+'{}_haze_{}_{}'.format(category, network_height, network_width))\n\n    # --- Use the evaluation model in testing --- #\n    net.eval()\n\n    val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n    one_epoch_time = time.time() - start_time\n    print_log(epoch+1, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, models+category)\n    \n    \n    if train_psnr< train_psnrold:\n        adjust_learning_rate_step(optimizer, category=category)            \n\n    # --- update the network weight --- #\n    if val_psnr >= old_val_psnr:\n        torch.save(net.state_dict(), models+'{}_haze_best_{}_{}'.format(category, network_height, network_width))\n        old_val_psnr = val_psnr\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:33:34.548545Z","iopub.status.idle":"2025-02-16T15:33:34.549121Z","shell.execute_reply":"2025-02-16T15:33:34.548896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\npaper: GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing\nfile: utils.py\nabout: all utilities\nauthor: Xiaohong Liu\ndate: 01/08/19\n\"\"\"\n\n# --- Imports --- #\nimport time\nimport torch\nimport torch.nn.functional as F\nimport torchvision.utils as utils\nfrom math import log10\nfrom skimage import measure\n\n\ndef to_psnr(dehaze, gt):\n    mse = F.mse_loss(dehaze, gt, reduction='none')\n    #print (mse)\n    mse_split = torch.split(mse, 1, dim=0)\n    mse_list = [torch.mean(torch.squeeze(mse_split[ind])).item() for ind in range(len(mse_split))]\n\n    intensity_max = 1.0\n    psnr_list = [10.0 * log10(intensity_max / min(max(mse,0.000001),1000)) for mse in mse_list]\n    return psnr_list\n\n\ndef to_ssim_skimage(dehaze, gt):\n    dehaze_list = torch.split(dehaze, 1, dim=0)\n    gt_list = torch.split(gt, 1, dim=0)\n\n    dehaze_list_np = [dehaze_list[ind].permute(0, 2, 3, 1).data.cpu().numpy().squeeze() for ind in range(len(dehaze_list))]\n    gt_list_np = [gt_list[ind].permute(0, 2, 3, 1).data.cpu().numpy().squeeze() for ind in range(len(dehaze_list))]\n    ssim_list = [measure.compare_ssim(dehaze_list_np[ind],  gt_list_np[ind], data_range=1, multichannel=True) for ind in range(len(dehaze_list))]\n\n    return ssim_list\n\n\n\n\ndef validationStlyle(net, val_data_loader, device, category, save_tag=False):\n    \"\"\"\n    :param net: GateDehazeNet\n    :param val_data_loader: validation loader\n    :param device: The GPU that loads the network\n    :param category: indoor or outdoor test dataset\n    :param save_tag: tag of saving image or not\n    :return: average PSNR value\n    \"\"\"\n    psnr_list = []\n    ssim_list = []\n\n    for batch_id, val_data in enumerate(val_data_loader):\n\n        with torch.no_grad():\n            haze, gt, image_name = val_data\n            haze = haze.to(device)\n            gt = gt.to(device)\n            hazing = net(gt,haze)\n\n        # --- Calculate the average PSNR --- #\n        psnr_list.extend(to_psnr(hazing, haze))\n\n        # --- Calculate the average SSIM --- #\n        ssim_list.extend(to_ssim_skimage(hazing, haze))\n\n        # --- Save image --- #\n        if save_tag:\n            save_image(dehaze, image_name, category)\n  \n    avr_psnr = sum(psnr_list) / len(psnr_list)\n    \n    avr_ssim = sum(ssim_list) / len(ssim_list)\n    return avr_psnr, avr_ssim\n    \n    \ndef validationB(net, val_data_loader, device, category, save_tag=False):\n    \"\"\"\n    :param net: GateDehazeNet\n    :param val_data_loader: validation loader\n    :param device: The GPU that loads the network\n    :param category: indoor or outdoor test dataset\n    :param save_tag: tag of saving image or not\n    :return: average PSNR value\n    \"\"\"\n    psnr_list = []\n    ssim_list = []\n\n    for batch_id, val_data in enumerate(val_data_loader):\n\n        with torch.no_grad():\n            haze, gt, image_name = val_data\n            haze = haze.to(device)\n            gt = gt.to(device)\n            dehaze, _ = net(haze)\n\n        # --- Calculate the average PSNR --- #\n        psnr_list.extend(to_psnr(dehaze, gt))\n\n        # --- Calculate the average SSIM --- #\n        ssim_list.extend(to_ssim_skimage(dehaze, gt))\n\n        # --- Save image --- #\n        if save_tag:\n            save_image(dehaze, image_name, category)\n  \n    avr_psnr = sum(psnr_list) / len(psnr_list)\n    \n    avr_ssim = sum(ssim_list) / len(ssim_list)\n    return avr_psnr, avr_ssim\n        \n    \n\ndef validation(net, val_data_loader, device, category, save_tag=False):\n    \"\"\"\n    :param net: GateDehazeNet\n    :param val_data_loader: validation loader\n    :param device: The GPU that loads the network\n    :param category: indoor or outdoor test dataset\n    :param save_tag: tag of saving image or not\n    :return: average PSNR value\n    \"\"\"\n    psnr_list = []\n    ssim_list = []\n\n    for batch_id, val_data in enumerate(val_data_loader):\n\n        with torch.no_grad():\n            haze, gt, image_name = val_data\n            haze = haze.to(device)\n            gt = gt.to(device)\n            dehaze = net(haze)\n\n        # --- Calculate the average PSNR --- #\n        psnr_list.extend(to_psnr(dehaze, gt))\n\n        # --- Calculate the average SSIM --- #\n        ssim_list.extend(to_ssim_skimage(dehaze, gt))\n\n        # --- Save image --- #\n        if save_tag:\n            save_image(dehaze, image_name, category)\n  \n    avr_psnr = sum(psnr_list) / len(psnr_list)\n    \n    avr_ssim = sum(ssim_list) / len(ssim_list)\n    return avr_psnr, avr_ssim\ndef validationN(net, val_data_loader, device, category, save_tag=False):\n    \"\"\"\n    :param net: GateDehazeNet\n    :param val_data_loader: validation loader\n    :param device: The GPU that loads the network\n    :param category: indoor or outdoor test dataset\n    :param save_tag: tag of saving image or not\n    :return: average PSNR value\n    \"\"\"\n    psnr_list = []\n    ssim_list = []\n\n    for batch_id, val_data in enumerate(val_data_loader):\n\n        with torch.no_grad():\n            haze, gt, image_name = val_data\n            haze = haze.to(device)\n            gt = gt.to(device)\n            dehaze,_,_ = net(haze)\n\n        # --- Calculate the average PSNR --- #\n        psnr_list.extend(to_psnr(dehaze, gt))\n\n        # --- Calculate the average SSIM --- #\n        ssim_list.extend(to_ssim_skimage(dehaze, gt))\n\n        # --- Save image --- #\n        if save_tag:\n            save_image(dehaze, image_name, category)\n\n    avr_psnr = sum(psnr_list) / len(psnr_list)\n    avr_ssim = sum(ssim_list) / len(ssim_list)\n    \n    return avr_psnr, avr_ssim\n\n\ndef save_image(dehaze, image_name, category):\n    dehaze_images = torch.split(dehaze, 1, dim=0)\n    batch_num = len(dehaze_images)\n\n    for ind in range(batch_num):\n        utils.save_image(dehaze_images[ind], './{}_results/{}'.format(category, image_name[ind][:-3] + 'png'))\n\n\ndef print_log(epoch, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, category):\n    print('({0:.0f}s) Epoch [{1}/{2}], Train_PSNR:{3:.2f}, Val_PSNR:{4:.2f}, Val_SSIM:{5:.4f}'\n          .format(one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim))\n\n    # --- Write the training log --- #\n    with open('./training_log/{}_log.txt'.format(category), 'a') as f:\n        print('Date: {0}s, Time_Cost: {1:.0f}s, Epoch: [{2}/{3}], Train_PSNR: {4:.2f}, Val_PSNR: {5:.2f}, Val_SSIM: {6:.4f}'\n              .format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n                      one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim), file=f)\n\n\ndef adjust_learning_rate_step(optimizer, category, lr_decay=0.95):\n\n    # --- Decay learning rate --- #\n\n    for param_group in optimizer.param_groups:\n       param_group['lr'] *= lr_decay\n       print('Learning rate sets to {}.'.format(param_group['lr']))\n\n            \n            \n            \ndef adjust_learning_rate(optimizer, epoch, category, lr_decay=0.90):\n\n    # --- Decay learning rate --- #\n    step = 18 if category == 'indoor' else 3\n    if category == 'NH':\n       step = 20\n    #if not category == 'indoor':\n       #for param_group in optimizer.param_groups:\n            #param_group['lr'] *= 0.99\n            #print('Learning rate sets to {}.'.format(param_group['lr']))\n    if not epoch % step and epoch > 0:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] *= lr_decay\n            print('Learning rate sets to {}.'.format(param_group['lr']))\n    else:\n        for param_group in optimizer.param_groups:\n            print('Learning rate sets to {}.'.format(param_group['lr']))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:33:34.550938Z","iopub.status.idle":"2025-02-16T15:33:34.551752Z","shell.execute_reply":"2025-02-16T15:33:34.551576Z"}},"outputs":[],"execution_count":null}]}