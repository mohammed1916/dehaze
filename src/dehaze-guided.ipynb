{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-31T15:30:50.318056Z",
     "iopub.status.busy": "2025-01-31T15:30:50.317646Z",
     "iopub.status.idle": "2025-01-31T15:30:59.721573Z",
     "shell.execute_reply": "2025-01-31T15:30:59.720630Z",
     "shell.execute_reply.started": "2025-01-31T15:30:50.318020Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "from timm.models.layers import to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class RLN(nn.Module):\n",
    "\tr\"\"\"Revised LayerNorm\"\"\"\n",
    "\tdef __init__(self, dim, eps=1e-5, detach_grad=False):\n",
    "\t\tsuper(RLN, self).__init__()\n",
    "\t\tself.eps = eps\n",
    "\t\tself.detach_grad = detach_grad\n",
    "\n",
    "\t\tself.weight = nn.Parameter(torch.ones((1, dim, 1, 1)))\n",
    "\t\tself.bias = nn.Parameter(torch.zeros((1, dim, 1, 1)))\n",
    "\n",
    "\t\tself.meta1 = nn.Conv2d(1, dim, 1)\n",
    "\t\tself.meta2 = nn.Conv2d(1, dim, 1)\n",
    "\n",
    "\t\ttrunc_normal_(self.meta1.weight, std=.02)\n",
    "\t\tnn.init.constant_(self.meta1.bias, 1)\n",
    "\n",
    "\t\ttrunc_normal_(self.meta2.weight, std=.02)\n",
    "\t\tnn.init.constant_(self.meta2.bias, 0)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tmean = torch.mean(input, dim=(1, 2, 3), keepdim=True)\n",
    "\t\tstd = torch.sqrt((input - mean).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
    "\n",
    "\t\tnormalized_input = (input - mean) / std\n",
    "\n",
    "\t\tif self.detach_grad:\n",
    "\t\t\trescale, rebias = self.meta1(std.detach()), self.meta2(mean.detach())\n",
    "\t\telse:\n",
    "\t\t\trescale, rebias = self.meta1(std), self.meta2(mean)\n",
    "\n",
    "\t\tout = normalized_input * self.weight + self.bias\n",
    "\t\treturn out, rescale, rebias\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "\tdef __init__(self, network_depth, in_features, hidden_features=None, out_features=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tout_features = out_features or in_features\n",
    "\t\thidden_features = hidden_features or in_features\n",
    "\n",
    "\t\tself.network_depth = network_depth\n",
    "\n",
    "\t\tself.mlp = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_features, hidden_features, 1),\n",
    "\t\t\tnn.ReLU(True),\n",
    "\t\t\tnn.Conv2d(hidden_features, out_features, 1)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.apply(self._init_weights)\n",
    "\n",
    "\tdef _init_weights(self, m):\n",
    "\t\tif isinstance(m, nn.Conv2d):\n",
    "\t\t\tgain = (8 * self.network_depth) ** (-1/4)\n",
    "\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n",
    "\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\t\t\ttrunc_normal_(m.weight, std=std)\n",
    "\t\t\tif m.bias !=   None:\n",
    "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.mlp(x)\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "\tB, H, W, C = x.shape\n",
    "\tx = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "\twindows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, C)\n",
    "\treturn windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "\tB = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "\tx = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "\tx = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "\treturn x\n",
    "\n",
    "\n",
    "def get_relative_positions(window_size):\n",
    "\tcoords_h = torch.arange(window_size)\n",
    "\tcoords_w = torch.arange(window_size)\n",
    "\n",
    "\tcoords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "\tcoords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "\trelative_positions = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "\n",
    "\trelative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "\trelative_positions_log  = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "\treturn relative_positions_log\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "\tdef __init__(self, dim, window_size, num_heads):\n",
    "\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.window_size = window_size  # Wh, Ww\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\thead_dim = dim // num_heads\n",
    "\t\tself.scale = head_dim ** -0.5\n",
    "\n",
    "\t\trelative_positions = get_relative_positions(self.window_size)\n",
    "\t\tself.register_buffer(\"relative_positions\", relative_positions)\n",
    "\t\tself.meta = nn.Sequential(\n",
    "\t\t\tnn.Linear(2, 256, bias=True),\n",
    "\t\t\tnn.ReLU(True),\n",
    "\t\t\tnn.Linear(256, num_heads, bias=True)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\tdef forward(self, qkv):\n",
    "\t\tB_, N, _ = qkv.shape\n",
    "\n",
    "\t\tqkv = qkv.reshape(B_, N, 3, self.num_heads, self.dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "\t\tq, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "\t\tq = q * self.scale\n",
    "\t\tattn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "\t\trelative_position_bias = self.meta(self.relative_positions)\n",
    "\t\trelative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "\t\tattn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "\t\tattn = self.softmax(attn)\n",
    "\n",
    "\t\tx = (attn @ v).transpose(1, 2).reshape(B_, N, self.dim)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, num_heads, window_size, shift_size, use_attn=False, conv_type=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.head_dim = int(dim // num_heads)\n",
    "\t\tself.num_heads = num_heads\n",
    "\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.shift_size = shift_size\n",
    "\n",
    "\t\tself.network_depth = network_depth\n",
    "\t\tself.use_attn = use_attn\n",
    "\t\tself.conv_type = conv_type\n",
    "\n",
    "\t\tif self.conv_type == 'Conv':\n",
    "\t\t\tself.conv = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "\t\t\t)\n",
    "\n",
    "\t\tif self.conv_type == 'DWConv':\n",
    "\t\t\tself.conv = nn.Conv2d(dim, dim, kernel_size=5, padding=2, groups=dim, padding_mode='reflect')\n",
    "\n",
    "\t\tif self.conv_type == 'DWConv' or self.use_attn:\n",
    "\t\t\tself.V = nn.Conv2d(dim, dim, 1)\n",
    "\t\t\tself.proj = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "\t\tif self.use_attn:\n",
    "\t\t\tself.QK = nn.Conv2d(dim, dim * 2, 1)\n",
    "\t\t\tself.attn = WindowAttention(dim, window_size, num_heads)\n",
    "\n",
    "\t\tself.apply(self._init_weights)\n",
    "\n",
    "\tdef _init_weights(self, m):\n",
    "\t\tif isinstance(m, nn.Conv2d):\n",
    "\t\t\tw_shape = m.weight.shape\n",
    "\t\t\t\n",
    "\t\t\tif w_shape[0] == self.dim * 2:\t# QK\n",
    "\t\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n",
    "\t\t\t\tstd = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\t\t\t\ttrunc_normal_(m.weight, std=std)\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t\tgain = (8 * self.network_depth) ** (-1/4)\n",
    "\t\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n",
    "\t\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\t\t\t\ttrunc_normal_(m.weight, std=std)\n",
    "\n",
    "\t\t\tif m.bias !=  None:\n",
    "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
    "\n",
    "\tdef check_size(self, x, shift=False):\n",
    "\t\t_, _, h, w = x.size()\n",
    "\t\tmod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
    "\t\tmod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
    "\n",
    "\t\tif shift:\n",
    "\t\t\tx = F.pad(x, (self.shift_size, (self.window_size-self.shift_size+mod_pad_w) % self.window_size,\n",
    "\t\t\t\t\t\t  self.shift_size, (self.window_size-self.shift_size+mod_pad_h) % self.window_size), mode='reflect')\n",
    "\t\telse:\n",
    "\t\t\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tB, C, H, W = X.shape\n",
    "\n",
    "\t\tif self.conv_type == 'DWConv' or self.use_attn:\n",
    "\t\t\tV = self.V(X)\n",
    "\t\t#print(self.use_attn)\n",
    "\t\tif self.use_attn:\n",
    "\t\t\t#print('attention')      \n",
    "\t\t\tQK = self.QK(X)\n",
    "\t\t\tQKV = torch.cat([QK, V], dim=1)\n",
    "\n",
    "\t\t\t# shift\n",
    "\t\t\tshifted_QKV = self.check_size(QKV, self.shift_size > 0)\n",
    "\t\t\tHt, Wt = shifted_QKV.shape[2:]\n",
    "\n",
    "\t\t\t# partition windows\n",
    "\t\t\tshifted_QKV = shifted_QKV.permute(0, 2, 3, 1)\n",
    "\t\t\tqkv = window_partition(shifted_QKV, self.window_size)  # nW*B, window_size**2, C\n",
    "\n",
    "\t\t\tattn_windows = self.attn(qkv)\n",
    "\n",
    "\t\t\t# merge windows\n",
    "\t\t\tshifted_out = window_reverse(attn_windows, self.window_size, Ht, Wt)  # B H' W' C\n",
    "\n",
    "\t\t\t# reverse cyclic shift\n",
    "\t\t\tout = shifted_out[:, self.shift_size:(self.shift_size+H), self.shift_size:(self.shift_size+W), :]\n",
    "\t\t\tattn_out = out.permute(0, 3, 1, 2)\n",
    "\n",
    "\t\t\tif self.conv_type in ['Conv', 'DWConv']:\n",
    "\t\t\t\tconv_out = self.conv(V)\n",
    "\t\t\t\tout = self.proj(conv_out + attn_out)\n",
    "\t\t\telse:\n",
    "\t\t\t\tout = self.proj(attn_out)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tif self.conv_type == 'Conv':\n",
    "\t\t\t\tout = self.conv(X)\t\t\t\t# no attention and use conv, no projection\n",
    "\t\t\telif self.conv_type == 'DWConv':\n",
    "\t\t\t\tout = self.proj(self.conv(V))\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, num_heads, mlp_ratio=4.,\n",
    "\t\t\t\t norm_layer=nn.LayerNorm, mlp_norm=False,\n",
    "\t\t\t\t window_size=8, shift_size=0, use_attn=True, conv_type=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.use_attn = use_attn\n",
    "\t\tself.mlp_norm = mlp_norm\n",
    "\n",
    "\t\tself.norm1 = norm_layer(dim) if use_attn else nn.Identity()\n",
    "\t\tself.attn = Attention(network_depth, dim, num_heads=num_heads, window_size=window_size,\n",
    "\t\t\t\t\t\t\t  shift_size=shift_size, use_attn=use_attn, conv_type=conv_type)\n",
    "\n",
    "\t\tself.norm2 = norm_layer(dim) if use_attn and mlp_norm else nn.Identity()\n",
    "\t\tself.mlp = Mlp(network_depth, dim, hidden_features=int(dim * mlp_ratio))\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tidentity = x\n",
    "\t\tif self.use_attn: x, rescale, rebias = self.norm1(x)\n",
    "\t\tx = self.attn(x)\n",
    "\t\tif self.use_attn: x = x * rescale + rebias\n",
    "\t\tx = identity + x\n",
    "\n",
    "\t\tidentity = x\n",
    "\t\tif self.use_attn and self.mlp_norm: x, rescale, rebias = self.norm2(x)\n",
    "\t\tx = self.mlp(x)\n",
    "\t\tif self.use_attn and self.mlp_norm: x = x * rescale + rebias\n",
    "\t\tx = identity + x\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, depth, num_heads, mlp_ratio=4.,\n",
    "\t\t\t\t norm_layer=nn.LayerNorm, window_size=8,\n",
    "\t\t\t\t attn_ratio=0., attn_loc='last', conv_type=None):\n",
    "\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.depth = depth\n",
    "\n",
    "\t\tattn_depth = attn_ratio * depth\n",
    "\n",
    "\t\tif attn_loc == 'last':\n",
    "\t\t\tuse_attns = [i >= depth-attn_depth for i in range(depth)]\n",
    "\t\telif attn_loc == 'first':\n",
    "\t\t\tuse_attns = [i < attn_depth for i in range(depth)]\n",
    "\t\telif attn_loc == 'middle':\n",
    "\t\t\tuse_attns = [i >= (depth-attn_depth)//2 and i < (depth+attn_depth)//2 for i in range(depth)]\n",
    "\n",
    "\t\t# build blocks\n",
    "\t\tself.blocks = nn.ModuleList([\n",
    "\t\t\tTransformerBlock(network_depth=network_depth,\n",
    "\t\t\t\t\t\t\t dim=dim, \n",
    "\t\t\t\t\t\t\t num_heads=num_heads,\n",
    "\t\t\t\t\t\t\t mlp_ratio=mlp_ratio,\n",
    "\t\t\t\t\t\t\t norm_layer=norm_layer,\n",
    "\t\t\t\t\t\t\t window_size=window_size,\n",
    "\t\t\t\t\t\t\t shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "\t\t\t\t\t\t\t use_attn=use_attns[i], conv_type=conv_type)\n",
    "\t\t\tfor i in range(depth)])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor blk in self.blocks:\n",
    "\t\t\tx = blk(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\tdef __init__(self, patch_size=4, in_chans=3, embed_dim=96, kernel_size=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.in_chans = in_chans\n",
    "\t\tself.embed_dim = embed_dim\n",
    "\n",
    "\t\tif kernel_size is None:\n",
    "\t\t\tkernel_size = patch_size\n",
    "\n",
    "\t\tself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "\t\t\t\t\t\t\t  padding=(kernel_size-patch_size+1)//2, padding_mode='reflect')\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.proj(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class PatchUnEmbed(nn.Module):\n",
    "\tdef __init__(self, patch_size=4, out_chans=3, embed_dim=96, kernel_size=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.out_chans = out_chans\n",
    "\t\tself.embed_dim = embed_dim\n",
    "\n",
    "\t\tif kernel_size is None:\n",
    "\t\t\tkernel_size = 1\n",
    "\n",
    "\t\tself.proj = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(embed_dim, out_chans*patch_size**2, kernel_size=kernel_size,\n",
    "\t\t\t\t\t  padding=kernel_size//2, padding_mode='reflect'),\n",
    "\t\t\tnn.PixelShuffle(patch_size)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.proj(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class SKFusion(nn.Module):\n",
    "\tdef __init__(self, dim, height=2, reduction=8):\n",
    "\t\tsuper(SKFusion, self).__init__()\n",
    "\t\t\n",
    "\t\tself.height = height\n",
    "\t\td = max(int(dim/reduction), 4)\n",
    "\t\t\n",
    "\t\tself.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\t\tself.mlp = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(dim, d, 1, bias=False), \n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(d, dim*height, 1, bias=False)\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\tdef forward(self, in_feats):\n",
    "\t\tB, C, H, W = in_feats[0].shape\n",
    "\t\t\n",
    "\t\tin_feats = torch.cat(in_feats, dim=1)\n",
    "\t\tin_feats = in_feats.view(B, self.height, C, H, W)\n",
    "\t\t\n",
    "\t\tfeats_sum = torch.sum(in_feats, dim=1)\n",
    "\t\tattn = self.mlp(self.avg_pool(feats_sum))\n",
    "\t\tattn = self.softmax(attn.view(B, self.height, C, 1, 1))\n",
    "\n",
    "\t\tout = torch.sum(in_feats*attn, dim=1)\n",
    "\t\treturn out      \n",
    "\n",
    "\n",
    "class DehazeFormer(nn.Module):\n",
    "\tdef __init__(self, in_chans=3, out_chans=4, window_size=8,\n",
    "\t\t\t\t embed_dims=[24, 48, 96, 48, 24],\n",
    "\t\t\t\t mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\t\t\t depths=[16, 16, 16, 8, 8],\n",
    "\t\t\t\t num_heads=[2, 4, 6, 1, 1],\n",
    "\t\t\t\t attn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\t\t\t conv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "\t\t\t\t norm_layer=[RLN, RLN, RLN, RLN, RLN]):\n",
    "\t\tsuper(DehazeFormer, self).__init__()\n",
    "\n",
    "\t\t# setting\n",
    "\t\tself.patch_size = 4\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.mlp_ratios = mlp_ratios\n",
    "\n",
    "\t\t# split image into non-overlapping patches\n",
    "\t\tself.patch_embed = PatchEmbed(\n",
    "\t\t\tpatch_size=1, in_chans=in_chans, embed_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "\t\t# backbone\n",
    "\t\tself.layer1 = BasicLayer(network_depth=sum(depths), dim=embed_dims[0], depth=depths[0],\n",
    "\t\t\t\t\t   \t\t\t num_heads=num_heads[0], mlp_ratio=mlp_ratios[0],\n",
    "\t\t\t\t\t   \t\t\t norm_layer=norm_layer[0], window_size=window_size,\n",
    "\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio[0], attn_loc='last', conv_type=conv_type[0])\n",
    "\n",
    "\t\tself.patch_merge1 = PatchEmbed(\n",
    "\t\t\tpatch_size=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])\n",
    "\n",
    "\t\tself.skip1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "\n",
    "\t\tself.layer2 = BasicLayer(network_depth=sum(depths), dim=embed_dims[1], depth=depths[1],\n",
    "\t\t\t\t\t\t\t\t num_heads=num_heads[1], mlp_ratio=mlp_ratios[1],\n",
    "\t\t\t\t\t\t\t\t norm_layer=norm_layer[1], window_size=window_size,\n",
    "\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[1], attn_loc='last', conv_type=conv_type[1])\n",
    "\n",
    "\t\tself.patch_merge2 = PatchEmbed(\n",
    "\t\t\tpatch_size=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])\n",
    "\n",
    "\t\tself.skip2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "\n",
    "\t\tself.layer3 = BasicLayer(network_depth=sum(depths), dim=embed_dims[2], depth=depths[2],\n",
    "\t\t\t\t\t\t\t\t num_heads=num_heads[2], mlp_ratio=mlp_ratios[2],\n",
    "\t\t\t\t\t\t\t\t norm_layer=norm_layer[2], window_size=window_size,\n",
    "\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[2], attn_loc='last', conv_type=conv_type[2])\n",
    "\n",
    "\t\tself.patch_split1 = PatchUnEmbed(\n",
    "\t\t\tpatch_size=2, out_chans=embed_dims[3], embed_dim=embed_dims[2])\n",
    "\n",
    "\t\tassert embed_dims[1] == embed_dims[3]\n",
    "\t\tself.fusion1 = SKFusion(embed_dims[3])\n",
    "\n",
    "\t\tself.layer4 = BasicLayer(network_depth=sum(depths), dim=embed_dims[3], depth=depths[3],\n",
    "\t\t\t\t\t\t\t\t num_heads=num_heads[3], mlp_ratio=mlp_ratios[3],\n",
    "\t\t\t\t\t\t\t\t norm_layer=norm_layer[3], window_size=window_size,\n",
    "\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[3], attn_loc='last', conv_type=conv_type[3])\n",
    "\n",
    "\t\tself.patch_split2 = PatchUnEmbed(\n",
    "\t\t\tpatch_size=2, out_chans=embed_dims[4], embed_dim=embed_dims[3])\n",
    "\n",
    "\t\tassert embed_dims[0] == embed_dims[4]\n",
    "\t\tself.fusion2 = SKFusion(embed_dims[4])\t\t\t\n",
    "\n",
    "\t\tself.layer5 = BasicLayer(network_depth=sum(depths), dim=embed_dims[4], depth=depths[4],\n",
    "\t\t\t\t\t   \t\t\t num_heads=num_heads[4], mlp_ratio=mlp_ratios[4],\n",
    "\t\t\t\t\t   \t\t\t norm_layer=norm_layer[4], window_size=window_size,\n",
    "\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio[4], attn_loc='last', conv_type=conv_type[4])\n",
    "\n",
    "\t\t# merge non-overlapping patches into image\n",
    "\t\tself.patch_unembed = PatchUnEmbed(\n",
    "\t\t\tpatch_size=1, out_chans=out_chans, embed_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "\n",
    "\tdef check_image_size(self, x):\n",
    "\t\t# NOTE: for I2I test\n",
    "\t\t_, _, h, w = x.size()\n",
    "\t\tmod_pad_h = (self.patch_size - h % self.patch_size) % self.patch_size\n",
    "\t\tmod_pad_w = (self.patch_size - w % self.patch_size) % self.patch_size\n",
    "\t\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward_features(self, x):\n",
    "\t\tx = self.patch_embed(x)\n",
    "\t\tx = self.layer1(x)\n",
    "\t\tskip1 = x\n",
    "\n",
    "\t\tx = self.patch_merge1(x)\n",
    "\t\tx = self.layer2(x)\n",
    "\t\tskip2 = x\n",
    "\n",
    "\t\tx = self.patch_merge2(x)\n",
    "\t\tx = self.layer3(x)\n",
    "\t\tx = self.patch_split1(x)\n",
    "\n",
    "\t\tx = self.fusion1([x, self.skip2(skip2)]) + x\n",
    "\t\tx = self.layer4(x)\n",
    "\t\tx = self.patch_split2(x)\n",
    "\n",
    "\t\tx = self.fusion2([x, self.skip1(skip1)]) + x\n",
    "\t\tx = self.layer5(x)\n",
    "\t\tx = self.patch_unembed(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tH, W = x.shape[2:]\n",
    "\t\tx = self.check_image_size(x)\n",
    "\n",
    "\t\tfeat = self.forward_features(x)\n",
    "\t\tK, B = torch.split(feat, (1, 3), dim=1)\n",
    "\n",
    "\t\tx = K * x - B + x\n",
    "\t\tx = x[:, :, :H, :W]\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def dehazeformer_t():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[4, 4, 4, 2, 2],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[0, 1/2, 1, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_s():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[8, 8, 8, 4, 4],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_b():\n",
    "    return DehazeFormer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[16, 16, 16, 8, 8],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_d():\n",
    "    return DehazeFormer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[32, 32, 32, 16, 16],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_w():\n",
    "    return DehazeFormer(\n",
    "        embed_dims=[48, 96, 192, 96, 48],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[16, 16, 16, 8, 8],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'])\n",
    "\n",
    "\n",
    "def dehazeformer_m():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[12, 12, 12, 6, 6],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['Conv', 'Conv', 'Conv', 'Conv', 'Conv'])\n",
    "\n",
    "\n",
    "def dehazeformer_l():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[48, 96, 192, 96, 48],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[16, 16, 16, 12, 12],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['Conv', 'Conv', 'Conv', 'Conv', 'Conv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T15:30:59.722870Z",
     "iopub.status.busy": "2025-01-31T15:30:59.722610Z",
     "iopub.status.idle": "2025-01-31T15:30:59.872129Z",
     "shell.execute_reply": "2025-01-31T15:30:59.870902Z",
     "shell.execute_reply.started": "2025-01-31T15:30:59.722848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from dehazeformer import *\n",
    "from torch.nn import functional as F\n",
    "from model_utils import AdaptiveInstanceNorm\n",
    "from ops import unpixel_shuffle\n",
    "\n",
    "import torch.nn as nn\n",
    "from residual_dense_block import SRDB\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, depth, num_heads, mlp_ratio=4.,\n",
    "\t\t\t\t norm_layer=nn.LayerNorm, window_size=8,\n",
    "\t\t\t\t attn_ratio=0., attn_loc='last', conv_type=None):\n",
    "\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.depth = depth\n",
    "\t\tself.gf=FastGuidedFilter(r=1)\n",
    "\t\tself.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "    \n",
    "\t\tdepth_rate=24\n",
    "\t\tkernel_size=3\n",
    "\t\tin_channels=3\n",
    "\t\tself.conv_out = nn.Conv2d(depth_rate*2, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "\t\tself.relu1=nn.ReLU(inplace=True)\n",
    "\t\tself.relu2=nn.ReLU(inplace=True)\n",
    "\t\tself.norm1=AdaptiveInstanceNorm(depth_rate)\n",
    "\t\tself.norm2=AdaptiveInstanceNorm(depth_rate) \n",
    "\t\tattn_depth = attn_ratio * depth\n",
    "\t\t#print(attn_depth,attn_ratio,depth)\n",
    "\t\tif attn_loc == 'last':\n",
    "\t\t\tuse_attns = [i >= depth-attn_depth for i in range(depth)]\n",
    "\t\telif attn_loc == 'first':\n",
    "\t\t\tuse_attns = [i < attn_depth for i in range(depth)]\n",
    "\t\telif attn_loc == 'middle':\n",
    "\t\t\tuse_attns = [i >= (depth-attn_depth)//2 and i < (depth+attn_depth)//2 for i in range(depth)]\n",
    "\n",
    "\t\t# build blocks\n",
    "\t\tself.blocks = nn.ModuleList([\n",
    "\t\t\tTransformerBlock(network_depth=network_depth,\n",
    "\t\t\t\t\t\t\t dim=dim, \n",
    "\t\t\t\t\t\t\t num_heads=num_heads,\n",
    "\t\t\t\t\t\t\t mlp_ratio=mlp_ratio,\n",
    "\t\t\t\t\t\t\t norm_layer=norm_layer,\n",
    "\t\t\t\t\t\t\t window_size=window_size,\n",
    "\t\t\t\t\t\t\t shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "\t\t\t\t\t\t\t use_attn=use_attns[i], conv_type=conv_type)\n",
    "\t\t\tfor i in range(depth)])\n",
    "\n",
    "\tdef forward(self, x_hr):\n",
    "\t\tx_lr = self.downsample(x_hr)\n",
    "   \n",
    "\t\tx_lr_new=self.norm1(x_lr)\n",
    "\t\tx_lr_new=self.relu1( x_lr_new)\n",
    "\t\tfor blc in self.blocks:\n",
    "   \n",
    "\t\t    x_lr_new = blc(x_lr_new)\n",
    "    \n",
    "\t\tg_hr= self.gf(x_lr, x_lr_new, x_hr)\n",
    "\t\tgx_cat=torch.cat([g_hr,x_hr],1)\n",
    "\t\tg_hr=self.conv_out(gx_cat)\n",
    "\t\tg_hr=self.norm2(g_hr)\n",
    "\t\tg_hr=self.relu2( g_hr)\n",
    "\t\tx=g_hr+x_hr\n",
    "   \n",
    "\t\treturn g_hr\n",
    "   \n",
    "\n",
    "\n",
    "class DeepGuidedFilterFormer(nn.Module):\n",
    "    def __init__(self,  radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        norm = AdaptiveInstanceNorm\n",
    "        depth_rate=24\n",
    "        kernel_size=3\n",
    "        in_channels=3\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.relu1=nn.ReLU(inplace=True)\n",
    "        self.block_num=3\n",
    "        network_depth=50\n",
    "        dim=depth_rate\n",
    "        mlp_ratio=2.0\n",
    "        norm_layer=RLN\n",
    "        window_size=16\n",
    "        conv_type='Conv'\n",
    "        depth=4\n",
    "        num_heads=4\n",
    "        attn_ratio=1/4\n",
    "        \n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "                   BasicBlock(network_depth=network_depth, dim=dim, depth=depth,\n",
    "\t\t\t\t\t   \t\t\t num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "\t\t\t\t\t   \t\t\t norm_layer=norm_layer, window_size=window_size,\n",
    "\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio, attn_loc='last', conv_type=conv_type)\n",
    "\t\t\t             for i in range(self.block_num)])\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x_hr):\n",
    "        x_hr=self.conv_in(x_hr)\n",
    "        #x_hr=self.relu1(x_hr)\n",
    "        #pixelshuffle_ratio=2\n",
    "        # Unpixelshuffle\n",
    "        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n",
    "        \n",
    "        for blc in self.blocks:\n",
    "            x_hr=blc(x_hr)\n",
    "            \n",
    "        x_hr=self.conv_out(x_hr)\n",
    "        # Pixelshuffle\n",
    "        #y_lr = F.pixel_shuffle(\n",
    "           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n",
    "        #)\n",
    "\n",
    "        return x_hr\n",
    "           \n",
    "   \n",
    "   \n",
    "class ConvGuidedFilter(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/wuhuikai/DeepGuidedFilter\n",
    "    \"\"\"\n",
    "    def __init__(self, radius=1, norm=nn.BatchNorm2d, conv_a_kernel_size: int = 1):\n",
    "        super(ConvGuidedFilter, self).__init__()\n",
    "\n",
    "        self.box_filter = nn.Conv2d(\n",
    "            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n",
    "        )\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                6,\n",
    "                32,\n",
    "                kernel_size=conv_a_kernel_size,\n",
    "                padding=conv_a_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                32,\n",
    "                kernel_size=conv_a_kernel_size,\n",
    "                padding=conv_a_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                3,\n",
    "                kernel_size=conv_a_kernel_size,\n",
    "                padding=conv_a_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.box_filter.weight.data[...] = 1.0\n",
    "\n",
    "    def forward(self, x_lr, y_lr, x_hr):\n",
    "        _, _, h_lrx, w_lrx = x_lr.size()\n",
    "        _, _, h_hrx, w_hrx = x_hr.size()\n",
    "\n",
    "        N = self.box_filter(x_lr.data.new().resize_((1, 3, h_lrx, w_lrx)).fill_(1.0))\n",
    "        ## mean_x\n",
    "        mean_x = self.box_filter(x_lr) / N\n",
    "        ## mean_y\n",
    "        mean_y = self.box_filter(y_lr) / N\n",
    "        ## cov_xy\n",
    "        cov_xy = self.box_filter(x_lr * y_lr) / N - mean_x * mean_y\n",
    "        ## var_x\n",
    "        var_x = self.box_filter(x_lr * x_lr) / N - mean_x * mean_x\n",
    "\n",
    "        ## A\n",
    "        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n",
    "        ## b\n",
    "        b = mean_y - A * mean_x\n",
    "\n",
    "        ## mean_A; mean_b\n",
    "        mean_A = F.interpolate(A, (h_hrx, w_hrx), mode=\"bilinear\", align_corners=True)\n",
    "        mean_b = F.interpolate(b, (h_hrx, w_hrx), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return mean_A * x_hr + mean_b\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class DeepGuideddetail(nn.Module):\n",
    "    def __init__(self,  radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        norm = AdaptiveInstanceNorm\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        #self.lr = dehazeformer_m()\n",
    "        kernel_size=3\n",
    "        depth_rate=16\n",
    "        in_channels=3\n",
    "        num_dense_layer=4\n",
    "        growth_rate=16\n",
    "        growth_rate=16\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.rdb1 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb2 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb3 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb4 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "\n",
    "        self.gf = ConvGuidedFilter(radius, norm=norm)\n",
    "\n",
    "        self.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x_hr):\n",
    "        x_lr = self.downsample(x_hr)\n",
    "        y_lr=self.conv_in(x_lr)\n",
    "        y_lr=self.rdb1(y_lr)\n",
    "        y_lr=self.rdb2(y_lr)\n",
    "        y_lr=self.rdb3(y_lr)\n",
    "        y_lr=self.rdb4(y_lr)\n",
    "        y_lr=self.conv_out(y_lr)\n",
    "        \n",
    "        #pixelshuffle_ratio=2\n",
    "        # Unpixelshuffle\n",
    "        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n",
    "        \n",
    "        #y_lr=self.lr(x_lr)\n",
    "        # Pixelshuffle\n",
    "        #y_lr = F.pixel_shuffle(\n",
    "           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n",
    "        #)\n",
    "\n",
    "        return F.tanh( self.gf(x_lr, y_lr, x_hr))\n",
    "                \n",
    "        \n",
    "\n",
    "class DeepGuidedall(nn.Module):\n",
    "    def __init__(self,  radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        norm = AdaptiveInstanceNorm\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        #self.lr = dehazeformer_m()\n",
    "        kernel_size=3\n",
    "        depth_rate=16\n",
    "        in_channels=3\n",
    "        num_dense_layer=4\n",
    "        growth_rate=16\n",
    "        growth_rate=16\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.rdb1 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb2 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb3 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb4 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "\n",
    "        self.gf = ConvGuidedFilter(radius, norm=norm)\n",
    "        self.lr = dehazeformer_m()\n",
    "\n",
    "        self.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "        self.upsample = nn.Upsample(\n",
    "            scale_factor=2, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x_hr):\n",
    "        x_lr = self.downsample(x_hr)\n",
    "        y_lr=self.conv_in(x_lr)\n",
    "        y_lr=self.rdb1(y_lr)\n",
    "        y_lr=self.rdb2(y_lr)\n",
    "        y_lr=self.rdb3(y_lr)\n",
    "        y_lr=self.rdb4(y_lr)\n",
    "        y_detail=self.conv_out(y_lr)\n",
    "        y_base=self.lr(x_lr)\n",
    "        y_lr=y_base+y_detail\n",
    "        y_base=self.upsample(y_base)\n",
    "        \n",
    "        #pixelshuffle_ratio=2\n",
    "        # Unpixelshuffle\n",
    "        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n",
    "        \n",
    "        #y_lr=self.lr(x_lr)\n",
    "        # Pixelshuffle\n",
    "        #y_lr = F.pixel_shuffle(\n",
    "           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n",
    "        #)\n",
    "\n",
    "        return F.tanh( self.gf(x_lr, y_lr, x_hr)), y_base   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepGuidednew(nn.Module):\n",
    "    def __init__(self,  radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        norm = AdaptiveInstanceNorm\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        #self.lr = dehazeformer_m()\n",
    "        kernel_size=3\n",
    "        depth_rate=16\n",
    "        in_channels=3\n",
    "        num_dense_layer=4\n",
    "        growth_rate=16\n",
    "        growth_rate=16\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.rdb1 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb2 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb3 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb4 = SRDB(depth_rate, num_dense_layer, growth_rate)\n",
    "\n",
    "        self.gf = ConvGuidedFilter(radius, norm=norm)\n",
    "        self.lr = dehazeformer_m()\n",
    "\n",
    "        self.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "        self.upsample = nn.Upsample(\n",
    "            scale_factor=2, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x_hr):\n",
    "        x_lr = self.downsample(x_hr)\n",
    "        y_lr=self.conv_in(x_lr)\n",
    "        y_lr=self.rdb1(y_lr)\n",
    "        y_lr=self.rdb2(y_lr)\n",
    "        y_lr=self.rdb3(y_lr)\n",
    "        y_lr=self.rdb4(y_lr)\n",
    "        y_detail=self.conv_out(y_lr)\n",
    "        y_base=self.lr(x_lr)\n",
    "        y_lr=y_base+y_detail\n",
    "        y_base=self.upsample(y_base)\n",
    "        \n",
    "        #pixelshuffle_ratio=2\n",
    "        # Unpixelshuffle\n",
    "        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n",
    "        \n",
    "        #y_lr=self.lr(x_lr)\n",
    "        # Pixelshuffle\n",
    "        #y_lr = F.pixel_shuffle(\n",
    "           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n",
    "        #)\n",
    "\n",
    "        return  self.gf(x_lr, y_lr, x_hr), y_base               \n",
    "        \n",
    "        \n",
    "\n",
    "class DeepAtrousGuidedFilter(nn.Module):\n",
    "    def __init__(self,  radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        norm = AdaptiveInstanceNorm\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        self.lr = dehazeformer_m()\n",
    "\n",
    "        self.gf = ConvGuidedFilter(radius, norm=norm)\n",
    "\n",
    "        self.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x_hr):\n",
    "        x_lr = self.downsample(x_hr)\n",
    "        #pixelshuffle_ratio=2\n",
    "        # Unpixelshuffle\n",
    "        #x_lr_unpixelshuffled = unpixel_shuffle(x_lr, pixelshuffle_ratio)\n",
    "        y_lr=self.lr(x_lr)\n",
    "        # Pixelshuffle\n",
    "        #y_lr = F.pixel_shuffle(\n",
    "           # self.lr(x_lr_unpixelshuffled), pixelshuffle_ratio\n",
    "        #)\n",
    "\n",
    "        return F.tanh( self.gf(x_lr, y_lr, x_hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-31T15:30:59.872606Z",
     "iopub.status.idle": "2025-01-31T15:30:59.872863Z",
     "shell.execute_reply": "2025-01-31T15:30:59.872754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AdaptiveInstanceNorm(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(AdaptiveInstanceNorm, self).__init__()\n",
    "\n",
    "        self.w_0 = nn.Parameter(torch.Tensor([1.0]))\n",
    "        self.w_1 = nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "        self.ins_norm = nn.InstanceNorm2d(n, momentum=0.999, eps=0.001, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_0 * x + self.w_1 * self.ins_norm(x)\n",
    "\n",
    "\n",
    "class PALayer(nn.Module):\n",
    "    def __init__(self, channel: int):\n",
    "        super(PALayer, self).__init__()\n",
    "        self.pa = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.pa(x)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel: int):\n",
    "        super(CALayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.ca = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.ca(y)\n",
    "        return x * y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def unpixel_shuffle(feature, r: int = 1):\n",
    "    b, c, h, w = feature.shape\n",
    "    out_channel = c * (r ** 2)\n",
    "    out_h = h // r\n",
    "    out_w = w // r\n",
    "    feature_view = feature.contiguous().view(b, c, out_h, r, out_w, r)\n",
    "    feature_prime = (\n",
    "        feature_view.permute(0, 1, 3, 5, 2, 4)\n",
    "        .contiguous()\n",
    "        .view(b, out_channel, out_h, out_w)\n",
    "    )\n",
    "    return feature_prime\n",
    "\n",
    "\n",
    "def sample_patches(\n",
    "    inputs: torch.Tensor, patch_size: int = 3, stride: int = 2\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    :param inputs: the input feature maps, shape: (n, c, h, w).\n",
    "    :param patch_size: the spatial size of sampled patches\n",
    "    :param stride: the stride of sampling.\n",
    "    :return: extracted patches, shape: (n, c, patch_size, patch_size, n_patches).\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Patch sampler for feature maps.\n",
    "    Parameters\n",
    "    ---\n",
    "    inputs : torch.Tensor\n",
    "        \n",
    "    patch_size : int, optional\n",
    "       \n",
    "    stride : int, optional\n",
    "        \n",
    "    Returns\n",
    "    ---\n",
    "    patches : torch.Tensor\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    n, c, h, w = inputs.shape\n",
    "    patches = (\n",
    "        inputs.unfold(2, patch_size, stride)\n",
    "        .unfold(3, patch_size, stride)\n",
    "        .reshape(n, c, -1, patch_size, patch_size)\n",
    "        .permute(0, 1, 3, 4, 2)\n",
    "    )\n",
    "    return patches\n",
    "\n",
    "\n",
    "def chop_patches(\n",
    "    img: torch.Tensor, patch_size_h: int = 256, patch_size_w: int = 512\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    :param inputs: the input feature maps, shape: (n, c, h, w).\n",
    "    :param patch_size: the spatial size of sampled patches\n",
    "    :param stride: the stride of sampling.\n",
    "    :return: extracted patches, shape: (n, c, patch_size, patch_size, n_patches).\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Patch sampler for feature maps.\n",
    "    Parameters\n",
    "    ---\n",
    "    inputs : torch.Tensor\n",
    "\n",
    "    patch_size : int, optional\n",
    "\n",
    "    stride : int, optional\n",
    "\n",
    "    Returns\n",
    "    ---\n",
    "    patches : torch.Tensor\n",
    "\n",
    "    \"\"\"\n",
    "    patches = (\n",
    "        img.unfold(2, patch_size_h, patch_size_h)\n",
    "        .unfold(3, patch_size_w, patch_size_w)\n",
    "        .contiguous()\n",
    "        .permute(2, 3, 0, 1, 4, 5)\n",
    "        .flatten(start_dim=0, end_dim=2)\n",
    "        # .reshape(-1, c, patch_size_h, patch_size_w)\n",
    "    )\n",
    "    return patches\n",
    "\n",
    "\n",
    "def unchop_patches(\n",
    "    patches: torch.Tensor, img_h: int = 1024, img_w: int = 2048, n: int = 1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Assumes non-overlapping patches\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/reshaping-windows-into-image/19805\n",
    "    \"\"\"\n",
    "    _, c, patch_size_h, patch_size_w = patches.shape\n",
    "    num_h = img_h // patch_size_h\n",
    "    num_w = img_w // patch_size_w\n",
    "\n",
    "    img = patches.reshape(n, num_h * num_w, patch_size_h * patch_size_w * c).permute(\n",
    "        0, 2, 1\n",
    "    )\n",
    "    img = F.fold(\n",
    "        img,\n",
    "        (img_h, img_w),\n",
    "        (patch_size_h, patch_size_w),\n",
    "        1,\n",
    "        0,\n",
    "        (patch_size_h, patch_size_w),\n",
    "    )\n",
    "    return img.reshape(n, c, img_h, img_w)\n",
    "\n",
    "def roll_n(X, axis, n):\n",
    "    f_idx = tuple(\n",
    "        slice(None, None, None) if i != axis else slice(0, n, None)\n",
    "        for i in range(X.dim())\n",
    "    )\n",
    "    b_idx = tuple(\n",
    "        slice(None, None, None) if i != axis else slice(n, None, None)\n",
    "        for i in range(X.dim())\n",
    "    )\n",
    "    front = X[f_idx]\n",
    "    back = X[b_idx]\n",
    "    return torch.cat([back, front], axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Imports --- #\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# --- Perceptual loss network  --- #\n",
    "class LossNetwork(torch.nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        super(LossNetwork, self).__init__()\n",
    "        self.vgg_layers = vgg_model\n",
    "        self.layer_name_mapping = {\n",
    "            '3': \"relu1_2\",\n",
    "            '8': \"relu2_2\",\n",
    "            '15': \"relu3_3\"\n",
    "        }\n",
    "\n",
    "    def output_features(self, x):\n",
    "        output = {}\n",
    "        for name, module in self.vgg_layers._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layer_name_mapping:\n",
    "                output[self.layer_name_mapping[name]] = x\n",
    "        return list(output.values())\n",
    "\n",
    "    def forward(self, dehaze, gt):\n",
    "        loss = []\n",
    "        dehaze_features = self.output_features(dehaze)\n",
    "        gt_features = self.output_features(gt)\n",
    "        for dehaze_feature, gt_feature in zip(dehaze_features, gt_features):\n",
    "            loss.append(F.mse_loss(dehaze_feature, gt_feature))\n",
    "\n",
    "        return sum(loss)/len(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Imports --- #\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 =nn.Conv2d(in_channel, in_channel, kernel_size=kernel_size, padding=(kernel_size - 1) // 2) #ConvLayer(in_channel, in_channel, 3)\n",
    "        self.conv2 = ConvLayer(in_channel, out_channel, 3, downsample=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv1(input)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        return out    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class CAB(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(CAB, self).__init__()\n",
    "        #new_features=features//2\n",
    "        features=features//2\n",
    "        self.reduce_fature=nn.Conv2d(features*2, features, kernel_size=1, bias=False)\n",
    "        self.delta_gen1 = nn.Sequential(\n",
    "                        nn.Conv2d(features*2, features, kernel_size=1, bias=False),\n",
    "                        nn.Conv2d(features, 2, kernel_size=3, padding=1, bias=False)\n",
    "                        )\n",
    "\n",
    "        self.delta_gen2 = nn.Sequential(\n",
    "                        nn.Conv2d(features*2, features, kernel_size=1, bias=False),\n",
    "                        nn.Conv2d(features, 2, kernel_size=3, padding=1, bias=False)\n",
    "                        )\n",
    "\n",
    "\n",
    "        #self.delta_gen1.weight.data.zero_()\n",
    "        #self.delta_gen2.weight.data.zero_()\n",
    "\n",
    "    # https://github.com/speedinghzl/AlignSeg/issues/7\n",
    "    # the normlization item is set to [w/s, h/s] rather than [h/s, w/s]\n",
    "    # the function bilinear_interpolate_torch_gridsample2 is standard implementation, please use bilinear_interpolate_torch_gridsample2 for training.\n",
    "    def bilinear_interpolate_torch_gridsample(self, input, size, delta=0):\n",
    "        out_h, out_w = size\n",
    "        n, c, h, w = input.shape\n",
    "        s = 1.0\n",
    "        norm = torch.tensor([[[[w/s, h/s]]]]).type_as(input).to(input.device)\n",
    "        w_list = torch.linspace(-1.0, 1.0, out_h).view(-1, 1).repeat(1, out_w)\n",
    "        h_list = torch.linspace(-1.0, 1.0, out_w).repeat(out_h, 1)\n",
    "        grid = torch.cat((h_list.unsqueeze(2), w_list.unsqueeze(2)), 2)\n",
    "        grid = grid.repeat(n, 1, 1, 1).type_as(input).to(input.device)\n",
    "        grid = grid + delta.permute(0, 2, 3, 1) / norm\n",
    "\n",
    "        output = F.grid_sample(input, grid)\n",
    "        return output\n",
    "\n",
    "    def bilinear_interpolate_torch_gridsample2(self, input, size, delta=0):\n",
    "        out_h, out_w = size\n",
    "        n, c, h, w = input.shape\n",
    "        s = 2.0\n",
    "        norm = torch.tensor([[[[(out_w-1)/s, (out_h-1)/s]]]]).type_as(input).to(input.device) # not [h/s, w/s]\n",
    "        w_list = torch.linspace(-1.0, 1.0, out_h).view(-1, 1).repeat(1, out_w)\n",
    "        h_list = torch.linspace(-1.0, 1.0, out_w).repeat(out_h, 1)\n",
    "        grid = torch.cat((h_list.unsqueeze(2), w_list.unsqueeze(2)), 2)\n",
    "        grid = grid.repeat(n, 1, 1, 1).type_as(input).to(input.device)\n",
    "        grid = grid + delta.permute(0, 2, 3, 1) / norm\n",
    "\n",
    "        output = F.grid_sample(input, grid, align_corners=True)\n",
    "        return output\n",
    "\n",
    "    def forward(self, low_stage, high_stage):\n",
    "        h, w = low_stage.size(2), low_stage.size(3)\n",
    "        high_stage=self.reduce_fature(high_stage)\n",
    "        high_stage = F.interpolate(input=high_stage, size=(h, w), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        concat = torch.cat((low_stage, high_stage), 1)\n",
    "        delta1 = self.delta_gen1(concat)\n",
    "        delta2 = self.delta_gen2(concat)\n",
    "        high_stage = self.bilinear_interpolate_torch_gridsample2(high_stage, (h, w), delta1)\n",
    "        low_stage = self.bilinear_interpolate_torch_gridsample2(low_stage, (h, w), delta2)\n",
    "\n",
    "        high_stage += low_stage\n",
    "        return high_stage\n",
    "\n",
    "class MakeDense(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, kernel_size=3):\n",
    "        super(MakeDense, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=kernel_size, padding=(kernel_size-1)//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv(x))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class PALayer(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(PALayer, self).__init__()\n",
    "        self.pa = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // 8, 1, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.pa(x)\n",
    "        return x * y\n",
    "\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(CALayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.ca = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.ca(y)\n",
    "        return x * y\n",
    "\n",
    "class SRDBDK(nn.Module):\n",
    "    def __init__(self, in_channels, num_dense_layer, growth_rate):\n",
    "        super(SRDBDK, self).__init__()\n",
    "        \n",
    "        modules = []\n",
    "        self.split_channel=in_channels//8\n",
    "        kernel_size=3\n",
    "        dilation=1\n",
    "        self.conv1 = nn.Conv2d(self.split_channel*1, self.split_channel, kernel_size=9, padding=4, dilation=1)\n",
    "        dilation=2\n",
    "        self.conv2 = nn.Conv2d(self.split_channel*2, self.split_channel*1, kernel_size=7, padding=3, dilation=1)\n",
    "        dilation=4\n",
    "        self.conv3 = nn.Conv2d(self.split_channel*4, self.split_channel*2, kernel_size=5,  padding=2, dilation=1)\n",
    "        dilation=8\n",
    "        self.conv4 = nn.Conv2d(self.split_channel*8, self.split_channel*4, kernel_size=3, padding=1, dilation=1)\n",
    "\n",
    "            \n",
    "        #self.residual_dense_layers = nn.Sequential(*modules)\n",
    "        _in_channels=in_channels\n",
    "        self.calayer=CALayer(in_channels)\n",
    "        self.palayer=PALayer(in_channels)\n",
    "        self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        splited = torch.split(x, [self.split_channel,self.split_channel*1,self.split_channel*2,self.split_channel*4], dim=1)\n",
    "        x0=F.relu(self.conv1(splited[0]))\n",
    "        tmp= torch.cat((splited[1], x0), 1)\n",
    "        x1=F.relu(self.conv2(tmp))\n",
    "        tmp= torch.cat((splited[2], x0, x1), 1)\n",
    "        x2=F.relu(self.conv3(tmp))\n",
    "        tmp= torch.cat((splited[3], x0, x1, x2), 1)\n",
    "        x3=F.relu(self.conv4(tmp))\n",
    "        tmp= torch.cat(( x0, x1, x2, x3), 1)\n",
    "        \n",
    "        out = self.conv_1x1(tmp)\n",
    "        out=self.calayer(out)\n",
    "        out=self.palayer(out)\n",
    "        out=out+x\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class SRDB(nn.Module):\n",
    "    def __init__(self, in_channels, num_dense_layer, growth_rate):\n",
    "        super(SRDB, self).__init__()\n",
    "        \n",
    "        modules = []\n",
    "        self.split_channel=in_channels//4\n",
    "        kernel_size=3\n",
    "        dilation=1\n",
    "        self.conv1 = nn.Conv2d(self.split_channel*1, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        dilation=2\n",
    "        self.conv2 = nn.Conv2d(self.split_channel*2, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        dilation=4\n",
    "        self.conv3 = nn.Conv2d(self.split_channel*3, self.split_channel, kernel_size=kernel_size,  padding=dilation, dilation=dilation)\n",
    "        dilation=8\n",
    "        self.conv4 = nn.Conv2d(self.split_channel*4, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "\n",
    "            \n",
    "        #self.residual_dense_layers = nn.Sequential(*modules)\n",
    "        _in_channels=in_channels\n",
    "        self.calayer=CALayer(in_channels)\n",
    "        self.palayer=PALayer(in_channels)\n",
    "        self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        splited = torch.split(x, self.split_channel, dim=1)\n",
    "        x0=F.relu(self.conv1(splited[0]))\n",
    "        tmp= torch.cat((splited[1], x0), 1)\n",
    "        x1=F.relu(self.conv2(tmp))\n",
    "        tmp= torch.cat((splited[2], x0, x1), 1)\n",
    "        x2=F.relu(self.conv3(tmp))\n",
    "        tmp= torch.cat((splited[3], x0, x1, x2), 1)\n",
    "        x3=F.relu(self.conv4(tmp))\n",
    "        tmp= torch.cat(( x0, x1, x2, x3), 1)\n",
    "        \n",
    "        out = self.conv_1x1(tmp)\n",
    "        out=self.calayer(out)\n",
    "        out=self.palayer(out)\n",
    "        #print(out.shape, x.shape)\n",
    "        out=out+x\n",
    "        return out\n",
    "\n",
    "\n",
    "class SRDBN(nn.Module):\n",
    "    def __init__(self, in_channels, num_dense_layer, growth_rate):\n",
    "        super(SRDBN, self).__init__()\n",
    "        modules = []\n",
    "        self.split_channel=in_channels//8\n",
    "        kernel_size=3\n",
    "        dilation=1\n",
    "        self.conv1 = nn.Conv2d(self.split_channel*1, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        dilation=2\n",
    "        self.conv2 = nn.Conv2d(self.split_channel*2, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        dilation=4\n",
    "        self.conv3 = nn.Conv2d(self.split_channel*3, self.split_channel, kernel_size=kernel_size,  padding=dilation, dilation=dilation)\n",
    "        dilation=8\n",
    "        self.conv4 = nn.Conv2d(self.split_channel*4, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        \n",
    "        dilation=8\n",
    "        self.conv5 = nn.Conv2d(self.split_channel*5, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        \n",
    "        dilation=4\n",
    "        self.conv6 = nn.Conv2d(self.split_channel*6, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        \n",
    "        dilation=2\n",
    "        self.conv7 = nn.Conv2d(self.split_channel*7, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "        \n",
    "        dilation=1\n",
    "        self.conv8 = nn.Conv2d(self.split_channel*8, self.split_channel, kernel_size=kernel_size, padding=dilation, dilation=dilation)\n",
    "\n",
    "            \n",
    "        #self.residual_dense_layers = nn.Sequential(*modules)\n",
    "        _in_channels=in_channels\n",
    "        self.calayer=CALayer(in_channels)\n",
    "        self.palayer=PALayer(in_channels)\n",
    "        self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        splited = torch.split(x, self.split_channel, dim=1)\n",
    "        x0=F.relu(self.conv1(splited[0]))\n",
    "        tmp= torch.cat((splited[1], x0), 1)\n",
    "        x1=F.relu(self.conv2(tmp))\n",
    "        tmp= torch.cat((splited[2], x0, x1), 1)\n",
    "        x2=F.relu(self.conv3(tmp))\n",
    "        tmp= torch.cat((splited[3], x0, x1, x2), 1)\n",
    "        x3=F.relu(self.conv4(tmp))\n",
    "        tmp= torch.cat(( splited[4],x0, x1, x2, x3), 1)\n",
    "        x4=F.relu(self.conv5(tmp))\n",
    "        \n",
    "        tmp= torch.cat(( splited[5],x0, x1, x2, x3,x4), 1)\n",
    "        x5=F.relu(self.conv6(tmp))\n",
    "        \n",
    "        tmp= torch.cat(( splited[6],x0, x1, x2, x3,x4,x5), 1)\n",
    "        x6=F.relu(self.conv7(tmp))\n",
    "        \n",
    "        tmp= torch.cat(( splited[7],x0, x1, x2, x3,x4,x5,x6), 1)\n",
    "        x7=F.relu(self.conv8(tmp))\n",
    "        \n",
    "       \n",
    "        tmp= torch.cat(( x0, x1, x2, x3,x4,x5,x6,x7), 1)\n",
    "        out = self.conv_1x1(tmp)\n",
    "        out=self.calayer(out)\n",
    "        out=self.palayer(out)\n",
    "        out=out+x\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n",
    "                \n",
    "\n",
    "class RDB(nn.Module):\n",
    "    def __init__(self, in_channels, num_dense_layer, growth_rate):\n",
    "        super(RDB, self).__init__()\n",
    "        _in_channels = in_channels\n",
    "        modules = []\n",
    "        for i in range(num_dense_layer):\n",
    "            modules.append(MakeDense(_in_channels, growth_rate))\n",
    "            _in_channels += growth_rate\n",
    "        self.residual_dense_layers = nn.Sequential(*modules)\n",
    "        self.conv_1x1 = nn.Conv2d(_in_channels, in_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.residual_dense_layers(x)\n",
    "        out = self.conv_1x1(out)\n",
    "        out = out + x\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports --- #\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from random import randrange\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "import glob\n",
    "# --- Training dataset --- #\n",
    "\n",
    "class TrainData512(data.Dataset):\n",
    "    def __init__(self, crop_size, train_data_dir):\n",
    "        super().__init__()\n",
    "        hazeeffected_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/reside/hazy/'\n",
    "        \n",
    "        hazy_data = glob.glob(hazeeffected_images_dir + \"*.png\")\n",
    "        hazefree_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/reside/clear/'\n",
    "        haze_names=[]\n",
    "        gt_names=[]\n",
    "        for h_image in hazy_data:\n",
    "\t\t        h_image = h_image.split(\"/\")[-1]\n",
    "\t\t        id_ = h_image.split(\"_\")[0]  + \".png\"\n",
    "\t\t        haze_names.append(hazeeffected_images_dir+h_image)\n",
    "\t\t        gt_names.append(hazefree_images_dir+id_)\n",
    "        self.haze_names = haze_names\n",
    "        self.gt_names = gt_names\n",
    "        self.crop_size = crop_size\n",
    "        self.train_data_dir = train_data_dir\n",
    "\n",
    "    def get_images(self, index):\n",
    "        crop_width, crop_height = self.crop_size\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        haze_img = Image.open(haze_name)\n",
    "\n",
    "        try:\n",
    "            gt_img = Image.open(gt_name)\n",
    "        except:\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # --- x,y coordinate of left-top corner --- #\n",
    "        \n",
    "        haze_crop_img = haze_img.resize((512, 512),Image.ANTIALIAS)\n",
    "        gt_crop_img = gt_img.resize((512, 512),Image.ANTIALIAS)\n",
    "\n",
    "        # --- Transform to tensor --- #\n",
    "        transform_all = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "       \n",
    "        haze = transform_all(haze_crop_img)\n",
    "        gt = transform_all(gt_crop_img)\n",
    "\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        res = self.get_images(index)\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)\n",
    "        \n",
    "        \n",
    "\n",
    "class TrainDataNew(data.Dataset):\n",
    "    def __init__(self, crop_size, train_data_dir):\n",
    "        super().__init__()\n",
    "        hazeeffected_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/light_dehazenet/data/'\n",
    "        \n",
    "        hazy_data = glob.glob(hazeeffected_images_dir + \"*.jpg\")\n",
    "        hazefree_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/light_dehazenet/image/'\n",
    "        haze_names=[]\n",
    "        gt_names=[]\n",
    "        for h_image in hazy_data:\n",
    "\t\t        h_image = h_image.split(\"/\")[-1]\n",
    "\t\t        id_ = h_image.split(\"_\")[0] + \"_\" + h_image.split(\"_\")[1] + \".jpg\"\n",
    "\t\t        haze_names.append(hazeeffected_images_dir+h_image)\n",
    "\t\t        gt_names.append(hazefree_images_dir+id_)\n",
    "        self.haze_names = haze_names\n",
    "        self.gt_names = gt_names\n",
    "        self.crop_size = crop_size\n",
    "        self.train_data_dir = train_data_dir\n",
    "\n",
    "    def get_images(self, index):\n",
    "        crop_width, crop_height = self.crop_size\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        haze_img = Image.open(haze_name)\n",
    "\n",
    "        try:\n",
    "            gt_img = Image.open(gt_name)\n",
    "        except:\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "\n",
    "        width, height = haze_img.size\n",
    "\n",
    "        if width < crop_width or height < crop_height:\n",
    "            raise Exception('Bad image size: {}'.format(gt_name))\n",
    "\n",
    "        # --- x,y coordinate of left-top corner --- #\n",
    "        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n",
    "        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "\n",
    "        # --- Transform to tensor --- #\n",
    "         #transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        transform_gt = Compose([ToTensor()])\n",
    "        haze = transform_gt(haze_crop_img)\n",
    "        gt = transform_gt(gt_crop_img)\n",
    "\n",
    "        # --- Check the channel is 3 or not --- #\n",
    "        if list(haze.shape)[0] !=  3 or list(gt.shape)[0] !=  3:\n",
    "            raise Exception('Bad image channel: {}'.format(gt_name))\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        res = self.get_images(index)\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)\n",
    "        \n",
    "        \n",
    "        \n",
    "class TrainData(data.Dataset):\n",
    "    def __init__(self, crop_size, train_data_dir):\n",
    "        super().__init__()\n",
    "        # hazeeffected_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/light_dehazenet/data/'\n",
    "        hazeeffected_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "        \n",
    "        hazy_data = glob.glob(hazeeffected_images_dir + \"*.jpg\")\n",
    "        # hazefree_images_dir='/home/zsd/data/dehazing/ITS_v2/indoor/light_dehazenet/image/'\n",
    "        hazefree_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "\n",
    "        haze_names=[]\n",
    "        gt_names=[]\n",
    "        for h_image in hazy_data:\n",
    "\t\t        h_image = h_image.split(\"/\")[-1]\n",
    "\t\t        id_ = h_image.split(\"_\")[0] + \"_\" + h_image.split(\"_\")[1] + \".jpg\"\n",
    "\t\t        haze_names.append(hazeeffected_images_dir+h_image)\n",
    "\t\t        gt_names.append(hazefree_images_dir+id_)\n",
    "        self.haze_names = haze_names\n",
    "        self.gt_names = gt_names\n",
    "        self.crop_size = crop_size\n",
    "        self.train_data_dir = train_data_dir\n",
    "\n",
    "    def get_images(self, index):\n",
    "        crop_width, crop_height = self.crop_size\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        haze_img = Image.open(haze_name)\n",
    "\n",
    "        try:\n",
    "            gt_img = Image.open(gt_name)\n",
    "        except:\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "\n",
    "        width, height = haze_img.size\n",
    "\n",
    "        if width < crop_width or height < crop_height:\n",
    "            raise Exception('Bad image size: {}'.format(gt_name))\n",
    "\n",
    "        # --- x,y coordinate of left-top corner --- #\n",
    "        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n",
    "        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "\n",
    "        # --- Transform to tensor --- #\n",
    "        transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        transform_gt = Compose([ToTensor()])\n",
    "        haze = transform_haze(haze_crop_img)\n",
    "        gt = transform_gt(gt_crop_img)\n",
    "\n",
    "        # --- Check the channel is 3 or not --- #\n",
    "        if list(haze.shape)[0] !=  3 or list(gt.shape)[0] !=  3:\n",
    "            raise Exception('Bad image channel: {}'.format(gt_name))\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        res = self.get_images(index)\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "\n",
    "# # Create widgets for each hyper-parameter\n",
    "# learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n",
    "# crop_size_widget = widgets.Text(value='360,360', description='Crop Size:')\n",
    "# train_batch_size_widget = widgets.IntText(value=6, description='Train Batch Size:')\n",
    "# network_height_widget = widgets.IntText(value=3, description='Network Height:')\n",
    "# network_width_widget = widgets.IntText(value=6, description='Network Width:')\n",
    "# num_dense_layer_widget = widgets.IntText(value=4, description='Num Dense Layer:')\n",
    "# growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n",
    "# lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n",
    "# val_batch_size_widget = widgets.IntText(value=1, description='Val Batch Size:')\n",
    "# category_widget = widgets.Dropdown(options=['indoor', 'outdoor'], value='indoor', description='Category:')\n",
    "\n",
    "# # Display the widgets\n",
    "# display(learning_rate_widget, crop_size_widget, train_batch_size_widget, network_height_widget, network_width_widget, num_dense_layer_widget, growth_rate_widget, lambda_loss_widget, val_batch_size_widget, category_widget)\n",
    "\n",
    "# # Function to parse the crop size\n",
    "# def parse_crop_size(crop_size_str):\n",
    "#     return [int(x) for x in crop_size_str.split(',')]\n",
    "\n",
    "# # Assign the widget values to variables\n",
    "# learning_rate = learning_rate_widget.value\n",
    "# crop_size = parse_crop_size(crop_size_widget.value)\n",
    "# train_batch_size = train_batch_size_widget.value\n",
    "# network_height = network_height_widget.value\n",
    "# network_width = network_width_widget.value\n",
    "# num_dense_layer = num_dense_layer_widget.value\n",
    "# growth_rate = growth_rate_widget.value\n",
    "# lambda_loss = lambda_loss_widget.value\n",
    "# val_batch_size = val_batch_size_widget.value\n",
    "# category = category_widget.value\n",
    "\n",
    "# print('Hyper-parameters set:')\n",
    "# print(f'learning_rate: {learning_rate}')\n",
    "# print(f'crop_size: {crop_size}')\n",
    "# print(f'train_batch_size: {train_batch_size}')\n",
    "# print(f'network_height: {network_height}')\n",
    "# print(f'network_width: {network_width}')\n",
    "# print(f'num_dense_layer: {num_dense_layer}')\n",
    "# print(f'growth_rate: {growth_rate}')\n",
    "# print(f'lambda_loss: {lambda_loss}')\n",
    "# print(f'val_batch_size: {val_batch_size}')\n",
    "# print(f'category: {category}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3d78539b664651a2e90b0242ea941c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Learning Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864188b0e87046fca43d223b9bffb766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='360,360', description='Crop Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44f93d2b6f1402eaef7a16be6f9e742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=6, description='Train Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf688d44bfa414fab9625001be10c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=3, description='Network Height:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb0ab6bd4ed47e6acc4a6eab2911ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=6, description='Network Width:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15b4bc59c4c40ebb6da50e55eef761e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=4, description='Num Dense Layer:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9ac38e17c94d93b5b14ed3f3eae7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='Growth Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8eb9a0782d49eeb2a0168e69caff67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.04, description='Lambda Loss:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ed04e8ee0746d897c63ef5590120ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='Val Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0a5924d2974ed7ac7752b4d4e527d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', index=2, options=('indoor', 'outdoor', 'nh'), value='nh')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f902b1ce244f798abc8af8645542b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Execution Env:', options=('local', 'kaggle'), value='local')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters set:\n",
      "learning_rate: 0.0001\n",
      "crop_size: [360, 360]\n",
      "train_batch_size: 6\n",
      "network_height: 3\n",
      "network_width: 6\n",
      "num_dense_layer: 4\n",
      "growth_rate: 16\n",
      "lambda_loss: 0.04\n",
      "val_batch_size: 1\n",
      "category: nh\n",
      "execution_env: local\n",
      "\n",
      "Final dataset paths:\n",
      "Training directory: /Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy\n",
      "Validation directory: /Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT\n",
      "Number of epochs: 10\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Create widgets for each hyper-parameter ---\n",
    "learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n",
    "crop_size_widget = widgets.Text(value='360,360', description='Crop Size:')\n",
    "train_batch_size_widget = widgets.IntText(value=6, description='Train Batch Size:')\n",
    "network_height_widget = widgets.IntText(value=3, description='Network Height:')\n",
    "network_width_widget = widgets.IntText(value=6, description='Network Width:')\n",
    "num_dense_layer_widget = widgets.IntText(value=4, description='Num Dense Layer:')\n",
    "growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n",
    "lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n",
    "val_batch_size_widget = widgets.IntText(value=1, description='Val Batch Size:')\n",
    "category_widget = widgets.Dropdown(options=['indoor', 'outdoor', 'nh'], value='nh', description='Category:')\n",
    "execution_env_widget = widgets.Dropdown(options=['local', 'kaggle'], value='local', description='Execution Env:')\n",
    "\n",
    "# --- Display the widgets ---\n",
    "display(\n",
    "    learning_rate_widget, crop_size_widget, train_batch_size_widget, network_height_widget, \n",
    "    network_width_widget, num_dense_layer_widget, growth_rate_widget, lambda_loss_widget, \n",
    "    val_batch_size_widget, category_widget, execution_env_widget\n",
    ")\n",
    "\n",
    "# --- Function to parse crop size ---\n",
    "def parse_crop_size(crop_size_str):\n",
    "    return [int(x) for x in crop_size_str.split(',')]\n",
    "\n",
    "# --- Assign the widget values to variables ---\n",
    "learning_rate = learning_rate_widget.value\n",
    "crop_size = parse_crop_size(crop_size_widget.value)\n",
    "train_batch_size = train_batch_size_widget.value\n",
    "network_height = network_height_widget.value\n",
    "network_width = network_width_widget.value\n",
    "num_dense_layer = num_dense_layer_widget.value\n",
    "growth_rate = growth_rate_widget.value\n",
    "lambda_loss = lambda_loss_widget.value\n",
    "val_batch_size = val_batch_size_widget.value\n",
    "category = category_widget.value\n",
    "execution_env = execution_env_widget.value  # Local or Kaggle\n",
    "\n",
    "print('\\nHyper-parameters set:')\n",
    "print(f'learning_rate: {learning_rate}')\n",
    "print(f'crop_size: {crop_size}')\n",
    "print(f'train_batch_size: {train_batch_size}')\n",
    "print(f'network_height: {network_height}')\n",
    "print(f'network_width: {network_width}')\n",
    "print(f'num_dense_layer: {num_dense_layer}')\n",
    "print(f'growth_rate: {growth_rate}')\n",
    "print(f'lambda_loss: {lambda_loss}')\n",
    "print(f'val_batch_size: {val_batch_size}')\n",
    "print(f'category: {category}')\n",
    "print(f'execution_env: {execution_env}')\n",
    "\n",
    "# --- Set category-specific hyper-parameters ---\n",
    "if category == 'indoor':\n",
    "    num_epochs = 1500\n",
    "    train_data_dir = './data/train/indoor/'\n",
    "    val_data_dir = './data/test/SOTS/indoor/'\n",
    "elif category == 'outdoor':\n",
    "    num_epochs = 10\n",
    "    train_data_dir = './data/train/outdoor/'\n",
    "    val_data_dir = './data/test/SOTS/outdoor/'\n",
    "elif category == 'nh':\n",
    "    num_epochs = 10\n",
    "    train_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "    val_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "else:\n",
    "    raise Exception('Wrong image category. Set it to indoor or outdoor for RESIDE dataset.')\n",
    "\n",
    "# --- Adjust paths based on execution environment ---\n",
    "if execution_env == 'kaggle':\n",
    "    train_data_dir = '/kaggle/input/reside-dataset/' + train_data_dir.strip('./')\n",
    "    val_data_dir = '/kaggle/input/reside-dataset/' + val_data_dir.strip('./')\n",
    "\n",
    "print('\\nFinal dataset paths:')\n",
    "print(f'Training directory: {train_data_dir}')\n",
    "print(f'Validation directory: {val_data_dir}')\n",
    "print(f'Number of epochs: {num_epochs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- no weight loaded ---\n",
      "Total_params: 4645694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/c7x3kkg169s8zlyh_hfm0_wm0000gp/T/ipykernel_1054/3320964144.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(models+'{}_haze_best_{}_{}'.format(category, network_height, network_width)))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal_params: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pytorch_total_params))\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# --- Load training data and validation/test data --- #\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m train_data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m val_data_loader \u001b[38;5;241m=\u001b[39m DataLoader(ValData(val_data_dir), batch_size\u001b[38;5;241m=\u001b[39mval_batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# --- Imports --- #\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "#from dehazeformer import *\n",
    "from torchvision.models import vgg16\n",
    "from perceptual import LossNetwork\n",
    "#plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Gpu device --- #\n",
    "device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# --- Define the network --- #\n",
    "net = DeepGuidednew() #GridDehazeNet(height=network_height, width=network_width, num_dense_layer=num_dense_layer, growth_rate=growth_rate)\n",
    "\n",
    "#net =FFA(3,19)\n",
    "# --- Build optimizer --- #\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# --- Multi-GPU --- #\n",
    "net = net.to(device)\n",
    "net = nn.DataParallel(net, device_ids=device_ids)\n",
    "\n",
    "\n",
    "# --- Define the perceptual loss network --- #\n",
    "vgg_model = vgg16(pretrained=True).features[:16]\n",
    "vgg_model = vgg_model.to(device)\n",
    "for param in vgg_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "loss_network = LossNetwork(vgg_model)\n",
    "loss_network.eval()\n",
    "models='formernew'\n",
    "\n",
    "# --- Load the network weight --- #\n",
    "try:\n",
    "    net.load_state_dict(torch.load(models+'{}_haze_best_{}_{}'.format(category, network_height, network_width)))\n",
    "    print('--- weight loaded ---')\n",
    "except:\n",
    "    print('--- no weight loaded ---')\n",
    "\n",
    "\n",
    "# --- Calculate all trainable parameters in network --- #\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total_params: {}\".format(pytorch_total_params))\n",
    "\n",
    "\n",
    "# --- Load training data and validation/test data --- #\n",
    "train_data_loader = DataLoader(TrainData(crop_size, train_data_dir), batch_size=train_batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(ValData(val_data_dir), batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- no weight loaded ---\n",
      "Total_params: 4645694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p4/c7x3kkg169s8zlyh_hfm0_wm0000gp/T/ipykernel_1054/1396023484.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(models+'{}_haze_best_{}_{}'.format(category, network_height, network_width)))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 70\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal_params: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pytorch_total_params))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# --- Load training data and validation/test data --- #\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m train_data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m val_data_loader \u001b[38;5;241m=\u001b[39m DataLoader(ValData(val_data_dir), batch_size\u001b[38;5;241m=\u001b[39mval_batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# --- Previous PSNR and SSIM in testing --- #\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- Previous PSNR and SSIM in testing --- #\n",
    "old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)\n",
    "print('old_val_psnr: {0:.2f}, old_val_ssim: {1:.4f}'.format(old_val_psnr, old_val_ssim))\n",
    "train_psnrold=0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    psnr_list = []\n",
    "    start_time = time.time()\n",
    "    adjust_learning_rate(optimizer, epoch, category=category)\n",
    "\n",
    "    for batch_id, train_data in enumerate(train_data_loader):\n",
    "\n",
    "        haze, gt = train_data\n",
    "        haze = haze.to(device)\n",
    "        gt = gt.to(device)\n",
    "\n",
    "        # --- Zero the parameter gradients --- #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # --- Forward + Backward + Optimize --- #\n",
    "        net.train()\n",
    "        dehaze,base = net(haze)\n",
    "        base_loss = F.smooth_l1_loss(base, gt)\n",
    "\n",
    "        smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "        perceptual_loss = loss_network(dehaze, gt)\n",
    "        loss = smooth_loss + lambda_loss*perceptual_loss+base_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- To calculate average PSNR --- #\n",
    "        psnr_list.extend(to_psnr(dehaze, gt))\n",
    "\n",
    "        if not (batch_id % 100):\n",
    "            print('Epoch: {0}, Iteration: {1}'.format(epoch, batch_id))\n",
    "\n",
    "    # --- Calculate the average training PSNR in one epoch --- #\n",
    "    train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "    # --- Save the network parameters --- #\n",
    "    torch.save(net.state_dict(), models+'{}_haze_{}_{}'.format(category, network_height, network_width))\n",
    "\n",
    "    # --- Use the evaluation model in testing --- #\n",
    "    net.eval()\n",
    "\n",
    "    val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "    one_epoch_time = time.time() - start_time\n",
    "    print_log(epoch+1, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, models+category)\n",
    "    \n",
    "    \n",
    "    if train_psnr< train_psnrold:\n",
    "        adjust_learning_rate_step(optimizer, category=category)            \n",
    "\n",
    "    # --- update the network weight --- #\n",
    "    if val_psnr >= old_val_psnr:\n",
    "        torch.save(net.state_dict(), models+'{}_haze_best_{}_{}'.format(category, network_height, network_width))\n",
    "        old_val_psnr = val_psnr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "paper: GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing\n",
    "file: utils.py\n",
    "about: all utilities\n",
    "author: Xiaohong Liu\n",
    "date: 01/08/19\n",
    "\"\"\"\n",
    "\n",
    "# --- Imports --- #\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as utils\n",
    "from math import log10\n",
    "from skimage import measure\n",
    "\n",
    "\n",
    "def to_psnr(dehaze, gt):\n",
    "    mse = F.mse_loss(dehaze, gt, reduction='none')\n",
    "    #print (mse)\n",
    "    mse_split = torch.split(mse, 1, dim=0)\n",
    "    mse_list = [torch.mean(torch.squeeze(mse_split[ind])).item() for ind in range(len(mse_split))]\n",
    "\n",
    "    intensity_max = 1.0\n",
    "    psnr_list = [10.0 * log10(intensity_max / min(max(mse,0.000001),1000)) for mse in mse_list]\n",
    "    return psnr_list\n",
    "\n",
    "\n",
    "def to_ssim_skimage(dehaze, gt):\n",
    "    dehaze_list = torch.split(dehaze, 1, dim=0)\n",
    "    gt_list = torch.split(gt, 1, dim=0)\n",
    "\n",
    "    dehaze_list_np = [dehaze_list[ind].permute(0, 2, 3, 1).data.cpu().numpy().squeeze() for ind in range(len(dehaze_list))]\n",
    "    gt_list_np = [gt_list[ind].permute(0, 2, 3, 1).data.cpu().numpy().squeeze() for ind in range(len(dehaze_list))]\n",
    "    ssim_list = [measure.compare_ssim(dehaze_list_np[ind],  gt_list_np[ind], data_range=1, multichannel=True) for ind in range(len(dehaze_list))]\n",
    "\n",
    "    return ssim_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validationStlyle(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: GateDehazeNet\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: The GPU that loads the network\n",
    "    :param category: indoor or outdoor test dataset\n",
    "    :param save_tag: tag of saving image or not\n",
    "    :return: average PSNR value\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "\n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            haze, gt, image_name = val_data\n",
    "            haze = haze.to(device)\n",
    "            gt = gt.to(device)\n",
    "            hazing = net(gt,haze)\n",
    "\n",
    "        # --- Calculate the average PSNR --- #\n",
    "        psnr_list.extend(to_psnr(hazing, haze))\n",
    "\n",
    "        # --- Calculate the average SSIM --- #\n",
    "        ssim_list.extend(to_ssim_skimage(hazing, haze))\n",
    "\n",
    "        # --- Save image --- #\n",
    "        if save_tag:\n",
    "            save_image(dehaze, image_name, category)\n",
    "  \n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list)\n",
    "    \n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list)\n",
    "    return avr_psnr, avr_ssim\n",
    "    \n",
    "    \n",
    "def validationB(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: GateDehazeNet\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: The GPU that loads the network\n",
    "    :param category: indoor or outdoor test dataset\n",
    "    :param save_tag: tag of saving image or not\n",
    "    :return: average PSNR value\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "\n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            haze, gt, image_name = val_data\n",
    "            haze = haze.to(device)\n",
    "            gt = gt.to(device)\n",
    "            dehaze, _ = net(haze)\n",
    "\n",
    "        # --- Calculate the average PSNR --- #\n",
    "        psnr_list.extend(to_psnr(dehaze, gt))\n",
    "\n",
    "        # --- Calculate the average SSIM --- #\n",
    "        ssim_list.extend(to_ssim_skimage(dehaze, gt))\n",
    "\n",
    "        # --- Save image --- #\n",
    "        if save_tag:\n",
    "            save_image(dehaze, image_name, category)\n",
    "  \n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list)\n",
    "    \n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list)\n",
    "    return avr_psnr, avr_ssim\n",
    "        \n",
    "    \n",
    "\n",
    "def validation(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: GateDehazeNet\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: The GPU that loads the network\n",
    "    :param category: indoor or outdoor test dataset\n",
    "    :param save_tag: tag of saving image or not\n",
    "    :return: average PSNR value\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "\n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            haze, gt, image_name = val_data\n",
    "            haze = haze.to(device)\n",
    "            gt = gt.to(device)\n",
    "            dehaze = net(haze)\n",
    "\n",
    "        # --- Calculate the average PSNR --- #\n",
    "        psnr_list.extend(to_psnr(dehaze, gt))\n",
    "\n",
    "        # --- Calculate the average SSIM --- #\n",
    "        ssim_list.extend(to_ssim_skimage(dehaze, gt))\n",
    "\n",
    "        # --- Save image --- #\n",
    "        if save_tag:\n",
    "            save_image(dehaze, image_name, category)\n",
    "  \n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list)\n",
    "    \n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list)\n",
    "    return avr_psnr, avr_ssim\n",
    "def validationN(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: GateDehazeNet\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: The GPU that loads the network\n",
    "    :param category: indoor or outdoor test dataset\n",
    "    :param save_tag: tag of saving image or not\n",
    "    :return: average PSNR value\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "\n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            haze, gt, image_name = val_data\n",
    "            haze = haze.to(device)\n",
    "            gt = gt.to(device)\n",
    "            dehaze,_,_ = net(haze)\n",
    "\n",
    "        # --- Calculate the average PSNR --- #\n",
    "        psnr_list.extend(to_psnr(dehaze, gt))\n",
    "\n",
    "        # --- Calculate the average SSIM --- #\n",
    "        ssim_list.extend(to_ssim_skimage(dehaze, gt))\n",
    "\n",
    "        # --- Save image --- #\n",
    "        if save_tag:\n",
    "            save_image(dehaze, image_name, category)\n",
    "\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list)\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list)\n",
    "    \n",
    "    return avr_psnr, avr_ssim\n",
    "\n",
    "\n",
    "def save_image(dehaze, image_name, category):\n",
    "    dehaze_images = torch.split(dehaze, 1, dim=0)\n",
    "    batch_num = len(dehaze_images)\n",
    "\n",
    "    for ind in range(batch_num):\n",
    "        utils.save_image(dehaze_images[ind], './{}_results/{}'.format(category, image_name[ind][:-3] + 'png'))\n",
    "\n",
    "\n",
    "def print_log(epoch, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, category):\n",
    "    print('({0:.0f}s) Epoch [{1}/{2}], Train_PSNR:{3:.2f}, Val_PSNR:{4:.2f}, Val_SSIM:{5:.4f}'\n",
    "          .format(one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim))\n",
    "\n",
    "    # --- Write the training log --- #\n",
    "    with open('./training_log/{}_log.txt'.format(category), 'a') as f:\n",
    "        print('Date: {0}s, Time_Cost: {1:.0f}s, Epoch: [{2}/{3}], Train_PSNR: {4:.2f}, Val_PSNR: {5:.2f}, Val_SSIM: {6:.4f}'\n",
    "              .format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "                      one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim), file=f)\n",
    "\n",
    "\n",
    "def adjust_learning_rate_step(optimizer, category, lr_decay=0.95):\n",
    "\n",
    "    # --- Decay learning rate --- #\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "       param_group['lr'] *= lr_decay\n",
    "       print('Learning rate sets to {}.'.format(param_group['lr']))\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "def adjust_learning_rate(optimizer, epoch, category, lr_decay=0.90):\n",
    "\n",
    "    # --- Decay learning rate --- #\n",
    "    step = 18 if category == 'indoor' else 3\n",
    "    if category == 'NH':\n",
    "       step = 20\n",
    "    #if not category == 'indoor':\n",
    "       #for param_group in optimizer.param_groups:\n",
    "            #param_group['lr'] *= 0.99\n",
    "            #print('Learning rate sets to {}.'.format(param_group['lr']))\n",
    "    if not epoch % step and epoch > 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= lr_decay\n",
    "            print('Learning rate sets to {}.'.format(param_group['lr']))\n",
    "    else:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('Learning rate sets to {}.'.format(param_group['lr']))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
