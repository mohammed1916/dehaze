{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1587463,"sourceType":"datasetVersion","datasetId":937211},{"sourceId":4853613,"sourceType":"datasetVersion","datasetId":2813430},{"sourceId":10417877,"sourceType":"datasetVersion","datasetId":6456606},{"sourceId":10443410,"sourceType":"datasetVersion","datasetId":6464114},{"sourceId":222875724,"sourceType":"kernelVersion"},{"sourceId":287852,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":246650,"modelId":268224},{"sourceId":288973,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":247592,"modelId":269124},{"sourceId":297672,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":247592,"modelId":269124},{"sourceId":299643,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":247592,"modelId":269124},{"sourceId":335035,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":247592,"modelId":269124}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install scikit-image ipywidgets torchmetrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport numpy as np\nfrom torch.nn.init import _calculate_fan_in_and_fan_out\nfrom timm.layers import to_2tuple, trunc_normal_\nimport os\nimport torchvision.utils as utils\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nimport glob\nfrom torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\nfrom torchvision.models import vgg16\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nfrom random import randrange\nimport time\nfrom math import log10\nfrom skimage import measure\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom torch.nn.init import trunc_normal_\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:31.628884Z","iopub.execute_input":"2025-04-13T15:59:31.629287Z","iopub.status.idle":"2025-04-13T15:59:44.617394Z","shell.execute_reply.started":"2025-04-13T15:59:31.629256Z","shell.execute_reply":"2025-04-13T15:59:44.616223Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torchvision import transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:59:44.618962Z","iopub.execute_input":"2025-04-13T15:59:44.619475Z","iopub.status.idle":"2025-04-13T15:59:44.624500Z","shell.execute_reply.started":"2025-04-13T15:59:44.619442Z","shell.execute_reply":"2025-04-13T15:59:44.623133Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## RevisedLayerNorm","metadata":{"editable":false}},{"cell_type":"code","source":"class RevisedLayerNorm(nn.Module):\n    \"\"\"Revised LayerNorm\"\"\"\n    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n        super(RevisedLayerNorm, self).__init__()\n        self.epsilon = epsilon\n        self.detach_gradient = detach_gradient\n\n        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n\n        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n\n        trunc_normal_(self.scale_mlp.weight, std=.02)\n        nn.init.constant_(self.scale_mlp.bias, 1)\n\n        trunc_normal_(self.shift_mlp.weight, std=.02)\n        nn.init.constant_(self.shift_mlp.bias, 0)\n\n    def forward(self, input_tensor):\n        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n\n        normalized_tensor = (input_tensor - mean_value) / std_value\n\n        if self.detach_gradient:\n            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n        else:\n            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n\n        output = normalized_tensor * self.scale + self.shift\n        return output, rescale, rebias\n\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:44.626891Z","iopub.execute_input":"2025-04-13T15:59:44.627278Z","iopub.status.idle":"2025-04-13T15:59:44.648069Z","shell.execute_reply.started":"2025-04-13T15:59:44.627248Z","shell.execute_reply":"2025-04-13T15:59:44.647045Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MultiLayerPerceptron(nn.Module):\n    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n        super().__init__()\n        output_channels = output_channels or input_channels\n        hidden_channels = hidden_channels or input_channels\n\n        self.depth = depth\n\n        self.mlp_layers = nn.Sequential(\n            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n        )\n\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, layer):\n        if isinstance(layer, nn.Conv2d):\n            gain = (8 * self.depth) ** (-1 / 4)\n            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n            trunc_normal_(layer.weight, std=std)\n            if layer.bias is not None:\n                nn.init.constant_(layer.bias, 0)\n\n    def forward(self, x):\n        return self.mlp_layers(x)\n\n\ndef partition_into_windows(tensor, window_size):\n    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n    batch_size, height, width, channels = tensor.shape\n    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n\n    tensor = tensor.view(\n        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n    )\n    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n    return windows\n\n\ndef merge_windows(windows, window_size, height, width):\n    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n    tensor = windows.view(\n        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n    )\n    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n    return tensor\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:44.649738Z","iopub.execute_input":"2025-04-13T15:59:44.650178Z","iopub.status.idle":"2025-04-13T15:59:44.671732Z","shell.execute_reply.started":"2025-04-13T15:59:44.650134Z","shell.execute_reply":"2025-04-13T15:59:44.670663Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### test","metadata":{"editable":false}},{"cell_type":"code","source":"import torch\n\n# Initialize the MultiLayerPerceptron with sample parameters\ndepth = 4\ninput_channels = 64\nhidden_channels = 128\noutput_channels = 64\n\nmlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n\n# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\ninput_tensor = torch.randn(2, 64, 16, 16)\noutput_tensor = mlp(input_tensor)\n\n# Check output shape\nmlp_output_shape = output_tensor.shape\n\n# Test window partition and merging\nbatch_size, height, width, channels = 2, 16, 16, 64\nwindow_size = 4\n\n# Create a random tensor for window functions (B, H, W, C) format\ninput_window_tensor = torch.randn(batch_size, height, width, channels)\n\n# Apply partitioning and merging\nwindows = partition_into_windows(input_window_tensor, window_size)\nreconstructed_tensor = merge_windows(windows, window_size, height, width)\n\n# Check shapes\nwindows_shape = windows.shape\nreconstructed_shape = reconstructed_tensor.shape\n\n# Validate if the reconstruction matches the original input shape\nis_shape_correct = reconstructed_shape == input_window_tensor.shape\n\n# Output results\nmlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:44.672772Z","iopub.execute_input":"2025-04-13T15:59:44.673121Z","iopub.status.idle":"2025-04-13T15:59:44.863325Z","shell.execute_reply.started":"2025-04-13T15:59:44.673082Z","shell.execute_reply":"2025-04-13T15:59:44.862202Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(torch.Size([2, 64, 16, 16]),\n torch.Size([32, 16, 64]),\n torch.Size([2, 16, 16, 64]),\n True)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"class LocalWindowAttention(nn.Module):\n    def __init__(self, embed_dim, window_size, num_heads):\n        \"\"\"Self-attention mechanism within local windows.\"\"\"\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.window_size = window_size  # (height, width)\n        self.num_heads = num_heads\n        head_dim = embed_dim // num_heads\n        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n\n        # Compute and store relative positional encodings\n        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n\n        # Learnable transformation of relative position embeddings\n        self.relative_mlp = nn.Sequential(\n            nn.Linear(2, 256, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, num_heads, bias=True)\n        )\n\n        self.attention_softmax = nn.Softmax(dim=-1)\n\n    def forward(self, qkv):\n        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n        batch_size, num_tokens, _ = qkv.shape\n\n        # Reshape qkv into separate query, key, and value tensors\n        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n\n        # Scale query for stable attention computation\n        q = q * self.scaling_factor\n        attention_scores = q @ k.transpose(-2, -1)\n\n        # Compute relative position bias\n        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n\n        # Apply softmax and compute weighted values\n        attention_weights = self.attention_softmax(attention_scores)\n        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n\n        return output\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:44.864279Z","iopub.execute_input":"2025-04-13T15:59:44.864555Z","iopub.status.idle":"2025-04-13T15:59:44.873779Z","shell.execute_reply.started":"2025-04-13T15:59:44.864533Z","shell.execute_reply":"2025-04-13T15:59:44.872524Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def compute_log_relative_positions(window_size):\n    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n    coord_range = torch.arange(window_size)\n\n    # Create coordinate grid\n    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n    \n    # Flatten coordinates\n    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n\n    # Compute relative positions\n    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n\n    # Format and apply log transformation\n    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n\n    return log_relative_positions\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:44.875459Z","iopub.execute_input":"2025-04-13T15:59:44.875959Z","iopub.status.idle":"2025-04-13T15:59:44.895186Z","shell.execute_reply.started":"2025-04-13T15:59:44.875925Z","shell.execute_reply":"2025-04-13T15:59:44.894134Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class AdaptiveAttention(nn.Module):\n    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.head_dim = embed_dim // num_heads\n        self.num_heads = num_heads\n\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n        self.network_depth = network_depth\n        self.enable_attention = enable_attention\n        self.conv_mode = conv_mode\n\n        # Define convolutional processing based on mode\n        if self.conv_mode == 'Conv':\n            self.conv_layer = nn.Sequential(\n                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n            )\n\n        if self.conv_mode == 'DWConv':\n            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n\n        if self.conv_mode == 'DWConv' or self.enable_attention:\n            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n\n        if self.enable_attention:\n            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module):\n        \"\"\"Custom weight initialization.\"\"\"\n        if isinstance(module, nn.Conv2d):\n            weight_shape = module.weight.shape\n\n            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n                std = math.sqrt(2.0 / float(fan_in + fan_out))\n                trunc_normal_(module.weight, std=std)\n            else:\n                gain = (8 * self.network_depth) ** (-1/4)\n                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n                trunc_normal_(module.weight, std=std)\n\n            if module.bias is not None:\n                nn.init.constant_(module.bias, 0)\n\n    def pad_for_window_processing(self, x, shift=False):\n        \"\"\"Pads the input tensor to fit window processing requirements.\"\"\"\n        _, _, height, width = x.size()\n        pad_h = (self.window_size - height % self.window_size) % self.window_size\n        pad_w = (self.window_size - width % self.window_size) % self.window_size\n\n        if shift:\n            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n                          self.shift_size, (self.window_size - self.shift_size + pad_h) % self.window_size), mode='reflect')\n        else:\n            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n        return x\n\n    def forward(self, x):\n        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n        batch_size, channels, height, width = x.shape\n\n        if self.conv_mode == 'DWConv' or self.enable_attention:\n            v_proj = self.value_projection(x)\n\n        if self.enable_attention:\n            qk_proj = self.query_key_projection(x)\n            qkv = torch.cat([qk_proj, v_proj], dim=1)\n\n            # Apply padding for shifted window processing\n            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n            padded_height, padded_width = padded_qkv.shape[2:]\n\n            # Partition into windows\n            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n\n            # Apply window-based attention\n            attn_windows = self.window_attention(qkv_windows)\n\n            # Merge back to original spatial dimensions\n            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n\n            # Reverse the cyclic shift\n            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), self.shift_size:(self.shift_size + width), :]\n            attn_output = attn_output.permute(0, 3, 1, 2)\n\n            if self.conv_mode in ['Conv', 'DWConv']:\n                conv_output = self.conv_layer(v_proj)\n                output = self.output_projection(conv_output + attn_output)\n            else:\n                output = self.output_projection(attn_output)\n\n        else:\n            if self.conv_mode == 'Conv':\n                output = self.conv_layer(x)  # No attention, using convolution only\n            elif self.conv_mode == 'DWConv':\n                output = self.output_projection(self.conv_layer(v_proj))\n\n        return output","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:44.898878Z","iopub.execute_input":"2025-04-13T15:59:44.899241Z","iopub.status.idle":"2025-04-13T15:59:44.920176Z","shell.execute_reply.started":"2025-04-13T15:59:44.899213Z","shell.execute_reply":"2025-04-13T15:59:44.918620Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class VisionTransformerBlock(nn.Module):\n    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n        \"\"\"\n        A transformer block that includes attention (optional) and MLP layers.\n        \"\"\"\n        super().__init__()\n        self.enable_attention = enable_attention\n        self.enable_mlp_norm = enable_mlp_norm\n\n        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n        self.attention_layer = AdaptiveAttention(\n            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n        )\n\n        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the transformer block.\n        \"\"\"\n        residual = x\n        if self.enable_attention:\n            x, rescale, rebias = self.pre_norm(x)\n        x = self.attention_layer(x)\n        if self.enable_attention:\n            x = x * rescale + rebias\n        x = residual + x  # Residual connection\n\n        residual = x\n        if self.enable_attention and self.enable_mlp_norm:\n            x, rescale, rebias = self.post_norm(x)\n        x = self.mlp_layer(x)\n        if self.enable_attention and self.enable_mlp_norm:\n            x = x * rescale + rebias\n        x = residual + x  # Residual connection\n\n        return x\n\n\nclass TransformerStage(nn.Module):\n    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n                 norm_layer=nn.LayerNorm, window_size=8,\n                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n        \"\"\"\n        A stage of transformer blocks with configurable attention placement.\n        \"\"\"\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_layers = num_layers\n\n        attention_layers = int(attention_ratio * num_layers)\n\n        if attention_placement == 'last':\n            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n        elif attention_placement == 'first':\n            enable_attentions = [i < attention_layers for i in range(num_layers)]\n        elif attention_placement == 'middle':\n            enable_attentions = [\n                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n                for i in range(num_layers)\n            ]\n\n        # Build transformer blocks\n        self.blocks = nn.ModuleList([\n            VisionTransformerBlock(\n                network_depth=network_depth,\n                embed_dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                norm_layer=norm_layer,\n                window_size=window_size,\n                shift_size=0 if (i % 2 == 0) else window_size // 2,\n                enable_attention=enable_attentions[i],\n                conv_mode=conv_mode\n            ) for i in range(num_layers)\n        ])\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the transformer stage.\n        \"\"\"\n        for block in self.blocks:\n            x = block(x)\n        return x\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:44.922032Z","iopub.execute_input":"2025-04-13T15:59:44.922415Z","iopub.status.idle":"2025-04-13T15:59:44.941895Z","shell.execute_reply.started":"2025-04-13T15:59:44.922383Z","shell.execute_reply":"2025-04-13T15:59:44.940788Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n        \"\"\"\n        Patch embedding module that projects input images into token embeddings.\n        \"\"\"\n        super().__init__()\n        self.input_channels = input_channels\n        self.embedding_dim = embedding_dim\n\n        if kernel_size is None:\n            kernel_size = patch_size\n\n        self.projection = nn.Conv2d(\n            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass to generate patch embeddings.\n        \"\"\"\n        return self.projection(x)\n\n\nclass PatchReconstruction(nn.Module):\n    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n        \"\"\"\n        Patch reconstruction module that converts token embeddings back to image patches.\n        \"\"\"\n        super().__init__()\n        self.output_channels = output_channels\n        self.embedding_dim = embedding_dim\n\n        if kernel_size is None:\n            kernel_size = 1\n\n        self.projection = nn.Sequential(\n            nn.Conv2d(\n                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n                padding=kernel_size // 2, padding_mode='reflect'\n            ),\n            nn.PixelShuffle(patch_size)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass to reconstruct image from embeddings.\n        \"\"\"\n        return self.projection(x)\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:44.943716Z","iopub.execute_input":"2025-04-13T15:59:44.944134Z","iopub.status.idle":"2025-04-13T15:59:44.964610Z","shell.execute_reply.started":"2025-04-13T15:59:44.944093Z","shell.execute_reply":"2025-04-13T15:59:44.963239Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class SelectiveKernelFusion(nn.Module):\n    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n        \"\"\"\n        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n\n        Args:\n            channels (int): Number of input channels.\n            num_branches (int): Number of feature branches to fuse.\n            reduction_ratio (int): Reduction ratio for the attention mechanism.\n        \"\"\"\n        super().__init__()\n        \n        self.num_branches = num_branches\n        reduced_channels = max(int(channels / reduction_ratio), 4)\n        \n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.channel_attention = nn.Sequential(\n            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n            nn.ReLU(),\n            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n        )\n        \n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, feature_maps):\n        \"\"\"\n        Forward pass for selective kernel fusion.\n\n        Args:\n            feature_maps (list of tensors): A list of feature maps to be fused.\n\n        Returns:\n            torch.Tensor: The adaptively fused feature map.\n        \"\"\"\n        batch_size, channels, height, width = feature_maps[0].shape\n        \n        # Concatenate feature maps along a new dimension (num_branches)\n        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n        \n        # Compute attention weights\n        aggregated_features = torch.sum(stacked_features, dim=1)\n        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n\n        # Weighted sum of input feature maps\n        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n        return fused_output\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:44.965833Z","iopub.execute_input":"2025-04-13T15:59:44.966240Z","iopub.status.idle":"2025-04-13T15:59:44.988261Z","shell.execute_reply.started":"2025-04-13T15:59:44.966198Z","shell.execute_reply":"2025-04-13T15:59:44.986960Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class DehazingTransformer(nn.Module):\n    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n                 embed_dims=[24, 48, 96, 48, 24],\n                 mlp_ratios=[2., 4., 4., 2., 2.],\n                 layer_depths=[16, 16, 16, 8, 8],\n                 num_heads=[2, 4, 6, 1, 1],\n                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n        super().__init__()\n\n        # Patch embedding settings\n        self.patch_size = 4\n        self.window_size = window_size\n\n        # Initial patch embedding\n        self.patch_embed = PatchEmbedding(\n            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n\n        # Backbone layers\n        self.encoder_stage1 = TransformerStage(\n            network_depth=sum(layer_depths),\n            embed_dim=embed_dims[0],\n            num_layers=layer_depths[0],\n            num_heads=num_heads[0],\n            mlp_ratio=mlp_ratios[0],\n            norm_layer=norm_layers[0],\n            window_size=window_size,\n            attention_ratio=attention_ratios[0],\n            attention_placement='last',\n            conv_mode=conv_types[0]\n        )\n        \n        self.downsample1 = PatchEmbedding(\n            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n        )\n        \n        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n        \n        self.encoder_stage2 = TransformerStage(\n            network_depth=sum(layer_depths),\n            embed_dim=embed_dims[1],\n            num_layers=layer_depths[1],\n            num_heads=num_heads[1],\n            mlp_ratio=mlp_ratios[1],\n            norm_layer=norm_layers[1],\n            window_size=window_size,\n            attention_ratio=attention_ratios[1],\n            attention_placement='last',\n            conv_mode=conv_types[1]\n        )\n        \n        self.downsample2 = PatchEmbedding(\n            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n        )\n        \n        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n        \n        self.encoder_stage3 = TransformerStage(\n            network_depth=sum(layer_depths),\n            embed_dim=embed_dims[2],\n            num_layers=layer_depths[2],\n            num_heads=num_heads[2],\n            mlp_ratio=mlp_ratios[2],\n            norm_layer=norm_layers[2],\n            window_size=window_size,\n            attention_ratio=attention_ratios[2],\n            attention_placement='last',\n            conv_mode=conv_types[2]\n        )\n        \n        self.upsample1 = PatchReconstruction(\n            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n        )\n        \n        assert embed_dims[1] == embed_dims[3]\n        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n        \n        self.decoder_stage1 = TransformerStage(\n            network_depth=sum(layer_depths),\n            embed_dim=embed_dims[3],\n            num_layers=layer_depths[3],\n            num_heads=num_heads[3],\n            mlp_ratio=mlp_ratios[3],\n            norm_layer=norm_layers[3],\n            window_size=window_size,\n            attention_ratio=attention_ratios[3],\n            attention_placement='last',\n            conv_mode=conv_types[3]\n        )\n        \n        self.upsample2 = PatchReconstruction(\n            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n        )\n        \n        assert embed_dims[0] == embed_dims[4]\n        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n        \n        self.decoder_stage2 = TransformerStage(\n            network_depth=sum(layer_depths),\n            embed_dim=embed_dims[4],\n            num_layers=layer_depths[4],\n            num_heads=num_heads[4],\n            mlp_ratio=mlp_ratios[4],\n            norm_layer=norm_layers[4],\n            window_size=window_size,\n            attention_ratio=attention_ratios[4],\n            attention_placement='last',\n            conv_mode=conv_types[4]\n        )\n\n        # Final patch reconstruction\n        self.patch_reconstruction = PatchReconstruction(\n            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n\n    def adjust_image_size(self, x):\n        # Ensures the input image size is compatible with the patch size\n        _, _, height, width = x.size()\n        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n        return x\n\n    def extract_features(self, x):\n        x = self.patch_embed(x)\n        x = self.encoder_stage1(x)\n        skip1 = x\n\n        x = self.downsample1(x)\n        x = self.encoder_stage2(x)\n        skip2 = x\n\n        x = self.downsample2(x)\n        x = self.encoder_stage3(x)\n        x = self.upsample1(x)\n\n        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n        x = self.decoder_stage1(x)\n        x = self.upsample2(x)\n\n        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n        x = self.decoder_stage2(x)\n        x = self.patch_reconstruction(x)\n        return x\n\n    def forward(self, x):\n        original_height, original_width = x.shape[2:]\n        x = self.adjust_image_size(x)\n\n        features = self.extract_features(x)\n        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n\n        # Dehazing formula: I = J * t + A * (1 - t)\n        x = transmission_map * x - atmospheric_light + x\n        x = x[:, :, :original_height, :original_width]\n        return x","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:44.989325Z","iopub.execute_input":"2025-04-13T15:59:44.989765Z","iopub.status.idle":"2025-04-13T15:59:45.015022Z","shell.execute_reply.started":"2025-04-13T15:59:44.989722Z","shell.execute_reply":"2025-04-13T15:59:45.013748Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def build_dehazing_transformer():\n    return DehazingTransformer(\n        embed_dims=[24, 48, 96, 48, 24],\n        mlp_ratios=[2., 4., 4., 2., 2.],\n        layer_depths=[12, 12, 12, 6, 6],\n        num_heads=[2, 4, 6, 1, 1],\n        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n    )","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:45.016314Z","iopub.execute_input":"2025-04-13T15:59:45.016755Z","iopub.status.idle":"2025-04-13T15:59:45.037984Z","shell.execute_reply.started":"2025-04-13T15:59:45.016716Z","shell.execute_reply":"2025-04-13T15:59:45.036693Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class ConvolutionalGuidedFilter(nn.Module):\n    def __init__(self, radius=1, norm_layer=nn.BatchNorm2d, conv_kernel_size: int = 1):\n        super(ConvolutionalGuidedFilter, self).__init__()\n\n        self.box_filter = nn.Conv2d(\n            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n        )\n        self.conv_a = nn.Sequential(\n            nn.Conv2d(\n                6,\n                32,\n                kernel_size=conv_kernel_size,\n                padding=conv_kernel_size // 2,\n                bias=False,\n            ),\n            norm_layer(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                32,\n                32,\n                kernel_size=conv_kernel_size,\n                padding=conv_kernel_size // 2,\n                bias=False,\n            ),\n            norm_layer(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                32,\n                3,\n                kernel_size=conv_kernel_size,\n                padding=conv_kernel_size // 2,\n                bias=False,\n            ),\n        )\n        self.box_filter.weight.data[...] = 1.0\n\n    def forward(self, x_low_res, y_low_res, x_high_res):\n        _, _, h_lr, w_lr = x_low_res.size()\n        _, _, h_hr, w_hr = x_high_res.size()\n\n        N = self.box_filter(x_low_res.data.new().resize_((1, 3, h_lr, w_lr)).fill_(1.0))\n        ## mean_x\n        mean_x = self.box_filter(x_low_res) / N\n        ## mean_y\n        mean_y = self.box_filter(y_low_res) / N\n        ## cov_xy\n        cov_xy = self.box_filter(x_low_res * y_low_res) / N - mean_x * mean_y\n        ## var_x\n        var_x = self.box_filter(x_low_res * x_low_res) / N - mean_x * mean_x\n\n        ## A\n        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n        ## b\n        b = mean_y - A * mean_x\n\n        ## mean_A; mean_b\n        mean_A = F.interpolate(A, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n        mean_b = F.interpolate(b, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n\n        return mean_A * x_high_res + mean_b","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:45.039155Z","iopub.execute_input":"2025-04-13T15:59:45.039529Z","iopub.status.idle":"2025-04-13T15:59:45.061551Z","shell.execute_reply.started":"2025-04-13T15:59:45.039486Z","shell.execute_reply":"2025-04-13T15:59:45.060393Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class PixelAttentionLayer(nn.Module):\n    def __init__(self, channels):\n        super(PixelAttentionLayer, self).__init__()\n        self.attention = nn.Sequential(\n                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channels // 8, 1, kernel_size=1, padding=0, bias=True),\n                nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        attention_map = self.attention(x)\n        return x * attention_map\n\nclass ChannelAttentionLayer(nn.Module):\n    def __init__(self, channels):\n        super(ChannelAttentionLayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.attention = nn.Sequential(\n                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channels // 8, channels, kernel_size=1, padding=0, bias=True),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        pooled = self.avg_pool(x)\n        attention_map = self.attention(pooled)\n        return x * attention_map\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:45.062530Z","iopub.execute_input":"2025-04-13T15:59:45.062886Z","iopub.status.idle":"2025-04-13T15:59:45.079332Z","shell.execute_reply.started":"2025-04-13T15:59:45.062857Z","shell.execute_reply":"2025-04-13T15:59:45.078045Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class SuperResolutionDilationBlock(nn.Module):\n    def __init__(self, in_channels, num_dense_layers, growth_rate):\n        super(SuperResolutionDilationBlock, self).__init__()\n\n        self.split_channels = in_channels // 4\n        kernel_size = 3\n\n        # Dilated convolutions with increasing dilation rates\n        self.conv1 = nn.Conv2d(self.split_channels, self.split_channels, kernel_size=kernel_size, padding=1, dilation=1)\n        self.conv2 = nn.Conv2d(self.split_channels * 2, self.split_channels, kernel_size=kernel_size, padding=2, dilation=2)\n        self.conv3 = nn.Conv2d(self.split_channels * 3, self.split_channels, kernel_size=kernel_size, padding=4, dilation=4)\n        self.conv4 = nn.Conv2d(self.split_channels * 4, self.split_channels, kernel_size=kernel_size, padding=8, dilation=8)\n\n        # Attention mechanisms\n        self.channel_attention = ChannelAttentionLayer(in_channels)\n        self.pixel_attention = PixelAttentionLayer(in_channels)\n\n        # Final 1x1 convolution for feature fusion\n        self.conv_1x1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0)\n\n    def forward(self, x):\n        # Split input into 4 equal parts along channel dimension\n        split_features = torch.split(x, self.split_channels, dim=1)\n\n        x0 = F.relu(self.conv1(split_features[0]))\n        tmp = torch.cat((split_features[1], x0), dim=1)\n        x1 = F.relu(self.conv2(tmp))\n\n        tmp = torch.cat((split_features[2], x0, x1), dim=1)\n        x2 = F.relu(self.conv3(tmp))\n\n        tmp = torch.cat((split_features[3], x0, x1, x2), dim=1)\n        x3 = F.relu(self.conv4(tmp))\n\n        # Concatenate all outputs\n        merged_features = torch.cat((x0, x1, x2, x3), dim=1)\n\n        # Apply 1x1 convolution for feature refinement\n        out = self.conv_1x1(merged_features)\n\n        # Apply attention mechanisms\n        out = self.channel_attention(out)\n        out = self.pixel_attention(out)\n\n        # Residual connection\n        return out + x","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:45.080485Z","iopub.execute_input":"2025-04-13T15:59:45.080817Z","iopub.status.idle":"2025-04-13T15:59:45.093830Z","shell.execute_reply.started":"2025-04-13T15:59:45.080789Z","shell.execute_reply":"2025-04-13T15:59:45.092330Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class AdaptiveInstanceNormalization(nn.Module):\n    def __init__(self, num_channels):\n        super(AdaptiveInstanceNormalization, self).__init__()\n\n        # Learnable scaling factors\n        self.scale_x = nn.Parameter(torch.tensor(1.0))  # Identity scaling\n        self.scale_norm = nn.Parameter(torch.tensor(0.0))  # Initially no effect\n\n        # Instance normalization layer with affine transformation enabled\n        self.instance_norm = nn.InstanceNorm2d(num_channels, momentum=0.999, eps=0.001, affine=True)\n\n    def forward(self, x):\n        normalized_x = self.instance_norm(x)\n        return self.scale_x * x + self.scale_norm * normalized_x","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:45.095057Z","iopub.execute_input":"2025-04-13T15:59:45.095478Z","iopub.status.idle":"2025-04-13T15:59:45.112416Z","shell.execute_reply.started":"2025-04-13T15:59:45.095449Z","shell.execute_reply":"2025-04-13T15:59:45.110573Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class DeepGuidedNetwork(nn.Module):\n    def __init__(self, radius=1):\n        super().__init__()\n\n        # Adaptive Normalization for Guided Filtering\n        norm = AdaptiveInstanceNormalization\n        kernel_size = 3\n        depth_rate = 16\n        in_channels = 3\n        num_dense_layer = 4\n        growth_rate = 16\n\n        # Initial convolution layers\n        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n\n        # Residual Dense Blocks (RDBs)\n        self.rdb1 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n        self.rdb2 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n        self.rdb3 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n        self.rdb4 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n\n        # Guided Filter & Dehazing Transformer\n        self.guided_filter = ConvolutionalGuidedFilter(radius, norm_layer=norm)\n        self.dehaze_network = build_dehazing_transformer()\n\n        # Downsampling & Upsampling Layers\n        self.downsample = nn.Upsample(scale_factor=0.5, mode=\"bilinear\", align_corners=True)\n        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n\n    def forward(self, x_hr):\n        x_lr = self.downsample(x_hr)\n    \n        # Initial conv\n        y_features = self.conv_in(x_lr)\n    \n        # RDBs + collect features\n        feat1 = self.rdb1(y_features)\n        feat2 = self.rdb2(feat1)\n        feat3 = self.rdb3(feat2)\n        feat4 = self.rdb4(feat3)\n        y_detail = self.conv_out(feat4)\n    \n        # Base image\n        y_base = self.dehaze_network(x_lr)\n    \n        # Combine\n        y_lr = y_base + y_detail\n        y_base_hr = self.upsample(y_base)\n    \n        # Guided output\n        refined_output = self.guided_filter(x_lr, y_lr, x_hr)\n    \n        return refined_output, y_base_hr, [feat1, feat2, feat3, feat4]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:59:45.113970Z","iopub.execute_input":"2025-04-13T15:59:45.114447Z","iopub.status.idle":"2025-04-13T15:59:45.135959Z","shell.execute_reply.started":"2025-04-13T15:59:45.114415Z","shell.execute_reply":"2025-04-13T15:59:45.134289Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class DeepGuidedNetwork(nn.Module):\n    def __init__(self, radius=1):\n        super().__init__()\n\n        # Adaptive Normalization for Guided Filtering\n        norm = AdaptiveInstanceNormalization\n        kernel_size = 3\n        depth_rate = 16\n        in_channels = 3\n        num_dense_layer = 4\n        growth_rate = 16\n\n        # Initial convolution layers\n        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n\n        # Residual Dense Blocks (RDBs)\n        self.rdb1 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n        self.rdb2 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n        self.rdb3 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n        self.rdb4 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n\n        # Guided Filter & Dehazing Transformer\n        self.guided_filter = ConvolutionalGuidedFilter(radius, norm_layer=norm)\n        self.dehaze_network = build_dehazing_transformer()\n\n        # Downsampling & Upsampling Layers\n        self.downsample = nn.Upsample(scale_factor=0.5, mode=\"bilinear\", align_corners=True)\n        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n\n    def forward(self, x_hr):\n        # Low-resolution processing\n        x_lr = self.downsample(x_hr)\n\n        # Detail extraction through Residual Dense Blocks\n        y_features = self.conv_in(x_lr)\n        y_features = self.rdb1(y_features)\n        y_features = self.rdb2(y_features)\n        y_features = self.rdb3(y_features)\n        y_features = self.rdb4(y_features)\n        y_detail = self.conv_out(y_features)\n\n        # Base image estimation using DehazeFormer\n        y_base = self.dehaze_network(x_lr)\n\n        # Combining base and details\n        y_lr = y_base + y_detail\n        y_base_hr = self.upsample(y_base)\n\n        # Final guided filtering refinement\n        refined_output = self.guided_filter(x_lr, y_lr, x_hr)\n        \n        return refined_output, y_base_hr\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:45.137578Z","iopub.execute_input":"2025-04-13T15:59:45.138146Z","iopub.status.idle":"2025-04-13T15:59:45.158302Z","shell.execute_reply.started":"2025-04-13T15:59:45.138075Z","shell.execute_reply":"2025-04-13T15:59:45.157325Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def parse_crop_size(crop_size_str):\n    try:\n        return [int(x.strip()) for x in crop_size_str.split(',')]\n    except ValueError:\n        raise ValueError(f\"Invalid crop size format: '{crop_size_str}'. Expected comma-separated integers.\")","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:45.159310Z","iopub.execute_input":"2025-04-13T15:59:45.159581Z","iopub.status.idle":"2025-04-13T15:59:45.186784Z","shell.execute_reply.started":"2025-04-13T15:59:45.159559Z","shell.execute_reply":"2025-04-13T15:59:45.185626Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import Compose, ToTensor, Normalize\nfrom PIL import Image, UnidentifiedImageError\nfrom random import randrange\n\nclass TrainData(Dataset):\n    def __init__(self, crop_size, hazeeffected_images_dir, hazefree_images_dir):\n        super().__init__()\n        \n        # --- Ensure valid file extensions --- #\n        valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n        hazy_data = [\n            f for f in glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n            if f.lower().endswith(valid_extensions)\n        ]\n\n        if not hazy_data:\n            raise ValueError(f\"No valid images found in {hazeeffected_images_dir}\")\n\n        self.hazeeffected_images_dir = hazeeffected_images_dir\n        self.hazefree_images_dir = hazefree_images_dir\n\n        self.haze_names = []\n        self.gt_names = []\n        \n        for h_image in hazy_data:\n            filename = os.path.basename(h_image)\n            haze_path = os.path.join(self.hazeeffected_images_dir, filename)\n            gt_path = os.path.join(self.hazefree_images_dir, filename)\n\n            if not os.path.exists(gt_path):\n                print(f\"Warning: Ground-truth missing for {filename}, skipping.\")\n                continue\n\n            self.haze_names.append(haze_path)\n            self.gt_names.append(gt_path)\n\n        if not self.haze_names:\n            raise ValueError(\"No matching ground-truth images found.\")\n\n        self.crop_size = crop_size\n\n    def get_images(self, index):\n        crop_width, crop_height = self.crop_size\n        haze_name = self.haze_names[index]\n        gt_name = self.gt_names[index]\n\n        try:\n            haze_img = Image.open(haze_name).convert('RGB')\n            gt_img = Image.open(gt_name).convert('RGB')\n        except UnidentifiedImageError:\n            raise ValueError(f\"Invalid image format: {haze_name} or {gt_name}\")\n\n        width, height = haze_img.size\n\n        # --- Handle small images --- #\n        if width < crop_width or height < crop_height:\n            raise ValueError(f\"Image too small for cropping: {haze_name}\")\n\n        # --- Random crop --- #\n        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n\n        # --- Transform to tensor --- #\n        transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        transform_gt = Compose([ToTensor()])\n        haze = transform_haze(haze_crop_img)\n        gt = transform_gt(gt_crop_img)\n\n        # --- Check channels --- #\n        if haze.shape[0] != 3 or gt.shape[0] != 3:\n            raise ValueError(f\"Invalid image channels: {haze_name}\")\n\n        return haze, gt\n\n    def __getitem__(self, index):\n        return self.get_images(index)\n\n    def __len__(self):\n        return len(self.haze_names)\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:45.187989Z","iopub.execute_input":"2025-04-13T15:59:45.188464Z","iopub.status.idle":"2025-04-13T15:59:45.208914Z","shell.execute_reply.started":"2025-04-13T15:59:45.188347Z","shell.execute_reply":"2025-04-13T15:59:45.207726Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import Compose, ToTensor, Normalize\nfrom PIL import Image, UnidentifiedImageError\nfrom random import randrange, shuffle\n\nclass HazeDataset(Dataset):\n    def __init__(self, crop_size, hazeeffected_images_dir, hazefree_images_dir, split=\"train\", split_ratio=0.8):\n        \"\"\"\n        Dataset class for handling both training and validation dynamically.\n        \n        Args:\n            crop_size (tuple): (width, height) of the random crop.\n            hazeeffected_images_dir (str): Directory for hazy images.\n            hazefree_images_dir (str): Directory for ground-truth images.\n            split (str): \"train\" or \"valid\" (determines data split).\n            split_ratio (float): Percentage of images to use for training (default 80% train, 20% validation).\n        \"\"\"\n        super().__init__()\n        \n        # --- Ensure valid file extensions --- #\n        valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n        hazy_data = [\n            f for f in glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n            if f.lower().endswith(valid_extensions)\n        ]\n\n        if not hazy_data:\n            raise ValueError(f\"No valid images found in {hazeeffected_images_dir}\")\n\n        # # --- Sort and shuffle to ensure random split --- #\n        hazy_data.sort()\n        # shuffle(hazy_data)  \n\n        # --- Split into train and validation --- #\n        split_idx = int(len(hazy_data) * split_ratio)\n        if split == \"train\":\n            hazy_data = hazy_data[:split_idx]\n        else:  # \"valid\"\n            hazy_data = hazy_data[split_idx:]\n\n        self.haze_names = []\n        self.gt_names = []\n        \n        for h_image in hazy_data:\n            filename = os.path.basename(h_image)\n            haze_path = os.path.join(hazeeffected_images_dir, filename)\n            gt_path = os.path.join(hazefree_images_dir, filename)\n\n            if not os.path.exists(gt_path):\n                print(f\"Warning: Ground-truth missing for {filename}, skipping.\")\n                continue\n\n            self.haze_names.append(haze_path)\n            self.gt_names.append(gt_path)\n\n        if not self.haze_names:\n            raise ValueError(\"No matching ground-truth images found.\")\n\n        self.crop_size = crop_size\n\n    def get_images(self, index):\n        crop_width, crop_height = self.crop_size\n        haze_name = self.haze_names[index]\n        gt_name = self.gt_names[index]\n\n        try:\n            haze_img = Image.open(haze_name).convert('RGB')\n            gt_img = Image.open(gt_name).convert('RGB')\n        except UnidentifiedImageError:\n            raise ValueError(f\"Invalid image format: {haze_name} or {gt_name}\")\n\n        width, height = haze_img.size\n\n        # --- Handle small images --- #\n        if width < crop_width or height < crop_height:\n            raise ValueError(f\"Image too small for cropping: {haze_name}\")\n\n        # --- Random crop --- #\n        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n\n        # --- Transform to tensor --- #\n        transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        transform_gt = Compose([ToTensor()])\n        haze = transform_haze(haze_crop_img)\n        gt = transform_gt(gt_crop_img)\n\n        # --- Check channels --- #\n        if haze.shape[0] != 3 or gt.shape[0] != 3:\n            raise ValueError(f\"Invalid image channels: {haze_name}\")\n\n        return haze, gt\n\n    def __getitem__(self, index):\n        return self.get_images(index)\n\n    def __len__(self):\n        return len(self.haze_names)\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:45.209964Z","iopub.execute_input":"2025-04-13T15:59:45.210280Z","iopub.status.idle":"2025-04-13T15:59:45.230114Z","shell.execute_reply.started":"2025-04-13T15:59:45.210252Z","shell.execute_reply":"2025-04-13T15:59:45.229017Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Validation","metadata":{"editable":false}},{"cell_type":"code","source":"def to_psnr(dehaze, gt):\n    \"\"\"\n    Compute PSNR (Peak Signal-to-Noise Ratio) between dehazed and ground truth images.\n\n    Args:\n        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n\n    Returns:\n        List[float]: PSNR values for each image in the batch.\n    \"\"\"\n    mse = F.mse_loss(dehaze, gt, reduction='none').mean(dim=[1, 2, 3])  # Compute MSE per image\n    intensity_max = 1.0\n\n    # Compute PSNR safely, avoiding division by zero and extreme values\n    psnr_list = [10.0 * log10(intensity_max / max(mse_val.item(), 1e-6)) for mse_val in mse]\n\n    return psnr_list\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:45.236776Z","iopub.execute_input":"2025-04-13T15:59:45.237125Z","iopub.status.idle":"2025-04-13T15:59:45.251824Z","shell.execute_reply.started":"2025-04-13T15:59:45.237098Z","shell.execute_reply":"2025-04-13T15:59:45.250527Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from torchmetrics.image import StructuralSimilarityIndexMeasure\n\n# Define SSIM metric\nssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0, reduction='none')\n\ndef to_ssim(dehaze: torch.Tensor, gt: torch.Tensor):\n    \"\"\"\n    Compute SSIM directly on the GPU using torchmetrics.\n\n    Args:\n        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n\n    Returns:\n        List[float]: SSIM values for each image in the batch.\n    \"\"\"\n    ssim_values = ssim_metric(dehaze, gt)  # Shape: [B]\n    # print(\"1\",ssim_values)\n    # print(\"2\",[ssim_values])\n    ssim_values = ssim_values.tolist() \n    # print(type(ssim_values))\n    if isinstance(ssim_values, float):  # Correct way to check for a float\n        return [ssim_values]  # Convert single float to a list\n    return ssim_values  # Otherwise, return as is\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:45.254269Z","iopub.execute_input":"2025-04-13T15:59:45.254645Z","iopub.status.idle":"2025-04-13T15:59:49.015159Z","shell.execute_reply.started":"2025-04-13T15:59:45.254581Z","shell.execute_reply":"2025-04-13T15:59:49.013963Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Test with a dummy tensor\ndehaze = torch.rand(1, 3, 360, 360)  # Random batch of images\ngt = torch.rand(1, 3, 360, 360)  # Random ground truth images\n\nssim_scores = to_ssim(dehaze, gt)\nprint(ssim_scores)  # Should print a list of 6 SSIM values","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.015990Z","iopub.execute_input":"2025-04-13T15:59:49.016508Z","iopub.status.idle":"2025-04-13T15:59:49.183218Z","shell.execute_reply.started":"2025-04-13T15:59:49.016477Z","shell.execute_reply":"2025-04-13T15:59:49.182054Z"}},"outputs":[{"name":"stdout","text":"[0.004063480533659458]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def validationB(net, val_data_loader, device, category, save_tag=False):\n    \"\"\"\n    :param net: Your deep learning model\n    :param val_data_loader: validation loader\n    :param device: GPU/CPU device\n    :param category: dataset type (indoor/outdoor)\n    :param save_tag: whether to save images\n    :return: average PSNR & SSIM values\n    \"\"\"\n    psnr_list = []\n    ssim_list = []\n    \n    for batch_id, val_data in enumerate(val_data_loader):\n        with torch.no_grad():\n            haze, gt = val_data\n            haze, gt = haze.to(device), gt.to(device)\n            dehaze, _ = net(haze)\n\n        # --- Compute PSNR & SSIM --- #\n        batch_psnr = to_psnr(dehaze, gt)  # This returns a list\n        # print(batch_psnr)\n        batch_ssim = to_ssim(dehaze, gt)  # This returns a list\n        # print(batch_ssim)\n\n        psnr_list.extend(batch_psnr)  # Flatten the list\n        ssim_list.extend(batch_ssim)  # Flatten the list\n\n    # --- Ensure lists are not empty to avoid division by zero --- #\n    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n\n    return avr_psnr, avr_ssim\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.184291Z","iopub.execute_input":"2025-04-13T15:59:49.184744Z","iopub.status.idle":"2025-04-13T15:59:49.192853Z","shell.execute_reply.started":"2025-04-13T15:59:49.184705Z","shell.execute_reply":"2025-04-13T15:59:49.191476Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def validation_sr(net, sr_val_loader, device):\n    psnr_list = []\n    ssim_list = []\n    for lr, hr in sr_val_loader:\n        with torch.no_grad():\n            lr, hr = lr.to(device), hr.to(device)\n            sr_out, _ = net(lr)\n        psnr_list.extend(to_psnr(sr_out, hr))\n        ssim_list.extend(to_ssim(sr_out, hr))\n\n    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n    return avr_psnr, avr_ssim\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.194399Z","iopub.execute_input":"2025-04-13T15:59:49.194829Z","iopub.status.idle":"2025-04-13T15:59:49.217167Z","shell.execute_reply.started":"2025-04-13T15:59:49.194798Z","shell.execute_reply":"2025-04-13T15:59:49.216088Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### test","metadata":{"editable":false}},{"cell_type":"code","source":"# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.218157Z","iopub.execute_input":"2025-04-13T15:59:49.218534Z","iopub.status.idle":"2025-04-13T15:59:49.235275Z","shell.execute_reply.started":"2025-04-13T15:59:49.218505Z","shell.execute_reply":"2025-04-13T15:59:49.234130Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# psnr, ssim = validationB(net, val_data_loader, device, category)\n# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# # psnr, ssim = validationB(model, val_loader, device, \"indoor\", save_tag=True)\n# print(f\"Validation PSNR: {psnr:.2f}, SSIM: {ssim:.4f}\")\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.236480Z","iopub.execute_input":"2025-04-13T15:59:49.236990Z","iopub.status.idle":"2025-04-13T15:59:49.255468Z","shell.execute_reply.started":"2025-04-13T15:59:49.236955Z","shell.execute_reply":"2025-04-13T15:59:49.254361Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"execution_env_widget = widgets.Dropdown(options=['local', 'kaggle'], value='local', description='Execution Env:')\ndisplay(execution_env_widget)\n\nif os.path.exists('/kaggle'):\n    execution_env_widget.value = 'kaggle' ","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.256617Z","iopub.execute_input":"2025-04-13T15:59:49.257005Z","iopub.status.idle":"2025-04-13T15:59:49.285256Z","shell.execute_reply.started":"2025-04-13T15:59:49.256964Z","shell.execute_reply":"2025-04-13T15:59:49.284127Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Dropdown(description='Execution Env:', options=('local', 'kaggle'), value='local')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"091e942fc761467a9ceaeefa1c7bcc63"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"\n# --- Create widgets for each hyper-parameter ---\nlearning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\ncrop_size_widget = widgets.Text(value='128,128', description='Crop Size:')\ntrain_batch_size_widget = widgets.IntText(value=6, description='Train Batch Size:')\nversion_widget = widgets.IntText(value=0, description='Version:')\ngrowth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\nlambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\nval_batch_size_widget = widgets.IntText(value=2, description='Val Batch Size:')\ncategory_widget = widgets.Dropdown(options=['indoor', 'outdoor', 'reside', 'nh'], value='reside', description='Category:')\n\n# --- Display the widgets ---\ndisplay(\n    learning_rate_widget, crop_size_widget, train_batch_size_widget, version_widget,\n    growth_rate_widget, lambda_loss_widget, \n    val_batch_size_widget, category_widget\n)\n\n# --- Function to parse crop size ---\ndef parse_crop_size(crop_size_str):\n    return [int(x) for x in crop_size_str.split(',')]\n\n# --- Assign the widget values to variables ---\nlearning_rate = learning_rate_widget.value\ncrop_size = parse_crop_size(crop_size_widget.value)\ntrain_batch_size = train_batch_size_widget.value\nversion = version_widget.value\ngrowth_rate = growth_rate_widget.value\nlambda_loss = lambda_loss_widget.value\nval_batch_size = val_batch_size_widget.value\ncategory = category_widget.value\n\nexecution_env = execution_env_widget.value  # Local or Kaggle\n\n\nprint('\\nHyper-parameters set:')\nprint(f'learning_rate: {learning_rate}')\nprint(f'crop_size: {crop_size}')\nprint(f'train_batch_size: {train_batch_size}')\nprint(f'version: {version}')\nprint(f'growth_rate: {growth_rate}')\nprint(f'lambda_loss: {lambda_loss}')\nprint(f'val_batch_size: {val_batch_size}')\nprint(f'category: {category}')\nprint(f'execution_env: {execution_env}')\n\n# --- Set category-specific hyper-parameters ---\nif category == 'indoor':\n    num_epochs = 1500\n    train_data_dir = './data/train/indoor/'\n    val_data_dir = './data/test/SOTS/indoor/'\nelif category == 'outdoor':\n    num_epochs = 10\n    train_data_dir = './data/train/outdoor/'\n    val_data_dir = './data/test/SOTS/outdoor/'\nelif category == 'reside':\n    num_epochs = 50\n    train_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n    val_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n    test_data_dir = '/kaggle/input/reside6k/RESIDE-6K/test'\nelif category == 'nh':\n    num_epochs = 50\n    train_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n    val_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\nelse:\n    raise Exception('Wrong image category. Set it to indoor or outdoor for RESIDE dataset.')\n\n# --- Adjust paths based on execution environment ---\n# if execution_env == 'kaggle':\n    # train_data_dir = '/kaggle/input/reside-dataset/' + train_data_dir.strip('./')\n    # val_data_dir = '/kaggle/input/reside-dataset/' + val_data_dir.strip('./')\n    # train_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T'\n    # val_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V' \n    # train_data_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n    # val_data_dir = '/kaggle/input/o-haze/O-HAZY/GT' \nprint('\\nFinal dataset paths:')\nprint(f'Training directory: {train_data_dir}')\nprint(f'Validation directory: {val_data_dir}')\nprint(f'Number of epochs: {num_epochs}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.286220Z","iopub.execute_input":"2025-04-13T15:59:49.286528Z","iopub.status.idle":"2025-04-13T15:59:49.345003Z","shell.execute_reply.started":"2025-04-13T15:59:49.286502Z","shell.execute_reply":"2025-04-13T15:59:49.342149Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"FloatText(value=0.0001, description='Learning Rate:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71234f2e2f0848d2bf78cd7974601fdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Text(value='128,128', description='Crop Size:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"281bf772a02541a2a7b934c2e58878d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IntText(value=6, description='Train Batch Size:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6f3c96539b947b785eb12ab95e841f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IntText(value=0, description='Version:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3912abd8e5a44b49bde5694a7a73689"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IntText(value=16, description='Growth Rate:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaea9a07a2a240958535987b107743f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"FloatText(value=0.04, description='Lambda Loss:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69deacbc96064140b7d2328eb218a5d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"IntText(value=2, description='Val Batch Size:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3110e834d89f4f2cb7d50e5ed6da2f37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dropdown(description='Category:', index=2, options=('indoor', 'outdoor', 'reside', 'nh'), value='reside')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"add7b102b44049b48e27f32a6e626805"}},"metadata":{}},{"name":"stdout","text":"\nHyper-parameters set:\nlearning_rate: 0.0001\ncrop_size: [128, 128]\ntrain_batch_size: 6\nversion: 0\ngrowth_rate: 16\nlambda_loss: 0.04\nval_batch_size: 2\ncategory: reside\nexecution_env: kaggle\n\nFinal dataset paths:\nTraining directory: /kaggle/input/reside6k/RESIDE-6K/train\nValidation directory: /kaggle/input/reside6k/RESIDE-6K/train\nNumber of epochs: 85\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# hazeeffected_images_dir_train = f\"{train_data_dir}/IN\"\nhazeeffected_images_dir_train = f\"{train_data_dir}/hazy\"\nhazefree_images_dir_train = f\"{train_data_dir}/GT\"\n\n# hazeeffected_images_dir_valid = f\"{val_data_dir}/IN\"\nhazeeffected_images_dir_valid = f\"{val_data_dir}/hazy\"\nhazefree_images_dir_valid = f\"{val_data_dir}/GT\"","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.346168Z","iopub.execute_input":"2025-04-13T15:59:49.346550Z","iopub.status.idle":"2025-04-13T15:59:49.352464Z","shell.execute_reply.started":"2025-04-13T15:59:49.346510Z","shell.execute_reply":"2025-04-13T15:59:49.351097Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.353893Z","iopub.execute_input":"2025-04-13T15:59:49.354234Z","iopub.status.idle":"2025-04-13T15:59:49.371774Z","shell.execute_reply.started":"2025-04-13T15:59:49.354197Z","shell.execute_reply":"2025-04-13T15:59:49.370575Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# import os\n# import glob\n# import shutil\n\n# hazeeffected_images_dir_train = f\"{train_data_dir}/IN\"\n# hazefree_images_dir_train = f\"{train_data_dir}/GT\"\n\n# hazeeffected_images_dir_valid = f\"{val_data_dir}/IN\"\n# hazefree_images_dir_valid = f\"{val_data_dir}/GT\"\n\n# # Create validation directories if they don't exist\n# os.makedirs(hazeeffected_images_dir_valid, exist_ok=True)\n# os.makedirs(hazefree_images_dir_valid, exist_ok=True)\n\n# # List all hazy and clean images\n# hazy_images = sorted(glob.glob(f\"{hazeeffected_images_dir_train}/*\"))\n# clean_images = sorted(glob.glob(f\"{hazefree_images_dir_train}/*\"))\n\n# # Ensure matching hazy-clean pairs\n# assert len(hazy_images) == len(clean_images), \"Mismatch in hazy and clean images count!\"\n\n# # Shuffle while keeping the hazy-clean correspondence\n# paired_images = list(zip(hazy_images, clean_images))\n# # random.shuffle(paired_images)\n\n# # Define split ratio (e.g., 80% train, 20% validation)\n# split_ratio = 0.8\n# split_idx = int(len(paired_images) * split_ratio)\n\n# # Split into train and validation\n# train_pairs = paired_images[:split_idx]\n# valid_pairs = paired_images[split_idx:]\n\n# # Move validation images\n# for hazy_path, clean_path in valid_pairs:\n#     shutil.move(hazy_path, hazeeffected_images_dir_valid)\n#     shutil.move(clean_path, hazefree_images_dir_valid)\n\n# print(f\"Moved {len(valid_pairs)} image pairs to validation set.\")\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.373028Z","iopub.execute_input":"2025-04-13T15:59:49.373439Z","iopub.status.idle":"2025-04-13T15:59:49.389842Z","shell.execute_reply.started":"2025-04-13T15:59:49.373394Z","shell.execute_reply":"2025-04-13T15:59:49.388675Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# # hazeeffected_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n# # hazefree_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n# # hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n# # hazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/GT'\n\n# hazeeffected_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN'\n# hazefree_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT'\n# hazeeffected_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN'\n# hazefree_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/GT'","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.391264Z","iopub.execute_input":"2025-04-13T15:59:49.391724Z","iopub.status.idle":"2025-04-13T15:59:49.409336Z","shell.execute_reply.started":"2025-04-13T15:59:49.391690Z","shell.execute_reply":"2025-04-13T15:59:49.407953Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def print_log(epoch, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, category):\n    log_dir = \"./training_log\"\n    os.makedirs(log_dir, exist_ok=True)  # Ensure the directory exists\n\n    log_path = os.path.join(log_dir, f\"{category}_log.txt\")\n\n    print('({0:.0f}s) Epoch [{1}/{2}], Train_PSNR:{3:.2f}, Val_PSNR:{4:.2f}, Val_SSIM:{5:.4f}'\n          .format(one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim))\n\n    # --- Write the training log --- #\n    with open(log_path, 'a') as f:\n        print('Date: {0}, Time_Cost: {1:.0f}s, Epoch: [{2}/{3}], Train_PSNR: {4:.2f}, Val_PSNR: {5:.2f}, Val_SSIM: {6:.4f}'\n              .format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n                      one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim), file=f)","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.410628Z","iopub.execute_input":"2025-04-13T15:59:49.411071Z","iopub.status.idle":"2025-04-13T15:59:49.435571Z","shell.execute_reply.started":"2025-04-13T15:59:49.411036Z","shell.execute_reply":"2025-04-13T15:59:49.434239Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def adjust_learning_rate(optimizer, epoch, category, lr_decay=0.90):\n    \"\"\"\n    Adjusts the learning rate based on the epoch and dataset category.\n\n    :param optimizer: The optimizer (e.g., Adam, SGD).\n    :param epoch: Current epoch number.\n    :param category: Dataset category ('indoor', 'outdoor', or 'NH').\n    :param lr_decay: Multiplicative factor for learning rate decay.\n    \"\"\"\n    # Define learning rate decay steps based on category\n    step_dict = {'indoor': 18, 'outdoor': 3, 'NH': 20}\n    step = step_dict.get(category, 3)  # Default step size if category is unknown\n\n    # Decay learning rate at the specified step\n    if epoch > 0 and epoch % step == 0:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] *= lr_decay\n            print(f\"Epoch {epoch}: Learning rate adjusted to {param_group['lr']:.6f}\")\n","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.437008Z","iopub.execute_input":"2025-04-13T15:59:49.437405Z","iopub.status.idle":"2025-04-13T15:59:49.455199Z","shell.execute_reply.started":"2025-04-13T15:59:49.437370Z","shell.execute_reply":"2025-04-13T15:59:49.454121Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"## Perceptual Loss","metadata":{"editable":false}},{"cell_type":"code","source":"# --- Perceptual Feature Loss Network --- #\nclass PerceptualLossNet(nn.Module):\n    def __init__(self, vgg_model):\n        super().__init__()\n        self.feature_extractor = vgg_model\n        self.feature_layers = {'3': \"low_level\", '8': \"mid_level\", '15': \"high_level\"}\n\n    def get_feature_maps(self, x):\n        feature_maps = []\n        for layer_id, layer in self.feature_extractor.named_children():\n            x = layer(x)\n            if layer_id in self.feature_layers:\n                feature_maps.append(x)\n        return feature_maps\n\n    def forward(self, predicted, target):\n        pred_features = self.get_feature_maps(predicted)\n        target_features = self.get_feature_maps(target)\n        \n        # Compute perceptual loss as mean squared error across feature maps\n        loss = torch.stack([F.mse_loss(p, t) for p, t in zip(pred_features, target_features)]).mean()\n        \n        return loss","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.456414Z","iopub.execute_input":"2025-04-13T15:59:49.456846Z","iopub.status.idle":"2025-04-13T15:59:49.478411Z","shell.execute_reply.started":"2025-04-13T15:59:49.456804Z","shell.execute_reply":"2025-04-13T15:59:49.477210Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# --- Imports --- #\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import vgg16\n\n# --- Device Setup --- #\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice_ids = list(range(torch.cuda.device_count()))\n\n# --- Initialize Model --- #\nnet = DeepGuidedNetwork().to(device)\n\n# --- Enable Multi-GPU (if available) --- #\nif len(device_ids) > 1:\n    net = nn.DataParallel(net, device_ids=device_ids)\n\n# --- Optimizer --- #\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n\n# --- Load Pretrained VGG16 for Perceptual Loss --- #\nvgg_features = vgg16(pretrained=True).features[:16].to(device)\nfor param in vgg_features.parameters():\n    param.requires_grad = False\n\nloss_network = PerceptualLossNet(vgg_features)\nloss_network.eval()\n\n# --- Load Model Weights (if available) --- #\nmodel_name = 'formernew'\n# checkpoint_path = f\"{model_name}_{category}_haze_best_{version}\"\ncheckpoint_path = \"/kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\" \n\ntry:\n    net.load_state_dict(torch.load(checkpoint_path, weights_only=False, map_location=torch.device('cpu')))\n    print(f\"✅ Model weights loaded from {checkpoint_path}\")\nexcept FileNotFoundError:\n    print(f\"⚠️ No pretrained weights found at {checkpoint_path}\")\n\n# --- Compute Total Trainable Parameters --- #\ntotal_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\nprint(f\"📊 Total Trainable Parameters: {total_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:59:49.479500Z","iopub.execute_input":"2025-04-13T15:59:49.479895Z","iopub.status.idle":"2025-04-13T15:59:55.913355Z","shell.execute_reply.started":"2025-04-13T15:59:49.479854Z","shell.execute_reply":"2025-04-13T15:59:55.912096Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:03<00:00, 158MB/s] \n","output_type":"stream"},{"name":"stdout","text":"✅ Model weights loaded from /kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\n📊 Total Trainable Parameters: 4,645,694\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FeatureAffinityModule(nn.Module):\n    def __init__(self, channels):\n        super(FeatureAffinityModule, self).__init__()\n        self.channels = channels\n\n    def forward(self, student_features, teacher_features):\n        # Normalize features\n        student_norm = F.normalize(student_features.view(student_features.size(0), self.channels, -1), dim=2)\n        teacher_norm = F.normalize(teacher_features.view(teacher_features.size(0), self.channels, -1), dim=2)\n\n        # Compute affinity matrices\n        student_affinity = torch.bmm(student_norm, student_norm.transpose(1, 2))\n        teacher_affinity = torch.bmm(teacher_norm, teacher_norm.transpose(1, 2))\n\n        # Compute KL divergence\n        loss = F.kl_div(F.log_softmax(student_affinity, dim=-1),\n                        F.softmax(teacher_affinity, dim=-1),\n                        reduction='batchmean')\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T15:59:55.914455Z","iopub.execute_input":"2025-04-13T15:59:55.914802Z","iopub.status.idle":"2025-04-13T15:59:55.922182Z","shell.execute_reply.started":"2025-04-13T15:59:55.914772Z","shell.execute_reply":"2025-04-13T15:59:55.920737Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Create train and validation datasets\ntrain_dataset = HazeDataset(crop_size=crop_size, \n                            hazeeffected_images_dir=hazeeffected_images_dir_train,\n                            hazefree_images_dir=hazefree_images_dir_train,\n                            split=\"train\")\n\nval_dataset = HazeDataset(crop_size=crop_size, \n                          hazeeffected_images_dir=hazeeffected_images_dir_train,\n                          hazefree_images_dir=hazefree_images_dir_train,\n                          split=\"valid\")\n\nprint(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T15:59:55.923349Z","iopub.execute_input":"2025-04-13T15:59:55.923752Z","iopub.status.idle":"2025-04-13T16:00:08.765056Z","shell.execute_reply.started":"2025-04-13T15:59:55.923720Z","shell.execute_reply":"2025-04-13T16:00:08.763334Z"}},"outputs":[{"name":"stdout","text":"Train samples: 4800, Validation samples: 1200\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\nval_data_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T16:00:08.766232Z","iopub.execute_input":"2025-04-13T16:00:08.766695Z","iopub.status.idle":"2025-04-13T16:00:08.773326Z","shell.execute_reply.started":"2025-04-13T16:00:08.766650Z","shell.execute_reply":"2025-04-13T16:00:08.772069Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import ToTensor\nfrom PIL import Image, UnidentifiedImageError\n\nclass SRDataset(Dataset):\n    def __init__(self, lr_dir, hr_dir, scale='x2', split='train', split_ratio=0.9):\n        \"\"\"\n        Super-Resolution dataset that matches LR and HR image pairs based on naming pattern,\n        with support for train/val split.\n\n        Args:\n            lr_dir (str): Directory containing low-resolution images (e.g., x2, x3, x4).\n            hr_dir (str): Directory containing high-resolution images.\n            scale (str): Scale suffix (e.g., 'x2', 'x3', 'x4').\n            split (str): Either 'train' or 'val'.\n            split_ratio (float): Ratio of training data (e.g., 0.9 means 90% train, 10% val).\n        \"\"\"\n        super().__init__()\n        self.lr_dir = lr_dir\n        self.hr_dir = hr_dir\n        self.scale = scale\n        self.split = split.lower()\n\n        valid_ext = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n        lr_images = sorted([\n            f for f in glob.glob(os.path.join(lr_dir, \"*.*\"))\n            if f.lower().endswith(valid_ext)\n        ])\n\n        lr_hr_pairs = []\n        for lr_path in lr_images:\n            lr_name = os.path.basename(lr_path)\n            hr_name = lr_name.replace(scale, '')\n            hr_path = os.path.join(hr_dir, hr_name)\n\n            if not os.path.exists(hr_path):\n                print(f\"Warning: Ground-truth missing for {lr_name}, skipping.\")\n                continue\n\n            lr_hr_pairs.append((lr_path, hr_path))\n\n        if not lr_hr_pairs:\n            raise ValueError(\"No matching LR-HR image pairs found.\")\n\n        # Split dataset\n        split_idx = int(len(lr_hr_pairs) * split_ratio)\n        if self.split == 'train':\n            self.lr_hr_pairs = lr_hr_pairs[:split_idx]\n        elif self.split == 'val':\n            self.lr_hr_pairs = lr_hr_pairs[split_idx:]\n        else:\n            raise ValueError(\"split must be either 'train' or 'val'\")\n\n    def __len__(self):\n        return len(self.lr_hr_pairs)\n\n    def __getitem__(self, idx):\n        lr_path, hr_path = self.lr_hr_pairs[idx]\n\n        try:\n            lr_img = Image.open(lr_path).convert('RGB')\n            hr_img = Image.open(hr_path).convert('RGB')\n        except UnidentifiedImageError:\n            raise ValueError(f\"Unidentified image at {lr_path} or {hr_path}\")\n\n        return ToTensor()(lr_img), ToTensor()(hr_img)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:00:08.774570Z","iopub.execute_input":"2025-04-13T16:00:08.774982Z","iopub.status.idle":"2025-04-13T16:00:10.743942Z","shell.execute_reply.started":"2025-04-13T16:00:08.774938Z","shell.execute_reply":"2025-04-13T16:00:10.742093Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"from torchvision.transforms import functional as TF\n\ndef custom_collate_fn(batch):\n    min_height = min([x[0].shape[1] for x in batch])//4\n    min_width = min([x[0].shape[2] for x in batch])//4\n    resized_batch = [(TF.resize(x[0], [min_height, min_width]), TF.resize(x[1], [min_height, min_width])) for x in batch]\n    return torch.utils.data.dataloader.default_collate(resized_batch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:00:10.745136Z","iopub.execute_input":"2025-04-13T16:00:10.745489Z","iopub.status.idle":"2025-04-13T16:00:10.771236Z","shell.execute_reply.started":"2025-04-13T16:00:10.745458Z","shell.execute_reply":"2025-04-13T16:00:10.769687Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# # Paths\n# sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n# sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X2'\n\n# # Train SR DataLoader\n# sr_train_dataset = SRDataset(lr_dir, hr_dir, scale='x2', split='train')\n# sr_valid_dataset = SRDataset(lr_dir, hr_dir, scale='x2', split='val')\n\n# sr_train_loader = DataLoader(sr_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=custom_collate_fn)\n\n# # If you have a separate validation split:\n# sr_val_loader = DataLoader(sr_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:00:10.772952Z","iopub.execute_input":"2025-04-13T16:00:10.773336Z","iopub.status.idle":"2025-04-13T16:00:10.789632Z","shell.execute_reply.started":"2025-04-13T16:00:10.773305Z","shell.execute_reply":"2025-04-13T16:00:10.788465Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# train_data_loader = DataLoader(TrainData(crop_size, hazeeffected_images_dir_train, hazefree_images_dir_train), batch_size=train_batch_size, shuffle=True)\n# val_data_loader = DataLoader(TrainData(crop_size, hazeeffected_images_dir_valid, hazefree_images_dir_valid), batch_size=val_batch_size, shuffle=False)","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T16:00:10.790767Z","iopub.execute_input":"2025-04-13T16:00:10.791182Z","iopub.status.idle":"2025-04-13T16:00:10.809326Z","shell.execute_reply.started":"2025-04-13T16:00:10.791143Z","shell.execute_reply":"2025-04-13T16:00:10.808162Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# train_size = len(TrainData(crop_size, hazeeffected_images_dir_train, hazefree_images_dir_train))\n# val_size = len(TrainData(crop_size, hazeeffected_images_dir_valid, hazefree_images_dir_valid))\n\n# print(f\"Train Size: {train_size}, Val Size: {val_size}\")","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T16:00:10.810467Z","iopub.execute_input":"2025-04-13T16:00:10.810900Z","iopub.status.idle":"2025-04-13T16:00:10.828851Z","shell.execute_reply.started":"2025-04-13T16:00:10.810862Z","shell.execute_reply":"2025-04-13T16:00:10.827753Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# --- SR Dataset Setup --- #\nsr_enabled = True\nif sr_enabled:\n    sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n    sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X2'\n    sr_train_dataset = SRDataset(lr_dir=sr_lr_dir, hr_dir=sr_hr_dir, scale='x2', split='train')\n    sr_val_dataset = SRDataset(lr_dir=sr_lr_dir, hr_dir=sr_hr_dir, scale='x2', split='val')\n    sr_train_loader = DataLoader(sr_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=custom_collate_fn)\n    sr_val_loader = DataLoader(sr_val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=custom_collate_fn)\n    sr_iter = iter(sr_train_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:00:10.830139Z","iopub.execute_input":"2025-04-13T16:00:10.830471Z","iopub.status.idle":"2025-04-13T16:00:22.922394Z","shell.execute_reply.started":"2025-04-13T16:00:10.830443Z","shell.execute_reply":"2025-04-13T16:00:22.919878Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"for i,o in train_data_loader:\n    print(i.shape, o.shape)\n    break","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T16:00:22.924882Z","iopub.execute_input":"2025-04-13T16:00:22.925631Z","iopub.status.idle":"2025-04-13T16:00:23.193377Z","shell.execute_reply.started":"2025-04-13T16:00:22.925539Z","shell.execute_reply":"2025-04-13T16:00:23.189849Z"}},"outputs":[{"name":"stdout","text":"torch.Size([6, 3, 128, 128]) torch.Size([6, 3, 128, 128])\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"for i,o in sr_val_loader:\n    print(i.shape, o.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:00:23.197364Z","iopub.execute_input":"2025-04-13T16:00:23.197795Z","iopub.status.idle":"2025-04-13T16:00:26.974426Z","shell.execute_reply.started":"2025-04-13T16:00:23.197760Z","shell.execute_reply":"2025-04-13T16:00:26.972768Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 3, 175, 255]) torch.Size([2, 3, 175, 255])\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SSFM(nn.Module):\n    def __init__(self, loss_type='l1'):\n        super(SSFM, self).__init__()\n        assert loss_type in ['l1', 'l2'], \"loss_type must be 'l1' or 'l2'\"\n        self.loss_type = loss_type\n\n    def forward(self, student_feats, teacher_feats):\n        \"\"\"\n        student_feats: List of feature maps from student RDBs [rdb1, rdb2, rdb3, rdb4]\n        teacher_feats: List of corresponding feature maps from teacher\n        \"\"\"\n        assert len(student_feats) == len(teacher_feats), \"Feature lists must match\"\n\n        total_loss = 0.0\n        for s_feat, t_feat in zip(student_feats, teacher_feats):\n            # Match resolution\n            if s_feat.shape != t_feat.shape:\n                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n            \n            if self.loss_type == 'l1':\n                loss = F.l1_loss(s_feat, t_feat)\n            else:\n                loss = F.mse_loss(s_feat, t_feat)\n            \n            total_loss += loss\n\n        return total_loss / len(student_feats)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:00:26.975991Z","iopub.execute_input":"2025-04-13T16:00:26.976476Z","iopub.status.idle":"2025-04-13T16:00:26.985093Z","shell.execute_reply.started":"2025-04-13T16:00:26.976404Z","shell.execute_reply":"2025-04-13T16:00:26.983664Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# --- Teacher Network --- #\nteacher_net = DeepGuidedNetwork(radius=1).to(device)\n# teacher_net.load_state_dict(torch.load('teacher_model.pth'))\n# teacher_net.eval()\n\n# --- Feature Affinity Module --- #\nfam = FeatureAffinityModule(channels=64).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:00:26.986434Z","iopub.execute_input":"2025-04-13T16:00:26.986909Z","iopub.status.idle":"2025-04-13T16:00:27.216972Z","shell.execute_reply.started":"2025-04-13T16:00:26.986864Z","shell.execute_reply":"2025-04-13T16:00:27.215507Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:08:03.487077Z","iopub.status.idle":"2025-04-13T16:08:03.487458Z","shell.execute_reply":"2025-04-13T16:08:03.487301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Initial Validation --- #\nold_val_psnr, old_val_ssim = validation_sr(teacher_net, sr_val_loader, device)\nprint(f\"[Teacher SR Init Val] PSNR: {old_val_psnr:.2f}, SSIM: {old_val_ssim:.4f}\")\n\n# --- Training Loop for Teacher Model --- #\nbest_psnr = old_val_psnr\ntrain_psnr_prev = 0\ndistillation_weight = 1  # Not needed for teacher, but included for consistency\n\nfor epoch in range(num_epochs):\n    psnr_list = []\n    start_time = time.time()\n\n    adjust_learning_rate(optimizer, epoch, category=category)\n    teacher_net.train()\n\n    for batch_id, (sr_lr, sr_hr) in enumerate(sr_train_loader):\n        sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n        optimizer.zero_grad()\n\n        # Forward Pass - Teacher (SR Model)\n        sr_out, _ = teacher_net(sr_lr)\n\n        # SR Loss (L1 loss)\n        sr_loss = F.l1_loss(sr_out, sr_hr)\n        \n        # Backpropagation\n        sr_loss.backward()\n        optimizer.step()\n        psnr_list.extend(to_psnr(sr_out, sr_hr))\n\n        if batch_id % 50 == 0:  # adjust this based on how frequently you want to print\n            print(f\"Epoch [{epoch}/{num_epochs}], Iteration [{batch_id}], Loss: {sr_loss.item():.4f}\")\n\n    # Save model checkpoint\n    if epoch % 5 == 0:\n        iter_model_path = f\"{model_name}{category}_teacher_sr_iter_{epoch}.pth\"\n        torch.save(teacher_net.state_dict(), iter_model_path)\n        print(f\"Teacher model saved in epoch {epoch}.\")\n\n    train_psnr = sum(psnr_list) / len(psnr_list)\n    model_path = f\"{model_name}{category}_teacher_sr_{version}.pth\"\n\n    # --- Validation --- #\n    teacher_net.eval()\n    val_psnr, val_ssim = validation_sr(teacher_net, sr_val_loader, device)\n    epoch_duration = time.time() - start_time\n    print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n\n    if train_psnr < train_psnr_prev:\n        adjust_learning_rate(optimizer, num_epochs, category=category)\n\n    if val_psnr >= best_psnr:\n        best_model_path = f\"{model_name}{category}_teacher_sr_best_{version}.pth\"\n        torch.save(teacher_net.state_dict(), best_model_path)\n        best_psnr = val_psnr\n\n    train_psnr_prev = train_psnr\n\n# Final save for teacher model\nfinal_path = f\"{model_name}{category}_teacher_sr_final_{epoch}.pth\"\ntorch.save(teacher_net.state_dict(), final_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:08:09.484565Z","iopub.execute_input":"2025-04-13T16:08:09.486645Z","iopub.status.idle":"2025-04-13T16:11:26.295645Z","shell.execute_reply.started":"2025-04-13T16:08:09.486513Z","shell.execute_reply":"2025-04-13T16:11:26.292920Z"}},"outputs":[{"name":"stdout","text":"[Teacher SR Init Val] PSNR: 15.25, SSIM: 0.4047\nEpoch [0/85], Iteration [0], Loss: 0.1303\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-17bc443d2ee4>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Forward Pass - Teacher (SR Model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# SR Loss (L1 loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-3cd0401b8698>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_hr)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Base image estimation using DehazeFormer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0my_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdehaze_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Combining base and details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-9ae08e574e27>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_image_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mtransmission_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matmospheric_light\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-9ae08e574e27>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_stage3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-ec5d2a034454>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \"\"\"\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-ec5d2a034454>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrebias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-6c1b014a642c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mrescale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrebias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mrescale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrebias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_tensor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":56},{"cell_type":"code","source":"# # --- Initial Validation --- #\n# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)\n# print(f\"[Dehazing Init Val] PSNR: {old_val_psnr:.2f}, SSIM: {old_val_ssim:.4f}\")\n# if sr_enabled:\n#     sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n#     print(f\"[SR Init Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n\n# # --- Training Loop --- #\n# best_psnr = old_val_psnr\n# train_psnr_prev = 0\n# distillation_weight = 1\n\n# for epoch in range(num_epochs):\n#     psnr_list = []\n#     start_time = time.time()\n\n#     adjust_learning_rate(optimizer, epoch, category=category)\n#     net.train()\n\n#     for batch_id, (haze, gt) in enumerate(train_data_loader):\n#         haze, gt = haze.to(device), gt.to(device)\n#         optimizer.zero_grad()\n\n#         # Forward Pass - Student\n#         dehaze, base = net(haze)\n\n#         # Teacher Output\n#         with torch.no_grad():\n#             teacher_dehaze, _ = teacher_net(haze)\n\n#         # Losses\n#         base_loss = F.smooth_l1_loss(base, gt)\n#         smooth_loss = F.smooth_l1_loss(dehaze, gt)\n#         perceptual_loss = loss_network(dehaze, gt)\n#         distillation_loss = fam(dehaze, teacher_dehaze)\n#         # print(\"distillation_loss: \", distillation_loss)\n#         total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss + distillation_weight * distillation_loss\n\n#         # --- SR Training --- #\n#         if sr_enabled:\n#             try:\n#                 sr_lr, sr_hr = next(sr_iter)\n#             except StopIteration:\n#                 sr_iter = iter(sr_train_loader)\n#                 sr_lr, sr_hr = next(sr_iter)\n#             sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n#             sr_out, _ = net(sr_lr)\n#             sr_loss = F.l1_loss(sr_out, sr_hr)\n#             total_loss += sr_loss\n\n#         total_loss.backward()\n#         optimizer.step()\n#         psnr_list.extend(to_psnr(dehaze, gt))\n\n#         if batch_id % num_epochs == 0:\n#             print(f\"Epoch [{epoch}/{num_epochs}], Iteration [{batch_id}]\")\n\n#     # Save model checkpoint\n#     if epoch % 5 == 0:\n#         iter_model_path = f\"{model_name}{category}_haze_iter_{epoch}.pth\"\n#         torch.save(net.state_dict(), iter_model_path)\n#         print(f\"Model saved in epoch {epoch}.\")\n\n#     train_psnr = sum(psnr_list) / len(psnr_list)\n#     model_path = f\"{model_name}{category}_haze_{version}.pth\"\n\n#     # --- Validation --- #\n#     net.eval()\n#     val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n#     if sr_enabled:\n#         sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n#         print(f\"[SR Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n\n#     epoch_duration = time.time() - start_time\n#     print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n\n#     if train_psnr < train_psnr_prev:\n#         adjust_learning_rate(optimizer, num_epochs, category=category)\n\n#     if val_psnr >= best_psnr:\n#         best_model_path = f\"{model_name}{category}_haze_best_{version}.pth\"\n#         torch.save(net.state_dict(), best_model_path)\n#         best_psnr = val_psnr\n\n#     train_psnr_prev = train_psnr\n\n# # Final save\n# final_path = f\"{model_name}{category}_final_{epoch}.pth\"\n# torch.save(net.state_dict(), final_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:11:26.296954Z","iopub.status.idle":"2025-04-13T16:11:26.297372Z","shell.execute_reply":"2025-04-13T16:11:26.297206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nimport numpy as np\n\ndef visualize_validation_results(teacher_net, val_data_loader, device, num_images=5):\n    \"\"\"\n    Visualize a few validation images along with the high-res ground truth and the model output.\n    \n    :param teacher_net: The teacher model.\n    :param val_data_loader: Validation data loader.\n    :param device: Device (CPU or GPU).\n    :param num_images: Number of images to visualize.\n    \"\"\"\n    teacher_net.eval()  # Set model to evaluation mode\n\n    # Get a few validation samples\n    with torch.no_grad():\n        for idx, (lr, hr) in enumerate(val_data_loader):\n            if idx >= num_images:\n                break\n            \n            lr, hr = lr.to(device), hr.to(device)  # Move data to device\n\n            # Forward pass through the teacher network\n            sr_output, _ = teacher_net(lr)  # Assuming model returns (sr_output, _) tuple\n\n            # Convert to numpy for visualization (detach from GPU if needed)\n            lr = lr.cpu().numpy().transpose(0, 2, 3, 1)[0]  # (C, H, W) -> (H, W, C)\n            hr = hr.cpu().numpy().transpose(0, 2, 3, 1)[0]  # (C, H, W) -> (H, W, C)\n            sr_output = sr_output.cpu().numpy().transpose(0, 2, 3, 1)[0]  # (C, H, W) -> (H, W, C)\n\n            # Plot the images\n            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n            axes[0].imshow(lr)\n            axes[0].set_title(f\"Low-Resolution Image {idx+1}\")\n            axes[0].axis(\"off\")\n            \n            axes[1].imshow(hr)\n            axes[1].set_title(f\"Ground Truth (HR) {idx+1}\")\n            axes[1].axis(\"off\")\n            \n            axes[2].imshow(sr_output)\n            axes[2].set_title(f\"Teacher Model Output {idx+1}\")\n            axes[2].axis(\"off\")\n\n            plt.show()\n\nvisualize_validation_results(teacher_net, sr_val_loader, device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize model\n# # model_path = \"/kaggle/input/rdb-and-transformer/pytorch/default/1/formernewnh_final_49.pth\"\n# # model_path = \"/kaggle/input/reside-dehaze/pytorch/default/3/formernewreside_haze_best_0.pth\"\n# model_path = \"/kaggle/input/reside-dehaze/pytorch/default/4/formernewreside_haze_iter_60.pth\"\n# # model = DehazingNet().to(device)\n# # model = SR_model(upscale_factor=1).to(device)\n# # net.load_state_dict(torch.load(model_path, map_location=device))\n# net.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:08:03.478653Z","iopub.status.idle":"2025-04-13T16:08:03.479026Z","shell.execute_reply":"2025-04-13T16:08:03.478886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# LOAD TEST DATA\n# -----------------------------\ntest_hazy_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/hazy\"\ntest_gt_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/GT\"\n# test_hazy_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN\"\n# test_gt_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT\"\n\nhazy_images = sorted(glob.glob(os.path.join(test_hazy_dir, \"*.*\")))\ngt_images = sorted(glob.glob(os.path.join(test_gt_dir, \"*.*\")))\n\ntransform = Compose([\n    ToTensor(),\n    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nto_pil = ToPILImage()","metadata":{"trusted":true,"editable":false,"execution":{"iopub.status.busy":"2025-04-13T16:08:03.480071Z","iopub.status.idle":"2025-04-13T16:08:03.480479Z","shell.execute_reply":"2025-04-13T16:08:03.480305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # -----------------------------\n# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n# # -----------------------------\n# image_indices = [11, 12, 13,14]  # Indices of images to visualize\n\n# plt.figure(figsize=(10, len(image_indices) * 5))\n\n# for idx, i in enumerate(image_indices):\n#     hazy_img = Image.open(hazy_images[i+1])\n#     gt_img = Image.open(gt_images[i+1])\n\n#     # Transform for model input\n#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n\n#     # Inference\n#     with torch.no_grad():\n#         res = net(input_tensor)\n#         print(res[0].shape)\n#         output_tensor = res[0].cpu().squeeze(0)\n\n#     # Convert back to image\n#     output_img = to_pil(output_tensor)\n\n#     # Display results\n#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n#     plt.imshow(hazy_img)\n#     plt.title(f\"Hazy Input \")\n#     plt.axis(\"off\")\n\n#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n#     plt.imshow(output_img)\n#     plt.title(f\"Dehazed Output \")\n#     plt.axis(\"off\")\n\n#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n#     plt.imshow(gt_img)\n#     plt.title(f\"Ground Truth \")\n#     plt.axis(\"off\")\n\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:08:03.481706Z","iopub.status.idle":"2025-04-13T16:08:03.482110Z","shell.execute_reply":"2025-04-13T16:08:03.481974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # -----------------------------\n# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n# # -----------------------------\n# image_indices = [70, 75, 90, 100]  # Indices of images to visualize\n\n# plt.figure(figsize=(10, len(image_indices) * 5))\n\n# for idx, i in enumerate(image_indices):\n#     hazy_img = Image.open(hazy_images[i+1])\n#     gt_img = Image.open(gt_images[i+1])\n\n#     # Transform for model input\n#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n\n#     # Inference\n#     with torch.no_grad():\n#         res = net(input_tensor)\n#         print(res[0].shape)\n#         output_tensor = res[0].cpu().squeeze(0)\n\n#     # Convert back to image\n#     output_img = to_pil(output_tensor)\n\n#     # Display results\n#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n#     plt.imshow(hazy_img)\n#     plt.title(f\"Hazy Input {i}\")\n#     plt.axis(\"off\")\n\n#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n#     plt.imshow(output_img)\n#     plt.title(f\"Dehazed Output {i}\")\n#     plt.axis(\"off\")\n\n#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n#     plt.imshow(gt_img)\n#     plt.title(f\"Ground Truth {i}\")\n#     plt.axis(\"off\")\n\n# plt.tight_layout()\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:08:03.483337Z","iopub.status.idle":"2025-04-13T16:08:03.484001Z","shell.execute_reply":"2025-04-13T16:08:03.483768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # -----------------------------\n# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n# # -----------------------------\n# image_indices = [1, 3, 5]  # Indices of images to visualize\n\n# plt.figure(figsize=(10, len(image_indices) * 5))\n\n# for idx, i in enumerate(image_indices):\n#     hazy_img = Image.open(hazy_images[i+1])\n#     gt_img = Image.open(gt_images[i+1])\n\n#     # Transform for model input\n#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n\n#     # Inference\n#     with torch.no_grad():\n#         res = net(input_tensor)\n#         print(res[0].shape)\n#         output_tensor = res[0].cpu().squeeze(0)\n\n#     # Convert back to image\n#     output_img = to_pil(output_tensor)\n\n#     # Display results\n#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n#     plt.imshow(hazy_img)\n#     plt.title(f\"Hazy Input {i}\")\n#     plt.axis(\"off\")\n\n#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n#     plt.imshow(output_img)\n#     plt.title(f\"Dehazed Output {i}\")\n#     plt.axis(\"off\")\n\n#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n#     plt.imshow(gt_img)\n#     plt.title(f\"Ground Truth {i}\")\n#     plt.axis(\"off\")\n\n# plt.tight_layout()\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:08:03.485186Z","iopub.status.idle":"2025-04-13T16:08:03.485642Z","shell.execute_reply":"2025-04-13T16:08:03.485433Z"}},"outputs":[],"execution_count":null}]}