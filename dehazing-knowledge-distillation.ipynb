{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10417877,"sourceType":"datasetVersion","datasetId":6456606},{"sourceId":10443410,"sourceType":"datasetVersion","datasetId":6464114},{"sourceId":273590,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":234257,"modelId":255960},{"sourceId":274811,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":234257,"modelId":255960},{"sourceId":274976,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":234257,"modelId":255960}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport glob\nimport os\nfrom torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage\nfrom PIL import Image\nfrom random import randrange\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport matplotlib.pyplot as plt\nimport math\nfrom math import log10\nimport numpy as np\nfrom torch.nn.init import _calculate_fan_in_and_fan_out\nfrom timm.layers import to_2tuple, trunc_normal_\nimport torchvision.utils as utils\nimport torch.utils.data as data\nfrom torchvision.models import vgg16\nfrom torch.utils.data import Dataset\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\nimport time\nfrom skimage import measure\nimport ipywidgets as widgets\nfrom IPython.display import display\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.414386Z","iopub.execute_input":"2025-03-08T11:10:26.414636Z","iopub.status.idle":"2025-03-08T11:10:26.422149Z","shell.execute_reply.started":"2025-03-08T11:10:26.414605Z","shell.execute_reply":"2025-03-08T11:10:26.421007Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.423463Z","iopub.execute_input":"2025-03-08T11:10:26.423761Z","iopub.status.idle":"2025-03-08T11:10:26.451470Z","shell.execute_reply.started":"2025-03-08T11:10:26.423737Z","shell.execute_reply":"2025-03-08T11:10:26.450328Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Basenet","metadata":{}},{"cell_type":"code","source":"class RLN(nn.Module):\n\tr\"\"\"Revised LayerNorm\"\"\"\n\tdef __init__(self, dim, eps=1e-5, detach_grad=False):\n\t\tsuper(RLN, self).__init__()\n\t\tself.eps = eps\n\t\tself.detach_grad = detach_grad\n\n\t\tself.weight = nn.Parameter(torch.ones((1, dim, 1, 1)))\n\t\tself.bias = nn.Parameter(torch.zeros((1, dim, 1, 1)))\n\n\t\tself.meta1 = nn.Conv2d(1, dim, 1)\n\t\tself.meta2 = nn.Conv2d(1, dim, 1)\n\n\t\ttrunc_normal_(self.meta1.weight, std=.02)\n\t\tnn.init.constant_(self.meta1.bias, 1)\n\n\t\ttrunc_normal_(self.meta2.weight, std=.02)\n\t\tnn.init.constant_(self.meta2.bias, 0)\n\n\tdef forward(self, input):\n\t\tmean = torch.mean(input, dim=(1, 2, 3), keepdim=True)\n\t\tstd = torch.sqrt((input - mean).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.eps)\n\n\t\tnormalized_input = (input - mean) / std\n\n\t\tif self.detach_grad:\n\t\t\trescale, rebias = self.meta1(std.detach()), self.meta2(mean.detach())\n\t\telse:\n\t\t\trescale, rebias = self.meta1(std), self.meta2(mean)\n\n\t\tout = normalized_input * self.weight + self.bias\n\t\treturn out, rescale, rebias\n\n\nclass Mlp(nn.Module):\n\tdef __init__(self, network_depth, in_features, hidden_features=None, out_features=None):\n\t\tsuper().__init__()\n\t\tout_features = out_features or in_features\n\t\thidden_features = hidden_features or in_features\n\n\t\tself.network_depth = network_depth\n\n\t\tself.mlp = nn.Sequential(\n\t\t\tnn.Conv2d(in_features, hidden_features, 1),\n\t\t\tnn.ReLU(True),\n\t\t\tnn.Conv2d(hidden_features, out_features, 1)\n\t\t)\n\n\t\tself.apply(self._init_weights)\n\n\tdef _init_weights(self, m):\n\t\tif isinstance(m, nn.Conv2d):\n\t\t\tgain = (8 * self.network_depth) ** (-1/4)\n\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n\t\t\ttrunc_normal_(m.weight, std=std)\n\t\t\tif m.bias !=   None:\n\t\t\t\tnn.init.constant_(m.bias, 0)\n\n\tdef forward(self, x):\n\t\treturn self.mlp(x)\n\n\ndef window_partition(x, window_size):\n\tB, H, W, C = x.shape\n\tx = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n\twindows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, C)\n\treturn windows\n\n\ndef window_reverse(windows, window_size, H, W):\n\tB = int(windows.shape[0] / (H * W / window_size / window_size))\n\tx = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n\tx = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n\treturn x\n\n\ndef get_relative_positions(window_size):\n\tcoords_h = torch.arange(window_size)\n\tcoords_w = torch.arange(window_size)\n\n\tcoords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n\tcoords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n\trelative_positions = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n\n\trelative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n\trelative_positions_log  = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n\n\treturn relative_positions_log\n\n\nclass WindowAttention(nn.Module):\n\tdef __init__(self, dim, window_size, num_heads):\n\n\t\tsuper().__init__()\n\t\tself.dim = dim\n\t\tself.window_size = window_size  # Wh, Ww\n\t\tself.num_heads = num_heads\n\t\thead_dim = dim // num_heads\n\t\tself.scale = head_dim ** -0.5\n\n\t\trelative_positions = get_relative_positions(self.window_size)\n\t\tself.register_buffer(\"relative_positions\", relative_positions)\n\t\tself.meta = nn.Sequential(\n\t\t\tnn.Linear(2, 256, bias=True),\n\t\t\tnn.ReLU(True),\n\t\t\tnn.Linear(256, num_heads, bias=True)\n\t\t)\n\n\t\tself.softmax = nn.Softmax(dim=-1)\n\n\tdef forward(self, qkv):\n\t\tB_, N, _ = qkv.shape\n\n\t\tqkv = qkv.reshape(B_, N, 3, self.num_heads, self.dim // self.num_heads).permute(2, 0, 3, 1, 4)\n\n\t\tq, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n\t\tq = q * self.scale\n\t\tattn = (q @ k.transpose(-2, -1))\n\n\t\trelative_position_bias = self.meta(self.relative_positions)\n\t\trelative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\t\tattn = attn + relative_position_bias.unsqueeze(0)\n\n\t\tattn = self.softmax(attn)\n\n\t\tx = (attn @ v).transpose(1, 2).reshape(B_, N, self.dim)\n\t\treturn x\n\n\nclass Attention(nn.Module):\n\tdef __init__(self, network_depth, dim, num_heads, window_size, shift_size, use_attn=False, conv_type=None):\n\t\tsuper().__init__()\n\t\tself.dim = dim\n\t\tself.head_dim = int(dim // num_heads)\n\t\tself.num_heads = num_heads\n\n\t\tself.window_size = window_size\n\t\tself.shift_size = shift_size\n\n\t\tself.network_depth = network_depth\n\t\tself.use_attn = use_attn\n\t\tself.conv_type = conv_type\n\n\t\tif self.conv_type == 'Conv':\n\t\t\tself.conv = nn.Sequential(\n\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1, padding_mode='reflect'),\n\t\t\t\tnn.ReLU(True),\n\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1, padding_mode='reflect')\n\t\t\t)\n\n\t\tif self.conv_type == 'DWConv':\n\t\t\tself.conv = nn.Conv2d(dim, dim, kernel_size=5, padding=2, groups=dim, padding_mode='reflect')\n\n\t\tif self.conv_type == 'DWConv' or self.use_attn:\n\t\t\tself.V = nn.Conv2d(dim, dim, 1)\n\t\t\tself.proj = nn.Conv2d(dim, dim, 1)\n\n\t\tif self.use_attn:\n\t\t\tself.QK = nn.Conv2d(dim, dim * 2, 1)\n\t\t\tself.attn = WindowAttention(dim, window_size, num_heads)\n\n\t\tself.apply(self._init_weights)\n\n\tdef _init_weights(self, m):\n\t\tif isinstance(m, nn.Conv2d):\n\t\t\tw_shape = m.weight.shape\n\t\t\t\n\t\t\tif w_shape[0] == self.dim * 2:\t# QK\n\t\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n\t\t\t\tstd = math.sqrt(2.0 / float(fan_in + fan_out))\n\t\t\t\ttrunc_normal_(m.weight, std=std)\t\t\n\t\t\telse:\n\t\t\t\tgain = (8 * self.network_depth) ** (-1/4)\n\t\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n\t\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n\t\t\t\ttrunc_normal_(m.weight, std=std)\n\n\t\t\tif m.bias !=  None:\n\t\t\t\tnn.init.constant_(m.bias, 0)\n\n\tdef check_size(self, x, shift=False):\n\t\t_, _, h, w = x.size()\n\t\tmod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n\t\tmod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n\n\t\tif shift:\n\t\t\tx = F.pad(x, (self.shift_size, (self.window_size-self.shift_size+mod_pad_w) % self.window_size,\n\t\t\t\t\t\t  self.shift_size, (self.window_size-self.shift_size+mod_pad_h) % self.window_size), mode='reflect')\n\t\telse:\n\t\t\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n\t\treturn x\n\n\tdef forward(self, X):\n\t\tB, C, H, W = X.shape\n\n\t\tif self.conv_type == 'DWConv' or self.use_attn:\n\t\t\tV = self.V(X)\n\t\t#print(self.use_attn)\n\t\tif self.use_attn:\n\t\t\t#print('attention')      \n\t\t\tQK = self.QK(X)\n\t\t\tQKV = torch.cat([QK, V], dim=1)\n\n\t\t\t# shift\n\t\t\tshifted_QKV = self.check_size(QKV, self.shift_size > 0)\n\t\t\tHt, Wt = shifted_QKV.shape[2:]\n\n\t\t\t# partition windows\n\t\t\tshifted_QKV = shifted_QKV.permute(0, 2, 3, 1)\n\t\t\tqkv = window_partition(shifted_QKV, self.window_size)  # nW*B, window_size**2, C\n\n\t\t\tattn_windows = self.attn(qkv)\n\n\t\t\t# merge windows\n\t\t\tshifted_out = window_reverse(attn_windows, self.window_size, Ht, Wt)  # B H' W' C\n\n\t\t\t# reverse cyclic shift\n\t\t\tout = shifted_out[:, self.shift_size:(self.shift_size+H), self.shift_size:(self.shift_size+W), :]\n\t\t\tattn_out = out.permute(0, 3, 1, 2)\n\n\t\t\tif self.conv_type in ['Conv', 'DWConv']:\n\t\t\t\tconv_out = self.conv(V)\n\t\t\t\tout = self.proj(conv_out + attn_out)\n\t\t\telse:\n\t\t\t\tout = self.proj(attn_out)\n\n\t\telse:\n\t\t\tif self.conv_type == 'Conv':\n\t\t\t\tout = self.conv(X)\t\t\t\t# no attention and use conv, no projection\n\t\t\telif self.conv_type == 'DWConv':\n\t\t\t\tout = self.proj(self.conv(V))\n\n\t\treturn out\n\n\nclass TransformerBlock(nn.Module):\n\tdef __init__(self, network_depth, dim, num_heads, mlp_ratio=4.,\n\t\t\t\t norm_layer=nn.LayerNorm, mlp_norm=False,\n\t\t\t\t window_size=8, shift_size=0, use_attn=True, conv_type=None):\n\t\tsuper().__init__()\n\t\tself.use_attn = use_attn\n\t\tself.mlp_norm = mlp_norm\n\n\t\tself.norm1 = norm_layer(dim) if use_attn else nn.Identity()\n\t\tself.attn = Attention(network_depth, dim, num_heads=num_heads, window_size=window_size,\n\t\t\t\t\t\t\t  shift_size=shift_size, use_attn=use_attn, conv_type=conv_type)\n\n\t\tself.norm2 = norm_layer(dim) if use_attn and mlp_norm else nn.Identity()\n\t\tself.mlp = Mlp(network_depth, dim, hidden_features=int(dim * mlp_ratio))\n\n\tdef forward(self, x):\n\t\tidentity = x\n\t\tif self.use_attn: x, rescale, rebias = self.norm1(x)\n\t\tx = self.attn(x)\n\t\tif self.use_attn: x = x * rescale + rebias\n\t\tx = identity + x\n\n\t\tidentity = x\n\t\tif self.use_attn and self.mlp_norm: x, rescale, rebias = self.norm2(x)\n\t\tx = self.mlp(x)\n\t\tif self.use_attn and self.mlp_norm: x = x * rescale + rebias\n\t\tx = identity + x\n\t\treturn x\n\n\nclass BasicLayer(nn.Module):\n\tdef __init__(self, network_depth, dim, depth, num_heads, mlp_ratio=4.,\n\t\t\t\t norm_layer=nn.LayerNorm, window_size=8,\n\t\t\t\t attn_ratio=0., attn_loc='last', conv_type=None):\n\n\t\tsuper().__init__()\n\t\tself.dim = dim\n\t\tself.depth = depth\n\n\t\tattn_depth = attn_ratio * depth\n\n\t\tif attn_loc == 'last':\n\t\t\tuse_attns = [i >= depth-attn_depth for i in range(depth)]\n\t\telif attn_loc == 'first':\n\t\t\tuse_attns = [i < attn_depth for i in range(depth)]\n\t\telif attn_loc == 'middle':\n\t\t\tuse_attns = [i >= (depth-attn_depth)//2 and i < (depth+attn_depth)//2 for i in range(depth)]\n\n\t\t# build blocks\n\t\tself.blocks = nn.ModuleList([\n\t\t\tTransformerBlock(network_depth=network_depth,\n\t\t\t\t\t\t\t dim=dim, \n\t\t\t\t\t\t\t num_heads=num_heads,\n\t\t\t\t\t\t\t mlp_ratio=mlp_ratio,\n\t\t\t\t\t\t\t norm_layer=norm_layer,\n\t\t\t\t\t\t\t window_size=window_size,\n\t\t\t\t\t\t\t shift_size=0 if (i % 2 == 0) else window_size // 2,\n\t\t\t\t\t\t\t use_attn=use_attns[i], conv_type=conv_type)\n\t\t\tfor i in range(depth)])\n\n\tdef forward(self, x):\n\t\tfor blk in self.blocks:\n\t\t\tx = blk(x)\n\t\treturn x\n\n\nclass PatchEmbed(nn.Module):\n\tdef __init__(self, patch_size=4, in_chans=3, embed_dim=96, kernel_size=None):\n\t\tsuper().__init__()\n\t\tself.in_chans = in_chans\n\t\tself.embed_dim = embed_dim\n\n\t\tif kernel_size is None:\n\t\t\tkernel_size = patch_size\n\n\t\tself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=kernel_size, stride=patch_size,\n\t\t\t\t\t\t\t  padding=(kernel_size-patch_size+1)//2, padding_mode='reflect')\n\n\tdef forward(self, x):\n\t\tx = self.proj(x)\n\t\treturn x\n\n\nclass PatchUnEmbed(nn.Module):\n\tdef __init__(self, patch_size=4, out_chans=3, embed_dim=96, kernel_size=None):\n\t\tsuper().__init__()\n\t\tself.out_chans = out_chans\n\t\tself.embed_dim = embed_dim\n\n\t\tif kernel_size is None:\n\t\t\tkernel_size = 1\n\n\t\tself.proj = nn.Sequential(\n\t\t\tnn.Conv2d(embed_dim, out_chans*patch_size**2, kernel_size=kernel_size,\n\t\t\t\t\t  padding=kernel_size//2, padding_mode='reflect'),\n\t\t\tnn.PixelShuffle(patch_size)\n\t\t)\n\n\tdef forward(self, x):\n\t\tx = self.proj(x)\n\t\treturn x\n\n\nclass SKFusion(nn.Module):\n\tdef __init__(self, dim, height=2, reduction=8):\n\t\tsuper(SKFusion, self).__init__()\n\t\t\n\t\tself.height = height\n\t\td = max(int(dim/reduction), 4)\n\t\t\n\t\tself.avg_pool = nn.AdaptiveAvgPool2d(1)\n\t\tself.mlp = nn.Sequential(\n\t\t\tnn.Conv2d(dim, d, 1, bias=False), \n\t\t\tnn.ReLU(),\n\t\t\tnn.Conv2d(d, dim*height, 1, bias=False)\n\t\t)\n\t\t\n\t\tself.softmax = nn.Softmax(dim=1)\n\n\tdef forward(self, in_feats):\n\t\tB, C, H, W = in_feats[0].shape\n\t\t\n\t\tin_feats = torch.cat(in_feats, dim=1)\n\t\tin_feats = in_feats.view(B, self.height, C, H, W)\n\t\t\n\t\tfeats_sum = torch.sum(in_feats, dim=1)\n\t\tattn = self.mlp(self.avg_pool(feats_sum))\n\t\tattn = self.softmax(attn.view(B, self.height, C, 1, 1))\n\n\t\tout = torch.sum(in_feats*attn, dim=1)\n\t\treturn out      ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.645225Z","iopub.execute_input":"2025-03-08T11:10:26.645743Z","iopub.status.idle":"2025-03-08T11:10:26.698297Z","shell.execute_reply.started":"2025-03-08T11:10:26.645693Z","shell.execute_reply":"2025-03-08T11:10:26.697168Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class DehazeFormer(nn.Module):\n\tdef __init__(self, in_chans=3, out_chans=4, window_size=8,\n\t\t\t\t embed_dims=[24, 48, 96, 48, 24],\n\t\t\t\t mlp_ratios=[2., 4., 4., 2., 2.],\n\t\t\t\t depths=[16, 16, 16, 8, 8],\n\t\t\t\t num_heads=[2, 4, 6, 1, 1],\n\t\t\t\t attn_ratio=[1/4, 1/2, 3/4, 0, 0],\n\t\t\t\t conv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n\t\t\t\t norm_layer=[RLN, RLN, RLN, RLN, RLN]):\n\t\tsuper(DehazeFormer, self).__init__()\n\n\t\t# setting\n\t\tself.patch_size = 4\n\t\tself.window_size = window_size\n\t\tself.mlp_ratios = mlp_ratios\n\n\t\t# split image into non-overlapping patches\n\t\tself.patch_embed = PatchEmbed(\n\t\t\tpatch_size=1, in_chans=in_chans, embed_dim=embed_dims[0], kernel_size=3)\n\n\t\t# backbone\n\t\tself.layer1 = BasicLayer(network_depth=sum(depths), dim=embed_dims[0], depth=depths[0],\n\t\t\t\t\t   \t\t\t num_heads=num_heads[0], mlp_ratio=mlp_ratios[0],\n\t\t\t\t\t   \t\t\t norm_layer=norm_layer[0], window_size=window_size,\n\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio[0], attn_loc='last', conv_type=conv_type[0])\n\n\t\tself.patch_merge1 = PatchEmbed(\n\t\t\tpatch_size=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])\n\n\t\tself.skip1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n\n\t\tself.layer2 = BasicLayer(network_depth=sum(depths), dim=embed_dims[1], depth=depths[1],\n\t\t\t\t\t\t\t\t num_heads=num_heads[1], mlp_ratio=mlp_ratios[1],\n\t\t\t\t\t\t\t\t norm_layer=norm_layer[1], window_size=window_size,\n\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[1], attn_loc='last', conv_type=conv_type[1])\n\n\t\tself.patch_merge2 = PatchEmbed(\n\t\t\tpatch_size=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])\n\n\t\tself.skip2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n\n\t\tself.layer3 = BasicLayer(network_depth=sum(depths), dim=embed_dims[2], depth=depths[2],\n\t\t\t\t\t\t\t\t num_heads=num_heads[2], mlp_ratio=mlp_ratios[2],\n\t\t\t\t\t\t\t\t norm_layer=norm_layer[2], window_size=window_size,\n\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[2], attn_loc='last', conv_type=conv_type[2])\n\n\t\tself.patch_split1 = PatchUnEmbed(\n\t\t\tpatch_size=2, out_chans=embed_dims[3], embed_dim=embed_dims[2])\n\n\t\tassert embed_dims[1] == embed_dims[3]\n\t\tself.fusion1 = SKFusion(embed_dims[3])\n\n\t\tself.layer4 = BasicLayer(network_depth=sum(depths), dim=embed_dims[3], depth=depths[3],\n\t\t\t\t\t\t\t\t num_heads=num_heads[3], mlp_ratio=mlp_ratios[3],\n\t\t\t\t\t\t\t\t norm_layer=norm_layer[3], window_size=window_size,\n\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[3], attn_loc='last', conv_type=conv_type[3])\n\n\t\tself.patch_split2 = PatchUnEmbed(\n\t\t\tpatch_size=2, out_chans=embed_dims[4], embed_dim=embed_dims[3])\n\n\t\tassert embed_dims[0] == embed_dims[4]\n\t\tself.fusion2 = SKFusion(embed_dims[4])\t\t\t\n\n\t\tself.layer5 = BasicLayer(network_depth=sum(depths), dim=embed_dims[4], depth=depths[4],\n\t\t\t\t\t   \t\t\t num_heads=num_heads[4], mlp_ratio=mlp_ratios[4],\n\t\t\t\t\t   \t\t\t norm_layer=norm_layer[4], window_size=window_size,\n\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio[4], attn_loc='last', conv_type=conv_type[4])\n\n\t\t# merge non-overlapping patches into image\n\t\tself.patch_unembed = PatchUnEmbed(\n\t\t\tpatch_size=1, out_chans=out_chans, embed_dim=embed_dims[4], kernel_size=3)\n\n\n\tdef check_image_size(self, x):\n\t\t# NOTE: for I2I test\n\t\t_, _, h, w = x.size()\n\t\tmod_pad_h = (self.patch_size - h % self.patch_size) % self.patch_size\n\t\tmod_pad_w = (self.patch_size - w % self.patch_size) % self.patch_size\n\t\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n\t\treturn x\n\n\tdef forward_features(self, x):\n\t\tx = self.patch_embed(x)\n\t\tx = self.layer1(x)\n\t\tskip1 = x\n\n\t\tx = self.patch_merge1(x)\n\t\tx = self.layer2(x)\n\t\tskip2 = x\n\n\t\tx = self.patch_merge2(x)\n\t\tx = self.layer3(x)\n\t\tx = self.patch_split1(x)\n\n\t\tx = self.fusion1([x, self.skip2(skip2)]) + x\n\t\tx = self.layer4(x)\n\t\tx = self.patch_split2(x)\n\n\t\tx = self.fusion2([x, self.skip1(skip1)]) + x\n\t\tx = self.layer5(x)\n\t\tx = self.patch_unembed(x)\n\t\treturn x\n\n\tdef forward(self, x):\n\t\tH, W = x.shape[2:]\n\t\tx = self.check_image_size(x)\n\n\t\tfeat = self.forward_features(x)\n\t\tK, B = torch.split(feat, (1, 3), dim=1)\n\n\t\tx = K * x - B + x\n\t\tx = x[:, :, :H, :W]\n\t\treturn x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.699893Z","iopub.execute_input":"2025-03-08T11:10:26.700262Z","iopub.status.idle":"2025-03-08T11:10:26.721552Z","shell.execute_reply.started":"2025-03-08T11:10:26.700210Z","shell.execute_reply":"2025-03-08T11:10:26.720353Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def dehazeformer_m():\n    return DehazeFormer(\n\t\tembed_dims=[24, 48, 96, 48, 24],\n\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n\t\tdepths=[12, 12, 12, 6, 6],\n\t\tnum_heads=[2, 4, 6, 1, 1],\n\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n\t\tconv_type=['Conv', 'Conv', 'Conv', 'Conv', 'Conv'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.723990Z","iopub.execute_input":"2025-03-08T11:10:26.724354Z","iopub.status.idle":"2025-03-08T11:10:26.749828Z","shell.execute_reply.started":"2025-03-08T11:10:26.724310Z","shell.execute_reply":"2025-03-08T11:10:26.748604Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# DetailNet","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# TEACHER MODEL\n# -----------------------------\nclass SR_model(nn.Module):\n    def __init__(self, upscale_factor=1):\n        super(SR_model, self).__init__()\n        self.downsample = nn.Upsample(\n            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n        )\n        self.feature_extraction = nn.Sequential(\n            nn.Conv2d(3, 56, kernel_size=5, padding=2),\n            nn.PReLU()\n        )\n        self.shrinking = nn.Sequential(\n            nn.Conv2d(56, 24, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.mapping = nn.Sequential(\n            nn.Conv2d(24, 24, kernel_size=3, padding=1),\n            nn.PReLU(),\n            nn.Conv2d(24, 24, kernel_size=3, padding=1),\n            nn.PReLU(),\n            nn.Conv2d(24, 24, kernel_size=3, padding=1),\n            nn.PReLU(),\n            nn.Conv2d(24, 24, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.expanding = nn.Sequential(\n            nn.Conv2d(24, 56, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.deconvolution = nn.Sequential(\n            nn.Conv2d(56, 3, kernel_size=3, padding=1),\n            nn.Tanh()  # Ensure output is normalized\n        )\n\n    def forward(self, x):\n        x = self.downsample(x)\n        residual = x  # Store input for residual connection\n        x = self.feature_extraction(x)\n        x = self.shrinking(x)\n        x = self.mapping(x)\n        x = self.expanding(x)\n        x = self.deconvolution(x)\n        return x + residual  # Add residual for stability\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.751508Z","iopub.execute_input":"2025-03-08T11:10:26.751816Z","iopub.status.idle":"2025-03-08T11:10:26.770918Z","shell.execute_reply.started":"2025-03-08T11:10:26.751790Z","shell.execute_reply":"2025-03-08T11:10:26.769751Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# -----------------------------\n# STUDENT MODEL\n# -----------------------------\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        )\n    \n    def forward(self, x):\n        return x + self.block(x)  # Residual connection\n\nclass SpectralAttention(nn.Module):\n    def __init__(self, channels):\n        super(SpectralAttention, self).__init__()\n        self.fc = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, channels // 16, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv2d(channels // 16, channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        return x * self.fc(x)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, channels):\n        super(SpatialAttention, self).__init__()\n        self.conv = nn.Conv2d(channels, 1, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        return x * self.sigmoid(self.conv(x))\n\nclass MultiScaleFeatureFusion(nn.Module):\n    def __init__(self, channels):\n        super(MultiScaleFeatureFusion, self).__init__()\n        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(channels, channels, kernel_size=5, padding=2)\n        self.conv5 = nn.Conv2d(channels, channels, kernel_size=7, padding=3)\n    \n    def forward(self, x):\n        return self.conv1(x) + self.conv3(x) + self.conv5(x)\n\nclass DehazingNet(nn.Module):\n    def __init__(self):\n        super(DehazingNet, self).__init__()\n        self.initial_conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.residual_blocks = nn.Sequential(\n            ResidualBlock(64),\n            ResidualBlock(64),\n            ResidualBlock(64)\n        )\n        self.downsample = nn.Upsample(\n            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n        )\n        self.spectral_attention = SpectralAttention(64)\n        self.spatial_attention = SpatialAttention(64)\n        self.multi_scale_fusion = MultiScaleFeatureFusion(64)\n        self.final_conv = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n    \n    def forward(self, x):\n        x = self.downsample(x)\n        x = F.relu(self.initial_conv(x))\n        x = self.residual_blocks(x)\n        x = self.spectral_attention(x)\n        x = self.spatial_attention(x)\n        x = self.multi_scale_fusion(x)\n        x = self.final_conv(x)\n        return x \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.772097Z","iopub.execute_input":"2025-03-08T11:10:26.772429Z","iopub.status.idle":"2025-03-08T11:10:26.794276Z","shell.execute_reply.started":"2025-03-08T11:10:26.772392Z","shell.execute_reply":"2025-03-08T11:10:26.792955Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# -----------------------------\n# FEATURE AFFINITY MODULE (FAM) USING KL DIVERGENCE\n# -----------------------------\nclass FeatureAffinityModule(nn.Module):\n    def __init__(self):\n        super(FeatureAffinityModule, self).__init__()\n        self.pool = nn.AdaptiveAvgPool2d((16, 16))\n\n    def forward(self, features_a, features_b):\n        # Pool and flatten features\n        feat_a = self.pool(features_a).view(features_a.size(0), -1)\n        feat_b = self.pool(features_b).view(features_b.size(0), -1)\n\n        # Normalize features (important for stable KL divergence)\n        feat_a = F.normalize(feat_a, p=2, dim=-1)\n        feat_b = F.normalize(feat_b, p=2, dim=-1)\n\n        # Compute normalized affinity matrices\n        affinity_a = torch.mm(feat_a, feat_a.T) / feat_a.size(1)\n        affinity_b = torch.mm(feat_b, feat_b.T) / feat_b.size(1)\n\n        # Compute symmetric KL divergence loss\n        loss = 0.5 * (F.kl_div(F.log_softmax(affinity_a, dim=-1), F.softmax(affinity_b, dim=-1), reduction='batchmean') +\n                      F.kl_div(F.log_softmax(affinity_b, dim=-1), F.softmax(affinity_a, dim=-1), reduction='batchmean'))\n\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.795662Z","iopub.execute_input":"2025-03-08T11:10:26.796041Z","iopub.status.idle":"2025-03-08T11:10:26.819832Z","shell.execute_reply.started":"2025-03-08T11:10:26.796003Z","shell.execute_reply":"2025-03-08T11:10:26.818683Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Guided Filter","metadata":{}},{"cell_type":"code","source":"class ConvGuidedFilter(nn.Module):\n    \"\"\"\n    Adapted from https://github.com/wuhuikai/DeepGuidedFilter\n    \"\"\"\n    def __init__(self, radius=1, norm=nn.BatchNorm2d, conv_a_kernel_size: int = 1):\n        super(ConvGuidedFilter, self).__init__()\n\n        self.box_filter = nn.Conv2d(\n            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n        )\n        self.conv_a = nn.Sequential(\n            nn.Conv2d(\n                6,\n                32,\n                kernel_size=conv_a_kernel_size,\n                padding=conv_a_kernel_size // 2,\n                bias=False,\n            ),\n            norm(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                32,\n                32,\n                kernel_size=conv_a_kernel_size,\n                padding=conv_a_kernel_size // 2,\n                bias=False,\n            ),\n            norm(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                32,\n                3,\n                kernel_size=conv_a_kernel_size,\n                padding=conv_a_kernel_size // 2,\n                bias=False,\n            ),\n        )\n        self.box_filter.weight.data[...] = 1.0\n\n    def forward(self, x_lr, y_lr, x_hr):\n        _, _, h_lrx, w_lrx = x_lr.size()\n        _, _, h_hrx, w_hrx = x_hr.size()\n\n        N = self.box_filter(x_lr.data.new().resize_((1, 3, h_lrx, w_lrx)).fill_(1.0))\n        ## mean_x\n        mean_x = self.box_filter(x_lr) / N\n        ## mean_y\n        mean_y = self.box_filter(y_lr) / N\n        ## cov_xy\n        cov_xy = self.box_filter(x_lr * y_lr) / N - mean_x * mean_y\n        ## var_x\n        var_x = self.box_filter(x_lr * x_lr) / N - mean_x * mean_x\n\n        ## A\n        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n        ## b\n        b = mean_y - A * mean_x\n\n        ## mean_A; mean_b\n        mean_A = F.interpolate(A, (h_hrx, w_hrx), mode=\"bilinear\", align_corners=True)\n        mean_b = F.interpolate(b, (h_hrx, w_hrx), mode=\"bilinear\", align_corners=True)\n\n        return mean_A * x_hr + mean_b","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.820851Z","iopub.execute_input":"2025-03-08T11:10:26.821151Z","iopub.status.idle":"2025-03-08T11:10:26.840471Z","shell.execute_reply.started":"2025-03-08T11:10:26.821125Z","shell.execute_reply":"2025-03-08T11:10:26.839355Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class AdaptiveInstanceNorm(nn.Module):\n    def __init__(self, n):\n        super(AdaptiveInstanceNorm, self).__init__()\n\n        self.w_0 = nn.Parameter(torch.Tensor([1.0]))\n        self.w_1 = nn.Parameter(torch.Tensor([0.0]))\n\n        self.ins_norm = nn.InstanceNorm2d(n, momentum=0.999, eps=0.001, affine=True)\n\n    def forward(self, x):\n        return self.w_0 * x + self.w_1 * self.ins_norm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.841552Z","iopub.execute_input":"2025-03-08T11:10:26.841907Z","iopub.status.idle":"2025-03-08T11:10:26.866110Z","shell.execute_reply.started":"2025-03-08T11:10:26.841871Z","shell.execute_reply":"2025-03-08T11:10:26.864874Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class DeepGuidednew(nn.Module):\n    def __init__(self, radius=1):\n        super().__init__()\n        norm = AdaptiveInstanceNorm\n        kernel_size=3\n        depth_rate=16\n        in_channels=3\n        num_dense_layer=4\n        growth_rate=16\n        growth_rate=16\n\n        # self.local = local\n        \n        # self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n        # self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n\n        self.gf = ConvGuidedFilter(radius, norm=norm)\n        self.lr = dehazeformer_m()\n\n        self.downsample = nn.Upsample(\n            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n        )\n        self.upsample = nn.Upsample(\n            scale_factor=2, mode=\"bilinear\", align_corners=True\n        )\n\n    def forward(self, x_hr, y_detail):\n        x_lr = self.downsample(x_hr)\n        # y_lr=self.conv_in(x_lr)\n        # y_lr= self.local(y_lr)\n        # y_detail=self.conv_out(y_lr)\n        y_base=self.lr(x_lr)\n        # print(y_base.shape, y_detail.shape)\n        y_lr=y_base+ y_detail\n        y_base=self.upsample(y_base)\n        return  self.gf(x_lr, y_lr, x_hr), y_base      ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.869253Z","iopub.execute_input":"2025-03-08T11:10:26.869620Z","iopub.status.idle":"2025-03-08T11:10:26.890834Z","shell.execute_reply.started":"2025-03-08T11:10:26.869590Z","shell.execute_reply":"2025-03-08T11:10:26.889407Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# CUSTOM DATASET LOADER\n# -----------------------------\nclass TrainData(Dataset):\n    def __init__(self, crop_size, hazeeffected_images_dir, hazefree_images_dir):\n        super().__init__()\n        hazy_data = glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n        self.haze_names = [os.path.join(hazeeffected_images_dir, os.path.basename(h)) for h in hazy_data]\n        self.gt_names = [os.path.join(hazefree_images_dir, os.path.basename(h)) for h in hazy_data]\n        self.crop_size = crop_size\n    \n    def get_images(self, index):\n        crop_width, crop_height = self.crop_size\n        haze_img = Image.open(self.haze_names[index]).convert('RGB')\n        gt_img = Image.open(self.gt_names[index]).convert('RGB')\n        \n        width, height = haze_img.size\n        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n        \n        transform = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        return transform(haze_crop_img), transform(gt_crop_img)\n\n    def __getitem__(self, index):\n        return self.get_images(index)\n\n    def __len__(self):\n        return len(self.haze_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.892440Z","iopub.execute_input":"2025-03-08T11:10:26.892824Z","iopub.status.idle":"2025-03-08T11:10:26.915840Z","shell.execute_reply.started":"2025-03-08T11:10:26.892776Z","shell.execute_reply":"2025-03-08T11:10:26.914819Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"crop_size = (360, 360)\n# train_data = TrainData(crop_size, '/kaggle/input/nh-dense-haze/Dense-Haze-T/Dense-Haze-T/IN', '/kaggle/input/ nh-dense-haze/Dense-Haze-T/Dense-Haze-T/GT')\ntrain_data = TrainData(crop_size, '/kaggle/input/reside6k/RESIDE-6K/train/hazy', '/kaggle/input/reside6k/RESIDE-6K/train/GT')\ndataloader = DataLoader(train_data, batch_size=4, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:26.917027Z","iopub.execute_input":"2025-03-08T11:10:26.917421Z","iopub.status.idle":"2025-03-08T11:10:27.148808Z","shell.execute_reply.started":"2025-03-08T11:10:26.917383Z","shell.execute_reply":"2025-03-08T11:10:27.147726Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Get a single batch from the dataloader\nfor hazy_images, clear_images in dataloader:\n    print(f\"Hazy Images Shape: {hazy_images.shape}\")\n    print(f\"Clear Images Shape: {clear_images.shape}\")\n    break  # Only check one batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:27.150136Z","iopub.execute_input":"2025-03-08T11:10:27.151095Z","iopub.status.idle":"2025-03-08T11:10:27.477209Z","shell.execute_reply.started":"2025-03-08T11:10:27.151047Z","shell.execute_reply":"2025-03-08T11:10:27.475855Z"}},"outputs":[{"name":"stdout","text":"Hazy Images Shape: torch.Size([4, 3, 360, 360])\nClear Images Shape: torch.Size([4, 3, 360, 360])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Perceptual Loss","metadata":{}},{"cell_type":"code","source":"from torchvision.models import vgg16\nloss_model = vgg16(pretrained=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:27.478500Z","iopub.execute_input":"2025-03-08T11:10:27.478869Z","iopub.status.idle":"2025-03-08T11:10:32.653104Z","shell.execute_reply.started":"2025-03-08T11:10:27.478832Z","shell.execute_reply":"2025-03-08T11:10:32.651799Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:03<00:00, 182MB/s]  \n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"loss_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:32.654080Z","iopub.execute_input":"2025-03-08T11:10:32.654402Z","iopub.status.idle":"2025-03-08T11:10:32.662038Z","shell.execute_reply.started":"2025-03-08T11:10:32.654375Z","shell.execute_reply":"2025-03-08T11:10:32.660923Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"loss_model = loss_model.features\nloss_model = loss_model.to(device)\nfor param in loss_model.parameters():\n    param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:32.663111Z","iopub.execute_input":"2025-03-08T11:10:32.663501Z","iopub.status.idle":"2025-03-08T11:10:32.691287Z","shell.execute_reply.started":"2025-03-08T11:10:32.663459Z","shell.execute_reply":"2025-03-08T11:10:32.689888Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class FeatureLossNetwork(torch.nn.Module):\n    def __init__(self, feature_extractor):\n        super(FeatureLossNetwork, self).__init__()\n        self.feature_layers = feature_extractor\n        self.layer_name_mapping = {\n            '1': \"relu1_1\",\n            # '3': \"relu1_2\",\n            # '6': \"relu2_1\",\n            # '8': \"relu2_2\",\n            # '11': \"relu3_1\",\n            # '13': \"relu3_2\",\n            # '15': \"relu3_3\",\n            '18': \"relu4_1\",\n            # '20': \"relu4_2\",\n            # '22': \"relu4_3\",\n            # '25': \"relu5_1\",\n            # '27': \"relu5_2\",\n            '29': \"relu5_3\"\n        }\n\n    def extract_features(self, x):\n        output = {}\n        for name, module in self.feature_layers._modules.items():\n            x = module(x)\n            if name in self.layer_name_mapping:\n                output[self.layer_name_mapping[name]] = x\n        return list(output.values())\n\n    def forward(self, predicted, ground_truth):\n        loss = []\n        scale_factor = 1000  \n        predicted_features = self.extract_features(predicted)\n        ground_truth_features = self.extract_features(ground_truth)\n        for pred_feature, gt_feature in zip(predicted_features, ground_truth_features):\n            loss.append(F.mse_loss(pred_feature, gt_feature))\n\n        return sum(loss) / (len(loss) * scale_factor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:32.695352Z","iopub.execute_input":"2025-03-08T11:10:32.695777Z","iopub.status.idle":"2025-03-08T11:10:32.718381Z","shell.execute_reply.started":"2025-03-08T11:10:32.695745Z","shell.execute_reply":"2025-03-08T11:10:32.717276Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"loss_network = FeatureLossNetwork(loss_model)\nloss_network.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:32.720698Z","iopub.execute_input":"2025-03-08T11:10:32.721327Z","iopub.status.idle":"2025-03-08T11:10:32.748336Z","shell.execute_reply.started":"2025-03-08T11:10:32.721294Z","shell.execute_reply":"2025-03-08T11:10:32.747100Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"FeatureLossNetwork(\n  (feature_layers): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# # Test loss network with generated image and original image\n# if isinstance(dehazed_image, np.ndarray):\n#     dehazed_image = torch.from_numpy(dehazed_image).float().permute(2, 0, 1).unsqueeze(0).to(device)\n\n# if isinstance(image, np.ndarray):\n#     image = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0).to(device)\n\n# # Compute loss\n# dehazed_image = dehazed_image.to(device)\n# image.to(device)\n# loss = loss_network(dehazed_image, image)\n# print(\"Feature Loss:\", loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:32.749430Z","iopub.execute_input":"2025-03-08T11:10:32.749823Z","iopub.status.idle":"2025-03-08T11:10:32.771796Z","shell.execute_reply.started":"2025-03-08T11:10:32.749793Z","shell.execute_reply":"2025-03-08T11:10:32.770477Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"learning_rate = 1e-4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:32.772941Z","iopub.execute_input":"2025-03-08T11:10:32.773204Z","iopub.status.idle":"2025-03-08T11:10:32.803629Z","shell.execute_reply.started":"2025-03-08T11:10:32.773180Z","shell.execute_reply":"2025-03-08T11:10:32.802450Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# --- GPU device --- #\ndevice_ids = list(range(torch.cuda.device_count()))\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# --- Define the network --- #\nnet = DeepGuidednew()\n\n# --- Multi-GPU (correct order) --- #\nnet = nn.DataParallel(net, device_ids=device_ids).to(device)\n\n# --- Build optimizer --- #\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n\n# # --- Define the perceptual loss network --- #\n# vgg_model = vgg16(pretrained=True).features[:16].to(device)\n# for param in vgg_model.parameters():\n#     param.requires_grad = False\n\n# loss_network = LossNetwork(vgg_model)\n# loss_network.eval()\n\n# models = 'formernew'\n\n# --- Load the network weight --- #\n# weight_path = \"{}_{}_haze_best_{}\".format(models, category, version)\n# try:\n#     net.load_state_dict(torch.load(weight_path))\n#     print('--- weight loaded ---')\n# except FileNotFoundError:\n#     print('--- no weight loaded ---')\n\n# --- Calculate all trainable parameters in network --- #\npytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\nprint(\"Total_params: {}\".format(pytorch_total_params))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:32.804879Z","iopub.execute_input":"2025-03-08T11:10:32.805342Z","iopub.status.idle":"2025-03-08T11:10:33.136492Z","shell.execute_reply.started":"2025-03-08T11:10:32.805262Z","shell.execute_reply":"2025-03-08T11:10:33.135268Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Total_params: 4637423\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"lambda_loss = 0.84\nprint(f'lambda_loss: {lambda_loss}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:33.137723Z","iopub.execute_input":"2025-03-08T11:10:33.138146Z","iopub.status.idle":"2025-03-08T11:10:33.144834Z","shell.execute_reply.started":"2025-03-08T11:10:33.138107Z","shell.execute_reply":"2025-03-08T11:10:33.143298Z"}},"outputs":[{"name":"stdout","text":"lambda_loss: 0.84\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# -----------------------------\n# CO-DISTILLATION TRAINING\n# -----------------------------\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport csv\n\ndef calculate_psnr(output, target, max_pixel_value=1.0):\n    mse = F.mse_loss(output, target)\n    # print(f\"MSE: {mse.item()}\")\n    if mse == 0:\n        return 100  # Avoid log(0) case, return max PSNR\n    psnr = 20 * math.log10(max_pixel_value) - 10 * math.log10(mse.item())\n    # print(f\"PSNR: {psnr}\")\n    return psnr\n\n# -----------------------------\n# CO-DISTILLATION TRAINING\n# -----------------------------\ndef train(net, teacher, student, fam, dataloader, num_epochs=10, lambda_fam=0.25, log_file=\"training_log.csv\"):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    # net.to(device).train()\n    teacher.to(device).train()\n    student.to(device).train()\n    fam.to(device)\n\n    optimizer_t = torch.optim.Adam(teacher.parameters(), lr=1e-2)\n    optimizer_s = torch.optim.Adam(student.parameters(), lr=1e-2)\n    optimizer_d = torch.optim.Adam(net.parameters(), lr=1e-2)\n    \n    scheduler_t = CosineAnnealingLR(optimizer_t, T_max=num_epochs, eta_min=1e-3)\n    scheduler_s = CosineAnnealingLR(optimizer_s, T_max=num_epochs, eta_min=1e-3)\n    scheduler_d = CosineAnnealingLR(optimizer_d, T_max=num_epochs, eta_min=1e-3)\n    \n    best_loss = float('inf')\n    best_psnr = 0\n\n    with open(log_file, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Epoch\", \"Loss\", \"Teacher PSNR\", \"Student PSNR\"])\n        \n        for epoch in range(num_epochs):\n            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n            total_loss = 0\n            total_psnr_t = 0  # Teacher PSNR\n            total_psnr_s = 0  # Student PSNR\n            num_batches = len(dataloader)\n            teacher_output = 0\n            student_output = 0\n            \n            for hazy_images, clear_images in dataloader:\n                hazy_images, clear_images = hazy_images.to(device), clear_images.to(device)\n                print(f\"Hazy images shape: {hazy_images.shape}, Clear images shape: {clear_images.shape}\")\n                \n                teacher_output = teacher(clear_images)\n                student_output = student(hazy_images)\n                print(f\"Teacher output shape: {teacher_output.shape}, Student output shape: {student_output.shape}\")\n\n                dehaze,base = net(hazy_images, student_output)\n                \n                base_loss = F.smooth_l1_loss(base, clear_images)\n                smooth_loss = F.smooth_l1_loss(dehaze, clear_images)\n                perceptual_loss = loss_network(dehaze, clear_images)\n\n                # print(\"Type: Teacher:\", type(teacher_output))\n                # print(\"Type: student_output:\", type(student_output))\n                # print(\"Type: detail_output:\", type(detail_output))\n                downsample = nn.Upsample(\n                    scale_factor=0.5, mode=\"bilinear\", align_corners=True\n                )\n                clear_images = downsample(clear_images)\n\n                # mse_loss_d = F.mse_loss(detail_output, clear_images)\n                mse_loss_t = F.mse_loss(teacher_output, clear_images)\n                mse_loss_s = F.mse_loss(student_output, clear_images)\n                # print(f\"MSE Loss - Teacher: {mse_loss_t.item()}, Student: {mse_loss_s.item()}\")\n                \n                fam_loss = fam(teacher_output, student_output) \n                # print(f\"FAM Loss: {fam_loss.item()}\")\n                print(\"Base Loss:\", base_loss)\n                print(\"Smooth Loss:\", smooth_loss)\n                print(\"Lambda Loss * Perceptual Loss:\", lambda_loss * perceptual_loss)\n                print(\"FAM Loss:\", fam_loss)\n\n                \n                loss = base_loss + smooth_loss + lambda_loss * perceptual_loss + fam_loss \n                \n                # loss = base_loss + smooth_loss + lambda_loss * perceptual_loss+ fam_loss + mse_loss_t + mse_loss_s\n                print(f\"Total Loss: {loss.item()}\")\n                \n                optimizer_t.zero_grad()\n                optimizer_s.zero_grad()\n                optimizer_d.zero_grad()\n                loss.backward()\n                optimizer_t.step()\n                optimizer_s.step()\n                optimizer_d.step()\n                \n                # print(f\"PSNR - Teacher: {psnr_t}, Student: {psnr_s}\")\n                \n                total_loss += loss.item()\n            # Compute PSNR for teacher and student\n            psnr_t = calculate_psnr(teacher_output, clear_images)\n            psnr_s = calculate_psnr(student_output, clear_images)\n            total_psnr_t += psnr_t\n            total_psnr_s += psnr_s\n\n            avg_loss = total_loss / num_batches\n            avg_psnr_t = total_psnr_t / num_batches\n            avg_psnr_s = total_psnr_s / num_batches\n            print(f\"Epoch {epoch + 1} - Avg Loss: {avg_loss}, Avg PSNR (Teacher): {avg_psnr_t}, Avg PSNR (Student): {avg_psnr_s}\")\n\n\n            log_entry = f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.6f}, Avg Teacher PSNR: {avg_psnr_t:.2f}, Avg Student PSNR: {avg_psnr_s:.2f}\"\n            print(log_entry)\n\n            # Write log to file\n            writer.writerow([epoch + 1, avg_loss, avg_psnr_t, avg_psnr_s])\n\n            # Update schedulers\n            scheduler_t.step()\n            scheduler_s.step()\n            scheduler_d.step()\n\n            # Save only if the model improves\n            # if avg_loss < best_loss or epoch %50 ==0:\n            best_loss = avg_loss\n            best_psnr = avg_psnr_s\n            torch.save(student.state_dict(), str(epoch)+\"best_dehazing_student.pth\")\n            torch.save(teacher.state_dict(), str(epoch)+\"best_sr_teacher.pth\")\n            print(f\"Saved Best Model (Loss: {best_loss:.6f}, Student PSNR: {best_psnr:.2f})\")\n\n    print(\"Training complete. Logs saved in\", log_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:33.146311Z","iopub.execute_input":"2025-03-08T11:10:33.147009Z","iopub.status.idle":"2025-03-08T11:10:33.176816Z","shell.execute_reply.started":"2025-03-08T11:10:33.146899Z","shell.execute_reply":"2025-03-08T11:10:33.175740Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# -----------------------------\n# TRAINING SETUP \n# -----------------------------\nteacher_model = SR_model(upscale_factor=1)\nstudent_model = DehazingNet()\nfam_module = FeatureAffinityModule()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:33.178090Z","iopub.execute_input":"2025-03-08T11:10:33.178508Z","iopub.status.idle":"2025-03-08T11:10:33.220092Z","shell.execute_reply.started":"2025-03-08T11:10:33.178468Z","shell.execute_reply":"2025-03-08T11:10:33.219103Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"train(net,teacher_model, student_model, fam_module, dataloader, num_epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:10:33.221313Z","iopub.execute_input":"2025-03-08T11:10:33.221732Z","iopub.status.idle":"2025-03-08T11:11:40.719640Z","shell.execute_reply.started":"2025-03-08T11:10:33.221693Z","shell.execute_reply":"2025-03-08T11:11:40.717858Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nEpoch 1/10\nHazy images shape: torch.Size([4, 3, 360, 360]), Clear images shape: torch.Size([4, 3, 360, 360])\nTeacher output shape: torch.Size([4, 3, 180, 180]), Student output shape: torch.Size([4, 3, 180, 180])\nBase Loss: tensor(0.1466, grad_fn=<SmoothL1LossBackward0>)\nSmooth Loss: tensor(0.1338, grad_fn=<SmoothL1LossBackward0>)\nLambda Loss * Perceptual Loss: tensor(0.0011, grad_fn=<MulBackward0>)\nFAM Loss: tensor(2.0489e-07, grad_fn=<MulBackward0>)\nTotal Loss: 0.281588077545166\nHazy images shape: torch.Size([4, 3, 360, 360]), Clear images shape: torch.Size([4, 3, 360, 360])\nTeacher output shape: torch.Size([4, 3, 180, 180]), Student output shape: torch.Size([4, 3, 180, 180])\nBase Loss: tensor(3166731.2500, grad_fn=<SmoothL1LossBackward0>)\nSmooth Loss: tensor(3141624., grad_fn=<SmoothL1LossBackward0>)\nLambda Loss * Perceptual Loss: tensor(1.0751e+10, grad_fn=<MulBackward0>)\nFAM Loss: tensor(7.4506e-08, grad_fn=<MulBackward0>)\nTotal Loss: 10757286912.0\nHazy images shape: torch.Size([4, 3, 360, 360]), Clear images shape: torch.Size([4, 3, 360, 360])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-210705458094>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mteacher_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfam_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-24-3bc7ccb109bb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, teacher, student, fam, dataloader, num_epochs, lambda_fam, log_file)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mteacher_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mstudent_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhazy_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Teacher output shape: {teacher_output.shape}, Student output shape: {student_output.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-4a4fe2de913c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectral_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_scale_fusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-4a4fe2de913c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDehazingNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"# -----------------------------\n# LOAD MODEL\n# -----------------------------\n# model_path = \"/kaggle/input/dehazing_sr/pytorch/default/1/best_dehazing_student.pth\"\nmodel_path = \"/kaggle/input/dehazing_sr/pytorch/default/3/9best_sr_teacher.pth\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:11:40.720602Z","iopub.status.idle":"2025-03-08T11:11:40.721053Z","shell.execute_reply":"2025-03-08T11:11:40.720885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model\n# model = DehazingNet().to(device)\nmodel = SR_model(upscale_factor=1).to(device)\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:11:40.722645Z","iopub.status.idle":"2025-03-08T11:11:40.723101Z","shell.execute_reply":"2025-03-08T11:11:40.722958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# LOAD TEST DATA\n# -----------------------------\ntest_hazy_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/hazy\"\ntest_gt_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/GT\"\n# test_hazy_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN\"\n# test_gt_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT\"\n\nhazy_images = sorted(glob.glob(os.path.join(test_hazy_dir, \"*.*\")))\ngt_images = sorted(glob.glob(os.path.join(test_gt_dir, \"*.*\")))\n\ntransform = Compose([\n    ToTensor(),\n    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nto_pil = ToPILImage()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:11:40.723993Z","iopub.status.idle":"2025-03-08T11:11:40.724468Z","shell.execute_reply":"2025-03-08T11:11:40.724232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":".. block execution","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:11:40.725697Z","iopub.status.idle":"2025-03-08T11:11:40.726049Z","shell.execute_reply":"2025-03-08T11:11:40.725913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# INFERENCE & VISUALIZATION\n# -----------------------------\nnum_samples = 20  # Change as needed\nplt.figure(figsize=(10, num_samples * 5))\n\nfor i in range(num_samples):\n    hazy_img = Image.open(hazy_images[i])\n\n    # Transform for model input\n    input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n\n    # Inference\n    with torch.no_grad():\n        output_tensor = model(input_tensor).cpu().squeeze(0)\n\n    # Convert back to image\n    output_img = to_pil(output_tensor)\n\n    # Display results\n    plt.subplot(num_samples, 2, 2 * i + 1)\n    plt.imshow(hazy_img)\n    plt.title(\"Hazy Input\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_samples, 2, 2 * i + 2)\n    plt.imshow(output_img)\n    plt.title(\"Output\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:11:40.727200Z","iopub.status.idle":"2025-03-08T11:11:40.727576Z","shell.execute_reply":"2025-03-08T11:11:40.727443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n# -----------------------------\nimage_indices = [70, 75, 89, 100]  # Indices of images to visualize\n\nplt.figure(figsize=(10, len(image_indices) * 5))\n\nfor idx, i in enumerate(image_indices):\n    hazy_img = Image.open(hazy_images[i+1])\n    gt_img = Image.open(gt_images[i+1])\n\n    # Transform for model input\n    input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n\n    # Inference\n    with torch.no_grad():\n        output_tensor = model(input_tensor).cpu().squeeze(0)\n\n    # Convert back to image\n    output_img = to_pil(output_tensor)\n\n    # Display results\n    plt.subplot(len(image_indices), 3, 3 * idx + 1)\n    plt.imshow(hazy_img)\n    plt.title(f\"Hazy Input {i}\")\n    plt.axis(\"off\")\n\n    plt.subplot(len(image_indices), 3, 3 * idx + 2)\n    plt.imshow(output_img)\n    plt.title(f\"Dehazed Output {i}\")\n    plt.axis(\"off\")\n\n    plt.subplot(len(image_indices), 3, 3 * idx + 3)\n    plt.imshow(gt_img)\n    plt.title(f\"Ground Truth {i}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:11:40.728633Z","iopub.status.idle":"2025-03-08T11:11:40.729055Z","shell.execute_reply":"2025-03-08T11:11:40.728917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# INFERENCE & VISUALIZATION\n# -----------------------------\nnum_samples = 5  # Change as needed\nplt.figure(figsize=(10, num_samples * 5))\n\nfor i in range(num_samples):\n    hazy_img = Image.open(hazy_images[i]).convert('RGB')\n    gt_img = Image.open(gt_images[i]).convert('RGB')\n\n    # Transform for model input\n    input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n\n    # Inference\n    with torch.no_grad():\n        output_tensor = teacher_model(input_tensor).cpu().squeeze(0)\n    \n    # Convert back to image\n    output_img = to_pil(output_tensor)\n\n    # Display results\n    plt.subplot(num_samples, 3, 3 * i + 1)\n    plt.imshow(hazy_img)\n    plt.title(\"Hazy Input\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_samples, 3, 3 * i + 2)\n    plt.imshow(output_img)\n    plt.title(\"Dehazed Output\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_samples, 3, 3 * i + 3)\n    plt.imshow(gt_img)\n    plt.title(\"Ground Truth\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:11:40.730061Z","iopub.status.idle":"2025-03-08T11:11:40.730424Z","shell.execute_reply":"2025-03-08T11:11:40.730292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path = \"/kaggle/input/dehazing_sr/pytorch/default/3/9best_dehazing_student.pth\"\nmodel = DehazingNet().to(device)\n# model = SR_model(upscale_factor=1).to(device)\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:11:40.731145Z","iopub.status.idle":"2025-03-08T11:11:40.731512Z","shell.execute_reply":"2025-03-08T11:11:40.731353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# INFERENCE & VISUALIZATION\n# -----------------------------\nnum_samples = 20  # Change as needed\nplt.figure(figsize=(10, num_samples * 5))\n\nfor i in range(num_samples):\n    hazy_img = Image.open(hazy_images[i])\n\n    # Transform for model input\n    input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n\n    # Inference\n    with torch.no_grad():\n        output_tensor = model(input_tensor).cpu().squeeze(0)\n\n    # Convert back to image\n    output_img = to_pil(output_tensor)\n\n    # Display results\n    plt.subplot(num_samples, 2, 2 * i + 1)\n    plt.imshow(hazy_img)\n    plt.title(\"Hazy Input\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_samples, 2, 2 * i + 2)\n    plt.imshow(output_img)\n    plt.title(\"Output\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:11:40.732342Z","iopub.status.idle":"2025-03-08T11:11:40.732949Z","shell.execute_reply":"2025-03-08T11:11:40.732551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# INFERENCE & VISUALIZATION\n# -----------------------------\nnum_samples = 5  # Change as needed\nplt.figure(figsize=(10, num_samples * 5))\n\nfor i in range(num_samples):\n    hazy_img = Image.open(hazy_images[i]).convert('RGB')\n    gt_img = Image.open(gt_images[i]).convert('RGB')\n\n    # Transform for model input\n    input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n\n    # Inference\n    with torch.no_grad():\n        output_tensor = teacher_model(input_tensor).cpu().squeeze(0)\n    \n    # Convert back to image\n    output_img = to_pil(output_tensor)\n\n    # Display results\n    plt.subplot(num_samples, 3, 3 * i + 1)\n    plt.imshow(hazy_img)\n    plt.title(\"Hazy Input\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_samples, 3, 3 * i + 2)\n    plt.imshow(output_img)\n    plt.title(\"Dehazed Output\")\n    plt.axis(\"off\")\n\n    plt.subplot(num_samples, 3, 3 * i + 3)\n    plt.imshow(gt_img)\n    plt.title(\"Ground Truth\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:11:40.734410Z","iopub.status.idle":"2025-03-08T11:11:40.734766Z","shell.execute_reply":"2025-03-08T11:11:40.734627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n# -----------------------------\nimage_indices = [70, 75, 89, 100]  # Indices of images to visualize\n\nplt.figure(figsize=(10, len(image_indices) * 5))\n\nfor idx, i in enumerate(image_indices):\n    hazy_img = Image.open(hazy_images[i+1])\n    gt_img = Image.open(gt_images[i+1])\n\n    # Transform for model input\n    input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n\n    # Inference\n    with torch.no_grad():\n        output_tensor = model(input_tensor).cpu().squeeze(0)\n\n    # Convert back to image\n    output_img = to_pil(output_tensor)\n\n    # Display results\n    plt.subplot(len(image_indices), 3, 3 * idx + 1)\n    plt.imshow(hazy_img)\n    plt.title(f\"Hazy Input {i}\")\n    plt.axis(\"off\")\n\n    plt.subplot(len(image_indices), 3, 3 * idx + 2)\n    plt.imshow(output_img)\n    plt.title(f\"Dehazed Output {i}\")\n    plt.axis(\"off\")\n\n    plt.subplot(len(image_indices), 3, 3 * idx + 3)\n    plt.imshow(gt_img)\n    plt.title(f\"Ground Truth {i}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T11:11:40.737956Z","iopub.status.idle":"2025-03-08T11:11:40.738355Z","shell.execute_reply":"2025-03-08T11:11:40.738155Z"}},"outputs":[],"execution_count":null}]}