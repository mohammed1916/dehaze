{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18039f3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:35:38.604853Z",
     "iopub.status.busy": "2025-04-14T02:35:38.604568Z",
     "iopub.status.idle": "2025-04-14T02:35:48.236850Z",
     "shell.execute_reply": "2025-04-14T02:35:48.235675Z"
    },
    "papermill": {
     "duration": 9.648788,
     "end_time": "2025-04-14T02:35:48.238892",
     "exception": false,
     "start_time": "2025-04-14T02:35:38.590104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/14.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\n",
      "\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/14.8 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/14.8 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/14.8 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m13.9/14.8 MB\u001b[0m \u001b[31m165.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading ipywidgets-8.1.6-py3-none-any.whl (139 kB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/139.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\r\n",
      "  Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/961.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.10/site-packages (from scikit-image) (11.1.0)\r\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/site-packages (from scikit-image) (3.4.2)\r\n",
      "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.10/site-packages (from scikit-image) (1.15.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tifffile>=2022.8.12\r\n",
      "  Downloading tifffile-2025.3.30-py3-none-any.whl (226 kB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting lazy-loader>=0.4\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/site-packages (from scikit-image) (2.0.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageio!=2.35.0,>=2.33\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/315.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/site-packages (from scikit-image) (24.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyterlab_widgets~=3.0.14\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading jupyterlab_widgets-3.0.14-py3-none-any.whl (213 kB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/214.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.0/214.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (8.32.0)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting widgetsnbextension~=4.0.14\r\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/site-packages (from torchmetrics) (2.5.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightning-utilities>=0.8.0\r\n",
      "  Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\r\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\r\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\r\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\r\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\r\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.8.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (2025.2.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\r\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\r\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: widgetsnbextension, tifffile, lightning-utilities, lazy-loader, jupyterlab_widgets, imageio, scikit-image, ipywidgets, torchmetrics\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed imageio-2.37.0 ipywidgets-8.1.6 jupyterlab_widgets-3.0.14 lazy-loader-0.4 lightning-utilities-0.14.3 scikit-image-0.25.2 tifffile-2025.3.30 torchmetrics-1.7.1 widgetsnbextension-4.0.14\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image ipywidgets torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b68a384",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:35:48.272564Z",
     "iopub.status.busy": "2025-04-14T02:35:48.272283Z",
     "iopub.status.idle": "2025-04-14T02:36:23.662141Z",
     "shell.execute_reply": "2025-04-14T02:36:23.660484Z"
    },
    "papermill": {
     "duration": 35.408176,
     "end_time": "2025-04-14T02:36:23.663720",
     "exception": false,
     "start_time": "2025-04-14T02:35:48.255544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "from timm.layers import to_2tuple, trunc_normal_\n",
    "import os\n",
    "import torchvision.utils as utils\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from random import randrange\n",
    "import time\n",
    "from math import log10\n",
    "from skimage import measure\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.nn.init import trunc_normal_\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53146314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:23.696639Z",
     "iopub.status.busy": "2025-04-14T02:36:23.696230Z",
     "iopub.status.idle": "2025-04-14T02:36:23.700521Z",
     "shell.execute_reply": "2025-04-14T02:36:23.699488Z"
    },
    "papermill": {
     "duration": 0.023264,
     "end_time": "2025-04-14T02:36:23.702285",
     "exception": false,
     "start_time": "2025-04-14T02:36:23.679021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4214af95",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.014733,
     "end_time": "2025-04-14T02:36:23.731679",
     "exception": false,
     "start_time": "2025-04-14T02:36:23.716946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RevisedLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6d14f4",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:23.761820Z",
     "iopub.status.busy": "2025-04-14T02:36:23.761489Z",
     "iopub.status.idle": "2025-04-14T02:36:23.769080Z",
     "shell.execute_reply": "2025-04-14T02:36:23.768321Z"
    },
    "papermill": {
     "duration": 0.025191,
     "end_time": "2025-04-14T02:36:23.770881",
     "exception": false,
     "start_time": "2025-04-14T02:36:23.745690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RevisedLayerNorm(nn.Module):\n",
    "    \"\"\"Revised LayerNorm\"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n",
    "        super(RevisedLayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.detach_gradient = detach_gradient\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n",
    "        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n",
    "\n",
    "        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "\n",
    "        trunc_normal_(self.scale_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.scale_mlp.bias, 1)\n",
    "\n",
    "        trunc_normal_(self.shift_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.shift_mlp.bias, 0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n",
    "        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n",
    "\n",
    "        normalized_tensor = (input_tensor - mean_value) / std_value\n",
    "\n",
    "        if self.detach_gradient:\n",
    "            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n",
    "        else:\n",
    "            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n",
    "\n",
    "        output = normalized_tensor * self.scale + self.shift\n",
    "        return output, rescale, rebias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af209b59",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:23.800788Z",
     "iopub.status.busy": "2025-04-14T02:36:23.800517Z",
     "iopub.status.idle": "2025-04-14T02:36:23.809773Z",
     "shell.execute_reply": "2025-04-14T02:36:23.808846Z"
    },
    "papermill": {
     "duration": 0.026167,
     "end_time": "2025-04-14T02:36:23.811258",
     "exception": false,
     "start_time": "2025-04-14T02:36:23.785091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n",
    "        super().__init__()\n",
    "        output_channels = output_channels or input_channels\n",
    "        hidden_channels = hidden_channels or input_channels\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            gain = (8 * self.depth) ** (-1 / 4)\n",
    "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "            trunc_normal_(layer.weight, std=std)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "\n",
    "def partition_into_windows(tensor, window_size):\n",
    "    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n",
    "    batch_size, height, width, channels = tensor.shape\n",
    "    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n",
    "\n",
    "    tensor = tensor.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n",
    "    )\n",
    "    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, height, width):\n",
    "    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n",
    "    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n",
    "    tensor = windows.view(\n",
    "        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77124a6",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.013953,
     "end_time": "2025-04-14T02:36:23.839565",
     "exception": false,
     "start_time": "2025-04-14T02:36:23.825612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded404c4",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:23.869943Z",
     "iopub.status.busy": "2025-04-14T02:36:23.869656Z",
     "iopub.status.idle": "2025-04-14T02:36:23.900606Z",
     "shell.execute_reply": "2025-04-14T02:36:23.899339Z"
    },
    "papermill": {
     "duration": 0.04876,
     "end_time": "2025-04-14T02:36:23.902469",
     "exception": false,
     "start_time": "2025-04-14T02:36:23.853709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 16, 16]),\n",
       " torch.Size([32, 16, 64]),\n",
       " torch.Size([2, 16, 16, 64]),\n",
       " True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the MultiLayerPerceptron with sample parameters\n",
    "depth = 4\n",
    "input_channels = 64\n",
    "hidden_channels = 128\n",
    "output_channels = 64\n",
    "\n",
    "mlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n",
    "\n",
    "# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\n",
    "input_tensor = torch.randn(2, 64, 16, 16)\n",
    "output_tensor = mlp(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "mlp_output_shape = output_tensor.shape\n",
    "\n",
    "# Test window partition and merging\n",
    "batch_size, height, width, channels = 2, 16, 16, 64\n",
    "window_size = 4\n",
    "\n",
    "# Create a random tensor for window functions (B, H, W, C) format\n",
    "input_window_tensor = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Apply partitioning and merging\n",
    "windows = partition_into_windows(input_window_tensor, window_size)\n",
    "reconstructed_tensor = merge_windows(windows, window_size, height, width)\n",
    "\n",
    "# Check shapes\n",
    "windows_shape = windows.shape\n",
    "reconstructed_shape = reconstructed_tensor.shape\n",
    "\n",
    "# Validate if the reconstruction matches the original input shape\n",
    "is_shape_correct = reconstructed_shape == input_window_tensor.shape\n",
    "\n",
    "# Output results\n",
    "mlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff932cb5",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:23.934325Z",
     "iopub.status.busy": "2025-04-14T02:36:23.934063Z",
     "iopub.status.idle": "2025-04-14T02:36:23.942408Z",
     "shell.execute_reply": "2025-04-14T02:36:23.941378Z"
    },
    "papermill": {
     "duration": 0.025339,
     "end_time": "2025-04-14T02:36:23.943690",
     "exception": false,
     "start_time": "2025-04-14T02:36:23.918351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, window_size, num_heads):\n",
    "        \"\"\"Self-attention mechanism within local windows.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # (height, width)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n",
    "\n",
    "        # Compute and store relative positional encodings\n",
    "        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n",
    "        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n",
    "\n",
    "        # Learnable transformation of relative position embeddings\n",
    "        self.relative_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_heads, bias=True)\n",
    "        )\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n",
    "        batch_size, num_tokens, _ = qkv.shape\n",
    "\n",
    "        # Reshape qkv into separate query, key, and value tensors\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n",
    "\n",
    "        # Scale query for stable attention computation\n",
    "        q = q * self.scaling_factor\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Compute relative position bias\n",
    "        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n",
    "        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n",
    "        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n",
    "\n",
    "        # Apply softmax and compute weighted values\n",
    "        attention_weights = self.attention_softmax(attention_scores)\n",
    "        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6209f5c",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:23.973153Z",
     "iopub.status.busy": "2025-04-14T02:36:23.972926Z",
     "iopub.status.idle": "2025-04-14T02:36:23.977343Z",
     "shell.execute_reply": "2025-04-14T02:36:23.976690Z"
    },
    "papermill": {
     "duration": 0.021702,
     "end_time": "2025-04-14T02:36:23.979067",
     "exception": false,
     "start_time": "2025-04-14T02:36:23.957365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_log_relative_positions(window_size):\n",
    "    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n",
    "    coord_range = torch.arange(window_size)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n",
    "\n",
    "    # Compute relative positions\n",
    "    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n",
    "\n",
    "    # Format and apply log transformation\n",
    "    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n",
    "    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "    return log_relative_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e3c2796",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.008636Z",
     "iopub.status.busy": "2025-04-14T02:36:24.008391Z",
     "iopub.status.idle": "2025-04-14T02:36:24.026191Z",
     "shell.execute_reply": "2025-04-14T02:36:24.025235Z"
    },
    "papermill": {
     "duration": 0.034449,
     "end_time": "2025-04-14T02:36:24.027850",
     "exception": false,
     "start_time": "2025-04-14T02:36:23.993401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n",
    "        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.network_depth = network_depth\n",
    "        self.enable_attention = enable_attention\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "        # Define convolutional processing based on mode\n",
    "        if self.conv_mode == 'Conv':\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "            )\n",
    "\n",
    "        if self.conv_mode == 'DWConv':\n",
    "            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n",
    "            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight_shape = module.weight.shape\n",
    "\n",
    "            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "            else:\n",
    "                gain = (8 * self.network_depth) ** (-1/4)\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def pad_for_window_processing(self, x, shift=False):\n",
    "        \"\"\"Pads the input tensor to fit window processing requirements.\"\"\"\n",
    "        _, _, height, width = x.size()\n",
    "        pad_h = (self.window_size - height % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - width % self.window_size) % self.window_size\n",
    "\n",
    "        if shift:\n",
    "            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n",
    "                          self.shift_size, (self.window_size - self.shift_size + pad_h) % self.window_size), mode='reflect')\n",
    "        else:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            v_proj = self.value_projection(x)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            qk_proj = self.query_key_projection(x)\n",
    "            qkv = torch.cat([qk_proj, v_proj], dim=1)\n",
    "\n",
    "            # Apply padding for shifted window processing\n",
    "            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n",
    "            padded_height, padded_width = padded_qkv.shape[2:]\n",
    "\n",
    "            # Partition into windows\n",
    "            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n",
    "            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n",
    "\n",
    "            # Apply window-based attention\n",
    "            attn_windows = self.window_attention(qkv_windows)\n",
    "\n",
    "            # Merge back to original spatial dimensions\n",
    "            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n",
    "\n",
    "            # Reverse the cyclic shift\n",
    "            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), self.shift_size:(self.shift_size + width), :]\n",
    "            attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.conv_mode in ['Conv', 'DWConv']:\n",
    "                conv_output = self.conv_layer(v_proj)\n",
    "                output = self.output_projection(conv_output + attn_output)\n",
    "            else:\n",
    "                output = self.output_projection(attn_output)\n",
    "\n",
    "        else:\n",
    "            if self.conv_mode == 'Conv':\n",
    "                output = self.conv_layer(x)  # No attention, using convolution only\n",
    "            elif self.conv_mode == 'DWConv':\n",
    "                output = self.output_projection(self.conv_layer(v_proj))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "278daf67",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.057580Z",
     "iopub.status.busy": "2025-04-14T02:36:24.057336Z",
     "iopub.status.idle": "2025-04-14T02:36:24.069836Z",
     "shell.execute_reply": "2025-04-14T02:36:24.068825Z"
    },
    "papermill": {
     "duration": 0.029075,
     "end_time": "2025-04-14T02:36:24.071214",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.042139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n",
    "                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n",
    "        \"\"\"\n",
    "        A transformer block that includes attention (optional) and MLP layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enable_attention = enable_attention\n",
    "        self.enable_mlp_norm = enable_mlp_norm\n",
    "\n",
    "        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n",
    "        self.attention_layer = AdaptiveAttention(\n",
    "            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n",
    "            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n",
    "        )\n",
    "\n",
    "        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n",
    "        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.enable_attention:\n",
    "            x, rescale, rebias = self.pre_norm(x)\n",
    "        x = self.attention_layer(x)\n",
    "        if self.enable_attention:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        residual = x\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x, rescale, rebias = self.post_norm(x)\n",
    "        x = self.mlp_layer(x)\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, window_size=8,\n",
    "                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n",
    "        \"\"\"\n",
    "        A stage of transformer blocks with configurable attention placement.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        attention_layers = int(attention_ratio * num_layers)\n",
    "\n",
    "        if attention_placement == 'last':\n",
    "            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'first':\n",
    "            enable_attentions = [i < attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'middle':\n",
    "            enable_attentions = [\n",
    "                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(\n",
    "                network_depth=network_depth,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                enable_attention=enable_attentions[i],\n",
    "                conv_mode=conv_mode\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer stage.\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3012507",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.102252Z",
     "iopub.status.busy": "2025-04-14T02:36:24.101952Z",
     "iopub.status.idle": "2025-04-14T02:36:24.109105Z",
     "shell.execute_reply": "2025-04-14T02:36:24.108223Z"
    },
    "papermill": {
     "duration": 0.025354,
     "end_time": "2025-04-14T02:36:24.110728",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.085374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch embedding module that projects input images into token embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to generate patch embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class PatchReconstruction(nn.Module):\n",
    "    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch reconstruction module that converts token embeddings back to image patches.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2, padding_mode='reflect'\n",
    "            ),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to reconstruct image from embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d14b9332",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.142712Z",
     "iopub.status.busy": "2025-04-14T02:36:24.142357Z",
     "iopub.status.idle": "2025-04-14T02:36:24.150500Z",
     "shell.execute_reply": "2025-04-14T02:36:24.149563Z"
    },
    "papermill": {
     "duration": 0.026415,
     "end_time": "2025-04-14T02:36:24.152090",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.125675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelectiveKernelFusion(nn.Module):\n",
    "    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n",
    "        \"\"\"\n",
    "        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n",
    "\n",
    "        Args:\n",
    "            channels (int): Number of input channels.\n",
    "            num_branches (int): Number of feature branches to fuse.\n",
    "            reduction_ratio (int): Reduction ratio for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_branches = num_branches\n",
    "        reduced_channels = max(int(channels / reduction_ratio), 4)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Forward pass for selective kernel fusion.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (list of tensors): A list of feature maps to be fused.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adaptively fused feature map.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = feature_maps[0].shape\n",
    "        \n",
    "        # Concatenate feature maps along a new dimension (num_branches)\n",
    "        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        aggregated_features = torch.sum(stacked_features, dim=1)\n",
    "        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n",
    "        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n",
    "\n",
    "        # Weighted sum of input feature maps\n",
    "        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b851da7b",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.183111Z",
     "iopub.status.busy": "2025-04-14T02:36:24.182848Z",
     "iopub.status.idle": "2025-04-14T02:36:24.201187Z",
     "shell.execute_reply": "2025-04-14T02:36:24.200032Z"
    },
    "papermill": {
     "duration": 0.035796,
     "end_time": "2025-04-14T02:36:24.202501",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.166705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DehazingTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "                 layer_depths=[16, 16, 16, 8, 8],\n",
    "                 num_heads=[2, 4, 6, 1, 1],\n",
    "                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding settings\n",
    "        self.patch_size = 4\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # Backbone layers\n",
    "        self.encoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[0],\n",
    "            num_layers=layer_depths[0],\n",
    "            num_heads=num_heads[0],\n",
    "            mlp_ratio=mlp_ratios[0],\n",
    "            norm_layer=norm_layers[0],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[0],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[0]\n",
    "        )\n",
    "        \n",
    "        self.downsample1 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "        \n",
    "        self.encoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[1],\n",
    "            num_layers=layer_depths[1],\n",
    "            num_heads=num_heads[1],\n",
    "            mlp_ratio=mlp_ratios[1],\n",
    "            norm_layer=norm_layers[1],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[1],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[1]\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "        \n",
    "        self.encoder_stage3 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[2],\n",
    "            num_layers=layer_depths[2],\n",
    "            num_heads=num_heads[2],\n",
    "            mlp_ratio=mlp_ratios[2],\n",
    "            norm_layer=norm_layers[2],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[2],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[2]\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n",
    "        \n",
    "        self.decoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[3],\n",
    "            num_layers=layer_depths[3],\n",
    "            num_heads=num_heads[3],\n",
    "            mlp_ratio=mlp_ratios[3],\n",
    "            norm_layer=norm_layers[3],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[3],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[3]\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n",
    "        \n",
    "        self.decoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[4],\n",
    "            num_layers=layer_depths[4],\n",
    "            num_heads=num_heads[4],\n",
    "            mlp_ratio=mlp_ratios[4],\n",
    "            norm_layer=norm_layers[4],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[4],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[4]\n",
    "        )\n",
    "\n",
    "        # Final patch reconstruction\n",
    "        self.patch_reconstruction = PatchReconstruction(\n",
    "            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def adjust_image_size(self, x):\n",
    "        # Ensures the input image size is compatible with the patch size\n",
    "        _, _, height, width = x.size()\n",
    "        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n",
    "        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder_stage1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.downsample1(x)\n",
    "        x = self.encoder_stage2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.downsample2(x)\n",
    "        x = self.encoder_stage3(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n",
    "        x = self.decoder_stage1(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n",
    "        x = self.decoder_stage2(x)\n",
    "        x = self.patch_reconstruction(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_height, original_width = x.shape[2:]\n",
    "        x = self.adjust_image_size(x)\n",
    "\n",
    "        features = self.extract_features(x)\n",
    "        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n",
    "\n",
    "        # Dehazing formula: I = J * t + A * (1 - t)\n",
    "        x = transmission_map * x - atmospheric_light + x\n",
    "        x = x[:, :, :original_height, :original_width]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef36d36",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.232785Z",
     "iopub.status.busy": "2025-04-14T02:36:24.232539Z",
     "iopub.status.idle": "2025-04-14T02:36:24.237069Z",
     "shell.execute_reply": "2025-04-14T02:36:24.236369Z"
    },
    "papermill": {
     "duration": 0.022117,
     "end_time": "2025-04-14T02:36:24.238842",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.216725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dehazing_transformer():\n",
    "    return DehazingTransformer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "        layer_depths=[12, 12, 12, 6, 6],\n",
    "        num_heads=[2, 4, 6, 1, 1],\n",
    "        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ddf6647",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.268371Z",
     "iopub.status.busy": "2025-04-14T02:36:24.268113Z",
     "iopub.status.idle": "2025-04-14T02:36:24.276383Z",
     "shell.execute_reply": "2025-04-14T02:36:24.275749Z"
    },
    "papermill": {
     "duration": 0.025108,
     "end_time": "2025-04-14T02:36:24.278260",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.253152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvolutionalGuidedFilter(nn.Module):\n",
    "    def __init__(self, radius=1, norm_layer=nn.BatchNorm2d, conv_kernel_size: int = 1):\n",
    "        super(ConvolutionalGuidedFilter, self).__init__()\n",
    "\n",
    "        self.box_filter = nn.Conv2d(\n",
    "            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n",
    "        )\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                6,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                3,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.box_filter.weight.data[...] = 1.0\n",
    "\n",
    "    def forward(self, x_low_res, y_low_res, x_high_res):\n",
    "        _, _, h_lr, w_lr = x_low_res.size()\n",
    "        _, _, h_hr, w_hr = x_high_res.size()\n",
    "\n",
    "        N = self.box_filter(x_low_res.data.new().resize_((1, 3, h_lr, w_lr)).fill_(1.0))\n",
    "        ## mean_x\n",
    "        mean_x = self.box_filter(x_low_res) / N\n",
    "        ## mean_y\n",
    "        mean_y = self.box_filter(y_low_res) / N\n",
    "        ## cov_xy\n",
    "        cov_xy = self.box_filter(x_low_res * y_low_res) / N - mean_x * mean_y\n",
    "        ## var_x\n",
    "        var_x = self.box_filter(x_low_res * x_low_res) / N - mean_x * mean_x\n",
    "\n",
    "        ## A\n",
    "        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n",
    "        ## b\n",
    "        b = mean_y - A * mean_x\n",
    "\n",
    "        ## mean_A; mean_b\n",
    "        mean_A = F.interpolate(A, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "        mean_b = F.interpolate(b, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return mean_A * x_high_res + mean_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8109b29",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.307139Z",
     "iopub.status.busy": "2025-04-14T02:36:24.306927Z",
     "iopub.status.idle": "2025-04-14T02:36:24.313331Z",
     "shell.execute_reply": "2025-04-14T02:36:24.312697Z"
    },
    "papermill": {
     "duration": 0.022679,
     "end_time": "2025-04-14T02:36:24.314864",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.292185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(PixelAttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, 1, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_map = self.attention(x)\n",
    "        return x * attention_map\n",
    "\n",
    "class ChannelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelAttentionLayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, channels, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = self.avg_pool(x)\n",
    "        attention_map = self.attention(pooled)\n",
    "        return x * attention_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9adfd5b9",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.343838Z",
     "iopub.status.busy": "2025-04-14T02:36:24.343624Z",
     "iopub.status.idle": "2025-04-14T02:36:24.351872Z",
     "shell.execute_reply": "2025-04-14T02:36:24.350955Z"
    },
    "papermill": {
     "duration": 0.024747,
     "end_time": "2025-04-14T02:36:24.353343",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.328596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SuperResolutionDilationBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_dense_layers, growth_rate):\n",
    "        super(SuperResolutionDilationBlock, self).__init__()\n",
    "\n",
    "        self.split_channels = in_channels // 4\n",
    "        kernel_size = 3\n",
    "\n",
    "        # Dilated convolutions with increasing dilation rates\n",
    "        self.conv1 = nn.Conv2d(self.split_channels, self.split_channels, kernel_size=kernel_size, padding=1, dilation=1)\n",
    "        self.conv2 = nn.Conv2d(self.split_channels * 2, self.split_channels, kernel_size=kernel_size, padding=2, dilation=2)\n",
    "        self.conv3 = nn.Conv2d(self.split_channels * 3, self.split_channels, kernel_size=kernel_size, padding=4, dilation=4)\n",
    "        self.conv4 = nn.Conv2d(self.split_channels * 4, self.split_channels, kernel_size=kernel_size, padding=8, dilation=8)\n",
    "\n",
    "        # Attention mechanisms\n",
    "        self.channel_attention = ChannelAttentionLayer(in_channels)\n",
    "        self.pixel_attention = PixelAttentionLayer(in_channels)\n",
    "\n",
    "        # Final 1x1 convolution for feature fusion\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into 4 equal parts along channel dimension\n",
    "        split_features = torch.split(x, self.split_channels, dim=1)\n",
    "\n",
    "        x0 = F.relu(self.conv1(split_features[0]))\n",
    "        tmp = torch.cat((split_features[1], x0), dim=1)\n",
    "        x1 = F.relu(self.conv2(tmp))\n",
    "\n",
    "        tmp = torch.cat((split_features[2], x0, x1), dim=1)\n",
    "        x2 = F.relu(self.conv3(tmp))\n",
    "\n",
    "        tmp = torch.cat((split_features[3], x0, x1, x2), dim=1)\n",
    "        x3 = F.relu(self.conv4(tmp))\n",
    "\n",
    "        # Concatenate all outputs\n",
    "        merged_features = torch.cat((x0, x1, x2, x3), dim=1)\n",
    "\n",
    "        # Apply 1x1 convolution for feature refinement\n",
    "        out = self.conv_1x1(merged_features)\n",
    "\n",
    "        # Apply attention mechanisms\n",
    "        out = self.channel_attention(out)\n",
    "        out = self.pixel_attention(out)\n",
    "\n",
    "        # Residual connection\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb6d9a51",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.382309Z",
     "iopub.status.busy": "2025-04-14T02:36:24.382084Z",
     "iopub.status.idle": "2025-04-14T02:36:24.386374Z",
     "shell.execute_reply": "2025-04-14T02:36:24.385743Z"
    },
    "papermill": {
     "duration": 0.020833,
     "end_time": "2025-04-14T02:36:24.387905",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.367072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNormalization(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(AdaptiveInstanceNormalization, self).__init__()\n",
    "\n",
    "        # Learnable scaling factors\n",
    "        self.scale_x = nn.Parameter(torch.tensor(1.0))  # Identity scaling\n",
    "        self.scale_norm = nn.Parameter(torch.tensor(0.0))  # Initially no effect\n",
    "\n",
    "        # Instance normalization layer with affine transformation enabled\n",
    "        self.instance_norm = nn.InstanceNorm2d(num_channels, momentum=0.999, eps=0.001, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized_x = self.instance_norm(x)\n",
    "        return self.scale_x * x + self.scale_norm * normalized_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "744202c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.416735Z",
     "iopub.status.busy": "2025-04-14T02:36:24.416530Z",
     "iopub.status.idle": "2025-04-14T02:36:24.424331Z",
     "shell.execute_reply": "2025-04-14T02:36:24.423640Z"
    },
    "papermill": {
     "duration": 0.024676,
     "end_time": "2025-04-14T02:36:24.426349",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.401673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepGuidedNetwork(nn.Module):\n",
    "    def __init__(self, radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Adaptive Normalization for Guided Filtering\n",
    "        norm = AdaptiveInstanceNormalization\n",
    "        kernel_size = 3\n",
    "        depth_rate = 16\n",
    "        in_channels = 3\n",
    "        num_dense_layer = 4\n",
    "        growth_rate = 16\n",
    "\n",
    "        # Initial convolution layers\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "        # Residual Dense Blocks (RDBs)\n",
    "        self.rdb1 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb2 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb3 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb4 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "\n",
    "        # Guided Filter & Dehazing Transformer\n",
    "        self.guided_filter = ConvolutionalGuidedFilter(radius, norm_layer=norm)\n",
    "        self.dehaze_network = build_dehazing_transformer()\n",
    "\n",
    "        # Downsampling & Upsampling Layers\n",
    "        self.downsample = nn.Upsample(scale_factor=0.5, mode=\"bilinear\", align_corners=True)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "    def forward(self, x_hr):\n",
    "        x_lr = self.downsample(x_hr)\n",
    "    \n",
    "        # Initial conv\n",
    "        y_features = self.conv_in(x_lr)\n",
    "    \n",
    "        # RDBs + collect features\n",
    "        feat1 = self.rdb1(y_features)\n",
    "        feat2 = self.rdb2(feat1)\n",
    "        feat3 = self.rdb3(feat2)\n",
    "        feat4 = self.rdb4(feat3)\n",
    "        y_detail = self.conv_out(feat4)\n",
    "    \n",
    "        # Base image\n",
    "        y_base = self.dehaze_network(x_lr)\n",
    "    \n",
    "        # Combine\n",
    "        y_lr = y_base + y_detail\n",
    "        y_base_hr = self.upsample(y_base)\n",
    "    \n",
    "        # Guided output\n",
    "        refined_output = self.guided_filter(x_lr, y_lr, x_hr)\n",
    "    \n",
    "        return refined_output, y_base_hr, [feat1, feat2, feat3, feat4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecef958b",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.455485Z",
     "iopub.status.busy": "2025-04-14T02:36:24.455249Z",
     "iopub.status.idle": "2025-04-14T02:36:24.463351Z",
     "shell.execute_reply": "2025-04-14T02:36:24.462326Z"
    },
    "papermill": {
     "duration": 0.024597,
     "end_time": "2025-04-14T02:36:24.464861",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.440264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class DeepGuidedNetwork(nn.Module):\n",
    "#     def __init__(self, radius=1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Adaptive Normalization for Guided Filtering\n",
    "#         norm = AdaptiveInstanceNormalization\n",
    "#         kernel_size = 3\n",
    "#         depth_rate = 16\n",
    "#         in_channels = 3\n",
    "#         num_dense_layer = 4\n",
    "#         growth_rate = 16\n",
    "\n",
    "#         # Initial convolution layers\n",
    "#         self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "#         self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "#         # Residual Dense Blocks (RDBs)\n",
    "#         self.rdb1 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "#         self.rdb2 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "#         self.rdb3 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "#         self.rdb4 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "\n",
    "#         # Guided Filter & Dehazing Transformer\n",
    "#         self.guided_filter = ConvolutionalGuidedFilter(radius, norm_layer=norm)\n",
    "#         self.dehaze_network = build_dehazing_transformer()\n",
    "\n",
    "#         # Downsampling & Upsampling Layers\n",
    "#         self.downsample = nn.Upsample(scale_factor=0.5, mode=\"bilinear\", align_corners=True)\n",
    "#         self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "#     def forward(self, x_hr):\n",
    "#         # Low-resolution processing\n",
    "#         x_lr = self.downsample(x_hr)\n",
    "\n",
    "#         # Detail extraction through Residual Dense Blocks\n",
    "#         y_features = self.conv_in(x_lr)\n",
    "#         y_features = self.rdb1(y_features)\n",
    "#         y_features = self.rdb2(y_features)\n",
    "#         y_features = self.rdb3(y_features)\n",
    "#         y_features = self.rdb4(y_features)\n",
    "#         y_detail = self.conv_out(y_features)\n",
    "\n",
    "#         # Base image estimation using DehazeFormer\n",
    "#         y_base = self.dehaze_network(x_lr)\n",
    "\n",
    "#         # Combining base and details\n",
    "#         y_lr = y_base + y_detail\n",
    "#         y_base_hr = self.upsample(y_base)\n",
    "\n",
    "#         # Final guided filtering refinement\n",
    "#         refined_output = self.guided_filter(x_lr, y_lr, x_hr)\n",
    "        \n",
    "#         return refined_output, y_base_hr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88e2929d",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.493604Z",
     "iopub.status.busy": "2025-04-14T02:36:24.493396Z",
     "iopub.status.idle": "2025-04-14T02:36:24.496941Z",
     "shell.execute_reply": "2025-04-14T02:36:24.496178Z"
    },
    "papermill": {
     "duration": 0.020137,
     "end_time": "2025-04-14T02:36:24.498598",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.478461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_crop_size(crop_size_str):\n",
    "    try:\n",
    "        return [int(x.strip()) for x in crop_size_str.split(',')]\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid crop size format: '{crop_size_str}'. Expected comma-separated integers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc42e2cf",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.527352Z",
     "iopub.status.busy": "2025-04-14T02:36:24.527111Z",
     "iopub.status.idle": "2025-04-14T02:36:24.539516Z",
     "shell.execute_reply": "2025-04-14T02:36:24.538498Z"
    },
    "papermill": {
     "duration": 0.028583,
     "end_time": "2025-04-14T02:36:24.540911",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.512328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from random import randrange\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self, crop_size, hazeeffected_images_dir, hazefree_images_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Ensure valid file extensions --- #\n",
    "        valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        hazy_data = [\n",
    "            f for f in glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_extensions)\n",
    "        ]\n",
    "\n",
    "        if not hazy_data:\n",
    "            raise ValueError(f\"No valid images found in {hazeeffected_images_dir}\")\n",
    "\n",
    "        self.hazeeffected_images_dir = hazeeffected_images_dir\n",
    "        self.hazefree_images_dir = hazefree_images_dir\n",
    "\n",
    "        self.haze_names = []\n",
    "        self.gt_names = []\n",
    "        \n",
    "        for h_image in hazy_data:\n",
    "            filename = os.path.basename(h_image)\n",
    "            haze_path = os.path.join(self.hazeeffected_images_dir, filename)\n",
    "            gt_path = os.path.join(self.hazefree_images_dir, filename)\n",
    "\n",
    "            if not os.path.exists(gt_path):\n",
    "                print(f\"Warning: Ground-truth missing for {filename}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            self.haze_names.append(haze_path)\n",
    "            self.gt_names.append(gt_path)\n",
    "\n",
    "        if not self.haze_names:\n",
    "            raise ValueError(\"No matching ground-truth images found.\")\n",
    "\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def get_images(self, index):\n",
    "        crop_width, crop_height = self.crop_size\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        try:\n",
    "            haze_img = Image.open(haze_name).convert('RGB')\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Invalid image format: {haze_name} or {gt_name}\")\n",
    "\n",
    "        width, height = haze_img.size\n",
    "\n",
    "        # --- Handle small images --- #\n",
    "        if width < crop_width or height < crop_height:\n",
    "            raise ValueError(f\"Image too small for cropping: {haze_name}\")\n",
    "\n",
    "        # --- Random crop --- #\n",
    "        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n",
    "        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "\n",
    "        # --- Transform to tensor --- #\n",
    "        transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        transform_gt = Compose([ToTensor()])\n",
    "        haze = transform_haze(haze_crop_img)\n",
    "        gt = transform_gt(gt_crop_img)\n",
    "\n",
    "        # --- Check channels --- #\n",
    "        if haze.shape[0] != 3 or gt.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {haze_name}\")\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_images(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc7f9ae3",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.571180Z",
     "iopub.status.busy": "2025-04-14T02:36:24.570935Z",
     "iopub.status.idle": "2025-04-14T02:36:24.583022Z",
     "shell.execute_reply": "2025-04-14T02:36:24.582106Z"
    },
    "papermill": {
     "duration": 0.029298,
     "end_time": "2025-04-14T02:36:24.584666",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.555368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from random import randrange, shuffle\n",
    "\n",
    "class HazeDataset(Dataset):\n",
    "    def __init__(self, crop_size, hazeeffected_images_dir, hazefree_images_dir, split=\"train\", split_ratio=0.8):\n",
    "        \"\"\"\n",
    "        Dataset class for handling both training and validation dynamically.\n",
    "        \n",
    "        Args:\n",
    "            crop_size (tuple): (width, height) of the random crop.\n",
    "            hazeeffected_images_dir (str): Directory for hazy images.\n",
    "            hazefree_images_dir (str): Directory for ground-truth images.\n",
    "            split (str): \"train\" or \"valid\" (determines data split).\n",
    "            split_ratio (float): Percentage of images to use for training (default 80% train, 20% validation).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Ensure valid file extensions --- #\n",
    "        valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        hazy_data = [\n",
    "            f for f in glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_extensions)\n",
    "        ]\n",
    "\n",
    "        if not hazy_data:\n",
    "            raise ValueError(f\"No valid images found in {hazeeffected_images_dir}\")\n",
    "\n",
    "        # # --- Sort and shuffle to ensure random split --- #\n",
    "        hazy_data.sort()\n",
    "        # shuffle(hazy_data)  \n",
    "\n",
    "        # --- Split into train and validation --- #\n",
    "        split_idx = int(len(hazy_data) * split_ratio)\n",
    "        if split == \"train\":\n",
    "            hazy_data = hazy_data[:split_idx]\n",
    "        else:  # \"valid\"\n",
    "            hazy_data = hazy_data[split_idx:]\n",
    "\n",
    "        self.haze_names = []\n",
    "        self.gt_names = []\n",
    "        \n",
    "        for h_image in hazy_data:\n",
    "            filename = os.path.basename(h_image)\n",
    "            haze_path = os.path.join(hazeeffected_images_dir, filename)\n",
    "            gt_path = os.path.join(hazefree_images_dir, filename)\n",
    "\n",
    "            if not os.path.exists(gt_path):\n",
    "                print(f\"Warning: Ground-truth missing for {filename}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            self.haze_names.append(haze_path)\n",
    "            self.gt_names.append(gt_path)\n",
    "\n",
    "        if not self.haze_names:\n",
    "            raise ValueError(\"No matching ground-truth images found.\")\n",
    "\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def get_images(self, index):\n",
    "        crop_width, crop_height = self.crop_size\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        try:\n",
    "            haze_img = Image.open(haze_name).convert('RGB')\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Invalid image format: {haze_name} or {gt_name}\")\n",
    "\n",
    "        width, height = haze_img.size\n",
    "\n",
    "        # --- Handle small images --- #\n",
    "        if width < crop_width or height < crop_height:\n",
    "            raise ValueError(f\"Image too small for cropping: {haze_name}\")\n",
    "\n",
    "        # --- Random crop --- #\n",
    "        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n",
    "        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "\n",
    "        # --- Transform to tensor --- #\n",
    "        transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        transform_gt = Compose([ToTensor()])\n",
    "        haze = transform_haze(haze_crop_img)\n",
    "        gt = transform_gt(gt_crop_img)\n",
    "\n",
    "        # --- Check channels --- #\n",
    "        if haze.shape[0] != 3 or gt.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {haze_name}\")\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_images(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc714ae",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.013681,
     "end_time": "2025-04-14T02:36:24.611981",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.598300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "570dd69e",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.641305Z",
     "iopub.status.busy": "2025-04-14T02:36:24.641042Z",
     "iopub.status.idle": "2025-04-14T02:36:24.645459Z",
     "shell.execute_reply": "2025-04-14T02:36:24.644608Z"
    },
    "papermill": {
     "duration": 0.021278,
     "end_time": "2025-04-14T02:36:24.646844",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.625566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_psnr(dehaze, gt):\n",
    "    \"\"\"\n",
    "    Compute PSNR (Peak Signal-to-Noise Ratio) between dehazed and ground truth images.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: PSNR values for each image in the batch.\n",
    "    \"\"\"\n",
    "    mse = F.mse_loss(dehaze, gt, reduction='none').mean(dim=[1, 2, 3])  # Compute MSE per image\n",
    "    intensity_max = 1.0\n",
    "\n",
    "    # Compute PSNR safely, avoiding division by zero and extreme values\n",
    "    psnr_list = [10.0 * log10(intensity_max / max(mse_val.item(), 1e-6)) for mse_val in mse]\n",
    "\n",
    "    return psnr_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc393006",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:24.675829Z",
     "iopub.status.busy": "2025-04-14T02:36:24.675612Z",
     "iopub.status.idle": "2025-04-14T02:36:33.696547Z",
     "shell.execute_reply": "2025-04-14T02:36:33.695478Z"
    },
    "papermill": {
     "duration": 9.037788,
     "end_time": "2025-04-14T02:36:33.698469",
     "exception": false,
     "start_time": "2025-04-14T02:36:24.660681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "# Define SSIM metric\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0, reduction='none')\n",
    "\n",
    "def to_ssim(dehaze: torch.Tensor, gt: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute SSIM directly on the GPU using torchmetrics.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: SSIM values for each image in the batch.\n",
    "    \"\"\"\n",
    "    ssim_values = ssim_metric(dehaze, gt)  # Shape: [B]\n",
    "    # print(\"1\",ssim_values)\n",
    "    # print(\"2\",[ssim_values])\n",
    "    ssim_values = ssim_values.tolist() \n",
    "    # print(type(ssim_values))\n",
    "    if isinstance(ssim_values, float):  # Correct way to check for a float\n",
    "        return [ssim_values]  # Convert single float to a list\n",
    "    return ssim_values  # Otherwise, return as is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed6628c8",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:33.729478Z",
     "iopub.status.busy": "2025-04-14T02:36:33.729021Z",
     "iopub.status.idle": "2025-04-14T02:36:33.776467Z",
     "shell.execute_reply": "2025-04-14T02:36:33.774673Z"
    },
    "papermill": {
     "duration": 0.064191,
     "end_time": "2025-04-14T02:36:33.777648",
     "exception": false,
     "start_time": "2025-04-14T02:36:33.713457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004686390049755573]\n"
     ]
    }
   ],
   "source": [
    "# Test with a dummy tensor\n",
    "dehaze = torch.rand(1, 3, 360, 360)  # Random batch of images\n",
    "gt = torch.rand(1, 3, 360, 360)  # Random ground truth images\n",
    "\n",
    "ssim_scores = to_ssim(dehaze, gt)\n",
    "print(ssim_scores)  # Should print a list of 6 SSIM values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "319221f8",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:33.811050Z",
     "iopub.status.busy": "2025-04-14T02:36:33.810778Z",
     "iopub.status.idle": "2025-04-14T02:36:33.816640Z",
     "shell.execute_reply": "2025-04-14T02:36:33.815822Z"
    },
    "papermill": {
     "duration": 0.023268,
     "end_time": "2025-04-14T02:36:33.818322",
     "exception": false,
     "start_time": "2025-04-14T02:36:33.795054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validationB(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: Your deep learning model\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: GPU/CPU device\n",
    "    :param category: dataset type (indoor/outdoor)\n",
    "    :param save_tag: whether to save images\n",
    "    :return: average PSNR & SSIM values\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    \n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            haze, gt = val_data\n",
    "            haze, gt = haze.to(device), gt.to(device)\n",
    "            dehaze, _ = net(haze)\n",
    "\n",
    "        # --- Compute PSNR & SSIM --- #\n",
    "        batch_psnr = to_psnr(dehaze, gt)  # This returns a list\n",
    "        # print(batch_psnr)\n",
    "        batch_ssim = to_ssim(dehaze, gt)  # This returns a list\n",
    "        # print(batch_ssim)\n",
    "\n",
    "        psnr_list.extend(batch_psnr)  # Flatten the list\n",
    "        ssim_list.extend(batch_ssim)  # Flatten the list\n",
    "\n",
    "    # --- Ensure lists are not empty to avoid division by zero --- #\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d91c319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:33.847662Z",
     "iopub.status.busy": "2025-04-14T02:36:33.847441Z",
     "iopub.status.idle": "2025-04-14T02:36:33.852357Z",
     "shell.execute_reply": "2025-04-14T02:36:33.851479Z"
    },
    "papermill": {
     "duration": 0.021349,
     "end_time": "2025-04-14T02:36:33.853813",
     "exception": false,
     "start_time": "2025-04-14T02:36:33.832464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation_sr(net, sr_val_loader, device):\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    for lr, hr in sr_val_loader:\n",
    "        with torch.no_grad():\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr_out, _ = net(lr)\n",
    "        psnr_list.extend(to_psnr(sr_out, hr))\n",
    "        ssim_list.extend(to_ssim(sr_out, hr))\n",
    "\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488e84a",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.013838,
     "end_time": "2025-04-14T02:36:33.881220",
     "exception": false,
     "start_time": "2025-04-14T02:36:33.867382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37ea80ea",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:33.909817Z",
     "iopub.status.busy": "2025-04-14T02:36:33.909554Z",
     "iopub.status.idle": "2025-04-14T02:36:33.912913Z",
     "shell.execute_reply": "2025-04-14T02:36:33.911998Z"
    },
    "papermill": {
     "duration": 0.019787,
     "end_time": "2025-04-14T02:36:33.914474",
     "exception": false,
     "start_time": "2025-04-14T02:36:33.894687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c2fdcb9",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:33.944167Z",
     "iopub.status.busy": "2025-04-14T02:36:33.943936Z",
     "iopub.status.idle": "2025-04-14T02:36:33.946962Z",
     "shell.execute_reply": "2025-04-14T02:36:33.946202Z"
    },
    "papermill": {
     "duration": 0.019867,
     "end_time": "2025-04-14T02:36:33.948536",
     "exception": false,
     "start_time": "2025-04-14T02:36:33.928669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# psnr, ssim = validationB(net, val_data_loader, device, category)\n",
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # psnr, ssim = validationB(model, val_loader, device, \"indoor\", save_tag=True)\n",
    "# print(f\"Validation PSNR: {psnr:.2f}, SSIM: {ssim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1f9ca33",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:33.977500Z",
     "iopub.status.busy": "2025-04-14T02:36:33.977290Z",
     "iopub.status.idle": "2025-04-14T02:36:33.985103Z",
     "shell.execute_reply": "2025-04-14T02:36:33.984189Z"
    },
    "papermill": {
     "duration": 0.023827,
     "end_time": "2025-04-14T02:36:33.986263",
     "exception": false,
     "start_time": "2025-04-14T02:36:33.962436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b1273ac60e419590e1e25b937355eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Execution Env:', options=('local', 'kaggle'), value='local')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execution_env_widget = widgets.Dropdown(options=['local', 'kaggle'], value='local', description='Execution Env:')\n",
    "display(execution_env_widget)\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "    execution_env_widget.value = 'kaggle' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "094b825e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:34.016135Z",
     "iopub.status.busy": "2025-04-14T02:36:34.015861Z",
     "iopub.status.idle": "2025-04-14T02:36:34.048956Z",
     "shell.execute_reply": "2025-04-14T02:36:34.047738Z"
    },
    "papermill": {
     "duration": 0.049712,
     "end_time": "2025-04-14T02:36:34.050105",
     "exception": false,
     "start_time": "2025-04-14T02:36:34.000393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1100583b924ab48b2753730b10eb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Learning Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de1f6c02ff349548ca97303a4571038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='128,128', description='Crop Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bc5b6ecf434b169579bb1583eabd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=6, description='Train Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f596cd722d9b4d2ab74527ff4774e0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=0, description='Version:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf8c6ab5350472386fd2f9718dea85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='Growth Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd25743293d5475387060b8ea433414b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.04, description='Lambda Loss:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2471ce9cae96466b841d9d2717a37d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Val Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a75abd8ce1a43e087220d15a414fb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', index=2, options=('indoor', 'outdoor', 'reside', 'nh'), value='reside')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters set:\n",
      "learning_rate: 0.0001\n",
      "crop_size: [128, 128]\n",
      "train_batch_size: 6\n",
      "version: 0\n",
      "growth_rate: 16\n",
      "lambda_loss: 0.04\n",
      "val_batch_size: 2\n",
      "category: reside\n",
      "execution_env: kaggle\n",
      "\n",
      "Final dataset paths:\n",
      "Training directory: /kaggle/input/reside6k/RESIDE-6K/train\n",
      "Validation directory: /kaggle/input/reside6k/RESIDE-6K/train\n",
      "Number of epochs: 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Create widgets for each hyper-parameter ---\n",
    "learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n",
    "crop_size_widget = widgets.Text(value='128,128', description='Crop Size:')\n",
    "train_batch_size_widget = widgets.IntText(value=6, description='Train Batch Size:')\n",
    "version_widget = widgets.IntText(value=0, description='Version:')\n",
    "growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n",
    "lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n",
    "val_batch_size_widget = widgets.IntText(value=2, description='Val Batch Size:')\n",
    "category_widget = widgets.Dropdown(options=['indoor', 'outdoor', 'reside', 'nh'], value='reside', description='Category:')\n",
    "\n",
    "# --- Display the widgets ---\n",
    "display(\n",
    "    learning_rate_widget, crop_size_widget, train_batch_size_widget, version_widget,\n",
    "    growth_rate_widget, lambda_loss_widget, \n",
    "    val_batch_size_widget, category_widget\n",
    ")\n",
    "\n",
    "# --- Function to parse crop size ---\n",
    "def parse_crop_size(crop_size_str):\n",
    "    return [int(x) for x in crop_size_str.split(',')]\n",
    "\n",
    "# --- Assign the widget values to variables ---\n",
    "learning_rate = learning_rate_widget.value\n",
    "crop_size = parse_crop_size(crop_size_widget.value)\n",
    "train_batch_size = train_batch_size_widget.value\n",
    "version = version_widget.value\n",
    "growth_rate = growth_rate_widget.value\n",
    "lambda_loss = lambda_loss_widget.value\n",
    "val_batch_size = val_batch_size_widget.value\n",
    "category = category_widget.value\n",
    "\n",
    "execution_env = execution_env_widget.value  # Local or Kaggle\n",
    "\n",
    "\n",
    "print('\\nHyper-parameters set:')\n",
    "print(f'learning_rate: {learning_rate}')\n",
    "print(f'crop_size: {crop_size}')\n",
    "print(f'train_batch_size: {train_batch_size}')\n",
    "print(f'version: {version}')\n",
    "print(f'growth_rate: {growth_rate}')\n",
    "print(f'lambda_loss: {lambda_loss}')\n",
    "print(f'val_batch_size: {val_batch_size}')\n",
    "print(f'category: {category}')\n",
    "print(f'execution_env: {execution_env}')\n",
    "\n",
    "# --- Set category-specific hyper-parameters ---\n",
    "if category == 'indoor':\n",
    "    num_epochs = 1500\n",
    "    train_data_dir = './data/train/indoor/'\n",
    "    val_data_dir = './data/test/SOTS/indoor/'\n",
    "elif category == 'outdoor':\n",
    "    num_epochs = 10\n",
    "    train_data_dir = './data/train/outdoor/'\n",
    "    val_data_dir = './data/test/SOTS/outdoor/'\n",
    "elif category == 'reside':\n",
    "    num_epochs = 50\n",
    "    train_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n",
    "    val_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n",
    "    test_data_dir = '/kaggle/input/reside6k/RESIDE-6K/test'\n",
    "elif category == 'nh':\n",
    "    num_epochs = 50\n",
    "    train_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "    val_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "else:\n",
    "    raise Exception('Wrong image category. Set it to indoor or outdoor for RESIDE dataset.')\n",
    "\n",
    "# --- Adjust paths based on execution environment ---\n",
    "# if execution_env == 'kaggle':\n",
    "    # train_data_dir = '/kaggle/input/reside-dataset/' + train_data_dir.strip('./')\n",
    "    # val_data_dir = '/kaggle/input/reside-dataset/' + val_data_dir.strip('./')\n",
    "    # train_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T'\n",
    "    # val_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V' \n",
    "    # train_data_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "    # val_data_dir = '/kaggle/input/o-haze/O-HAZY/GT' \n",
    "print('\\nFinal dataset paths:')\n",
    "print(f'Training directory: {train_data_dir}')\n",
    "print(f'Validation directory: {val_data_dir}')\n",
    "print(f'Number of epochs: {num_epochs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff601e8b",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:34.081954Z",
     "iopub.status.busy": "2025-04-14T02:36:34.081716Z",
     "iopub.status.idle": "2025-04-14T02:36:34.085136Z",
     "shell.execute_reply": "2025-04-14T02:36:34.084396Z"
    },
    "papermill": {
     "duration": 0.021081,
     "end_time": "2025-04-14T02:36:34.086501",
     "exception": false,
     "start_time": "2025-04-14T02:36:34.065420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hazeeffected_images_dir_train = f\"{train_data_dir}/IN\"\n",
    "hazeeffected_images_dir_train = f\"{train_data_dir}/hazy\"\n",
    "hazefree_images_dir_train = f\"{train_data_dir}/GT\"\n",
    "\n",
    "# hazeeffected_images_dir_valid = f\"{val_data_dir}/IN\"\n",
    "hazeeffected_images_dir_valid = f\"{val_data_dir}/hazy\"\n",
    "hazefree_images_dir_valid = f\"{val_data_dir}/GT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e30cd4",
   "metadata": {
    "papermill": {
     "duration": 0.01452,
     "end_time": "2025-04-14T02:36:34.115933",
     "exception": false,
     "start_time": "2025-04-14T02:36:34.101413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "247f9d47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:34.147935Z",
     "iopub.status.busy": "2025-04-14T02:36:34.147688Z",
     "iopub.status.idle": "2025-04-14T02:36:34.150840Z",
     "shell.execute_reply": "2025-04-14T02:36:34.150070Z"
    },
    "papermill": {
     "duration": 0.020969,
     "end_time": "2025-04-14T02:36:34.152491",
     "exception": false,
     "start_time": "2025-04-14T02:36:34.131522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75617e52",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:34.183559Z",
     "iopub.status.busy": "2025-04-14T02:36:34.183340Z",
     "iopub.status.idle": "2025-04-14T02:36:34.186962Z",
     "shell.execute_reply": "2025-04-14T02:36:34.186230Z"
    },
    "papermill": {
     "duration": 0.021404,
     "end_time": "2025-04-14T02:36:34.188525",
     "exception": false,
     "start_time": "2025-04-14T02:36:34.167121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import shutil\n",
    "\n",
    "# hazeeffected_images_dir_train = f\"{train_data_dir}/IN\"\n",
    "# hazefree_images_dir_train = f\"{train_data_dir}/GT\"\n",
    "\n",
    "# hazeeffected_images_dir_valid = f\"{val_data_dir}/IN\"\n",
    "# hazefree_images_dir_valid = f\"{val_data_dir}/GT\"\n",
    "\n",
    "# # Create validation directories if they don't exist\n",
    "# os.makedirs(hazeeffected_images_dir_valid, exist_ok=True)\n",
    "# os.makedirs(hazefree_images_dir_valid, exist_ok=True)\n",
    "\n",
    "# # List all hazy and clean images\n",
    "# hazy_images = sorted(glob.glob(f\"{hazeeffected_images_dir_train}/*\"))\n",
    "# clean_images = sorted(glob.glob(f\"{hazefree_images_dir_train}/*\"))\n",
    "\n",
    "# # Ensure matching hazy-clean pairs\n",
    "# assert len(hazy_images) == len(clean_images), \"Mismatch in hazy and clean images count!\"\n",
    "\n",
    "# # Shuffle while keeping the hazy-clean correspondence\n",
    "# paired_images = list(zip(hazy_images, clean_images))\n",
    "# # random.shuffle(paired_images)\n",
    "\n",
    "# # Define split ratio (e.g., 80% train, 20% validation)\n",
    "# split_ratio = 0.8\n",
    "# split_idx = int(len(paired_images) * split_ratio)\n",
    "\n",
    "# # Split into train and validation\n",
    "# train_pairs = paired_images[:split_idx]\n",
    "# valid_pairs = paired_images[split_idx:]\n",
    "\n",
    "# # Move validation images\n",
    "# for hazy_path, clean_path in valid_pairs:\n",
    "#     shutil.move(hazy_path, hazeeffected_images_dir_valid)\n",
    "#     shutil.move(clean_path, hazefree_images_dir_valid)\n",
    "\n",
    "# print(f\"Moved {len(valid_pairs)} image pairs to validation set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c9698f8",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:34.219851Z",
     "iopub.status.busy": "2025-04-14T02:36:34.219593Z",
     "iopub.status.idle": "2025-04-14T02:36:34.223174Z",
     "shell.execute_reply": "2025-04-14T02:36:34.222150Z"
    },
    "papermill": {
     "duration": 0.020989,
     "end_time": "2025-04-14T02:36:34.224706",
     "exception": false,
     "start_time": "2025-04-14T02:36:34.203717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # hazeeffected_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "# # hazefree_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "# # hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "# # hazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/GT'\n",
    "\n",
    "# hazeeffected_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN'\n",
    "# hazefree_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT'\n",
    "# hazeeffected_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN'\n",
    "# hazefree_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/GT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4adc8f6d",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:34.256338Z",
     "iopub.status.busy": "2025-04-14T02:36:34.256024Z",
     "iopub.status.idle": "2025-04-14T02:36:34.261772Z",
     "shell.execute_reply": "2025-04-14T02:36:34.260557Z"
    },
    "papermill": {
     "duration": 0.023312,
     "end_time": "2025-04-14T02:36:34.263029",
     "exception": false,
     "start_time": "2025-04-14T02:36:34.239717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_log(epoch, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, category):\n",
    "    log_dir = \"./training_log\"\n",
    "    os.makedirs(log_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    log_path = os.path.join(log_dir, f\"{category}_log.txt\")\n",
    "\n",
    "    print('({0:.0f}s) Epoch [{1}/{2}], Train_PSNR:{3:.2f}, Val_PSNR:{4:.2f}, Val_SSIM:{5:.4f}'\n",
    "          .format(one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim))\n",
    "\n",
    "    # --- Write the training log --- #\n",
    "    with open(log_path, 'a') as f:\n",
    "        print('Date: {0}, Time_Cost: {1:.0f}s, Epoch: [{2}/{3}], Train_PSNR: {4:.2f}, Val_PSNR: {5:.2f}, Val_SSIM: {6:.4f}'\n",
    "              .format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "                      one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b00e5a8c",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:34.295004Z",
     "iopub.status.busy": "2025-04-14T02:36:34.294719Z",
     "iopub.status.idle": "2025-04-14T02:36:34.299942Z",
     "shell.execute_reply": "2025-04-14T02:36:34.298696Z"
    },
    "papermill": {
     "duration": 0.02256,
     "end_time": "2025-04-14T02:36:34.300886",
     "exception": false,
     "start_time": "2025-04-14T02:36:34.278326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, category, lr_decay=0.90):\n",
    "    \"\"\"\n",
    "    Adjusts the learning rate based on the epoch and dataset category.\n",
    "\n",
    "    :param optimizer: The optimizer (e.g., Adam, SGD).\n",
    "    :param epoch: Current epoch number.\n",
    "    :param category: Dataset category ('indoor', 'outdoor', or 'NH').\n",
    "    :param lr_decay: Multiplicative factor for learning rate decay.\n",
    "    \"\"\"\n",
    "    # Define learning rate decay steps based on category\n",
    "    step_dict = {'indoor': 18, 'outdoor': 3, 'NH': 20}\n",
    "    step = step_dict.get(category, 3)  # Default step size if category is unknown\n",
    "\n",
    "    # Decay learning rate at the specified step\n",
    "    if epoch > 0 and epoch % step == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= lr_decay\n",
    "            print(f\"Epoch {epoch}: Learning rate adjusted to {param_group['lr']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e9742",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.014682,
     "end_time": "2025-04-14T02:36:34.330049",
     "exception": false,
     "start_time": "2025-04-14T02:36:34.315367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "301bb254",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:34.363135Z",
     "iopub.status.busy": "2025-04-14T02:36:34.362879Z",
     "iopub.status.idle": "2025-04-14T02:36:34.368423Z",
     "shell.execute_reply": "2025-04-14T02:36:34.367641Z"
    },
    "papermill": {
     "duration": 0.023427,
     "end_time": "2025-04-14T02:36:34.369896",
     "exception": false,
     "start_time": "2025-04-14T02:36:34.346469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Perceptual Feature Loss Network --- #\n",
    "class PerceptualLossNet(nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = vgg_model\n",
    "        self.feature_layers = {'3': \"low_level\", '8': \"mid_level\", '15': \"high_level\"}\n",
    "\n",
    "    def get_feature_maps(self, x):\n",
    "        feature_maps = []\n",
    "        for layer_id, layer in self.feature_extractor.named_children():\n",
    "            x = layer(x)\n",
    "            if layer_id in self.feature_layers:\n",
    "                feature_maps.append(x)\n",
    "        return feature_maps\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        pred_features = self.get_feature_maps(predicted)\n",
    "        target_features = self.get_feature_maps(target)\n",
    "        \n",
    "        # Compute perceptual loss as mean squared error across feature maps\n",
    "        loss = torch.stack([F.mse_loss(p, t) for p, t in zip(pred_features, target_features)]).mean()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97e6d008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:34.400767Z",
     "iopub.status.busy": "2025-04-14T02:36:34.400500Z",
     "iopub.status.idle": "2025-04-14T02:36:39.952802Z",
     "shell.execute_reply": "2025-04-14T02:36:39.951550Z"
    },
    "papermill": {
     "duration": 5.57038,
     "end_time": "2025-04-14T02:36:39.954929",
     "exception": false,
     "start_time": "2025-04-14T02:36:34.384549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0.00/528M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  1%|▏         | 7.00M/528M [00:00<00:07, 73.3MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  4%|▍         | 20.0M/528M [00:00<00:04, 110MB/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  8%|▊         | 40.0M/528M [00:00<00:03, 155MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 11%|█▏        | 59.5M/528M [00:00<00:02, 174MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 15%|█▌        | 79.4M/528M [00:00<00:02, 186MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 19%|█▊        | 98.8M/528M [00:00<00:02, 192MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 22%|██▏       | 119M/528M [00:00<00:02, 197MB/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 26%|██▌       | 138M/528M [00:00<00:02, 199MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 30%|██▉       | 158M/528M [00:00<00:01, 202MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 34%|███▎      | 177M/528M [00:01<00:01, 200MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 37%|███▋      | 197M/528M [00:01<00:01, 203MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 41%|████      | 217M/528M [00:01<00:01, 204MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 45%|████▍     | 237M/528M [00:01<00:01, 205MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 49%|████▊     | 257M/528M [00:01<00:01, 206MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 52%|█████▏    | 276M/528M [00:01<00:01, 205MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 56%|█████▌    | 297M/528M [00:01<00:01, 206MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60%|██████    | 317M/528M [00:01<00:01, 208MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 64%|██████▍   | 337M/528M [00:01<00:00, 208MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 68%|██████▊   | 357M/528M [00:01<00:00, 207MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 71%|███████▏  | 376M/528M [00:02<00:00, 206MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 75%|███████▌  | 396M/528M [00:02<00:00, 205MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 79%|███████▉  | 416M/528M [00:02<00:00, 207MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 83%|████████▎ | 436M/528M [00:02<00:00, 208MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 87%|████████▋ | 457M/528M [00:02<00:00, 209MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 90%|█████████ | 477M/528M [00:02<00:00, 210MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 94%|█████████▍| 497M/528M [00:02<00:00, 210MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 98%|█████████▊| 517M/528M [00:02<00:00, 207MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|██████████| 528M/528M [00:02<00:00, 199MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model weights loaded from /kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\n",
      "📊 Total Trainable Parameters: 4,645,694\n"
     ]
    }
   ],
   "source": [
    "# --- Imports --- #\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "# --- Device Setup --- #\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_ids = list(range(torch.cuda.device_count()))\n",
    "\n",
    "# --- Initialize Model --- #\n",
    "net = DeepGuidedNetwork().to(device)\n",
    "\n",
    "# --- Enable Multi-GPU (if available) --- #\n",
    "if len(device_ids) > 1:\n",
    "    net = nn.DataParallel(net, device_ids=device_ids)\n",
    "\n",
    "# --- Optimizer --- #\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Load Pretrained VGG16 for Perceptual Loss --- #\n",
    "vgg_features = vgg16(pretrained=True).features[:16].to(device)\n",
    "for param in vgg_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "loss_network = PerceptualLossNet(vgg_features)\n",
    "loss_network.eval()\n",
    "\n",
    "# --- Load Model Weights (if available) --- #\n",
    "model_name = 'formernew'\n",
    "# checkpoint_path = f\"{model_name}_{category}_haze_best_{version}\"\n",
    "checkpoint_path = \"/kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\" \n",
    "\n",
    "try:\n",
    "    net.load_state_dict(torch.load(checkpoint_path, weights_only=False, map_location=torch.device('cpu')))\n",
    "    print(f\"✅ Model weights loaded from {checkpoint_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️ No pretrained weights found at {checkpoint_path}\")\n",
    "\n",
    "# --- Compute Total Trainable Parameters --- #\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"📊 Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b0bc084",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:40.024287Z",
     "iopub.status.busy": "2025-04-14T02:36:40.023976Z",
     "iopub.status.idle": "2025-04-14T02:36:40.030450Z",
     "shell.execute_reply": "2025-04-14T02:36:40.029252Z"
    },
    "papermill": {
     "duration": 0.058118,
     "end_time": "2025-04-14T02:36:40.031697",
     "exception": false,
     "start_time": "2025-04-14T02:36:39.973579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeatureAffinityModule(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(FeatureAffinityModule, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def forward(self, student_features, teacher_features):\n",
    "        # Normalize features\n",
    "        student_norm = F.normalize(student_features.view(student_features.size(0), self.channels, -1), dim=2)\n",
    "        teacher_norm = F.normalize(teacher_features.view(teacher_features.size(0), self.channels, -1), dim=2)\n",
    "\n",
    "        # Compute affinity matrices\n",
    "        student_affinity = torch.bmm(student_norm, student_norm.transpose(1, 2))\n",
    "        teacher_affinity = torch.bmm(teacher_norm, teacher_norm.transpose(1, 2))\n",
    "\n",
    "        # Compute KL divergence\n",
    "        loss = F.kl_div(F.log_softmax(student_affinity, dim=-1),\n",
    "                        F.softmax(teacher_affinity, dim=-1),\n",
    "                        reduction='batchmean')\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21bc7ef8",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:36:40.067226Z",
     "iopub.status.busy": "2025-04-14T02:36:40.066971Z",
     "iopub.status.idle": "2025-04-14T02:37:01.487929Z",
     "shell.execute_reply": "2025-04-14T02:37:01.486988Z"
    },
    "papermill": {
     "duration": 21.440764,
     "end_time": "2025-04-14T02:37:01.489630",
     "exception": false,
     "start_time": "2025-04-14T02:36:40.048866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4800, Validation samples: 1200\n"
     ]
    }
   ],
   "source": [
    "# Create train and validation datasets\n",
    "train_dataset = HazeDataset(crop_size=crop_size, \n",
    "                            hazeeffected_images_dir=hazeeffected_images_dir_train,\n",
    "                            hazefree_images_dir=hazefree_images_dir_train,\n",
    "                            split=\"train\")\n",
    "\n",
    "val_dataset = HazeDataset(crop_size=crop_size, \n",
    "                          hazeeffected_images_dir=hazeeffected_images_dir_train,\n",
    "                          hazefree_images_dir=hazefree_images_dir_train,\n",
    "                          split=\"valid\")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdc913ee",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:01.526751Z",
     "iopub.status.busy": "2025-04-14T02:37:01.526474Z",
     "iopub.status.idle": "2025-04-14T02:37:01.530399Z",
     "shell.execute_reply": "2025-04-14T02:37:01.529537Z"
    },
    "papermill": {
     "duration": 0.024687,
     "end_time": "2025-04-14T02:37:01.532010",
     "exception": false,
     "start_time": "2025-04-14T02:37:01.507323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "134575cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:01.568315Z",
     "iopub.status.busy": "2025-04-14T02:37:01.568039Z",
     "iopub.status.idle": "2025-04-14T02:37:01.578532Z",
     "shell.execute_reply": "2025-04-14T02:37:01.577584Z"
    },
    "papermill": {
     "duration": 0.031135,
     "end_time": "2025-04-14T02:37:01.580107",
     "exception": false,
     "start_time": "2025-04-14T02:37:01.548972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, scale='x2', split='train', split_ratio=0.9):\n",
    "        \"\"\"\n",
    "        Super-Resolution dataset that matches LR and HR image pairs based on naming pattern,\n",
    "        with support for train/val split.\n",
    "\n",
    "        Args:\n",
    "            lr_dir (str): Directory containing low-resolution images (e.g., x2, x3, x4).\n",
    "            hr_dir (str): Directory containing high-resolution images.\n",
    "            scale (str): Scale suffix (e.g., 'x2', 'x3', 'x4').\n",
    "            split (str): Either 'train' or 'val'.\n",
    "            split_ratio (float): Ratio of training data (e.g., 0.9 means 90% train, 10% val).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.scale = scale\n",
    "        self.split = split.lower()\n",
    "\n",
    "        valid_ext = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        lr_images = sorted([\n",
    "            f for f in glob.glob(os.path.join(lr_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_ext)\n",
    "        ])\n",
    "\n",
    "        lr_hr_pairs = []\n",
    "        for lr_path in lr_images:\n",
    "            lr_name = os.path.basename(lr_path)\n",
    "            hr_name = lr_name.replace(scale, '')\n",
    "            hr_path = os.path.join(hr_dir, hr_name)\n",
    "\n",
    "            if not os.path.exists(hr_path):\n",
    "                print(f\"Warning: Ground-truth missing for {lr_name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            lr_hr_pairs.append((lr_path, hr_path))\n",
    "\n",
    "        if not lr_hr_pairs:\n",
    "            raise ValueError(\"No matching LR-HR image pairs found.\")\n",
    "\n",
    "        # Split dataset\n",
    "        split_idx = int(len(lr_hr_pairs) * split_ratio)\n",
    "        if self.split == 'train':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[:split_idx]\n",
    "        elif self.split == 'val':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[split_idx:]\n",
    "        else:\n",
    "            raise ValueError(\"split must be either 'train' or 'val'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_hr_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_path, hr_path = self.lr_hr_pairs[idx]\n",
    "\n",
    "        try:\n",
    "            lr_img = Image.open(lr_path).convert('RGB')\n",
    "            hr_img = Image.open(hr_path).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Unidentified image at {lr_path} or {hr_path}\")\n",
    "\n",
    "        return ToTensor()(lr_img), ToTensor()(hr_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "feceda13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:01.617061Z",
     "iopub.status.busy": "2025-04-14T02:37:01.616805Z",
     "iopub.status.idle": "2025-04-14T02:37:01.621677Z",
     "shell.execute_reply": "2025-04-14T02:37:01.620734Z"
    },
    "papermill": {
     "duration": 0.025566,
     "end_time": "2025-04-14T02:37:01.623248",
     "exception": false,
     "start_time": "2025-04-14T02:37:01.597682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    min_height = min([x[0].shape[1] for x in batch])//4\n",
    "    min_width = min([x[0].shape[2] for x in batch])//4\n",
    "    resized_batch = [(TF.resize(x[0], [min_height, min_width]), TF.resize(x[1], [min_height, min_width])) for x in batch]\n",
    "    return torch.utils.data.dataloader.default_collate(resized_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c963c47a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:01.659521Z",
     "iopub.status.busy": "2025-04-14T02:37:01.659301Z",
     "iopub.status.idle": "2025-04-14T02:37:01.663072Z",
     "shell.execute_reply": "2025-04-14T02:37:01.661905Z"
    },
    "papermill": {
     "duration": 0.023149,
     "end_time": "2025-04-14T02:37:01.663971",
     "exception": false,
     "start_time": "2025-04-14T02:37:01.640822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Paths\n",
    "# sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n",
    "# sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X2'\n",
    "\n",
    "# # Train SR DataLoader\n",
    "# sr_train_dataset = SRDataset(lr_dir, hr_dir, scale='x2', split='train')\n",
    "# sr_valid_dataset = SRDataset(lr_dir, hr_dir, scale='x2', split='val')\n",
    "\n",
    "# sr_train_loader = DataLoader(sr_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=custom_collate_fn)\n",
    "\n",
    "# # If you have a separate validation split:\n",
    "# sr_val_loader = DataLoader(sr_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c3f02d1",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:01.698962Z",
     "iopub.status.busy": "2025-04-14T02:37:01.698723Z",
     "iopub.status.idle": "2025-04-14T02:37:01.701898Z",
     "shell.execute_reply": "2025-04-14T02:37:01.700989Z"
    },
    "papermill": {
     "duration": 0.022728,
     "end_time": "2025-04-14T02:37:01.703419",
     "exception": false,
     "start_time": "2025-04-14T02:37:01.680691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_data_loader = DataLoader(TrainData(crop_size, hazeeffected_images_dir_train, hazefree_images_dir_train), batch_size=train_batch_size, shuffle=True)\n",
    "# val_data_loader = DataLoader(TrainData(crop_size, hazeeffected_images_dir_valid, hazefree_images_dir_valid), batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42cd1c1d",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:01.740307Z",
     "iopub.status.busy": "2025-04-14T02:37:01.740051Z",
     "iopub.status.idle": "2025-04-14T02:37:01.743601Z",
     "shell.execute_reply": "2025-04-14T02:37:01.742487Z"
    },
    "papermill": {
     "duration": 0.023486,
     "end_time": "2025-04-14T02:37:01.744829",
     "exception": false,
     "start_time": "2025-04-14T02:37:01.721343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_size = len(TrainData(crop_size, hazeeffected_images_dir_train, hazefree_images_dir_train))\n",
    "# val_size = len(TrainData(crop_size, hazeeffected_images_dir_valid, hazefree_images_dir_valid))\n",
    "\n",
    "# print(f\"Train Size: {train_size}, Val Size: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df85e2e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:01.779794Z",
     "iopub.status.busy": "2025-04-14T02:37:01.779529Z",
     "iopub.status.idle": "2025-04-14T02:37:12.282610Z",
     "shell.execute_reply": "2025-04-14T02:37:12.278978Z"
    },
    "papermill": {
     "duration": 10.523942,
     "end_time": "2025-04-14T02:37:12.285289",
     "exception": false,
     "start_time": "2025-04-14T02:37:01.761347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- SR Dataset Setup --- #\n",
    "sr_enabled = True\n",
    "if sr_enabled:\n",
    "    sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n",
    "    sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X2'\n",
    "    sr_train_dataset = SRDataset(lr_dir=sr_lr_dir, hr_dir=sr_hr_dir, scale='x2', split='train')\n",
    "    sr_val_dataset = SRDataset(lr_dir=sr_lr_dir, hr_dir=sr_hr_dir, scale='x2', split='val')\n",
    "    sr_train_loader = DataLoader(sr_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=custom_collate_fn)\n",
    "    sr_val_loader = DataLoader(sr_val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=custom_collate_fn)\n",
    "    sr_iter = iter(sr_train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32fba965",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:12.328995Z",
     "iopub.status.busy": "2025-04-14T02:37:12.328585Z",
     "iopub.status.idle": "2025-04-14T02:37:12.526477Z",
     "shell.execute_reply": "2025-04-14T02:37:12.525098Z"
    },
    "papermill": {
     "duration": 0.221805,
     "end_time": "2025-04-14T02:37:12.528600",
     "exception": false,
     "start_time": "2025-04-14T02:37:12.306795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 128, 128]) torch.Size([6, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for i,o in train_data_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3021951b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:12.569716Z",
     "iopub.status.busy": "2025-04-14T02:37:12.569376Z",
     "iopub.status.idle": "2025-04-14T02:37:14.099547Z",
     "shell.execute_reply": "2025-04-14T02:37:14.098236Z"
    },
    "papermill": {
     "duration": 1.553238,
     "end_time": "2025-04-14T02:37:14.101623",
     "exception": false,
     "start_time": "2025-04-14T02:37:12.548385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 175, 255]) torch.Size([2, 3, 175, 255])\n"
     ]
    }
   ],
   "source": [
    "for i,o in sr_val_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48f6d722",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:14.139957Z",
     "iopub.status.busy": "2025-04-14T02:37:14.139658Z",
     "iopub.status.idle": "2025-04-14T02:37:14.147488Z",
     "shell.execute_reply": "2025-04-14T02:37:14.146499Z"
    },
    "papermill": {
     "duration": 0.028532,
     "end_time": "2025-04-14T02:37:14.148910",
     "exception": false,
     "start_time": "2025-04-14T02:37:14.120378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SSFM(nn.Module):\n",
    "    def __init__(self, loss_type='l1'):\n",
    "        super(SSFM, self).__init__()\n",
    "        assert loss_type in ['l1', 'l2'], \"loss_type must be 'l1' or 'l2'\"\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def forward(self, student_feats, teacher_feats):\n",
    "        \"\"\"\n",
    "        student_feats: List of feature maps from student RDBs [rdb1, rdb2, rdb3, rdb4]\n",
    "        teacher_feats: List of corresponding feature maps from teacher\n",
    "        \"\"\"\n",
    "        assert len(student_feats) == len(teacher_feats), \"Feature lists must match\"\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for s_feat, t_feat in zip(student_feats, teacher_feats):\n",
    "            # Match resolution\n",
    "            if s_feat.shape != t_feat.shape:\n",
    "                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            if self.loss_type == 'l1':\n",
    "                loss = F.l1_loss(s_feat, t_feat)\n",
    "            else:\n",
    "                loss = F.mse_loss(s_feat, t_feat)\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss / len(student_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c021f70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:14.184327Z",
     "iopub.status.busy": "2025-04-14T02:37:14.184027Z",
     "iopub.status.idle": "2025-04-14T02:37:14.484294Z",
     "shell.execute_reply": "2025-04-14T02:37:14.482497Z"
    },
    "papermill": {
     "duration": 0.319583,
     "end_time": "2025-04-14T02:37:14.485527",
     "exception": false,
     "start_time": "2025-04-14T02:37:14.165944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Teacher Network --- #\n",
    "teacher_net = DeepGuidedNetwork(radius=1).to(device)\n",
    "# teacher_net.load_state_dict(torch.load('teacher_model.pth'))\n",
    "# teacher_net.eval()\n",
    "\n",
    "# --- Feature Affinity Module --- #\n",
    "fam = FeatureAffinityModule(channels=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "facc042e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:14.521858Z",
     "iopub.status.busy": "2025-04-14T02:37:14.521557Z",
     "iopub.status.idle": "2025-04-14T02:37:14.525641Z",
     "shell.execute_reply": "2025-04-14T02:37:14.524625Z"
    },
    "papermill": {
     "duration": 0.02407,
     "end_time": "2025-04-14T02:37:14.527082",
     "exception": false,
     "start_time": "2025-04-14T02:37:14.503012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98cfdbf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T02:37:14.563811Z",
     "iopub.status.busy": "2025-04-14T02:37:14.563494Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-04-14T02:37:14.544561",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Teacher SR Init Val] PSNR: 14.83, SSIM: 0.3481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [0], Loss: 0.1683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [50], Loss: 0.1672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [100], Loss: 0.1548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [150], Loss: 0.1518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [200], Loss: 0.1571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [250], Loss: 0.1808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [300], Loss: 0.1645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [350], Loss: 0.1595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [400], Loss: 0.1545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [450], Loss: 0.1261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [500], Loss: 0.1593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [550], Loss: 0.1193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [600], Loss: 0.1222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [650], Loss: 0.1265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [700], Loss: 0.1498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [750], Loss: 0.1642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [800], Loss: 0.1639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [850], Loss: 0.1581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [900], Loss: 0.1536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [950], Loss: 0.1650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [1000], Loss: 0.1805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [1050], Loss: 0.1489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [1100], Loss: 0.1485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Iteration [1150], Loss: 0.1561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model saved in epoch 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1876s) Epoch [1/50], Train_PSNR:14.91, Val_PSNR:14.83, Val_SSIM:0.3481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [0], Loss: 0.1658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [50], Loss: 0.1414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [100], Loss: 0.1546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [150], Loss: 0.1373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [200], Loss: 0.1953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [250], Loss: 0.1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [300], Loss: 0.1543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [350], Loss: 0.1576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [400], Loss: 0.1711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [450], Loss: 0.1489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [500], Loss: 0.1511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [550], Loss: 0.1534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [600], Loss: 0.1499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [650], Loss: 0.1593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [700], Loss: 0.1847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [750], Loss: 0.1630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [800], Loss: 0.1576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [850], Loss: 0.1671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [900], Loss: 0.1373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [950], Loss: 0.1420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [1000], Loss: 0.1197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [1050], Loss: 0.1545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [1100], Loss: 0.1229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Iteration [1150], Loss: 0.1523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1940s) Epoch [2/50], Train_PSNR:14.91, Val_PSNR:14.83, Val_SSIM:0.3481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [0], Loss: 0.1535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [50], Loss: 0.1825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [100], Loss: 0.1565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [150], Loss: 0.1615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [200], Loss: 0.1482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [250], Loss: 0.1513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [300], Loss: 0.1701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [350], Loss: 0.1757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [400], Loss: 0.1547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [450], Loss: 0.1090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [500], Loss: 0.1614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [550], Loss: 0.1448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [600], Loss: 0.1617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [650], Loss: 0.1685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [700], Loss: 0.1363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [750], Loss: 0.1569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [800], Loss: 0.1736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [850], Loss: 0.1405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [900], Loss: 0.1635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [950], Loss: 0.1546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [1000], Loss: 0.1550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [1050], Loss: 0.1695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [1100], Loss: 0.1570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Iteration [1150], Loss: 0.1357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1972s) Epoch [3/50], Train_PSNR:14.91, Val_PSNR:14.83, Val_SSIM:0.3481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Learning rate adjusted to 0.000090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [0], Loss: 0.1459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [50], Loss: 0.1654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [100], Loss: 0.1453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [150], Loss: 0.1553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [200], Loss: 0.1703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [250], Loss: 0.1725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [300], Loss: 0.1797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [350], Loss: 0.1709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [400], Loss: 0.1413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [450], Loss: 0.1544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [500], Loss: 0.1521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [550], Loss: 0.1685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [600], Loss: 0.1049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [650], Loss: 0.1656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [700], Loss: 0.1376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [750], Loss: 0.1509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [800], Loss: 0.1445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Iteration [850], Loss: 0.1931\n"
     ]
    }
   ],
   "source": [
    "# --- Initial Validation --- #\n",
    "old_val_psnr, old_val_ssim = validation_sr(teacher_net, sr_val_loader, device)\n",
    "print(f\"[Teacher SR Init Val] PSNR: {old_val_psnr:.2f}, SSIM: {old_val_ssim:.4f}\")\n",
    "\n",
    "# --- Training Loop for Teacher Model --- #\n",
    "best_psnr = old_val_psnr\n",
    "train_psnr_prev = 0\n",
    "distillation_weight = 1  # Not needed for teacher, but included for consistency\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    psnr_list = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch, category=category)\n",
    "    teacher_net.train()\n",
    "\n",
    "    for batch_id, (sr_lr, sr_hr) in enumerate(sr_train_loader):\n",
    "        sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass - Teacher (SR Model)\n",
    "        sr_out, _ = teacher_net(sr_lr)\n",
    "\n",
    "        # SR Loss (L1 loss)\n",
    "        sr_loss = F.l1_loss(sr_out, sr_hr)\n",
    "        \n",
    "        # Backpropagation\n",
    "        sr_loss.backward()\n",
    "        optimizer.step()\n",
    "        psnr_list.extend(to_psnr(sr_out, sr_hr))\n",
    "\n",
    "        if batch_id % 50 == 0:  # adjust this based on how frequently you want to print\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Iteration [{batch_id}], Loss: {sr_loss.item():.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if epoch % 5 == 0:\n",
    "        iter_model_path = f\"{model_name}{category}_teacher_sr_iter_{epoch}.pth\"\n",
    "        torch.save(teacher_net.state_dict(), iter_model_path)\n",
    "        print(f\"Teacher model saved in epoch {epoch}.\")\n",
    "\n",
    "    train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "    model_path = f\"{model_name}{category}_teacher_sr_{version}.pth\"\n",
    "\n",
    "    # --- Validation --- #\n",
    "    teacher_net.eval()\n",
    "    val_psnr, val_ssim = validation_sr(teacher_net, sr_val_loader, device)\n",
    "    epoch_duration = time.time() - start_time\n",
    "    print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "    if train_psnr < train_psnr_prev:\n",
    "        adjust_learning_rate(optimizer, num_epochs, category=category)\n",
    "\n",
    "    if val_psnr >= best_psnr:\n",
    "        best_model_path = f\"{model_name}{category}_teacher_sr_best_{version}.pth\"\n",
    "        torch.save(teacher_net.state_dict(), best_model_path)\n",
    "        best_psnr = val_psnr\n",
    "\n",
    "    train_psnr_prev = train_psnr\n",
    "\n",
    "# Final save for teacher model\n",
    "final_path = f\"{model_name}{category}_teacher_sr_final_{epoch}.pth\"\n",
    "torch.save(teacher_net.state_dict(), final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a0b51",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-13T16:11:26.296954Z",
     "iopub.status.idle": "2025-04-13T16:11:26.297372Z",
     "shell.execute_reply": "2025-04-13T16:11:26.297206Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # --- Initial Validation --- #\n",
    "# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)\n",
    "# print(f\"[Dehazing Init Val] PSNR: {old_val_psnr:.2f}, SSIM: {old_val_ssim:.4f}\")\n",
    "# if sr_enabled:\n",
    "#     sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#     print(f\"[SR Init Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "# # --- Training Loop --- #\n",
    "# best_psnr = old_val_psnr\n",
    "# train_psnr_prev = 0\n",
    "# distillation_weight = 1\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     psnr_list = []\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     adjust_learning_rate(optimizer, epoch, category=category)\n",
    "#     net.train()\n",
    "\n",
    "#     for batch_id, (haze, gt) in enumerate(train_data_loader):\n",
    "#         haze, gt = haze.to(device), gt.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward Pass - Student\n",
    "#         dehaze, base = net(haze)\n",
    "\n",
    "#         # Teacher Output\n",
    "#         with torch.no_grad():\n",
    "#             teacher_dehaze, _ = teacher_net(haze)\n",
    "\n",
    "#         # Losses\n",
    "#         base_loss = F.smooth_l1_loss(base, gt)\n",
    "#         smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "#         perceptual_loss = loss_network(dehaze, gt)\n",
    "#         distillation_loss = fam(dehaze, teacher_dehaze)\n",
    "#         # print(\"distillation_loss: \", distillation_loss)\n",
    "#         total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss + distillation_weight * distillation_loss\n",
    "\n",
    "#         # --- SR Training --- #\n",
    "#         if sr_enabled:\n",
    "#             try:\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             except StopIteration:\n",
    "#                 sr_iter = iter(sr_train_loader)\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n",
    "#             sr_out, _ = net(sr_lr)\n",
    "#             sr_loss = F.l1_loss(sr_out, sr_hr)\n",
    "#             total_loss += sr_loss\n",
    "\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "#         psnr_list.extend(to_psnr(dehaze, gt))\n",
    "\n",
    "#         if batch_id % num_epochs == 0:\n",
    "#             print(f\"Epoch [{epoch}/{num_epochs}], Iteration [{batch_id}]\")\n",
    "\n",
    "#     # Save model checkpoint\n",
    "#     if epoch % 5 == 0:\n",
    "#         iter_model_path = f\"{model_name}{category}_haze_iter_{epoch}.pth\"\n",
    "#         torch.save(net.state_dict(), iter_model_path)\n",
    "#         print(f\"Model saved in epoch {epoch}.\")\n",
    "\n",
    "#     train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "#     model_path = f\"{model_name}{category}_haze_{version}.pth\"\n",
    "\n",
    "#     # --- Validation --- #\n",
    "#     net.eval()\n",
    "#     val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "#     if sr_enabled:\n",
    "#         sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#         print(f\"[SR Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "#     epoch_duration = time.time() - start_time\n",
    "#     print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "#     if train_psnr < train_psnr_prev:\n",
    "#         adjust_learning_rate(optimizer, num_epochs, category=category)\n",
    "\n",
    "#     if val_psnr >= best_psnr:\n",
    "#         best_model_path = f\"{model_name}{category}_haze_best_{version}.pth\"\n",
    "#         torch.save(net.state_dict(), best_model_path)\n",
    "#         best_psnr = val_psnr\n",
    "\n",
    "#     train_psnr_prev = train_psnr\n",
    "\n",
    "# # Final save\n",
    "# final_path = f\"{model_name}{category}_final_{epoch}.pth\"\n",
    "# torch.save(net.state_dict(), final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1646a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def visualize_validation_results(teacher_net, val_data_loader, device, num_images=5):\n",
    "    \"\"\"\n",
    "    Visualize a few validation images along with the high-res ground truth and the model output.\n",
    "    \n",
    "    :param teacher_net: The teacher model.\n",
    "    :param val_data_loader: Validation data loader.\n",
    "    :param device: Device (CPU or GPU).\n",
    "    :param num_images: Number of images to visualize.\n",
    "    \"\"\"\n",
    "    teacher_net.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Get a few validation samples\n",
    "    with torch.no_grad():\n",
    "        for idx, (lr, hr) in enumerate(val_data_loader):\n",
    "            if idx >= num_images:\n",
    "                break\n",
    "            \n",
    "            lr, hr = lr.to(device), hr.to(device)  # Move data to device\n",
    "\n",
    "            # Forward pass through the teacher network\n",
    "            sr_output, _ = teacher_net(lr)  # Assuming model returns (sr_output, _) tuple\n",
    "\n",
    "            # Convert to numpy for visualization (detach from GPU if needed)\n",
    "            lr = lr.cpu().numpy().transpose(0, 2, 3, 1)[0]  # (C, H, W) -> (H, W, C)\n",
    "            hr = hr.cpu().numpy().transpose(0, 2, 3, 1)[0]  # (C, H, W) -> (H, W, C)\n",
    "            sr_output = sr_output.cpu().numpy().transpose(0, 2, 3, 1)[0]  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "            # Plot the images\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "            axes[0].imshow(lr)\n",
    "            axes[0].set_title(f\"Low-Resolution Image {idx+1}\")\n",
    "            axes[0].axis(\"off\")\n",
    "            \n",
    "            axes[1].imshow(hr)\n",
    "            axes[1].set_title(f\"Ground Truth (HR) {idx+1}\")\n",
    "            axes[1].axis(\"off\")\n",
    "            \n",
    "            axes[2].imshow(sr_output)\n",
    "            axes[2].set_title(f\"Teacher Model Output {idx+1}\")\n",
    "            axes[2].axis(\"off\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "visualize_validation_results(teacher_net, sr_val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6dace1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-13T16:08:03.478653Z",
     "iopub.status.idle": "2025-04-13T16:08:03.479026Z",
     "shell.execute_reply": "2025-04-13T16:08:03.478886Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initialize model\n",
    "# # model_path = \"/kaggle/input/rdb-and-transformer/pytorch/default/1/formernewnh_final_49.pth\"\n",
    "# # model_path = \"/kaggle/input/reside-dehaze/pytorch/default/3/formernewreside_haze_best_0.pth\"\n",
    "# model_path = \"/kaggle/input/reside-dehaze/pytorch/default/4/formernewreside_haze_iter_60.pth\"\n",
    "# # model = DehazingNet().to(device)\n",
    "# # model = SR_model(upscale_factor=1).to(device)\n",
    "# # net.load_state_dict(torch.load(model_path, map_location=device))\n",
    "# net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96897641",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.status.busy": "2025-04-13T16:08:03.480071Z",
     "iopub.status.idle": "2025-04-13T16:08:03.480479Z",
     "shell.execute_reply": "2025-04-13T16:08:03.480305Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# LOAD TEST DATA\n",
    "# -----------------------------\n",
    "test_hazy_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/hazy\"\n",
    "test_gt_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/GT\"\n",
    "# test_hazy_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN\"\n",
    "# test_gt_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT\"\n",
    "\n",
    "hazy_images = sorted(glob.glob(os.path.join(test_hazy_dir, \"*.*\")))\n",
    "gt_images = sorted(glob.glob(os.path.join(test_gt_dir, \"*.*\")))\n",
    "\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "to_pil = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a130aa",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-13T16:08:03.481706Z",
     "iopub.status.idle": "2025-04-13T16:08:03.482110Z",
     "shell.execute_reply": "2025-04-13T16:08:03.481974Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [11, 12, 13,14]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416044a8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-13T16:08:03.483337Z",
     "iopub.status.idle": "2025-04-13T16:08:03.484001Z",
     "shell.execute_reply": "2025-04-13T16:08:03.483768Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [70, 75, 90, 100]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6602683",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-13T16:08:03.485186Z",
     "iopub.status.idle": "2025-04-13T16:08:03.485642Z",
     "shell.execute_reply": "2025-04-13T16:08:03.485433Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [1, 3, 5]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 937211,
     "sourceId": 1587463,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2813430,
     "sourceId": 4853613,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6456606,
     "sourceId": 10417877,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6464114,
     "sourceId": 10443410,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 222875724,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 268224,
     "modelInstanceId": 246650,
     "sourceId": 287852,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 288973,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 297672,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 299643,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 335035,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-14T02:35:35.562970",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
