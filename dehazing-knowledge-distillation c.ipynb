{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6622e71",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-09T12:27:54.325602Z",
     "iopub.status.busy": "2025-03-09T12:27:54.325266Z",
     "iopub.status.idle": "2025-03-09T12:28:06.434549Z",
     "shell.execute_reply": "2025-03-09T12:28:06.433754Z"
    },
    "papermill": {
     "duration": 12.120034,
     "end_time": "2025-03-09T12:28:06.436129",
     "exception": false,
     "start_time": "2025-03-09T12:27:54.316095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import glob\n",
    "import os\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage\n",
    "from PIL import Image\n",
    "from random import randrange\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from math import log10\n",
    "import numpy as np\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "from timm.layers import to_2tuple, trunc_normal_\n",
    "import torchvision.utils as utils\n",
    "import torch.utils.data as data\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import time\n",
    "from skimage import measure\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16eaba1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:06.451979Z",
     "iopub.status.busy": "2025-03-09T12:28:06.451491Z",
     "iopub.status.idle": "2025-03-09T12:28:06.502596Z",
     "shell.execute_reply": "2025-03-09T12:28:06.501806Z"
    },
    "papermill": {
     "duration": 0.060172,
     "end_time": "2025-03-09T12:28:06.503880",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.443708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebca25a",
   "metadata": {
    "papermill": {
     "duration": 0.006955,
     "end_time": "2025-03-09T12:28:06.518322",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.511367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Basenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0677bdd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:06.533683Z",
     "iopub.status.busy": "2025-03-09T12:28:06.533374Z",
     "iopub.status.idle": "2025-03-09T12:28:06.760692Z",
     "shell.execute_reply": "2025-03-09T12:28:06.759745Z"
    },
    "papermill": {
     "duration": 0.236734,
     "end_time": "2025-03-09T12:28:06.762225",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.525491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RLN(nn.Module):\n",
    "\tr\"\"\"Revised LayerNorm\"\"\"\n",
    "\tdef __init__(self, dim, eps=1e-5, detach_grad=False):\n",
    "\t\tsuper(RLN, self).__init__()\n",
    "\t\tself.eps = eps\n",
    "\t\tself.detach_grad = detach_grad\n",
    "\n",
    "\t\tself.weight = nn.Parameter(torch.ones((1, dim, 1, 1)))\n",
    "\t\tself.bias = nn.Parameter(torch.zeros((1, dim, 1, 1)))\n",
    "\n",
    "\t\tself.meta1 = nn.Conv2d(1, dim, 1)\n",
    "\t\tself.meta2 = nn.Conv2d(1, dim, 1)\n",
    "\n",
    "\t\ttrunc_normal_(self.meta1.weight, std=.02)\n",
    "\t\tnn.init.constant_(self.meta1.bias, 1)\n",
    "\n",
    "\t\ttrunc_normal_(self.meta2.weight, std=.02)\n",
    "\t\tnn.init.constant_(self.meta2.bias, 0)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tmean = torch.mean(input, dim=(1, 2, 3), keepdim=True)\n",
    "\t\tstd = torch.sqrt((input - mean).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.eps)\n",
    "\n",
    "\t\tnormalized_input = (input - mean) / std\n",
    "\n",
    "\t\tif self.detach_grad:\n",
    "\t\t\trescale, rebias = self.meta1(std.detach()), self.meta2(mean.detach())\n",
    "\t\telse:\n",
    "\t\t\trescale, rebias = self.meta1(std), self.meta2(mean)\n",
    "\n",
    "\t\tout = normalized_input * self.weight + self.bias\n",
    "\t\treturn out, rescale, rebias\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "\tdef __init__(self, network_depth, in_features, hidden_features=None, out_features=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tout_features = out_features or in_features\n",
    "\t\thidden_features = hidden_features or in_features\n",
    "\n",
    "\t\tself.network_depth = network_depth\n",
    "\n",
    "\t\tself.mlp = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(in_features, hidden_features, 1),\n",
    "\t\t\tnn.ReLU(True),\n",
    "\t\t\tnn.Conv2d(hidden_features, out_features, 1)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.apply(self._init_weights)\n",
    "\n",
    "\tdef _init_weights(self, m):\n",
    "\t\tif isinstance(m, nn.Conv2d):\n",
    "\t\t\tgain = (8 * self.network_depth) ** (-1/4)\n",
    "\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n",
    "\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\t\t\ttrunc_normal_(m.weight, std=std)\n",
    "\t\t\tif m.bias !=   None:\n",
    "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.mlp(x)\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "\tB, H, W, C = x.shape\n",
    "\tx = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "\twindows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, C)\n",
    "\treturn windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "\tB = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "\tx = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "\tx = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "\treturn x\n",
    "\n",
    "\n",
    "def get_relative_positions(window_size):\n",
    "\tcoords_h = torch.arange(window_size)\n",
    "\tcoords_w = torch.arange(window_size)\n",
    "\n",
    "\tcoords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "\tcoords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "\trelative_positions = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "\n",
    "\trelative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "\trelative_positions_log  = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "\treturn relative_positions_log\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "\tdef __init__(self, dim, window_size, num_heads):\n",
    "\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.window_size = window_size  # Wh, Ww\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\thead_dim = dim // num_heads\n",
    "\t\tself.scale = head_dim ** -0.5\n",
    "\n",
    "\t\trelative_positions = get_relative_positions(self.window_size)\n",
    "\t\tself.register_buffer(\"relative_positions\", relative_positions)\n",
    "\t\tself.meta = nn.Sequential(\n",
    "\t\t\tnn.Linear(2, 256, bias=True),\n",
    "\t\t\tnn.ReLU(True),\n",
    "\t\t\tnn.Linear(256, num_heads, bias=True)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\tdef forward(self, qkv):\n",
    "\t\tB_, N, _ = qkv.shape\n",
    "\n",
    "\t\tqkv = qkv.reshape(B_, N, 3, self.num_heads, self.dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "\t\tq, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "\t\tq = q * self.scale\n",
    "\t\tattn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "\t\trelative_position_bias = self.meta(self.relative_positions)\n",
    "\t\trelative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "\t\tattn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "\t\tattn = self.softmax(attn)\n",
    "\n",
    "\t\tx = (attn @ v).transpose(1, 2).reshape(B_, N, self.dim)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, num_heads, window_size, shift_size, use_attn=False, conv_type=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.head_dim = int(dim // num_heads)\n",
    "\t\tself.num_heads = num_heads\n",
    "\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.shift_size = shift_size\n",
    "\n",
    "\t\tself.network_depth = network_depth\n",
    "\t\tself.use_attn = use_attn\n",
    "\t\tself.conv_type = conv_type\n",
    "\n",
    "\t\tif self.conv_type == 'Conv':\n",
    "\t\t\tself.conv = nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "\t\t\t\tnn.ReLU(True),\n",
    "\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "\t\t\t)\n",
    "\n",
    "\t\tif self.conv_type == 'DWConv':\n",
    "\t\t\tself.conv = nn.Conv2d(dim, dim, kernel_size=5, padding=2, groups=dim, padding_mode='reflect')\n",
    "\n",
    "\t\tif self.conv_type == 'DWConv' or self.use_attn:\n",
    "\t\t\tself.V = nn.Conv2d(dim, dim, 1)\n",
    "\t\t\tself.proj = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "\t\tif self.use_attn:\n",
    "\t\t\tself.QK = nn.Conv2d(dim, dim * 2, 1)\n",
    "\t\t\tself.attn = WindowAttention(dim, window_size, num_heads)\n",
    "\n",
    "\t\tself.apply(self._init_weights)\n",
    "\n",
    "\tdef _init_weights(self, m):\n",
    "\t\tif isinstance(m, nn.Conv2d):\n",
    "\t\t\tw_shape = m.weight.shape\n",
    "\t\t\t\n",
    "\t\t\tif w_shape[0] == self.dim * 2:\t# QK\n",
    "\t\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n",
    "\t\t\t\tstd = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\t\t\t\ttrunc_normal_(m.weight, std=std)\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t\tgain = (8 * self.network_depth) ** (-1/4)\n",
    "\t\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n",
    "\t\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "\t\t\t\ttrunc_normal_(m.weight, std=std)\n",
    "\n",
    "\t\t\tif m.bias !=  None:\n",
    "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
    "\n",
    "\tdef check_size(self, x, shift=False):\n",
    "\t\t_, _, h, w = x.size()\n",
    "\t\tmod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
    "\t\tmod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
    "\n",
    "\t\tif shift:\n",
    "\t\t\tx = F.pad(x, (self.shift_size, (self.window_size-self.shift_size+mod_pad_w) % self.window_size,\n",
    "\t\t\t\t\t\t  self.shift_size, (self.window_size-self.shift_size+mod_pad_h) % self.window_size), mode='reflect')\n",
    "\t\telse:\n",
    "\t\t\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tB, C, H, W = X.shape\n",
    "\n",
    "\t\tif self.conv_type == 'DWConv' or self.use_attn:\n",
    "\t\t\tV = self.V(X)\n",
    "\t\t#print(self.use_attn)\n",
    "\t\tif self.use_attn:\n",
    "\t\t\t#print('attention')      \n",
    "\t\t\tQK = self.QK(X)\n",
    "\t\t\tQKV = torch.cat([QK, V], dim=1)\n",
    "\n",
    "\t\t\t# shift\n",
    "\t\t\tshifted_QKV = self.check_size(QKV, self.shift_size > 0)\n",
    "\t\t\tHt, Wt = shifted_QKV.shape[2:]\n",
    "\n",
    "\t\t\t# partition windows\n",
    "\t\t\tshifted_QKV = shifted_QKV.permute(0, 2, 3, 1)\n",
    "\t\t\tqkv = window_partition(shifted_QKV, self.window_size)  # nW*B, window_size**2, C\n",
    "\n",
    "\t\t\tattn_windows = self.attn(qkv)\n",
    "\n",
    "\t\t\t# merge windows\n",
    "\t\t\tshifted_out = window_reverse(attn_windows, self.window_size, Ht, Wt)  # B H' W' C\n",
    "\n",
    "\t\t\t# reverse cyclic shift\n",
    "\t\t\tout = shifted_out[:, self.shift_size:(self.shift_size+H), self.shift_size:(self.shift_size+W), :]\n",
    "\t\t\tattn_out = out.permute(0, 3, 1, 2)\n",
    "\n",
    "\t\t\tif self.conv_type in ['Conv', 'DWConv']:\n",
    "\t\t\t\tconv_out = self.conv(V)\n",
    "\t\t\t\tout = self.proj(conv_out + attn_out)\n",
    "\t\t\telse:\n",
    "\t\t\t\tout = self.proj(attn_out)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tif self.conv_type == 'Conv':\n",
    "\t\t\t\tout = self.conv(X)\t\t\t\t# no attention and use conv, no projection\n",
    "\t\t\telif self.conv_type == 'DWConv':\n",
    "\t\t\t\tout = self.proj(self.conv(V))\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, num_heads, mlp_ratio=4.,\n",
    "\t\t\t\t norm_layer=nn.LayerNorm, mlp_norm=False,\n",
    "\t\t\t\t window_size=8, shift_size=0, use_attn=True, conv_type=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.use_attn = use_attn\n",
    "\t\tself.mlp_norm = mlp_norm\n",
    "\n",
    "\t\tself.norm1 = norm_layer(dim) if use_attn else nn.Identity()\n",
    "\t\tself.attn = Attention(network_depth, dim, num_heads=num_heads, window_size=window_size,\n",
    "\t\t\t\t\t\t\t  shift_size=shift_size, use_attn=use_attn, conv_type=conv_type)\n",
    "\n",
    "\t\tself.norm2 = norm_layer(dim) if use_attn and mlp_norm else nn.Identity()\n",
    "\t\tself.mlp = Mlp(network_depth, dim, hidden_features=int(dim * mlp_ratio))\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tidentity = x\n",
    "\t\tif self.use_attn: x, rescale, rebias = self.norm1(x)\n",
    "\t\tx = self.attn(x)\n",
    "\t\tif self.use_attn: x = x * rescale + rebias\n",
    "\t\tx = identity + x\n",
    "\n",
    "\t\tidentity = x\n",
    "\t\tif self.use_attn and self.mlp_norm: x, rescale, rebias = self.norm2(x)\n",
    "\t\tx = self.mlp(x)\n",
    "\t\tif self.use_attn and self.mlp_norm: x = x * rescale + rebias\n",
    "\t\tx = identity + x\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "\tdef __init__(self, network_depth, dim, depth, num_heads, mlp_ratio=4.,\n",
    "\t\t\t\t norm_layer=nn.LayerNorm, window_size=8,\n",
    "\t\t\t\t attn_ratio=0., attn_loc='last', conv_type=None):\n",
    "\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim = dim\n",
    "\t\tself.depth = depth\n",
    "\n",
    "\t\tattn_depth = attn_ratio * depth\n",
    "\n",
    "\t\tif attn_loc == 'last':\n",
    "\t\t\tuse_attns = [i >= depth-attn_depth for i in range(depth)]\n",
    "\t\telif attn_loc == 'first':\n",
    "\t\t\tuse_attns = [i < attn_depth for i in range(depth)]\n",
    "\t\telif attn_loc == 'middle':\n",
    "\t\t\tuse_attns = [i >= (depth-attn_depth)//2 and i < (depth+attn_depth)//2 for i in range(depth)]\n",
    "\n",
    "\t\t# build blocks\n",
    "\t\tself.blocks = nn.ModuleList([\n",
    "\t\t\tTransformerBlock(network_depth=network_depth,\n",
    "\t\t\t\t\t\t\t dim=dim, \n",
    "\t\t\t\t\t\t\t num_heads=num_heads,\n",
    "\t\t\t\t\t\t\t mlp_ratio=mlp_ratio,\n",
    "\t\t\t\t\t\t\t norm_layer=norm_layer,\n",
    "\t\t\t\t\t\t\t window_size=window_size,\n",
    "\t\t\t\t\t\t\t shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "\t\t\t\t\t\t\t use_attn=use_attns[i], conv_type=conv_type)\n",
    "\t\t\tfor i in range(depth)])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tfor blk in self.blocks:\n",
    "\t\t\tx = blk(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\tdef __init__(self, patch_size=4, in_chans=3, embed_dim=96, kernel_size=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.in_chans = in_chans\n",
    "\t\tself.embed_dim = embed_dim\n",
    "\n",
    "\t\tif kernel_size is None:\n",
    "\t\t\tkernel_size = patch_size\n",
    "\n",
    "\t\tself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "\t\t\t\t\t\t\t  padding=(kernel_size-patch_size+1)//2, padding_mode='reflect')\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.proj(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class PatchUnEmbed(nn.Module):\n",
    "\tdef __init__(self, patch_size=4, out_chans=3, embed_dim=96, kernel_size=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.out_chans = out_chans\n",
    "\t\tself.embed_dim = embed_dim\n",
    "\n",
    "\t\tif kernel_size is None:\n",
    "\t\t\tkernel_size = 1\n",
    "\n",
    "\t\tself.proj = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(embed_dim, out_chans*patch_size**2, kernel_size=kernel_size,\n",
    "\t\t\t\t\t  padding=kernel_size//2, padding_mode='reflect'),\n",
    "\t\t\tnn.PixelShuffle(patch_size)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.proj(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class SKFusion(nn.Module):\n",
    "\tdef __init__(self, dim, height=2, reduction=8):\n",
    "\t\tsuper(SKFusion, self).__init__()\n",
    "\t\t\n",
    "\t\tself.height = height\n",
    "\t\td = max(int(dim/reduction), 4)\n",
    "\t\t\n",
    "\t\tself.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\t\tself.mlp = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(dim, d, 1, bias=False), \n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(d, dim*height, 1, bias=False)\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\tdef forward(self, in_feats):\n",
    "\t\tB, C, H, W = in_feats[0].shape\n",
    "\t\t\n",
    "\t\tin_feats = torch.cat(in_feats, dim=1)\n",
    "\t\tin_feats = in_feats.view(B, self.height, C, H, W)\n",
    "\t\t\n",
    "\t\tfeats_sum = torch.sum(in_feats, dim=1)\n",
    "\t\tattn = self.mlp(self.avg_pool(feats_sum))\n",
    "\t\tattn = self.softmax(attn.view(B, self.height, C, 1, 1))\n",
    "\n",
    "\t\tout = torch.sum(in_feats*attn, dim=1)\n",
    "\t\treturn out      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8424dc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:06.778250Z",
     "iopub.status.busy": "2025-03-09T12:28:06.777947Z",
     "iopub.status.idle": "2025-03-09T12:28:06.791363Z",
     "shell.execute_reply": "2025-03-09T12:28:06.790614Z"
    },
    "papermill": {
     "duration": 0.022717,
     "end_time": "2025-03-09T12:28:06.792567",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.769850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DehazeFormer(nn.Module):\n",
    "\tdef __init__(self, in_chans=3, out_chans=4, window_size=8,\n",
    "\t\t\t\t embed_dims=[24, 48, 96, 48, 24],\n",
    "\t\t\t\t mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\t\t\t depths=[16, 16, 16, 8, 8],\n",
    "\t\t\t\t num_heads=[2, 4, 6, 1, 1],\n",
    "\t\t\t\t attn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\t\t\t conv_type=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "\t\t\t\t norm_layer=[RLN, RLN, RLN, RLN, RLN]):\n",
    "\t\tsuper(DehazeFormer, self).__init__()\n",
    "\n",
    "\t\t# setting\n",
    "\t\tself.patch_size = 4\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.mlp_ratios = mlp_ratios\n",
    "\n",
    "\t\t# split image into non-overlapping patches\n",
    "\t\tself.patch_embed = PatchEmbed(\n",
    "\t\t\tpatch_size=1, in_chans=in_chans, embed_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "\t\t# backbone\n",
    "\t\tself.layer1 = BasicLayer(network_depth=sum(depths), dim=embed_dims[0], depth=depths[0],\n",
    "\t\t\t\t\t   \t\t\t num_heads=num_heads[0], mlp_ratio=mlp_ratios[0],\n",
    "\t\t\t\t\t   \t\t\t norm_layer=norm_layer[0], window_size=window_size,\n",
    "\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio[0], attn_loc='last', conv_type=conv_type[0])\n",
    "\n",
    "\t\tself.patch_merge1 = PatchEmbed(\n",
    "\t\t\tpatch_size=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])\n",
    "\n",
    "\t\tself.skip1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "\n",
    "\t\tself.layer2 = BasicLayer(network_depth=sum(depths), dim=embed_dims[1], depth=depths[1],\n",
    "\t\t\t\t\t\t\t\t num_heads=num_heads[1], mlp_ratio=mlp_ratios[1],\n",
    "\t\t\t\t\t\t\t\t norm_layer=norm_layer[1], window_size=window_size,\n",
    "\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[1], attn_loc='last', conv_type=conv_type[1])\n",
    "\n",
    "\t\tself.patch_merge2 = PatchEmbed(\n",
    "\t\t\tpatch_size=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])\n",
    "\n",
    "\t\tself.skip2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "\n",
    "\t\tself.layer3 = BasicLayer(network_depth=sum(depths), dim=embed_dims[2], depth=depths[2],\n",
    "\t\t\t\t\t\t\t\t num_heads=num_heads[2], mlp_ratio=mlp_ratios[2],\n",
    "\t\t\t\t\t\t\t\t norm_layer=norm_layer[2], window_size=window_size,\n",
    "\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[2], attn_loc='last', conv_type=conv_type[2])\n",
    "\n",
    "\t\tself.patch_split1 = PatchUnEmbed(\n",
    "\t\t\tpatch_size=2, out_chans=embed_dims[3], embed_dim=embed_dims[2])\n",
    "\n",
    "\t\tassert embed_dims[1] == embed_dims[3]\n",
    "\t\tself.fusion1 = SKFusion(embed_dims[3])\n",
    "\n",
    "\t\tself.layer4 = BasicLayer(network_depth=sum(depths), dim=embed_dims[3], depth=depths[3],\n",
    "\t\t\t\t\t\t\t\t num_heads=num_heads[3], mlp_ratio=mlp_ratios[3],\n",
    "\t\t\t\t\t\t\t\t norm_layer=norm_layer[3], window_size=window_size,\n",
    "\t\t\t\t\t\t\t\t attn_ratio=attn_ratio[3], attn_loc='last', conv_type=conv_type[3])\n",
    "\n",
    "\t\tself.patch_split2 = PatchUnEmbed(\n",
    "\t\t\tpatch_size=2, out_chans=embed_dims[4], embed_dim=embed_dims[3])\n",
    "\n",
    "\t\tassert embed_dims[0] == embed_dims[4]\n",
    "\t\tself.fusion2 = SKFusion(embed_dims[4])\t\t\t\n",
    "\n",
    "\t\tself.layer5 = BasicLayer(network_depth=sum(depths), dim=embed_dims[4], depth=depths[4],\n",
    "\t\t\t\t\t   \t\t\t num_heads=num_heads[4], mlp_ratio=mlp_ratios[4],\n",
    "\t\t\t\t\t   \t\t\t norm_layer=norm_layer[4], window_size=window_size,\n",
    "\t\t\t\t\t   \t\t\t attn_ratio=attn_ratio[4], attn_loc='last', conv_type=conv_type[4])\n",
    "\n",
    "\t\t# merge non-overlapping patches into image\n",
    "\t\tself.patch_unembed = PatchUnEmbed(\n",
    "\t\t\tpatch_size=1, out_chans=out_chans, embed_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "\n",
    "\tdef check_image_size(self, x):\n",
    "\t\t# NOTE: for I2I test\n",
    "\t\t_, _, h, w = x.size()\n",
    "\t\tmod_pad_h = (self.patch_size - h % self.patch_size) % self.patch_size\n",
    "\t\tmod_pad_w = (self.patch_size - w % self.patch_size) % self.patch_size\n",
    "\t\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward_features(self, x):\n",
    "\t\tx = self.patch_embed(x)\n",
    "\t\tx = self.layer1(x)\n",
    "\t\tskip1 = x\n",
    "\n",
    "\t\tx = self.patch_merge1(x)\n",
    "\t\tx = self.layer2(x)\n",
    "\t\tskip2 = x\n",
    "\n",
    "\t\tx = self.patch_merge2(x)\n",
    "\t\tx = self.layer3(x)\n",
    "\t\tx = self.patch_split1(x)\n",
    "\n",
    "\t\tx = self.fusion1([x, self.skip2(skip2)]) + x\n",
    "\t\tx = self.layer4(x)\n",
    "\t\tx = self.patch_split2(x)\n",
    "\n",
    "\t\tx = self.fusion2([x, self.skip1(skip1)]) + x\n",
    "\t\tx = self.layer5(x)\n",
    "\t\tx = self.patch_unembed(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tH, W = x.shape[2:]\n",
    "\t\tx = self.check_image_size(x)\n",
    "\n",
    "\t\tfeat = self.forward_features(x)\n",
    "\t\tK, B = torch.split(feat, (1, 3), dim=1)\n",
    "\n",
    "\t\tx = K * x - B + x\n",
    "\t\tx = x[:, :, :H, :W]\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c1ed61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:06.807448Z",
     "iopub.status.busy": "2025-03-09T12:28:06.807192Z",
     "iopub.status.idle": "2025-03-09T12:28:06.811110Z",
     "shell.execute_reply": "2025-03-09T12:28:06.810485Z"
    },
    "papermill": {
     "duration": 0.012672,
     "end_time": "2025-03-09T12:28:06.812320",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.799648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dehazeformer_m():\n",
    "    return DehazeFormer(\n",
    "\t\tembed_dims=[24, 48, 96, 48, 24],\n",
    "\t\tmlp_ratios=[2., 4., 4., 2., 2.],\n",
    "\t\tdepths=[12, 12, 12, 6, 6],\n",
    "\t\tnum_heads=[2, 4, 6, 1, 1],\n",
    "\t\tattn_ratio=[1/4, 1/2, 3/4, 0, 0],\n",
    "\t\tconv_type=['Conv', 'Conv', 'Conv', 'Conv', 'Conv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c43a44",
   "metadata": {
    "papermill": {
     "duration": 0.006823,
     "end_time": "2025-03-09T12:28:06.826175",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.819352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DetailNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5bf8f99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:06.841064Z",
     "iopub.status.busy": "2025-03-09T12:28:06.840720Z",
     "iopub.status.idle": "2025-03-09T12:28:06.847516Z",
     "shell.execute_reply": "2025-03-09T12:28:06.846688Z"
    },
    "papermill": {
     "duration": 0.015807,
     "end_time": "2025-03-09T12:28:06.848757",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.832950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# TEACHER MODEL\n",
    "# -----------------------------\n",
    "class SR_model(nn.Module):\n",
    "    def __init__(self, upscale_factor=1):\n",
    "        super(SR_model, self).__init__()\n",
    "        self.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(3, 56, kernel_size=5, padding=2),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.shrinking = nn.Sequential(\n",
    "            nn.Conv2d(56, 24, kernel_size=3, padding=1),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.mapping = nn.Sequential(\n",
    "            nn.Conv2d(24, 24, kernel_size=3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(24, 24, kernel_size=3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(24, 24, kernel_size=3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(24, 24, kernel_size=3, padding=1),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.expanding = nn.Sequential(\n",
    "            nn.Conv2d(24, 56, kernel_size=3, padding=1),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.deconvolution = nn.Sequential(\n",
    "            nn.Conv2d(56, 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()  # Ensure output is normalized\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downsample(x)\n",
    "        residual = x  # Store input for residual connection\n",
    "        x = self.feature_extraction(x)\n",
    "        x = self.shrinking(x)\n",
    "        x = self.mapping(x)\n",
    "        x = self.expanding(x)\n",
    "        x = self.deconvolution(x)\n",
    "        return x + residual  # Add residual for stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422dc397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:06.864201Z",
     "iopub.status.busy": "2025-03-09T12:28:06.863914Z",
     "iopub.status.idle": "2025-03-09T12:28:06.874077Z",
     "shell.execute_reply": "2025-03-09T12:28:06.873217Z"
    },
    "papermill": {
     "duration": 0.019479,
     "end_time": "2025-03-09T12:28:06.875479",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.856000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# STUDENT MODEL\n",
    "# -----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)  # Residual connection\n",
    "\n",
    "class SpectralAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(SpectralAttention, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // 16, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // 16, channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.fc(x)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(channels, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(self.conv(x))\n",
    "\n",
    "class MultiScaleFeatureFusion(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(MultiScaleFeatureFusion, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(channels, channels, kernel_size=5, padding=2)\n",
    "        self.conv5 = nn.Conv2d(channels, channels, kernel_size=7, padding=3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv1(x) + self.conv3(x) + self.conv5(x)\n",
    "\n",
    "class DehazingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DehazingNet, self).__init__()\n",
    "        self.initial_conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            ResidualBlock(64),\n",
    "            ResidualBlock(64),\n",
    "            ResidualBlock(64)\n",
    "        )\n",
    "        self.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "        self.spectral_attention = SpectralAttention(64)\n",
    "        self.spatial_attention = SpatialAttention(64)\n",
    "        self.multi_scale_fusion = MultiScaleFeatureFusion(64)\n",
    "        self.final_conv = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.downsample(x)\n",
    "        x = F.relu(self.initial_conv(x))\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.spectral_attention(x)\n",
    "        x = self.spatial_attention(x)\n",
    "        x = self.multi_scale_fusion(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faeb496b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:06.890394Z",
     "iopub.status.busy": "2025-03-09T12:28:06.890140Z",
     "iopub.status.idle": "2025-03-09T12:28:06.895772Z",
     "shell.execute_reply": "2025-03-09T12:28:06.895039Z"
    },
    "papermill": {
     "duration": 0.014542,
     "end_time": "2025-03-09T12:28:06.896961",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.882419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# FEATURE AFFINITY MODULE (FAM) USING KL DIVERGENCE\n",
    "# -----------------------------\n",
    "class FeatureAffinityModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureAffinityModule, self).__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((16, 16))\n",
    "\n",
    "    def forward(self, features_a, features_b):\n",
    "        # Pool and flatten features\n",
    "        feat_a = self.pool(features_a).view(features_a.size(0), -1)\n",
    "        feat_b = self.pool(features_b).view(features_b.size(0), -1)\n",
    "\n",
    "        # Normalize features (important for stable KL divergence)\n",
    "        feat_a = F.normalize(feat_a, p=2, dim=-1)\n",
    "        feat_b = F.normalize(feat_b, p=2, dim=-1)\n",
    "\n",
    "        # Compute normalized affinity matrices\n",
    "        affinity_a = torch.mm(feat_a, feat_a.T) / feat_a.size(1)\n",
    "        affinity_b = torch.mm(feat_b, feat_b.T) / feat_b.size(1)\n",
    "\n",
    "        # Compute symmetric KL divergence loss\n",
    "        loss = 0.5 * (F.kl_div(F.log_softmax(affinity_a, dim=-1), F.softmax(affinity_b, dim=-1), reduction='batchmean') +\n",
    "                      F.kl_div(F.log_softmax(affinity_b, dim=-1), F.softmax(affinity_a, dim=-1), reduction='batchmean'))\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a753ff",
   "metadata": {
    "papermill": {
     "duration": 0.006829,
     "end_time": "2025-03-09T12:28:06.910878",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.904049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Guided Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54752fcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:06.925629Z",
     "iopub.status.busy": "2025-03-09T12:28:06.925310Z",
     "iopub.status.idle": "2025-03-09T12:28:06.933009Z",
     "shell.execute_reply": "2025-03-09T12:28:06.932206Z"
    },
    "papermill": {
     "duration": 0.016648,
     "end_time": "2025-03-09T12:28:06.934274",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.917626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvGuidedFilter(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/wuhuikai/DeepGuidedFilter\n",
    "    \"\"\"\n",
    "    def __init__(self, radius=1, norm=nn.BatchNorm2d, conv_a_kernel_size: int = 1):\n",
    "        super(ConvGuidedFilter, self).__init__()\n",
    "\n",
    "        self.box_filter = nn.Conv2d(\n",
    "            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n",
    "        )\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                6,\n",
    "                32,\n",
    "                kernel_size=conv_a_kernel_size,\n",
    "                padding=conv_a_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                32,\n",
    "                kernel_size=conv_a_kernel_size,\n",
    "                padding=conv_a_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                3,\n",
    "                kernel_size=conv_a_kernel_size,\n",
    "                padding=conv_a_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.box_filter.weight.data[...] = 1.0\n",
    "\n",
    "    def forward(self, x_lr, y_lr, x_hr):\n",
    "        _, _, h_lrx, w_lrx = x_lr.size()\n",
    "        _, _, h_hrx, w_hrx = x_hr.size()\n",
    "\n",
    "        N = self.box_filter(x_lr.data.new().resize_((1, 3, h_lrx, w_lrx)).fill_(1.0))\n",
    "        ## mean_x\n",
    "        mean_x = self.box_filter(x_lr) / N\n",
    "        ## mean_y\n",
    "        mean_y = self.box_filter(y_lr) / N\n",
    "        ## cov_xy\n",
    "        cov_xy = self.box_filter(x_lr * y_lr) / N - mean_x * mean_y\n",
    "        ## var_x\n",
    "        var_x = self.box_filter(x_lr * x_lr) / N - mean_x * mean_x\n",
    "\n",
    "        ## A\n",
    "        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n",
    "        ## b\n",
    "        b = mean_y - A * mean_x\n",
    "\n",
    "        ## mean_A; mean_b\n",
    "        mean_A = F.interpolate(A, (h_hrx, w_hrx), mode=\"bilinear\", align_corners=True)\n",
    "        mean_b = F.interpolate(b, (h_hrx, w_hrx), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return mean_A * x_hr + mean_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45a8ac22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:06.949796Z",
     "iopub.status.busy": "2025-03-09T12:28:06.949480Z",
     "iopub.status.idle": "2025-03-09T12:28:06.954183Z",
     "shell.execute_reply": "2025-03-09T12:28:06.953302Z"
    },
    "papermill": {
     "duration": 0.013524,
     "end_time": "2025-03-09T12:28:06.955417",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.941893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNorm(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(AdaptiveInstanceNorm, self).__init__()\n",
    "\n",
    "        self.w_0 = nn.Parameter(torch.Tensor([1.0]))\n",
    "        self.w_1 = nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "        self.ins_norm = nn.InstanceNorm2d(n, momentum=0.999, eps=0.001, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_0 * x + self.w_1 * self.ins_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c1c560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:06.970469Z",
     "iopub.status.busy": "2025-03-09T12:28:06.970187Z",
     "iopub.status.idle": "2025-03-09T12:28:06.975570Z",
     "shell.execute_reply": "2025-03-09T12:28:06.974744Z"
    },
    "papermill": {
     "duration": 0.01437,
     "end_time": "2025-03-09T12:28:06.976973",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.962603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepGuidednew(nn.Module):\n",
    "    def __init__(self, radius=1):\n",
    "        super().__init__()\n",
    "        norm = AdaptiveInstanceNorm\n",
    "        kernel_size=3\n",
    "        depth_rate=16\n",
    "        in_channels=3\n",
    "        num_dense_layer=4\n",
    "        growth_rate=16\n",
    "        growth_rate=16\n",
    "\n",
    "        # self.local = local\n",
    "        \n",
    "        # self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        # self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "        self.gf = ConvGuidedFilter(radius, norm=norm)\n",
    "        self.lr = dehazeformer_m()\n",
    "\n",
    "        self.downsample = nn.Upsample(\n",
    "            scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "        self.upsample = nn.Upsample(\n",
    "            scale_factor=2, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x_hr, y_detail):\n",
    "        x_lr = self.downsample(x_hr)\n",
    "        # y_lr=self.conv_in(x_lr)\n",
    "        # y_lr= self.local(y_lr)\n",
    "        # y_detail=self.conv_out(y_lr)\n",
    "        y_base=self.lr(x_lr)\n",
    "        # print(y_base.shape, y_detail.shape)\n",
    "        y_lr=y_base+ y_detail\n",
    "        y_base=self.upsample(y_base)\n",
    "        return  self.gf(x_lr, y_lr, x_hr), y_base      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5a83a",
   "metadata": {
    "papermill": {
     "duration": 0.007,
     "end_time": "2025-03-09T12:28:06.990901",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.983901",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18065e59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:07.005842Z",
     "iopub.status.busy": "2025-03-09T12:28:07.005444Z",
     "iopub.status.idle": "2025-03-09T12:28:07.012275Z",
     "shell.execute_reply": "2025-03-09T12:28:07.011496Z"
    },
    "papermill": {
     "duration": 0.015798,
     "end_time": "2025-03-09T12:28:07.013426",
     "exception": false,
     "start_time": "2025-03-09T12:28:06.997628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# CUSTOM DATASET LOADER\n",
    "# -----------------------------\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self, crop_size, hazeeffected_images_dir, hazefree_images_dir):\n",
    "        super().__init__()\n",
    "        hazy_data = glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n",
    "        self.haze_names = [os.path.join(hazeeffected_images_dir, os.path.basename(h)) for h in hazy_data]\n",
    "        self.gt_names = [os.path.join(hazefree_images_dir, os.path.basename(h)) for h in hazy_data]\n",
    "        self.crop_size = crop_size\n",
    "    \n",
    "    def get_images(self, index):\n",
    "        crop_width, crop_height = self.crop_size\n",
    "        haze_img = Image.open(self.haze_names[index]).convert('RGB')\n",
    "        gt_img = Image.open(self.gt_names[index]).convert('RGB')\n",
    "        \n",
    "        width, height = haze_img.size\n",
    "        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n",
    "        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "        \n",
    "        transform = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        return transform(haze_crop_img), transform(gt_crop_img)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_images(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2b540be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:07.028531Z",
     "iopub.status.busy": "2025-03-09T12:28:07.028275Z",
     "iopub.status.idle": "2025-03-09T12:28:07.244437Z",
     "shell.execute_reply": "2025-03-09T12:28:07.243452Z"
    },
    "papermill": {
     "duration": 0.22535,
     "end_time": "2025-03-09T12:28:07.246096",
     "exception": false,
     "start_time": "2025-03-09T12:28:07.020746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "crop_size = (360, 360)\n",
    "# train_data = TrainData(crop_size, '/kaggle/input/nh-dense-haze/Dense-Haze-T/Dense-Haze-T/IN', '/kaggle/input/ nh-dense-haze/Dense-Haze-T/Dense-Haze-T/GT')\n",
    "train_data = TrainData(crop_size, '/kaggle/input/reside6k/RESIDE-6K/train/hazy', '/kaggle/input/reside6k/RESIDE-6K/train/GT')\n",
    "dataloader = DataLoader(train_data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7861e652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:07.261446Z",
     "iopub.status.busy": "2025-03-09T12:28:07.261179Z",
     "iopub.status.idle": "2025-03-09T12:28:07.524284Z",
     "shell.execute_reply": "2025-03-09T12:28:07.523081Z"
    },
    "papermill": {
     "duration": 0.27272,
     "end_time": "2025-03-09T12:28:07.526048",
     "exception": false,
     "start_time": "2025-03-09T12:28:07.253328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hazy Images Shape: torch.Size([4, 3, 360, 360])\n",
      "Clear Images Shape: torch.Size([4, 3, 360, 360])\n"
     ]
    }
   ],
   "source": [
    "# Get a single batch from the dataloader\n",
    "for hazy_images, clear_images in dataloader:\n",
    "    print(f\"Hazy Images Shape: {hazy_images.shape}\")\n",
    "    print(f\"Clear Images Shape: {clear_images.shape}\")\n",
    "    break  # Only check one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136cc728",
   "metadata": {
    "papermill": {
     "duration": 0.006716,
     "end_time": "2025-03-09T12:28:07.541830",
     "exception": false,
     "start_time": "2025-03-09T12:28:07.535114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e1361a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:07.557129Z",
     "iopub.status.busy": "2025-03-09T12:28:07.556805Z",
     "iopub.status.idle": "2025-03-09T12:28:11.663699Z",
     "shell.execute_reply": "2025-03-09T12:28:11.662963Z"
    },
    "papermill": {
     "duration": 4.116436,
     "end_time": "2025-03-09T12:28:11.665298",
     "exception": false,
     "start_time": "2025-03-09T12:28:07.548862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0.00/528M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▎         | 19.6M/528M [00:00<00:02, 205MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 42.5M/528M [00:00<00:02, 225MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 65.2M/528M [00:00<00:02, 231MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 88.2M/528M [00:00<00:01, 235MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 111M/528M [00:00<00:01, 234MB/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 133M/528M [00:00<00:01, 235MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██▉       | 156M/528M [00:00<00:01, 233MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▎      | 178M/528M [00:00<00:01, 191MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 201M/528M [00:00<00:01, 204MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 224M/528M [00:01<00:01, 214MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 247M/528M [00:01<00:01, 222MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████     | 270M/528M [00:01<00:01, 227MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 293M/528M [00:01<00:01, 231MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████▉    | 316M/528M [00:01<00:00, 234MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 339M/528M [00:01<00:00, 235MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▊   | 362M/528M [00:01<00:00, 237MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 385M/528M [00:01<00:00, 236MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 407M/528M [00:01<00:00, 235MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████▏ | 430M/528M [00:01<00:00, 235MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 453M/528M [00:02<00:00, 236MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 476M/528M [00:02<00:00, 238MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 499M/528M [00:02<00:00, 238MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▉| 522M/528M [00:02<00:00, 238MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 528M/528M [00:02<00:00, 229MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "loss_model = vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41576954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:11.684323Z",
     "iopub.status.busy": "2025-03-09T12:28:11.684000Z",
     "iopub.status.idle": "2025-03-09T12:28:11.690074Z",
     "shell.execute_reply": "2025-03-09T12:28:11.689399Z"
    },
    "papermill": {
     "duration": 0.016883,
     "end_time": "2025-03-09T12:28:11.691158",
     "exception": false,
     "start_time": "2025-03-09T12:28:11.674275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd406ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:11.708964Z",
     "iopub.status.busy": "2025-03-09T12:28:11.708654Z",
     "iopub.status.idle": "2025-03-09T12:28:11.901582Z",
     "shell.execute_reply": "2025-03-09T12:28:11.900874Z"
    },
    "papermill": {
     "duration": 0.203512,
     "end_time": "2025-03-09T12:28:11.903269",
     "exception": false,
     "start_time": "2025-03-09T12:28:11.699757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_model = loss_model.features\n",
    "loss_model = loss_model.to(device)\n",
    "for param in loss_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93c07464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:11.922265Z",
     "iopub.status.busy": "2025-03-09T12:28:11.921982Z",
     "iopub.status.idle": "2025-03-09T12:28:11.927709Z",
     "shell.execute_reply": "2025-03-09T12:28:11.926965Z"
    },
    "papermill": {
     "duration": 0.016508,
     "end_time": "2025-03-09T12:28:11.928985",
     "exception": false,
     "start_time": "2025-03-09T12:28:11.912477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureLossNetwork(torch.nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(FeatureLossNetwork, self).__init__()\n",
    "        self.feature_layers = feature_extractor\n",
    "        self.layer_name_mapping = {\n",
    "            '1': \"relu1_1\",\n",
    "            # '3': \"relu1_2\",\n",
    "            # '6': \"relu2_1\",\n",
    "            # '8': \"relu2_2\",\n",
    "            # '11': \"relu3_1\",\n",
    "            # '13': \"relu3_2\",\n",
    "            # '15': \"relu3_3\",\n",
    "            '18': \"relu4_1\",\n",
    "            # '20': \"relu4_2\",\n",
    "            # '22': \"relu4_3\",\n",
    "            # '25': \"relu5_1\",\n",
    "            # '27': \"relu5_2\",\n",
    "            '29': \"relu5_3\"\n",
    "        }\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        output = {}\n",
    "        for name, module in self.feature_layers._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layer_name_mapping:\n",
    "                output[self.layer_name_mapping[name]] = x\n",
    "        return list(output.values())\n",
    "\n",
    "    def forward(self, predicted, ground_truth):\n",
    "        loss = []\n",
    "        scale_factor = 1000  \n",
    "        predicted_features = self.extract_features(predicted)\n",
    "        ground_truth_features = self.extract_features(ground_truth)\n",
    "        for pred_feature, gt_feature in zip(predicted_features, ground_truth_features):\n",
    "            loss.append(F.mse_loss(pred_feature, gt_feature))\n",
    "\n",
    "        return sum(loss) / (len(loss) * scale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "934213fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:11.947058Z",
     "iopub.status.busy": "2025-03-09T12:28:11.946790Z",
     "iopub.status.idle": "2025-03-09T12:28:11.952332Z",
     "shell.execute_reply": "2025-03-09T12:28:11.951496Z"
    },
    "papermill": {
     "duration": 0.015985,
     "end_time": "2025-03-09T12:28:11.953605",
     "exception": false,
     "start_time": "2025-03-09T12:28:11.937620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureLossNetwork(\n",
       "  (feature_layers): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_network = FeatureLossNetwork(loss_model)\n",
    "loss_network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "359e4ce0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:11.972134Z",
     "iopub.status.busy": "2025-03-09T12:28:11.971894Z",
     "iopub.status.idle": "2025-03-09T12:28:11.974999Z",
     "shell.execute_reply": "2025-03-09T12:28:11.974246Z"
    },
    "papermill": {
     "duration": 0.013732,
     "end_time": "2025-03-09T12:28:11.976205",
     "exception": false,
     "start_time": "2025-03-09T12:28:11.962473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Test loss network with generated image and original image\n",
    "# if isinstance(dehazed_image, np.ndarray):\n",
    "#     dehazed_image = torch.from_numpy(dehazed_image).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "# if isinstance(image, np.ndarray):\n",
    "#     image = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "# # Compute loss\n",
    "# dehazed_image = dehazed_image.to(device)\n",
    "# image.to(device)\n",
    "# loss = loss_network(dehazed_image, image)\n",
    "# print(\"Feature Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dc8fadc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:11.994283Z",
     "iopub.status.busy": "2025-03-09T12:28:11.994031Z",
     "iopub.status.idle": "2025-03-09T12:28:11.997348Z",
     "shell.execute_reply": "2025-03-09T12:28:11.996618Z"
    },
    "papermill": {
     "duration": 0.013903,
     "end_time": "2025-03-09T12:28:11.998629",
     "exception": false,
     "start_time": "2025-03-09T12:28:11.984726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92d4cc76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:12.017805Z",
     "iopub.status.busy": "2025-03-09T12:28:12.017422Z",
     "iopub.status.idle": "2025-03-09T12:28:12.282452Z",
     "shell.execute_reply": "2025-03-09T12:28:12.281380Z"
    },
    "papermill": {
     "duration": 0.275759,
     "end_time": "2025-03-09T12:28:12.283652",
     "exception": false,
     "start_time": "2025-03-09T12:28:12.007893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_params: 4637423\n"
     ]
    }
   ],
   "source": [
    "# --- GPU device --- #\n",
    "device_ids = list(range(torch.cuda.device_count()))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Define the network --- #\n",
    "net = DeepGuidednew()\n",
    "\n",
    "# --- Multi-GPU (correct order) --- #\n",
    "net = nn.DataParallel(net, device_ids=device_ids).to(device)\n",
    "\n",
    "# --- Build optimizer --- #\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# # --- Define the perceptual loss network --- #\n",
    "# vgg_model = vgg16(pretrained=True).features[:16].to(device)\n",
    "# for param in vgg_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# loss_network = LossNetwork(vgg_model)\n",
    "# loss_network.eval()\n",
    "\n",
    "# models = 'formernew'\n",
    "\n",
    "# --- Load the network weight --- #\n",
    "# weight_path = \"{}_{}_haze_best_{}\".format(models, category, version)\n",
    "# try:\n",
    "#     net.load_state_dict(torch.load(weight_path))\n",
    "#     print('--- weight loaded ---')\n",
    "# except FileNotFoundError:\n",
    "#     print('--- no weight loaded ---')\n",
    "\n",
    "# --- Calculate all trainable parameters in network --- #\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"Total_params: {}\".format(pytorch_total_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1c98c80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:12.302632Z",
     "iopub.status.busy": "2025-03-09T12:28:12.302320Z",
     "iopub.status.idle": "2025-03-09T12:28:12.307039Z",
     "shell.execute_reply": "2025-03-09T12:28:12.305777Z"
    },
    "papermill": {
     "duration": 0.015981,
     "end_time": "2025-03-09T12:28:12.308489",
     "exception": false,
     "start_time": "2025-03-09T12:28:12.292508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_loss: 0.84\n"
     ]
    }
   ],
   "source": [
    "lambda_loss = 0.84\n",
    "print(f'lambda_loss: {lambda_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "541fef8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:12.327627Z",
     "iopub.status.busy": "2025-03-09T12:28:12.327326Z",
     "iopub.status.idle": "2025-03-09T12:28:12.338260Z",
     "shell.execute_reply": "2025-03-09T12:28:12.337477Z"
    },
    "papermill": {
     "duration": 0.021951,
     "end_time": "2025-03-09T12:28:12.339518",
     "exception": false,
     "start_time": "2025-03-09T12:28:12.317567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# CO-DISTILLATION TRAINING\n",
    "# -----------------------------\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import csv\n",
    "\n",
    "def calculate_psnr(output, target, max_pixel_value=1.0):\n",
    "    mse = F.mse_loss(output, target)\n",
    "    # print(f\"MSE: {mse.item()}\")\n",
    "    if mse == 0:\n",
    "        return 100  # Avoid log(0) case, return max PSNR\n",
    "    psnr = 20 * math.log10(max_pixel_value) - 10 * math.log10(mse.item())\n",
    "    # print(f\"PSNR: {psnr}\")\n",
    "    return psnr\n",
    "\n",
    "# -----------------------------\n",
    "# CO-DISTILLATION TRAINING\n",
    "# -----------------------------\n",
    "def train(net, teacher, student, fam, dataloader, num_epochs=10, lambda_fam=0.25, log_file=\"training_log.csv\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    # net.to(device).train()\n",
    "    teacher.to(device).train()\n",
    "    student.to(device).train()\n",
    "    fam.to(device)\n",
    "\n",
    "    optimizer_t = torch.optim.Adam(teacher.parameters(), lr=1e-2)\n",
    "    # optimizer_s = torch.optim.Adam(student.parameters(), lr=1e-2)\n",
    "    # optimizer_d = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "    \n",
    "    scheduler_t = CosineAnnealingLR(optimizer_t, T_max=num_epochs, eta_min=1e-3)\n",
    "    # scheduler_s = CosineAnnealingLR(optimizer_s, T_max=num_epochs, eta_min=1e-3)\n",
    "    # scheduler_d = CosineAnnealingLR(optimizer_d, T_max=num_epochs, eta_min=1e-3)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_psnr = 0\n",
    "\n",
    "    with open(log_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Epoch\", \"Loss\", \"Teacher PSNR\", \"Student PSNR\"])\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            total_loss = 0\n",
    "            total_psnr_t = 0  # Teacher PSNR\n",
    "            # total_psnr_s = 0  # Student PSNR\n",
    "            num_batches = len(dataloader)\n",
    "            teacher_output = 0\n",
    "            # student_output = 0\n",
    "            \n",
    "            for hazy_images, clear_images in dataloader:\n",
    "                hazy_images, clear_images = hazy_images.to(device), clear_images.to(device)\n",
    "                # print(f\"Hazy images shape: {hazy_images.shape}, Clear images shape: {clear_images.shape}\")\n",
    "                \n",
    "                teacher_output = teacher(clear_images)\n",
    "                # student_output = student(hazy_images)\n",
    "                # print(f\"Teacher output shape: {teacher_output.shape}, Student output shape: {student_output.shape}\")\n",
    "\n",
    "                # dehaze,base = net(hazy_images, student_output)\n",
    "                \n",
    "                # base_loss = F.smooth_l1_loss(base, clear_images)\n",
    "                # smooth_loss = F.smooth_l1_loss(dehaze, clear_images)\n",
    "                # perceptual_loss = loss_network(dehaze, clear_images)\n",
    "\n",
    "                # print(\"Type: Teacher:\", type(teacher_output))\n",
    "                # print(\"Type: student_output:\", type(student_output))\n",
    "                # print(\"Type: detail_output:\", type(detail_output))\n",
    "                downsample = nn.Upsample(\n",
    "                    scale_factor=0.5, mode=\"bilinear\", align_corners=True\n",
    "                )\n",
    "                clear_images = downsample(clear_images)\n",
    "\n",
    "                # mse_loss_d = F.mse_loss(detail_output, clear_images)\n",
    "                mse_loss_t = F.mse_loss(teacher_output, clear_images)\n",
    "                perceptual_loss = loss_network(teacher_output, clear_images)\n",
    "            \n",
    "                # mse_loss_s = F.mse_loss(student_output, clear_images)\n",
    "                # print(f\"MSE Loss - Teacher: {mse_loss_t.item()}, Student: {mse_loss_s.item()}\")\n",
    "                \n",
    "                # fam_loss = fam(teacher_output, student_output) \n",
    "                # print(f\"FAM Loss: {fam_loss.item()}\")\n",
    "                \n",
    "                # print(\"Base Loss:\", base_loss)\n",
    "                # print(\"Smooth Loss:\", smooth_loss)\n",
    "                # print(\"Lambda Loss * Perceptual Loss:\", lambda_loss * perceptual_loss)\n",
    "                # print(\"FAM Loss:\", fam_loss)\n",
    "\n",
    "                \n",
    "                loss =  mse_loss_t + lambda_loss * perceptual_loss \n",
    "                # loss = base_loss + smooth_loss + lambda_loss * perceptual_loss + fam_loss \n",
    "                \n",
    "                # loss = base_loss + smooth_loss + lambda_loss * perceptual_loss+ fam_loss + mse_loss_t + mse_loss_s\n",
    "                # print(f\"Total Loss: {loss.item()}\")\n",
    "                \n",
    "                optimizer_t.zero_grad()\n",
    "                # optimizer_s.zero_grad()\n",
    "                # optimizer_d.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_t.step()\n",
    "                # optimizer_s.step()\n",
    "                # optimizer_d.step()\n",
    "                \n",
    "                # print(f\"PSNR - Teacher: {psnr_t}, Student: {psnr_s}\")\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            # Compute PSNR for teacher and student\n",
    "            psnr_t = calculate_psnr(teacher_output, clear_images)\n",
    "            # psnr_s = calculate_psnr(student_output, clear_images)\n",
    "            total_psnr_t += psnr_t\n",
    "            # total_psnr_s += psnr_s\n",
    "\n",
    "            avg_loss = total_loss / num_batches\n",
    "            avg_psnr_t = total_psnr_t / num_batches\n",
    "            # avg_psnr_s = total_psnr_s / num_batches\n",
    "            # print(f\"Epoch {epoch + 1} - Avg Loss: {avg_loss}, Avg PSNR (Teacher): {avg_psnr_t}, Avg PSNR (Student): {avg_psnr_s}\")\n",
    "            print(f\"Epoch {epoch + 1} - Avg Loss: {avg_loss}, Avg PSNR (Teacher): {avg_psnr_t}\")\n",
    "\n",
    "\n",
    "            # log_entry = f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.6f}, Avg Teacher PSNR: {avg_psnr_t:.2f}, Avg Student PSNR: {avg_psnr_s:.2f}\"\n",
    "            log_entry = f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.6f}, Avg Teacher PSNR: {avg_psnr_t:.2f}\"\n",
    "            print(log_entry)\n",
    "\n",
    "            # Write log to file\n",
    "            # writer.writerow([epoch + 1, avg_loss, avg_psnr_t, avg_psnr_s])\n",
    "            writer.writerow([epoch + 1, avg_loss, avg_psnr_t])\n",
    "\n",
    "            # Update schedulers\n",
    "            scheduler_t.step()\n",
    "            # scheduler_s.step()\n",
    "            # scheduler_d.step()\n",
    "\n",
    "            # Save only if the model improves\n",
    "            # if avg_loss < best_loss or epoch %50 ==0:\n",
    "            best_loss = avg_loss\n",
    "            # best_psnr = avg_psnr_s\n",
    "            best_psnr = avg_psnr_t\n",
    "            torch.save(student.state_dict(), str(epoch)+\"best_dehazing_student.pth\")\n",
    "            # torch.save(teacher.state_dict(), str(epoch)+\"best_sr_teacher.pth\")\n",
    "            # print(f\"Saved Best Model (Loss: {best_loss:.6f}, Student PSNR: {best_psnr:.2f})\")\n",
    "            print(f\"Saved Best Model (Loss: {best_loss:.6f}, PSNR: {best_psnr:.2f})\")\n",
    "\n",
    "    print(\"Training complete. Logs saved in\", log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "628efec0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:12.358835Z",
     "iopub.status.busy": "2025-03-09T12:28:12.358493Z",
     "iopub.status.idle": "2025-03-09T12:28:12.370483Z",
     "shell.execute_reply": "2025-03-09T12:28:12.369853Z"
    },
    "papermill": {
     "duration": 0.023019,
     "end_time": "2025-03-09T12:28:12.371843",
     "exception": false,
     "start_time": "2025-03-09T12:28:12.348824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# TRAINING SETUP \n",
    "# -----------------------------\n",
    "teacher_model = SR_model(upscale_factor=1)\n",
    "student_model = DehazingNet()\n",
    "fam_module = FeatureAffinityModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9baaa14c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T12:28:12.390593Z",
     "iopub.status.busy": "2025-03-09T12:28:12.390325Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-03-09T12:28:12.380842",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.9971499037338654, Avg PSNR (Teacher): 0.0\n",
      "Epoch 1/500, Avg Loss: 0.997150, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 0.997150, PSNR: 0.00)\n",
      "Epoch 2/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Avg Loss: 1.0007535820007325, Avg PSNR (Teacher): 0.0\n",
      "Epoch 2/500, Avg Loss: 1.000754, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000754, PSNR: 0.00)\n",
      "Epoch 3/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Avg Loss: 1.0007515109380087, Avg PSNR (Teacher): 0.0\n",
      "Epoch 3/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 4/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Avg Loss: 1.0007520149548847, Avg PSNR (Teacher): 0.0\n",
      "Epoch 4/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 5/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Avg Loss: 1.000751891851425, Avg PSNR (Teacher): 0.0\n",
      "Epoch 5/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 6/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Avg Loss: 1.0007522361278534, Avg PSNR (Teacher): 0.0\n",
      "Epoch 6/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 7/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Avg Loss: 1.0007523809274037, Avg PSNR (Teacher): 0.0\n",
      "Epoch 7/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 8/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Avg Loss: 1.000752190510432, Avg PSNR (Teacher): 0.0\n",
      "Epoch 8/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 9/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Avg Loss: 1.0007524010340372, Avg PSNR (Teacher): 0.0\n",
      "Epoch 9/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 10/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Avg Loss: 1.0007520370483398, Avg PSNR (Teacher): 0.0\n",
      "Epoch 10/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 11/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Avg Loss: 1.0007520078023275, Avg PSNR (Teacher): 0.0\n",
      "Epoch 11/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 12/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Avg Loss: 1.0007514360745748, Avg PSNR (Teacher): 0.0\n",
      "Epoch 12/500, Avg Loss: 1.000751, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000751, PSNR: 0.00)\n",
      "Epoch 13/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Avg Loss: 1.0007516624132793, Avg PSNR (Teacher): 0.0\n",
      "Epoch 13/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 14/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Avg Loss: 1.0007523275216421, Avg PSNR (Teacher): 0.0\n",
      "Epoch 14/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 15/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Avg Loss: 1.000751720269521, Avg PSNR (Teacher): 0.0\n",
      "Epoch 15/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 16/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Avg Loss: 1.0007523580392201, Avg PSNR (Teacher): 0.0\n",
      "Epoch 16/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 17/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Avg Loss: 1.0007515414555868, Avg PSNR (Teacher): 0.0\n",
      "Epoch 17/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 18/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Avg Loss: 1.0007517941792805, Avg PSNR (Teacher): 0.0\n",
      "Epoch 18/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 19/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Avg Loss: 1.0007522846857706, Avg PSNR (Teacher): 0.0\n",
      "Epoch 19/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 20/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Avg Loss: 1.0007516452471414, Avg PSNR (Teacher): 0.0\n",
      "Epoch 20/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 21/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Avg Loss: 1.000752527475357, Avg PSNR (Teacher): 0.0\n",
      "Epoch 21/500, Avg Loss: 1.000753, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000753, PSNR: 0.00)\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Avg Loss: 1.0007518406709035, Avg PSNR (Teacher): 0.0\n",
      "Epoch 22/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Avg Loss: 1.000751805305481, Avg PSNR (Teacher): 0.0\n",
      "Epoch 23/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Avg Loss: 1.0007515659332276, Avg PSNR (Teacher): 0.0\n",
      "Epoch 24/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Avg Loss: 1.000752698659897, Avg PSNR (Teacher): 0.0\n",
      "Epoch 25/500, Avg Loss: 1.000753, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000753, PSNR: 0.00)\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Avg Loss: 1.000752191543579, Avg PSNR (Teacher): 0.0\n",
      "Epoch 26/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Avg Loss: 1.0007524001598358, Avg PSNR (Teacher): 0.0\n",
      "Epoch 27/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Avg Loss: 1.0007518305778504, Avg PSNR (Teacher): 0.0\n",
      "Epoch 28/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Avg Loss: 1.0007522219816845, Avg PSNR (Teacher): 0.0\n",
      "Epoch 29/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 30/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Avg Loss: 1.0007519915103913, Avg PSNR (Teacher): 0.0\n",
      "Epoch 30/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 31/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Avg Loss: 1.0007520857652028, Avg PSNR (Teacher): 0.0\n",
      "Epoch 31/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 32/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Avg Loss: 1.000751939535141, Avg PSNR (Teacher): 0.0\n",
      "Epoch 32/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 33/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Avg Loss: 1.0007523392041524, Avg PSNR (Teacher): 0.0\n",
      "Epoch 33/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 34/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Avg Loss: 1.0007525963783264, Avg PSNR (Teacher): 0.0\n",
      "Epoch 34/500, Avg Loss: 1.000753, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000753, PSNR: 0.00)\n",
      "Epoch 35/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Avg Loss: 1.0007521199385325, Avg PSNR (Teacher): 0.0\n",
      "Epoch 35/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 36/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Avg Loss: 1.0007527764638264, Avg PSNR (Teacher): 0.0\n",
      "Epoch 36/500, Avg Loss: 1.000753, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000753, PSNR: 0.00)\n",
      "Epoch 37/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Avg Loss: 1.0007522921562195, Avg PSNR (Teacher): 0.0\n",
      "Epoch 37/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 38/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Avg Loss: 1.0007518643538158, Avg PSNR (Teacher): 0.0\n",
      "Epoch 38/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 39/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Avg Loss: 1.0007520609696705, Avg PSNR (Teacher): 0.0\n",
      "Epoch 39/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 40/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Avg Loss: 1.0007522761821748, Avg PSNR (Teacher): 0.0\n",
      "Epoch 40/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 41/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 - Avg Loss: 1.000751981417338, Avg PSNR (Teacher): 0.0\n",
      "Epoch 41/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 42/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 - Avg Loss: 1.0007521580855052, Avg PSNR (Teacher): 0.0\n",
      "Epoch 42/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 43/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 - Avg Loss: 1.000752034028371, Avg PSNR (Teacher): 0.0\n",
      "Epoch 43/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 44/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 - Avg Loss: 1.0007524245580037, Avg PSNR (Teacher): 0.0\n",
      "Epoch 44/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 45/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 - Avg Loss: 1.0007520248095194, Avg PSNR (Teacher): 0.0\n",
      "Epoch 45/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 46/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 - Avg Loss: 1.0007521653175353, Avg PSNR (Teacher): 0.0\n",
      "Epoch 46/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 47/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 - Avg Loss: 1.000752055088679, Avg PSNR (Teacher): 0.0\n",
      "Epoch 47/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 48/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 - Avg Loss: 1.0007518549760184, Avg PSNR (Teacher): 0.0\n",
      "Epoch 48/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 49/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Avg Loss: 1.0007520685195923, Avg PSNR (Teacher): 0.0\n",
      "Epoch 49/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 50/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 - Avg Loss: 1.000751586119334, Avg PSNR (Teacher): 0.0\n",
      "Epoch 50/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 - Avg Loss: 1.0007522214253743, Avg PSNR (Teacher): 0.0\n",
      "Epoch 51/500, Avg Loss: 1.000752, Avg Teacher PSNR: 0.00\n",
      "Saved Best Model (Loss: 1.000752, PSNR: 0.00)\n",
      "Epoch 52/500\n"
     ]
    }
   ],
   "source": [
    "train(net,teacher_model, student_model, fam_module, dataloader, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecfafd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T13:50:42.368873Z",
     "iopub.status.busy": "2025-03-08T13:50:42.368428Z",
     "iopub.status.idle": "2025-03-08T13:50:42.390673Z",
     "shell.execute_reply": "2025-03-08T13:50:42.389482Z",
     "shell.execute_reply.started": "2025-03-08T13:50:42.368832Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # LOAD MODEL\n",
    "# # -----------------------------\n",
    "# # model_path = \"/kaggle/input/dehazing_sr/pytorch/default/1/best_dehazing_student.pth\"\n",
    "# model_path = \"/kaggle/input/dehazing_sr/pytorch/default/4/9best_sr_teacher.pth\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8114e5d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T13:50:42.392303Z",
     "iopub.status.busy": "2025-03-08T13:50:42.391913Z",
     "iopub.status.idle": "2025-03-08T13:50:42.427537Z",
     "shell.execute_reply": "2025-03-08T13:50:42.426555Z",
     "shell.execute_reply.started": "2025-03-08T13:50:42.392264Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initialize model\n",
    "# # model = DehazingNet().to(device)\n",
    "# model = SR_model(upscale_factor=1).to(device)\n",
    "# model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b882e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T13:50:42.428899Z",
     "iopub.status.busy": "2025-03-08T13:50:42.428517Z",
     "iopub.status.idle": "2025-03-08T13:50:42.551013Z",
     "shell.execute_reply": "2025-03-08T13:50:42.549716Z",
     "shell.execute_reply.started": "2025-03-08T13:50:42.428870Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # LOAD TEST DATA\n",
    "# # -----------------------------\n",
    "# test_hazy_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/hazy\"\n",
    "# test_gt_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/GT\"\n",
    "# # test_hazy_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN\"\n",
    "# # test_gt_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT\"\n",
    "\n",
    "# hazy_images = sorted(glob.glob(os.path.join(test_hazy_dir, \"*.*\")))\n",
    "# gt_images = sorted(glob.glob(os.path.join(test_gt_dir, \"*.*\")))\n",
    "\n",
    "# transform = Compose([\n",
    "#     ToTensor(),\n",
    "#     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# to_pil = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800932e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T13:50:42.558529Z",
     "iopub.status.busy": "2025-03-08T13:50:42.558199Z",
     "iopub.status.idle": "2025-03-08T13:50:52.447295Z",
     "shell.execute_reply": "2025-03-08T13:50:52.444955Z",
     "shell.execute_reply.started": "2025-03-08T13:50:42.558495Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION\n",
    "# # -----------------------------\n",
    "# num_samples = 20  # Change as needed\n",
    "# plt.figure(figsize=(10, num_samples * 5))\n",
    "\n",
    "# for i in range(num_samples):\n",
    "#     hazy_img = Image.open(hazy_images[i])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         output_tensor = model(input_tensor).cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(num_samples, 2, 2 * i + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(\"Hazy Input\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(num_samples, 2, 2 * i + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(\"Output\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875aece1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T13:50:52.449264Z",
     "iopub.status.busy": "2025-03-08T13:50:52.448929Z",
     "iopub.status.idle": "2025-03-08T13:50:55.829741Z",
     "shell.execute_reply": "2025-03-08T13:50:55.828075Z",
     "shell.execute_reply.started": "2025-03-08T13:50:52.449236Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [70, 75, 89, 100]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         output_tensor = model(input_tensor).cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9749a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T13:50:55.832002Z",
     "iopub.status.busy": "2025-03-08T13:50:55.831378Z",
     "iopub.status.idle": "2025-03-08T13:50:58.405644Z",
     "shell.execute_reply": "2025-03-08T13:50:58.404007Z",
     "shell.execute_reply.started": "2025-03-08T13:50:55.831947Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION\n",
    "# # -----------------------------\n",
    "# num_samples = 5  # Change as needed\n",
    "# plt.figure(figsize=(10, num_samples * 5))\n",
    "\n",
    "# for i in range(num_samples):\n",
    "#     hazy_img = Image.open(hazy_images[i]).convert('RGB')\n",
    "#     gt_img = Image.open(gt_images[i]).convert('RGB')\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         output_tensor = teacher_model(input_tensor).cpu().squeeze(0)\n",
    "    \n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(num_samples, 3, 3 * i + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(\"Hazy Input\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(num_samples, 3, 3 * i + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(\"Dehazed Output\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(num_samples, 3, 3 * i + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(\"Ground Truth\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac278514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T13:50:58.407161Z",
     "iopub.status.busy": "2025-03-08T13:50:58.406772Z",
     "iopub.status.idle": "2025-03-08T13:50:58.463952Z",
     "shell.execute_reply": "2025-03-08T13:50:58.462930Z",
     "shell.execute_reply.started": "2025-03-08T13:50:58.407128Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_path = \"/kaggle/input/dehazing_sr/pytorch/default/4/9best_dehazing_student.pth\"\n",
    "# model = DehazingNet().to(device)\n",
    "# # model = SR_model(upscale_factor=1).to(device)\n",
    "# model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5566cb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T13:50:58.465380Z",
     "iopub.status.busy": "2025-03-08T13:50:58.465070Z",
     "iopub.status.idle": "2025-03-08T13:51:17.729232Z",
     "shell.execute_reply": "2025-03-08T13:51:17.725980Z",
     "shell.execute_reply.started": "2025-03-08T13:50:58.465349Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION\n",
    "# # -----------------------------\n",
    "# num_samples = 20  # Change as needed\n",
    "# plt.figure(figsize=(10, num_samples * 5))\n",
    "\n",
    "# for i in range(num_samples):\n",
    "#     hazy_img = Image.open(hazy_images[i])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         output_tensor = model(input_tensor).cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(num_samples, 2, 2 * i + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(\"Hazy Input\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(num_samples, 2, 2 * i + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(\"Output\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a3bde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T13:51:17.731051Z",
     "iopub.status.busy": "2025-03-08T13:51:17.730473Z",
     "iopub.status.idle": "2025-03-08T13:51:20.257841Z",
     "shell.execute_reply": "2025-03-08T13:51:20.255980Z",
     "shell.execute_reply.started": "2025-03-08T13:51:17.730994Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION\n",
    "# # -----------------------------\n",
    "# num_samples = 5  # Change as needed\n",
    "# plt.figure(figsize=(10, num_samples * 5))\n",
    "\n",
    "# for i in range(num_samples):\n",
    "#     hazy_img = Image.open(hazy_images[i]).convert('RGB')\n",
    "#     gt_img = Image.open(gt_images[i]).convert('RGB')\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         output_tensor = teacher_model(input_tensor).cpu().squeeze(0)\n",
    "    \n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(num_samples, 3, 3 * i + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(\"Hazy Input\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(num_samples, 3, 3 * i + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(\"Dehazed Output\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(num_samples, 3, 3 * i + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(\"Ground Truth\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97280555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T13:51:20.259399Z",
     "iopub.status.busy": "2025-03-08T13:51:20.259063Z",
     "iopub.status.idle": "2025-03-08T13:51:27.078456Z",
     "shell.execute_reply": "2025-03-08T13:51:27.077177Z",
     "shell.execute_reply.started": "2025-03-08T13:51:20.259369Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [70, 75, 89, 100]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         output_tensor = model(input_tensor).cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6456606,
     "sourceId": 10417877,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6464114,
     "sourceId": 10443410,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 255960,
     "modelInstanceId": 234257,
     "sourceId": 273590,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 255960,
     "modelInstanceId": 234257,
     "sourceId": 274811,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 255960,
     "modelInstanceId": 234257,
     "sourceId": 274976,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 255960,
     "modelInstanceId": 234257,
     "sourceId": 279190,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-09T12:27:51.634396",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}