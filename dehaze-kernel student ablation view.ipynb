{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 14 19:24:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.02                 Driver Version: 576.02         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   40C    P8              1W /  120W |     115MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           16260    C+G   ...s\\current\\emulator\\crosvm.exe      N/A      |\n",
      "|    0   N/A  N/A           30852    C+G   ...s\\current\\emulator\\crosvm.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from IPython import get_ipython\n",
    "\n",
    "if shutil.which(\"nvidia-smi\") is not None:\n",
    "    get_ipython().system(\"nvidia-smi\")\n",
    "else:\n",
    "    print(\"No NVIDIA GPU or driver detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Device IDs: [0]\n"
     ]
    }
   ],
   "source": [
    "# --- Device Setup --- #\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_ids = list(range(torch.cuda.device_count()))\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Device IDs: {device_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "Current GPU: 0\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU Memory Allocated: 0 bytes\n",
      "GPU Memory Cached: 0 bytes\n",
      "GPU Memory Allocated (Total): 0 bytes\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(device)}\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "    print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(device)} bytes\")\n",
    "    print(f\"GPU Memory Allocated (Total): {torch.cuda.memory_allocated()} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:01.883179Z",
     "iopub.status.busy": "2025-04-21T16:01:01.882223Z",
     "iopub.status.idle": "2025-04-21T16:01:16.592457Z",
     "shell.execute_reply": "2025-04-21T16:01:16.591461Z",
     "shell.execute_reply.started": "2025-04-21T16:01:01.883139Z"
    },
    "papermill": {
     "duration": 11.276571,
     "end_time": "2025-04-12T13:09:32.125878",
     "exception": false,
     "start_time": "2025-04-12T13:09:20.849307",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "from timm.layers import to_2tuple, trunc_normal_\n",
    "import os\n",
    "import torchvision.utils as utils\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from random import randrange\n",
    "import time\n",
    "from math import log10\n",
    "from skimage import measure\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.nn.init import trunc_normal_\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.593951Z",
     "iopub.status.busy": "2025-04-21T16:01:16.593512Z",
     "iopub.status.idle": "2025-04-21T16:01:16.598530Z",
     "shell.execute_reply": "2025-04-21T16:01:16.597465Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.593926Z"
    },
    "papermill": {
     "duration": 0.015315,
     "end_time": "2025-04-12T13:09:32.152405",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.137090",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009983,
     "end_time": "2025-04-12T13:09:32.172516",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.162533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RevisedLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.601109Z",
     "iopub.status.busy": "2025-04-21T16:01:16.600708Z",
     "iopub.status.idle": "2025-04-21T16:01:16.719788Z",
     "shell.execute_reply": "2025-04-21T16:01:16.718584Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.601080Z"
    },
    "papermill": {
     "duration": 0.01803,
     "end_time": "2025-04-12T13:09:32.200726",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.182696",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RevisedLayerNorm(nn.Module):\n",
    "    \"\"\"Revised LayerNorm\"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n",
    "        super(RevisedLayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.detach_gradient = detach_gradient\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n",
    "        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n",
    "\n",
    "        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "\n",
    "        trunc_normal_(self.scale_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.scale_mlp.bias, 1)\n",
    "\n",
    "        trunc_normal_(self.shift_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.shift_mlp.bias, 0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n",
    "        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n",
    "\n",
    "        normalized_tensor = (input_tensor - mean_value) / std_value\n",
    "\n",
    "        if self.detach_gradient:\n",
    "            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n",
    "        else:\n",
    "            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n",
    "\n",
    "        output = normalized_tensor * self.scale + self.shift\n",
    "        return output, rescale, rebias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.720946Z",
     "iopub.status.busy": "2025-04-21T16:01:16.720710Z",
     "iopub.status.idle": "2025-04-21T16:01:16.744866Z",
     "shell.execute_reply": "2025-04-21T16:01:16.743982Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.720927Z"
    },
    "papermill": {
     "duration": 0.019569,
     "end_time": "2025-04-12T13:09:32.230395",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.210826",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n",
    "        super().__init__()\n",
    "        output_channels = output_channels or input_channels\n",
    "        hidden_channels = hidden_channels or input_channels\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            gain = (8 * self.depth) ** (-1 / 4)\n",
    "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "            trunc_normal_(layer.weight, std=std)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "\n",
    "def partition_into_windows(tensor, window_size):\n",
    "    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n",
    "    batch_size, height, width, channels = tensor.shape\n",
    "    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n",
    "\n",
    "    tensor = tensor.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n",
    "    )\n",
    "    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, height, width):\n",
    "    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n",
    "    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n",
    "    tensor = windows.view(\n",
    "        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010551,
     "end_time": "2025-04-12T13:09:32.251201",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.240650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.746699Z",
     "iopub.status.busy": "2025-04-21T16:01:16.746311Z",
     "iopub.status.idle": "2025-04-21T16:01:16.935555Z",
     "shell.execute_reply": "2025-04-21T16:01:16.934584Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.746668Z"
    },
    "papermill": {
     "duration": 0.141806,
     "end_time": "2025-04-12T13:09:32.403159",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.261353",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 16, 16]),\n",
       " torch.Size([32, 16, 64]),\n",
       " torch.Size([2, 16, 16, 64]),\n",
       " True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize the MultiLayerPerceptron with sample parameters\n",
    "depth = 4\n",
    "input_channels = 64\n",
    "hidden_channels = 128\n",
    "output_channels = 64\n",
    "\n",
    "mlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n",
    "\n",
    "# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\n",
    "input_tensor = torch.randn(2, 64, 16, 16)\n",
    "output_tensor = mlp(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "mlp_output_shape = output_tensor.shape\n",
    "\n",
    "# Test window partition and merging\n",
    "batch_size, height, width, channels = 2, 16, 16, 64\n",
    "window_size = 4\n",
    "\n",
    "# Create a random tensor for window functions (B, H, W, C) format\n",
    "input_window_tensor = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Apply partitioning and merging\n",
    "windows = partition_into_windows(input_window_tensor, window_size)\n",
    "reconstructed_tensor = merge_windows(windows, window_size, height, width)\n",
    "\n",
    "# Check shapes\n",
    "windows_shape = windows.shape\n",
    "reconstructed_shape = reconstructed_tensor.shape\n",
    "\n",
    "# Validate if the reconstruction matches the original input shape\n",
    "is_shape_correct = reconstructed_shape == input_window_tensor.shape\n",
    "\n",
    "# Output results\n",
    "mlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LocalWindowAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.937244Z",
     "iopub.status.busy": "2025-04-21T16:01:16.936980Z",
     "iopub.status.idle": "2025-04-21T16:01:16.947402Z",
     "shell.execute_reply": "2025-04-21T16:01:16.946296Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.937222Z"
    },
    "papermill": {
     "duration": 0.019079,
     "end_time": "2025-04-12T13:09:32.432815",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.413736",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, window_size, num_heads):\n",
    "        \"\"\"Self-attention mechanism within local windows.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # (height, width)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n",
    "\n",
    "        # Compute and store relative positional encodings\n",
    "        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n",
    "        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n",
    "\n",
    "        # Learnable transformation of relative position embeddings\n",
    "        self.relative_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_heads, bias=True)\n",
    "        )\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n",
    "        batch_size, num_tokens, _ = qkv.shape\n",
    "\n",
    "        # Reshape qkv into separate query, key, and value tensors\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n",
    "\n",
    "        # Scale query for stable attention computation\n",
    "        q = q * self.scaling_factor\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Compute relative position bias\n",
    "        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n",
    "        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n",
    "        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n",
    "\n",
    "        # Apply softmax and compute weighted values\n",
    "        attention_weights = self.attention_softmax(attention_scores)\n",
    "        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute_log_relative_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.948815Z",
     "iopub.status.busy": "2025-04-21T16:01:16.948476Z",
     "iopub.status.idle": "2025-04-21T16:01:16.974830Z",
     "shell.execute_reply": "2025-04-21T16:01:16.973956Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.948785Z"
    },
    "papermill": {
     "duration": 0.015719,
     "end_time": "2025-04-12T13:09:32.458806",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.443087",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_log_relative_positions(window_size):\n",
    "    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n",
    "    coord_range = torch.arange(window_size)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n",
    "\n",
    "    # Compute relative positions\n",
    "    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n",
    "\n",
    "    # Format and apply log transformation\n",
    "    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n",
    "    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "    return log_relative_positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaptiveAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:16.976208Z",
     "iopub.status.busy": "2025-04-21T16:01:16.975861Z",
     "iopub.status.idle": "2025-04-21T16:01:17.000363Z",
     "shell.execute_reply": "2025-04-21T16:01:16.999498Z",
     "shell.execute_reply.started": "2025-04-21T16:01:16.976184Z"
    },
    "papermill": {
     "duration": 0.025485,
     "end_time": "2025-04-12T13:09:32.494578",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.469093",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n",
    "        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.network_depth = network_depth\n",
    "        self.enable_attention = enable_attention\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "        # Define convolutional processing based on mode\n",
    "        if self.conv_mode == 'Conv':\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "            )\n",
    "\n",
    "        if self.conv_mode == 'DWConv':\n",
    "            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n",
    "            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight_shape = module.weight.shape\n",
    "\n",
    "            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "            else:\n",
    "                gain = (8 * self.network_depth) ** (-1/4)\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def pad_for_window_processing(self, x, shift=False):\n",
    "        \"\"\"Pads the input tensor to fit window processing requirements with left and right shifts.\"\"\"\n",
    "        _, _, height, width = x.size()\n",
    "        pad_h = (self.window_size - height % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - width % self.window_size) % self.window_size\n",
    "\n",
    "        if shift:\n",
    "            # Apply left or right shift instead of cyclic shift\n",
    "            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n",
    "                          0, (self.window_size - pad_h) % self.window_size), mode='reflect')\n",
    "        else:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            v_proj = self.value_projection(x)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            qk_proj = self.query_key_projection(x)\n",
    "            qkv = torch.cat([qk_proj, v_proj], dim=1)\n",
    "\n",
    "            # Apply padding for shifted window processing\n",
    "            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n",
    "            padded_height, padded_width = padded_qkv.shape[2:]\n",
    "\n",
    "            # Partition into windows\n",
    "            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n",
    "            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n",
    "\n",
    "            # Apply window-based attention\n",
    "            attn_windows = self.window_attention(qkv_windows)\n",
    "\n",
    "            # Merge back to original spatial dimensions\n",
    "            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n",
    "\n",
    "            # Apply left or right shift (avoid cyclic)\n",
    "            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), 0:(self.shift_size + width), :]\n",
    "            attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.conv_mode in ['Conv', 'DWConv']:\n",
    "                conv_output = self.conv_layer(v_proj)\n",
    "                print(f\"conv_output shape: {conv_output.shape}\")\n",
    "                print(f\"attn_output shape: {attn_output.shape}\")\n",
    "                # print(f\"conv_output + attn_output shape: {conv_output + attn_output.shape}\")\n",
    "                print(f\"self.output_projection: {self.output_projection}\")\n",
    "                output = self.output_projection(conv_output + attn_output)\n",
    "            else:\n",
    "                output = self.output_projection(attn_output)\n",
    "\n",
    "        else:\n",
    "            if self.conv_mode == 'Conv':\n",
    "                output = self.conv_layer(x)  # No attention, using convolution only\n",
    "            elif self.conv_mode == 'DWConv':\n",
    "                output = self.output_projection(self.conv_layer(v_proj))\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisionTransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.003995Z",
     "iopub.status.busy": "2025-04-21T16:01:17.003674Z",
     "iopub.status.idle": "2025-04-21T16:01:17.024865Z",
     "shell.execute_reply": "2025-04-21T16:01:17.023832Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.003973Z"
    },
    "papermill": {
     "duration": 0.020653,
     "end_time": "2025-04-12T13:09:32.525353",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.504700",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n",
    "                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n",
    "        \"\"\"\n",
    "        A transformer block that includes attention (optional) and MLP layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enable_attention = enable_attention\n",
    "        self.enable_mlp_norm = enable_mlp_norm\n",
    "\n",
    "        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n",
    "        self.attention_layer = AdaptiveAttention(\n",
    "            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n",
    "            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n",
    "        )\n",
    "\n",
    "        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n",
    "        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.enable_attention:\n",
    "            x, rescale, rebias = self.pre_norm(x)\n",
    "        x = self.attention_layer(x)\n",
    "        if self.enable_attention:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        residual = x\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x, rescale, rebias = self.post_norm(x)\n",
    "        x = self.mlp_layer(x)\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchEmbedding and PatchReconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.026567Z",
     "iopub.status.busy": "2025-04-21T16:01:17.026179Z",
     "iopub.status.idle": "2025-04-21T16:01:17.055499Z",
     "shell.execute_reply": "2025-04-21T16:01:17.054636Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.026462Z"
    },
    "papermill": {
     "duration": 0.017283,
     "end_time": "2025-04-12T13:09:32.553948",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.536665",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch embedding module that projects input images into token embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to generate patch embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class PatchReconstruction(nn.Module):\n",
    "    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch reconstruction module that converts token embeddings back to image patches.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2, padding_mode='reflect'\n",
    "            ),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to reconstruct image from embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectiveKernelFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.056678Z",
     "iopub.status.busy": "2025-04-21T16:01:17.056406Z",
     "iopub.status.idle": "2025-04-21T16:01:17.078704Z",
     "shell.execute_reply": "2025-04-21T16:01:17.077764Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.056658Z"
    },
    "papermill": {
     "duration": 0.017407,
     "end_time": "2025-04-12T13:09:32.581526",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.564119",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SelectiveKernelFusion(nn.Module):\n",
    "    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n",
    "        \"\"\"\n",
    "        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n",
    "\n",
    "        Args:\n",
    "            channels (int): Number of input channels.\n",
    "            num_branches (int): Number of feature branches to fuse.\n",
    "            reduction_ratio (int): Reduction ratio for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_branches = num_branches\n",
    "        reduced_channels = max(int(channels / reduction_ratio), 4)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Forward pass for selective kernel fusion.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (list of tensors): A list of feature maps to be fused.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adaptively fused feature map.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = feature_maps[0].shape\n",
    "        \n",
    "        # Concatenate feature maps along a new dimension (num_branches)\n",
    "        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        aggregated_features = torch.sum(stacked_features, dim=1)\n",
    "        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n",
    "        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n",
    "\n",
    "        # Weighted sum of input feature maps\n",
    "        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerStage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, window_size=8,\n",
    "                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n",
    "        \"\"\"\n",
    "        A stage of transformer blocks with configurable attention placement.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        attention_layers = int(attention_ratio * num_layers)\n",
    "\n",
    "        if attention_placement == 'last':\n",
    "            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'first':\n",
    "            enable_attentions = [i < attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'middle':\n",
    "            enable_attentions = [\n",
    "                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(\n",
    "                network_depth=network_depth,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                enable_attention=enable_attentions[i],\n",
    "                conv_mode=conv_mode\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer stage.\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DehazingTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.080101Z",
     "iopub.status.busy": "2025-04-21T16:01:17.079760Z",
     "iopub.status.idle": "2025-04-21T16:01:17.103347Z",
     "shell.execute_reply": "2025-04-21T16:01:17.102401Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.080073Z"
    },
    "papermill": {
     "duration": 0.025553,
     "end_time": "2025-04-12T13:09:32.617120",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.591567",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DehazingTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "                 layer_depths=[16, 16, 16, 8, 8],\n",
    "                 num_heads=[2, 4, 6, 1, 1],\n",
    "                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding settings\n",
    "        self.patch_size = 4\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # Backbone layers\n",
    "        self.encoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[0],\n",
    "            num_layers=layer_depths[0],\n",
    "            num_heads=num_heads[0],\n",
    "            mlp_ratio=mlp_ratios[0],\n",
    "            norm_layer=norm_layers[0],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[0],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[0]\n",
    "        )\n",
    "        \n",
    "        self.downsample1 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "        \n",
    "        self.encoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[1],\n",
    "            num_layers=layer_depths[1],\n",
    "            num_heads=num_heads[1],\n",
    "            mlp_ratio=mlp_ratios[1],\n",
    "            norm_layer=norm_layers[1],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[1],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[1]\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "        \n",
    "        self.encoder_stage3 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[2],\n",
    "            num_layers=layer_depths[2],\n",
    "            num_heads=num_heads[2],\n",
    "            mlp_ratio=mlp_ratios[2],\n",
    "            norm_layer=norm_layers[2],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[2],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[2]\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n",
    "        \n",
    "        self.decoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[3],\n",
    "            num_layers=layer_depths[3],\n",
    "            num_heads=num_heads[3],\n",
    "            mlp_ratio=mlp_ratios[3],\n",
    "            norm_layer=norm_layers[3],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[3],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[3]\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n",
    "        \n",
    "        self.decoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[4],\n",
    "            num_layers=layer_depths[4],\n",
    "            num_heads=num_heads[4],\n",
    "            mlp_ratio=mlp_ratios[4],\n",
    "            norm_layer=norm_layers[4],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[4],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[4]\n",
    "        )\n",
    "\n",
    "        # Final patch reconstruction\n",
    "        self.patch_reconstruction = PatchReconstruction(\n",
    "            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def adjust_image_size(self, x):\n",
    "        # Ensures the input image size is compatible with the patch size\n",
    "        _, _, height, width = x.size()\n",
    "        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n",
    "        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder_stage1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.downsample1(x)\n",
    "        x = self.encoder_stage2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.downsample2(x)\n",
    "        x = self.encoder_stage3(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n",
    "        x = self.decoder_stage1(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n",
    "        x = self.decoder_stage2(x)\n",
    "        x = self.patch_reconstruction(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_height, original_width = x.shape[2:]\n",
    "        x = self.adjust_image_size(x)\n",
    "\n",
    "        features = self.extract_features(x)\n",
    "        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n",
    "\n",
    "        # Dehazing formula: I = J * t + A * (1 - t)\n",
    "        x = transmission_map * x - atmospheric_light + x\n",
    "        x = x[:, :, :original_height, :original_width]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build_dehazing_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.104568Z",
     "iopub.status.busy": "2025-04-21T16:01:17.104291Z",
     "iopub.status.idle": "2025-04-21T16:01:17.126572Z",
     "shell.execute_reply": "2025-04-21T16:01:17.125559Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.104548Z"
    },
    "papermill": {
     "duration": 0.015434,
     "end_time": "2025-04-12T13:09:32.642796",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.627362",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_dehazing_transformer():\n",
    "    return DehazingTransformer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "        layer_depths=[12, 12, 12, 6, 6],\n",
    "        num_heads=[2, 4, 6, 1, 1],\n",
    "        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RevisedLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevisedLayerNorm(nn.Module):\n",
    "    \"\"\"Revised LayerNorm\"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n",
    "        super(RevisedLayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.detach_gradient = detach_gradient\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n",
    "        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n",
    "\n",
    "        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "\n",
    "        trunc_normal_(self.scale_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.scale_mlp.bias, 1)\n",
    "\n",
    "        trunc_normal_(self.shift_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.shift_mlp.bias, 0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n",
    "        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n",
    "\n",
    "        normalized_tensor = (input_tensor - mean_value) / std_value\n",
    "\n",
    "        if self.detach_gradient:\n",
    "            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n",
    "        else:\n",
    "            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n",
    "\n",
    "        output = normalized_tensor * self.scale + self.shift\n",
    "        return output, rescale, rebias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n",
    "        super().__init__()\n",
    "        output_channels = output_channels or input_channels\n",
    "        hidden_channels = hidden_channels or input_channels\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            gain = (8 * self.depth) ** (-1 / 4)\n",
    "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "            trunc_normal_(layer.weight, std=std)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "\n",
    "def partition_into_windows(tensor, window_size):\n",
    "    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n",
    "    batch_size, height, width, channels = tensor.shape\n",
    "    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n",
    "\n",
    "    tensor = tensor.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n",
    "    )\n",
    "    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, height, width):\n",
    "    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n",
    "    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n",
    "    tensor = windows.view(\n",
    "        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 16, 16]),\n",
       " torch.Size([32, 16, 64]),\n",
       " torch.Size([2, 16, 16, 64]),\n",
       " True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the MultiLayerPerceptron with sample parameters\n",
    "depth = 4\n",
    "input_channels = 64\n",
    "hidden_channels = 128\n",
    "output_channels = 64\n",
    "\n",
    "mlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n",
    "\n",
    "# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\n",
    "input_tensor = torch.randn(2, 64, 16, 16)\n",
    "output_tensor = mlp(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "mlp_output_shape = output_tensor.shape\n",
    "\n",
    "# Test window partition and merging\n",
    "batch_size, height, width, channels = 2, 16, 16, 64\n",
    "window_size = 4\n",
    "\n",
    "# Create a random tensor for window functions (B, H, W, C) format\n",
    "input_window_tensor = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Apply partitioning and merging\n",
    "windows = partition_into_windows(input_window_tensor, window_size)\n",
    "reconstructed_tensor = merge_windows(windows, window_size, height, width)\n",
    "\n",
    "# Check shapes\n",
    "windows_shape = windows.shape\n",
    "reconstructed_shape = reconstructed_tensor.shape\n",
    "\n",
    "# Validate if the reconstruction matches the original input shape\n",
    "is_shape_correct = reconstructed_shape == input_window_tensor.shape\n",
    "\n",
    "# Output results\n",
    "mlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, window_size, num_heads):\n",
    "        \"\"\"Self-attention mechanism within local windows.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # (height, width)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n",
    "\n",
    "        # Compute and store relative positional encodings\n",
    "        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n",
    "        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n",
    "\n",
    "        # Learnable transformation of relative position embeddings\n",
    "        self.relative_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_heads, bias=True)\n",
    "        )\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n",
    "        batch_size, num_tokens, _ = qkv.shape\n",
    "\n",
    "        # Reshape qkv into separate query, key, and value tensors\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n",
    "\n",
    "        # Scale query for stable attention computation\n",
    "        q = q * self.scaling_factor\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Compute relative position bias\n",
    "        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n",
    "        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n",
    "        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n",
    "\n",
    "        # Apply softmax and compute weighted values\n",
    "        attention_weights = self.attention_softmax(attention_scores)\n",
    "        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_relative_positions(window_size):\n",
    "    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n",
    "    coord_range = torch.arange(window_size)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n",
    "\n",
    "    # Compute relative positions\n",
    "    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n",
    "\n",
    "    # Format and apply log transformation\n",
    "    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n",
    "    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "    return log_relative_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n",
    "        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.network_depth = network_depth\n",
    "        self.enable_attention = enable_attention\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "        # Define convolutional processing based on mode\n",
    "        if self.conv_mode == 'Conv':\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "            )\n",
    "\n",
    "        if self.conv_mode == 'DWConv':\n",
    "            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n",
    "            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight_shape = module.weight.shape\n",
    "\n",
    "            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "            else:\n",
    "                gain = (8 * self.network_depth) ** (-1/4)\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def pad_for_window_processing(self, x, shift=False):\n",
    "        \"\"\"Pads the input tensor to fit window processing requirements.\"\"\"\n",
    "        _, _, height, width = x.size()\n",
    "        pad_h = (self.window_size - height % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - width % self.window_size) % self.window_size\n",
    "\n",
    "        if shift:\n",
    "            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n",
    "                          self.shift_size, (self.window_size - self.shift_size + pad_h) % self.window_size), mode='reflect')\n",
    "        else:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            v_proj = self.value_projection(x)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            qk_proj = self.query_key_projection(x)\n",
    "            qkv = torch.cat([qk_proj, v_proj], dim=1)\n",
    "\n",
    "            # Apply padding for shifted window processing\n",
    "            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n",
    "            padded_height, padded_width = padded_qkv.shape[2:]\n",
    "\n",
    "            # Partition into windows\n",
    "            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n",
    "            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n",
    "\n",
    "            # Apply window-based attention\n",
    "            attn_windows = self.window_attention(qkv_windows)\n",
    "\n",
    "            # Merge back to original spatial dimensions\n",
    "            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n",
    "\n",
    "            # Reverse the cyclic shift\n",
    "            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), self.shift_size:(self.shift_size + width), :]\n",
    "            attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.conv_mode in ['Conv', 'DWConv']:\n",
    "                conv_output = self.conv_layer(v_proj)\n",
    "                output = self.output_projection(conv_output + attn_output)\n",
    "            else:\n",
    "                output = self.output_projection(attn_output)\n",
    "\n",
    "        else:\n",
    "            if self.conv_mode == 'Conv':\n",
    "                output = self.conv_layer(x)  # No attention, using convolution only\n",
    "            elif self.conv_mode == 'DWConv':\n",
    "                output = self.output_projection(self.conv_layer(v_proj))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n",
    "                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n",
    "        \"\"\"\n",
    "        A transformer block that includes attention (optional) and MLP layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enable_attention = enable_attention\n",
    "        self.enable_mlp_norm = enable_mlp_norm\n",
    "\n",
    "        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n",
    "        self.attention_layer = AdaptiveAttention(\n",
    "            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n",
    "            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n",
    "        )\n",
    "\n",
    "        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n",
    "        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.enable_attention:\n",
    "            x, rescale, rebias = self.pre_norm(x)\n",
    "        x = self.attention_layer(x)\n",
    "        if self.enable_attention:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        residual = x\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x, rescale, rebias = self.post_norm(x)\n",
    "        x = self.mlp_layer(x)\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, window_size=8,\n",
    "                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n",
    "        \"\"\"\n",
    "        A stage of transformer blocks with configurable attention placement.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        attention_layers = int(attention_ratio * num_layers)\n",
    "\n",
    "        if attention_placement == 'last':\n",
    "            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'first':\n",
    "            enable_attentions = [i < attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'middle':\n",
    "            enable_attentions = [\n",
    "                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(\n",
    "                network_depth=network_depth,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                enable_attention=enable_attentions[i],\n",
    "                conv_mode=conv_mode\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer stage.\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch embedding module that projects input images into token embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to generate patch embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class PatchReconstruction(nn.Module):\n",
    "    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch reconstruction module that converts token embeddings back to image patches.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2, padding_mode='reflect'\n",
    "            ),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to reconstruct image from embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveKernelFusion(nn.Module):\n",
    "    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n",
    "        \"\"\"\n",
    "        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n",
    "\n",
    "        Args:\n",
    "            channels (int): Number of input channels.\n",
    "            num_branches (int): Number of feature branches to fuse.\n",
    "            reduction_ratio (int): Reduction ratio for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_branches = num_branches\n",
    "        reduced_channels = max(int(channels / reduction_ratio), 4)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Forward pass for selective kernel fusion.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (list of tensors): A list of feature maps to be fused.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adaptively fused feature map.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = feature_maps[0].shape\n",
    "        \n",
    "        # Concatenate feature maps along a new dimension (num_branches)\n",
    "        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        aggregated_features = torch.sum(stacked_features, dim=1)\n",
    "        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n",
    "        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n",
    "\n",
    "        # Weighted sum of input feature maps\n",
    "        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DehazingTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "                 layer_depths=[16, 16, 16, 8, 8],\n",
    "                 num_heads=[2, 4, 6, 1, 1],\n",
    "                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding settings\n",
    "        self.patch_size = 4\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # Backbone layers\n",
    "        self.encoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[0],\n",
    "            num_layers=layer_depths[0],\n",
    "            num_heads=num_heads[0],\n",
    "            mlp_ratio=mlp_ratios[0],\n",
    "            norm_layer=norm_layers[0],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[0],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[0]\n",
    "        )\n",
    "        \n",
    "        self.downsample1 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "        \n",
    "        self.encoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[1],\n",
    "            num_layers=layer_depths[1],\n",
    "            num_heads=num_heads[1],\n",
    "            mlp_ratio=mlp_ratios[1],\n",
    "            norm_layer=norm_layers[1],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[1],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[1]\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "        \n",
    "        self.encoder_stage3 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[2],\n",
    "            num_layers=layer_depths[2],\n",
    "            num_heads=num_heads[2],\n",
    "            mlp_ratio=mlp_ratios[2],\n",
    "            norm_layer=norm_layers[2],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[2],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[2]\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n",
    "        \n",
    "        self.decoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[3],\n",
    "            num_layers=layer_depths[3],\n",
    "            num_heads=num_heads[3],\n",
    "            mlp_ratio=mlp_ratios[3],\n",
    "            norm_layer=norm_layers[3],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[3],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[3]\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n",
    "        \n",
    "        self.decoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[4],\n",
    "            num_layers=layer_depths[4],\n",
    "            num_heads=num_heads[4],\n",
    "            mlp_ratio=mlp_ratios[4],\n",
    "            norm_layer=norm_layers[4],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[4],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[4]\n",
    "        )\n",
    "\n",
    "        # Final patch reconstruction\n",
    "        self.patch_reconstruction = PatchReconstruction(\n",
    "            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def adjust_image_size(self, x):\n",
    "        # Ensures the input image size is compatible with the patch size\n",
    "        _, _, height, width = x.size()\n",
    "        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n",
    "        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder_stage1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.downsample1(x)\n",
    "        x = self.encoder_stage2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.downsample2(x)\n",
    "        x = self.encoder_stage3(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n",
    "        x = self.decoder_stage1(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n",
    "        x = self.decoder_stage2(x)\n",
    "        x = self.patch_reconstruction(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_height, original_width = x.shape[2:]\n",
    "        x = self.adjust_image_size(x)\n",
    "\n",
    "        features = self.extract_features(x)\n",
    "        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n",
    "\n",
    "        # Dehazing formula: I = J * t + A * (1 - t)\n",
    "        x = transmission_map * x - atmospheric_light + x\n",
    "        x = x[:, :, :original_height, :original_width]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dehazing_transformer():\n",
    "    return DehazingTransformer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "        layer_depths=[12, 12, 12, 6, 6],\n",
    "        num_heads=[2, 4, 6, 1, 1],\n",
    "        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvolutionalGuidedFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.127839Z",
     "iopub.status.busy": "2025-04-21T16:01:17.127597Z",
     "iopub.status.idle": "2025-04-21T16:01:17.152140Z",
     "shell.execute_reply": "2025-04-21T16:01:17.151236Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.127821Z"
    },
    "papermill": {
     "duration": 0.018977,
     "end_time": "2025-04-12T13:09:32.671995",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.653018",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvolutionalGuidedFilter(nn.Module):\n",
    "    def __init__(self, radius=1, norm_layer=nn.BatchNorm2d, conv_kernel_size: int = 1):\n",
    "        super(ConvolutionalGuidedFilter, self).__init__()\n",
    "\n",
    "        self.box_filter = nn.Conv2d(\n",
    "            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n",
    "        )\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                6,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                3,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.box_filter.weight.data[...] = 1.0\n",
    "\n",
    "    def forward(self, x_low_res, y_low_res, x_high_res):\n",
    "        _, _, h_lr, w_lr = x_low_res.size()\n",
    "        _, _, h_hr, w_hr = x_high_res.size()\n",
    "\n",
    "        N = self.box_filter(x_low_res.data.new().resize_((1, 3, h_lr, w_lr)).fill_(1.0))\n",
    "        ## mean_x\n",
    "        mean_x = self.box_filter(x_low_res) / N\n",
    "        ## mean_y\n",
    "        mean_y = self.box_filter(y_low_res) / N\n",
    "        ## cov_xy\n",
    "        cov_xy = self.box_filter(x_low_res * y_low_res) / N - mean_x * mean_y\n",
    "        ## var_x\n",
    "        var_x = self.box_filter(x_low_res * x_low_res) / N - mean_x * mean_x\n",
    "\n",
    "        ## A\n",
    "        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n",
    "        ## b\n",
    "        b = mean_y - A * mean_x\n",
    "\n",
    "        ## mean_A; mean_b\n",
    "        mean_A = F.interpolate(A, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "        mean_b = F.interpolate(b, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return mean_A * x_high_res + mean_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PixelAttentionLayer and ChannelAttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.153291Z",
     "iopub.status.busy": "2025-04-21T16:01:17.153032Z",
     "iopub.status.idle": "2025-04-21T16:01:17.175609Z",
     "shell.execute_reply": "2025-04-21T16:01:17.174643Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.153270Z"
    },
    "papermill": {
     "duration": 0.016895,
     "end_time": "2025-04-12T13:09:32.699032",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.682137",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PixelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(PixelAttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, 1, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_map = self.attention(x)\n",
    "        return x * attention_map\n",
    "\n",
    "class ChannelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelAttentionLayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, channels, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = self.avg_pool(x)\n",
    "        attention_map = self.attention(pooled)\n",
    "        return x * attention_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HybridResidualDenseBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.176736Z",
     "iopub.status.busy": "2025-04-21T16:01:17.176483Z",
     "iopub.status.idle": "2025-04-21T16:01:17.194108Z",
     "shell.execute_reply": "2025-04-21T16:01:17.193205Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.176717Z"
    },
    "papermill": {
     "duration": 0.018495,
     "end_time": "2025-04-12T13:09:32.727852",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.709357",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HybridResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_dense_layers=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.growth_rate = growth_rate\n",
    "        self.in_channels = in_channels\n",
    "        total_channels = in_channels\n",
    "\n",
    "        for i in range(num_dense_layers):\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(total_channels, growth_rate, kernel_size=3, padding=2**i,dilation=2**i)\n",
    "            )\n",
    "            total_channels += growth_rate\n",
    "\n",
    "        self.fusion = nn.Conv2d(total_channels, in_channels, kernel_size=1)\n",
    "        self.channel_attention = ChannelAttentionLayer(in_channels)\n",
    "        self.pixel_attention = PixelAttentionLayer(in_channels)\n",
    "\n",
    "        # Gated residual fusion\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"self.layers\", self.layers)\n",
    "        # print(\"HybridResidualDenseBlock: x.shape\", x.shape)\n",
    "        # x: (B, C, H, W)\n",
    "        features = [x]\n",
    "        # x: (B, C, H, W) -> (B, C, H, W) + (B, growth_rate, H, W) * num_dense_layers\n",
    "        # features: [(B, C, H, W), (B, growth_rate, H, W), ...] \n",
    "        for conv in self.layers:\n",
    "            # Apply convolution and ReLU activation\n",
    "            out = F.relu(conv(torch.cat(features, dim=1)))\n",
    "            # print(\"self.layers -> out.shape: \", out.shape)\n",
    "            features.append(out)\n",
    "        \n",
    "        # print(\"self.layers -> features: \", features)\n",
    "\n",
    "        dense_out = torch.cat(features, dim=1)\n",
    "        # print(\"self.layers -> dense_out.shape: \", dense_out.shape)\n",
    "        fused = self.fusion(dense_out)\n",
    "        # print(\"self.layers -> fused.shape: \", fused.shape)\n",
    "        # Apply channel attention and pixel attention\n",
    "        # fused: (B, C, H, W) -> (B, C, H, W) + (B, C, H, W) * 2\n",
    "        ca = self.channel_attention(fused)\n",
    "        # print(\"self.layers -> ca.shape: \", ca.shape)\n",
    "        pa = self.pixel_attention(ca)\n",
    "        # print(\"self.layers -> pa.shape: \", pa.shape)\n",
    "        # Gated residual fusion\n",
    "        # x: (B, C, H, W) + (B, C, H, W) * 2\n",
    "        gate_input = torch.cat([x, pa], dim=1)\n",
    "        # print(\"self.layers -> gate_input.shape: \", gate_input.shape)\n",
    "        gated_fusion = self.gate(gate_input)\n",
    "        # print(\"self.layers -> gated_fusion.shape: \", gated_fusion.shape)\n",
    "        # Apply gated fusion\n",
    "        return x * (1 - gated_fusion) + pa * gated_fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaptiveInstanceNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.195300Z",
     "iopub.status.busy": "2025-04-21T16:01:17.195064Z",
     "iopub.status.idle": "2025-04-21T16:01:17.218308Z",
     "shell.execute_reply": "2025-04-21T16:01:17.217401Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.195283Z"
    },
    "papermill": {
     "duration": 0.015456,
     "end_time": "2025-04-12T13:09:32.753293",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.737837",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNormalization(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(AdaptiveInstanceNormalization, self).__init__()\n",
    "\n",
    "        # Learnable scaling factors\n",
    "        self.scale_x = nn.Parameter(torch.tensor(1.0))  # Identity scaling\n",
    "        self.scale_norm = nn.Parameter(torch.tensor(0.0))  # Initially no effect\n",
    "\n",
    "        # Instance normalization layer with affine transformation enabled\n",
    "        self.instance_norm = nn.InstanceNorm2d(num_channels, momentum=0.999, eps=0.001, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized_x = self.instance_norm(x)\n",
    "        return self.scale_x * x + self.scale_norm * normalized_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEPGUIDEDNETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.219705Z",
     "iopub.status.busy": "2025-04-21T16:01:17.219346Z",
     "iopub.status.idle": "2025-04-21T16:01:17.243565Z",
     "shell.execute_reply": "2025-04-21T16:01:17.242600Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.219677Z"
    },
    "papermill": {
     "duration": 0.01754,
     "end_time": "2025-04-12T13:09:32.780956",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.763416",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DeepGuidedNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=3, radius=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        norm = AdaptiveInstanceNormalization  # define this separately\n",
    "        kernel_size = 3\n",
    "        depth_rate = 64\n",
    "        growth_rate = 16\n",
    "        num_dense_layer = 4\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "        self.rdb1 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb2 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb3 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb4 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(depth_rate, 256, 3, padding=1),       # 64 → 256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 48, 3, padding=1), # 256 → 48 for 3 × 4²\n",
    "            nn.PixelShuffle(4)                              # 48 → 3, upsample ×4\n",
    "        )\n",
    "\n",
    "    def forward(self, x, sr= False):\n",
    "        # print(f\"Shape of x: {x.shape}\")\n",
    "        x_in = self.conv_in(x)\n",
    "        # print(f\"Shape of x_in: {x_in.shape}\")\n",
    "        feat1 = self.rdb1(x_in)\n",
    "        # print(f\"Shape of feat1: {feat1.shape}\")\n",
    "        feat2 = self.rdb2(feat1)\n",
    "        # print(f\"Shape of feat2: {feat2.shape}\")\n",
    "        feat3 = self.rdb3(feat2)\n",
    "        # print(f\"Shape of feat3: {feat3.shape}\")\n",
    "        feat4 = self.rdb4(feat3)\n",
    "        # print(f\"Shape of feat4: {feat4.shape}\")\n",
    "        out_feat = self.conv_out(feat4)\n",
    "        # print(\"CONV Tail final\", self.tail)\n",
    "\n",
    "        sr = self.tail(out_feat)  # shape: (B, 3, H×4, W×4)\n",
    "        return sr, sr, [feat1, feat2, feat3, feat4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepGuidedNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, radius=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        norm = AdaptiveInstanceNormalization  # define this separately\n",
    "        kernel_size = 3\n",
    "        depth_rate = 64\n",
    "        growth_rate = 16\n",
    "        num_dense_layer = 4\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "        self.rdb1 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb2 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb3 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb4 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(depth_rate, 256, 3, padding=1),       # 64 → 256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 48, 3, padding=1), # 256 → 48 for 3 × 4²\n",
    "            nn.PixelShuffle(4)                              # 48 → 3, upsample ×4\n",
    "        )\n",
    "        self.downsample = nn.Upsample(scale_factor=0.25, mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "        # Guided Filter & Dehazing Transformer\n",
    "        self.guided_filter = ConvolutionalGuidedFilter(radius, norm_layer=norm)\n",
    "        self.dehaze_network = build_dehazing_transformer()\n",
    "\n",
    "    def forward(self, x, sr= False):\n",
    "        # print(f\"Shape of x: {x.shape}\")\n",
    "        x_lr = self.downsample(x)\n",
    "        x_in = self.conv_in(x_lr)\n",
    "        # print(f\"Shape of x_in: {x_in.shape}\")\n",
    "        feat1 = self.rdb1(x_in)\n",
    "        # print(f\"Shape of feat1: {feat1.shape}\")\n",
    "        feat2 = self.rdb2(feat1)\n",
    "        # print(f\"Shape of feat2: {feat2.shape}\")\n",
    "        feat3 = self.rdb3(feat2)\n",
    "        # print(f\"Shape of feat3: {feat3.shape}\")\n",
    "        feat4 = self.rdb4(feat3)\n",
    "        # print(f\"Shape of feat4: {feat4.shape}\")\n",
    "        out_feat = self.conv_out(feat4)\n",
    "        # print(\"CONV Tail final\", self.tail)\n",
    "        \n",
    "        y_base = self.dehaze_network(x_lr)\n",
    "        # print(\"y_base\", y_base.shape)\n",
    "        # print(\"x_in\", x_in.shape)\n",
    "        # print(\"x\", x.shape)\n",
    "        refined_output = self.guided_filter(x_lr, y_base, x)\n",
    "        # print(\"refined_output\", refined_output.shape)\n",
    "        # print(\"out_feat\", out_feat.shape)\n",
    "\n",
    "        sr = self.tail(out_feat)  # shape: (B, 3, H×4, W×4)\n",
    "        # print(\"sr\", sr.shape)\n",
    "        y_out  = refined_output + sr\n",
    "        # print(\"y_out\", y_out.shape)\n",
    "        return sr, refined_output, [feat1, feat2, feat3, feat4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test dehazing transformer\n",
    "d_transformer = build_dehazing_transformer()\n",
    "x = torch.randn(1, 3, 64, 64)  # Example input tensor\n",
    "t_out = d_transformer(x)\n",
    "t_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sr shape: torch.Size([1, 3, 128, 128])\n",
      "refined_output shape: torch.Size([1, 3, 128, 128])\n",
      "feat1 shape: torch.Size([1, 64, 32, 32])\n",
      "feat2 shape: torch.Size([1, 64, 32, 32])\n",
      "feat3 shape: torch.Size([1, 64, 32, 32])\n",
      "feat4 shape: torch.Size([1, 64, 32, 32])\n",
      "t_out shape: torch.Size([1, 3, 64, 64])\n",
      "guided_filter output shape: torch.Size([1, 3, 128, 128])\n",
      "PixelAttentionLayer output shape: torch.Size([1, 64, 32, 32])\n",
      "ChannelAttentionLayer output shape: torch.Size([1, 64, 32, 32])\n",
      "HybridResidualDenseBlock output shape: torch.Size([1, 64, 32, 32])\n",
      "AdaptiveInstanceNormalization output shape: torch.Size([1, 64, 32, 32])\n",
      "PatchEmbedding output shape: torch.Size([1, 96, 32, 32])\n",
      "PatchReconstruction output shape: torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "test_net = DeepGuidedNet()\n",
    "x = torch.randn(1, 3, 128, 128)  # Example input tensor\n",
    "y_t = test_net(x)\n",
    "sr, refined_output, [feat1, feat2, feat3, feat4] = y_t\n",
    "print(\"sr shape:\", sr.shape)  # Output shape after the network\n",
    "print(\"refined_output shape:\", refined_output.shape)  # Output shape after the network\n",
    "print(\"feat1 shape:\", feat1.shape)  # Output shape after the network\n",
    "print(\"feat2 shape:\", feat2.shape)  # Output shape after the network\n",
    "print(\"feat3 shape:\", feat3.shape)  # Output shape after the network\n",
    "print(\"feat4 shape:\", feat4.shape)  # Output shape after the network\n",
    "# test the dehazing transformer\n",
    "x = torch.randn(1, 3, 64, 64)  # Example input tensor\n",
    "t_out = d_transformer(x)\n",
    "print(\"t_out shape:\", t_out.shape)  # Output shape after the network\n",
    "# test the convolutional guided filter\n",
    "x_low_res = torch.randn(1, 3, 32, 32)  # Example low-resolution input tensor\n",
    "y_low_res = torch.randn(1, 3, 32, 32)  # Example low-resolution input tensor\n",
    "x_high_res = torch.randn(1, 3, 128, 128)  # Example high-resolution input tensor\n",
    "guided_filter = ConvolutionalGuidedFilter(radius=1)\n",
    "output = guided_filter(x_low_res, y_low_res, x_high_res)\n",
    "print(\"guided_filter output shape:\", output.shape)  # Output shape after the network\n",
    "# test the pixel attention layer\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "pixel_attention_layer = PixelAttentionLayer(channels=64)\n",
    "output = pixel_attention_layer(x)\n",
    "print(\"PixelAttentionLayer output shape:\", output.shape)  # Output shape after the network\n",
    "# test the channel attention layer\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "channel_attention_layer = ChannelAttentionLayer(channels=64)\n",
    "output = channel_attention_layer(x)\n",
    "print(\"ChannelAttentionLayer output shape:\", output.shape)  # Output shape after the network\n",
    "# test the hybrid residual dense block\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "hybrid_residual_dense_block = HybridResidualDenseBlock(in_channels=64, growth_rate=16, num_dense_layers=4)\n",
    "output = hybrid_residual_dense_block(x)\n",
    "print(\"HybridResidualDenseBlock output shape:\", output.shape)  # Output shape after the network\n",
    "# test the adaptive instance normalization\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "adaptive_instance_norm = AdaptiveInstanceNormalization(num_channels=64)\n",
    "output = adaptive_instance_norm(x)\n",
    "print(\"AdaptiveInstanceNormalization output shape:\", output.shape)  # Output shape after the network\n",
    "# test the patch embedding\n",
    "x = torch.randn(1, 3, 128, 128)  # Example input tensor\n",
    "# (batch_size, channels, height, width)\n",
    "patch_embedding = PatchEmbedding(patch_size=4, input_channels=3, embedding_dim=96, kernel_size=3)\n",
    "output = patch_embedding(x)\n",
    "print(\"PatchEmbedding output shape:\", output.shape)  # Output shape after the network\n",
    "# test the patch reconstruction\n",
    "x = torch.randn(1, 96, 32, 32)  # Example input tensor\n",
    "# (batch_size, channels, height, width)\n",
    "patch_reconstruction = PatchReconstruction(patch_size=4, output_channels=3, embedding_dim=96, kernel_size=3)\n",
    "output = patch_reconstruction(x)\n",
    "print(\"PatchReconstruction output shape:\", output.shape)  # Output shape after the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pixle shuffle 2 and conv out 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3, 512, 512])\n",
      "Feature shapes: [torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128])]\n"
     ]
    }
   ],
   "source": [
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, 3, 128, 128)  # Batch size of 1, 3 channels, 256x256 image\n",
    "\n",
    "# Initialize the DeepGuidedNetwork\n",
    "model = DeepGuidedNetwork(radius=1)\n",
    "   \n",
    "# Forward pass through the network\n",
    "output_tensor, dummy_out, features = model(input_tensor)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output_tensor.shape)\n",
    "print(\"Feature shapes:\", [f.shape for f in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pixle shuffle 2 and conv out 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3, 512, 512])\n",
      "Feature shapes: [torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128])]\n"
     ]
    }
   ],
   "source": [
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, 3, 128, 128)  # Batch size of 1, 3 channels, 256x256 image\n",
    "\n",
    "# Initialize the DeepGuidedNetwork\n",
    "model = DeepGuidedNetwork(radius=1)\n",
    "\n",
    "# Forward pass through the network\n",
    "output_tensor, dummy_out, features = model(input_tensor)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output_tensor.shape)\n",
    "print(\"Feature shapes:\", [f.shape for f in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to_psnr\n",
    "# to_psnr(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010053,
     "end_time": "2025-04-12T13:09:32.918333",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.908280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.317546Z",
     "iopub.status.busy": "2025-04-21T16:01:17.317245Z",
     "iopub.status.idle": "2025-04-21T16:01:17.339955Z",
     "shell.execute_reply": "2025-04-21T16:01:17.338827Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.317524Z"
    },
    "papermill": {
     "duration": 0.017326,
     "end_time": "2025-04-12T13:09:32.945817",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.928491",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def to_psnr(dehaze, gt):\n",
    "    \"\"\"\n",
    "    Compute PSNR (Peak Signal-to-Noise Ratio) between dehazed and ground truth images.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: PSNR values for each image in the batch.\n",
    "    \"\"\"\n",
    "    # print(\"Shapes: \", dehaze.shape, gt.shape)\n",
    "    mse = F.mse_loss(dehaze, gt, reduction='none').mean(dim=[1, 2, 3])  # Compute MSE per image\n",
    "    intensity_max = 1.0\n",
    "\n",
    "    # Compute PSNR safely, avoiding division by zero and extreme values\n",
    "    psnr_list = [10.0 * log10(intensity_max / max(mse_val.item(), 1e-6)) for mse_val in mse]\n",
    "\n",
    "    return psnr_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lpips\n",
      "  Using cached lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=0.4.0 in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from lpips) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from lpips) (0.20.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.14.3 in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from lpips) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from lpips) (1.15.2)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from lpips) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from torch>=0.4.0->lpips) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from torch>=0.4.0->lpips) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from torch>=0.4.0->lpips) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from torch>=0.4.0->lpips) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from torch>=0.4.0->lpips) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from torchvision>=0.2.1->lpips) (11.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from tqdm>=4.28.1->lpips) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abd\\d\\ai\\dehaze\\.venv\\lib\\site-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n",
      "Using cached lpips-0.1.4-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: lpips\n",
      "Successfully installed lpips-0.1.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips  # pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def to_lpips(dehaze, gt, net_type='alex'):\n",
    "    \"\"\"\n",
    "    Compute LPIPS between dehazed and ground truth images.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W), range [-1, 1]\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W), range [-1, 1]\n",
    "        net_type (str): Backbone network ('alex', 'vgg', 'squeeze')\n",
    "\n",
    "    Returns:\n",
    "        List[float]: LPIPS values for each image in the batch.\n",
    "    \"\"\"\n",
    "    loss_fn = lpips.LPIPS(net=net_type).to(dehaze.device)\n",
    "    with torch.no_grad():\n",
    "        lpips_scores = loss_fn(dehaze, gt)\n",
    "    return lpips_scores.squeeze().cpu().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ciede2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2lab, deltaE_ciede2000\n",
    "import numpy as np\n",
    "\n",
    "def to_ciede2000(dehaze, gt):\n",
    "    \"\"\"\n",
    "    Compute average CIEDE2000 color difference between dehazed and ground truth images.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W), range [0, 1]\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W), range [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        List[float]: CIEDE2000 values for each image in the batch.\n",
    "    \"\"\"\n",
    "    dehaze_np = dehaze.permute(0, 2, 3, 1).cpu().numpy()  # (B, H, W, C)\n",
    "    gt_np = gt.permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "    ciede_list = []\n",
    "    for dh, gt_img in zip(dehaze_np, gt_np):\n",
    "        dh_lab = rgb2lab(dh)\n",
    "        gt_lab = rgb2lab(gt_img)\n",
    "        delta_e = deltaE_ciede2000(dh_lab, gt_lab)\n",
    "        ciede_list.append(delta_e.mean())\n",
    "    \n",
    "    return ciede_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:17.341295Z",
     "iopub.status.busy": "2025-04-21T16:01:17.340966Z",
     "iopub.status.idle": "2025-04-21T16:01:22.663023Z",
     "shell.execute_reply": "2025-04-21T16:01:22.661971Z",
     "shell.execute_reply.started": "2025-04-21T16:01:17.341266Z"
    },
    "papermill": {
     "duration": 2.341767,
     "end_time": "2025-04-12T13:09:35.297645",
     "exception": false,
     "start_time": "2025-04-12T13:09:32.955878",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "# Define SSIM metric\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0, reduction='none')\n",
    "\n",
    "def to_ssim(dehaze: torch.Tensor, gt: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute SSIM directly on the GPU using torchmetrics.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: SSIM values for each image in the batch.\n",
    "    \"\"\"\n",
    "    ssim_values = ssim_metric(dehaze, gt)  # Shape: [B]\n",
    "    # print(\"1\",ssim_values)\n",
    "    # print(\"2\",[ssim_values])\n",
    "    ssim_values = ssim_values.tolist() \n",
    "    # print(type(ssim_values))\n",
    "    if isinstance(ssim_values, float):  # Correct way to check for a float\n",
    "        return [ssim_values]  # Convert single float to a list\n",
    "    return ssim_values  # Otherwise, return as is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.664561Z",
     "iopub.status.busy": "2025-04-21T16:01:22.664015Z",
     "iopub.status.idle": "2025-04-21T16:01:22.804857Z",
     "shell.execute_reply": "2025-04-21T16:01:22.804035Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.664534Z"
    },
    "papermill": {
     "duration": 0.183935,
     "end_time": "2025-04-12T13:09:35.493308",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.309373",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005094354972243309]\n"
     ]
    }
   ],
   "source": [
    "# Test with a dummy tensor\n",
    "dehaze = torch.rand(1, 3, 360, 360)  # Random batch of images\n",
    "gt = torch.rand(1, 3, 360, 360)  # Random ground truth images\n",
    "\n",
    "ssim_scores = to_ssim(dehaze, gt)\n",
    "print(ssim_scores)  # Should print a list of 6 SSIM values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Haze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.805937Z",
     "iopub.status.busy": "2025-04-21T16:01:22.805707Z",
     "iopub.status.idle": "2025-04-21T16:01:22.814610Z",
     "shell.execute_reply": "2025-04-21T16:01:22.813471Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.805921Z"
    },
    "papermill": {
     "duration": 0.018069,
     "end_time": "2025-04-12T13:09:35.522246",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.504177",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validationB(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: Your deep learning model\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: GPU/CPU device\n",
    "    :param category: dataset type (indoor/outdoor)\n",
    "    :param save_tag: whether to save images\n",
    "    :return: average PSNR & SSIM values\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    \n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            haze, gt = val_data\n",
    "            haze, gt = haze.to(device), gt.to(device)\n",
    "            dehaze, _, _ = net(haze)\n",
    "\n",
    "        # --- Compute PSNR & SSIM --- #\n",
    "        batch_psnr = to_psnr(dehaze, gt)  # This returns a list\n",
    "        # print(batch_psnr)\n",
    "        batch_ssim = to_ssim(dehaze, gt)  # This returns a list\n",
    "        # print(batch_ssim)\n",
    "\n",
    "        psnr_list.extend(batch_psnr)  # Flatten the list\n",
    "        ssim_list.extend(batch_ssim)  # Flatten the list\n",
    "\n",
    "    # --- Ensure lists are not empty to avoid division by zero --- #\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def testDeepguided(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: Your deep learning model\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: GPU/CPU device\n",
    "    :param category: dataset type (indoor/outdoor)\n",
    "    :param save_tag: whether to save images\n",
    "    :return: average PSNR, SSIM, LPIPS & CIEDE2000 values\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    lpips_list = []\n",
    "    ciede_list = []\n",
    "\n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            haze, gt = val_data\n",
    "            haze, gt = haze.to(device), gt.to(device)\n",
    "            _, dehaze, _ = net(haze)\n",
    "\n",
    "        # PSNR & SSIM\n",
    "        batch_psnr = to_psnr(dehaze, gt)\n",
    "        batch_ssim = to_ssim(dehaze, gt)\n",
    "\n",
    "        # LPIPS: expects [-1,1]\n",
    "        lpips_input_dehaze = (dehaze * 2) - 1\n",
    "        lpips_input_gt = (gt * 2) - 1\n",
    "        batch_lpips = to_lpips(lpips_input_dehaze, lpips_input_gt)\n",
    "\n",
    "        # CIEDE2000: expects [0,1]\n",
    "        batch_ciede = to_ciede2000(dehaze.clamp(0,1), gt.clamp(0,1))\n",
    "\n",
    "        psnr_list.extend(batch_psnr)\n",
    "        ssim_list.extend(batch_ssim)\n",
    "        lpips_list.extend(batch_lpips)\n",
    "        ciede_list.extend(batch_ciede)\n",
    "\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "    avr_lpips = sum(lpips_list) / len(lpips_list) if lpips_list else 0.0\n",
    "    avr_ciede = sum(ciede_list) / len(ciede_list) if ciede_list else 0.0\n",
    "\n",
    "    return avr_psnr, avr_ssim, avr_lpips, avr_ciede\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.816571Z",
     "iopub.status.busy": "2025-04-21T16:01:22.816059Z",
     "iopub.status.idle": "2025-04-21T16:01:22.840957Z",
     "shell.execute_reply": "2025-04-21T16:01:22.840120Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.816534Z"
    },
    "papermill": {
     "duration": 0.01909,
     "end_time": "2025-04-12T13:09:35.551459",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.532369",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validation_sr(net, sr_val_loader, device):\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    for lr, hr in sr_val_loader:\n",
    "        with torch.no_grad():\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr_out, _, _ = net(lr, sr = False)\n",
    "            hr = F.interpolate(hr, size=sr_out.shape[2:], mode='bilinear', align_corners=False)\n",
    "        # print(\"Shapes 1: \", sr_out.shape, hr.shape)\n",
    "        psnr_list.extend(to_psnr(sr_out, hr))\n",
    "        ssim_list.extend(to_ssim(sr_out, hr))\n",
    "\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009925,
     "end_time": "2025-04-12T13:09:35.578339",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.568414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.846831Z",
     "iopub.status.busy": "2025-04-21T16:01:22.846514Z",
     "iopub.status.idle": "2025-04-21T16:01:22.862331Z",
     "shell.execute_reply": "2025-04-21T16:01:22.861073Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.846807Z"
    },
    "papermill": {
     "duration": 0.016023,
     "end_time": "2025-04-12T13:09:35.604481",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.588458",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.863635Z",
     "iopub.status.busy": "2025-04-21T16:01:22.863314Z",
     "iopub.status.idle": "2025-04-21T16:01:22.880993Z",
     "shell.execute_reply": "2025-04-21T16:01:22.880020Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.863607Z"
    },
    "papermill": {
     "duration": 0.015183,
     "end_time": "2025-04-12T13:09:35.637575",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.622392",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# psnr, ssim = validationB(net, val_data_loader, device, category)\n",
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # psnr, ssim = validationB(model, val_loader, device, \"indoor\", save_tag=True)\n",
    "# print(f\"Validation PSNR: {psnr:.2f}, SSIM: {ssim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exec Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.882823Z",
     "iopub.status.busy": "2025-04-21T16:01:22.882494Z",
     "iopub.status.idle": "2025-04-21T16:01:22.912339Z",
     "shell.execute_reply": "2025-04-21T16:01:22.911133Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.882793Z"
    },
    "papermill": {
     "duration": 0.021172,
     "end_time": "2025-04-12T13:09:35.676615",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.655443",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf0bb6e6fd048b6904536e84ce41651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Execution Env:', index=1, options=('local', 'kaggle'), value='kaggle')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execution_env_widget = widgets.Dropdown(options=['local', 'kaggle'], value='kaggle', description='Execution Env:')\n",
    "display(execution_env_widget)\n",
    "\n",
    "# check if not in windows and not in kaggle\n",
    "if os.path.exists('/kaggle') and os.path.exists('/mnt'):\n",
    "    execution_env_widget.value = 'kaggle' \n",
    "else:\n",
    "    execution_env_widget.value = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.913804Z",
     "iopub.status.busy": "2025-04-21T16:01:22.913475Z",
     "iopub.status.idle": "2025-04-21T16:01:22.965654Z",
     "shell.execute_reply": "2025-04-21T16:01:22.964307Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.913775Z"
    },
    "papermill": {
     "duration": 0.054368,
     "end_time": "2025-04-12T13:09:35.750507",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.696139",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ce9e73e7844f1d92bfe4cd75f903ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Learning Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7d466c505644208443427a94dc9bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='128,128', description='Crop Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d375418d17b74c4ca85772a84edb8e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Train Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56a3139281945679bf9dcc9022cc5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=0, description='Version:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c6da37b1a04e6c93f42c6864b3e9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='Growth Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3db2a68e0924c60bec0c092c86c278a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.04, description='Lambda Loss:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3288d75ddb4f959bbbf68fc451416e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Val Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67f8a65782f4b71bd0d67e1b9e9860c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', index=2, options=('indoor', 'outdoor', 'reside', 'nh'), value='reside')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters set:\n",
      "learning_rate: 0.0001\n",
      "crop_size: [128, 128]\n",
      "train_batch_size: 2\n",
      "version: 0\n",
      "growth_rate: 16\n",
      "lambda_loss: 0.04\n",
      "val_batch_size: 2\n",
      "category: reside\n",
      "execution_env: local\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Create widgets for each hyper-parameter ---\n",
    "learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n",
    "crop_size_widget = widgets.Text(value='128,128', description='Crop Size:')\n",
    "train_batch_size_widget = widgets.IntText(value=2, description='Train Batch Size:')\n",
    "version_widget = widgets.IntText(value=0, description='Version:')\n",
    "growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n",
    "lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n",
    "val_batch_size_widget = widgets.IntText(value=2, description='Val Batch Size:')\n",
    "category_widget = widgets.Dropdown(options=['indoor', 'outdoor', 'reside', 'nh'], value='reside', description='Category:')\n",
    "\n",
    "# --- Display the widgets ---\n",
    "display(\n",
    "    learning_rate_widget, crop_size_widget, train_batch_size_widget, version_widget,\n",
    "    growth_rate_widget, lambda_loss_widget, \n",
    "    val_batch_size_widget, category_widget\n",
    ")\n",
    "\n",
    "# --- Function to parse crop size ---\n",
    "def parse_crop_size(crop_size_str):\n",
    "    return [int(x) for x in crop_size_str.split(',')]\n",
    "\n",
    "# --- Assign the widget values to variables ---\n",
    "learning_rate = learning_rate_widget.value\n",
    "crop_size = parse_crop_size(crop_size_widget.value)\n",
    "train_batch_size = train_batch_size_widget.value\n",
    "version = version_widget.value\n",
    "growth_rate = growth_rate_widget.value\n",
    "lambda_loss = lambda_loss_widget.value\n",
    "val_batch_size = val_batch_size_widget.value\n",
    "category = category_widget.value\n",
    "\n",
    "execution_env = execution_env_widget.value  # Local or Kaggle\n",
    "\n",
    "\n",
    "print('\\nHyper-parameters set:')\n",
    "print(f'learning_rate: {learning_rate}')\n",
    "print(f'crop_size: {crop_size}')\n",
    "print(f'train_batch_size: {train_batch_size}')\n",
    "print(f'version: {version}')\n",
    "print(f'growth_rate: {growth_rate}')\n",
    "print(f'lambda_loss: {lambda_loss}')\n",
    "print(f'val_batch_size: {val_batch_size}')\n",
    "print(f'category: {category}')\n",
    "print(f'execution_env: {execution_env}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths Dehaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RESIDE dataset\n",
      "Using local RESIDE dataset\n",
      "\n",
      "Final dataset paths:\n",
      "Training directory: ./dataset/reside_processed/kaggle/working/cropped_t\n",
      "Validation directory: ./dataset/reside_processed/kaggle/working/cropped_v\n",
      "Number of epochs: 200\n"
     ]
    }
   ],
   "source": [
    "# --- Set category-specific hyper-parameters ---\n",
    "if category == 'indoor':\n",
    "    num_epochs = 1500\n",
    "    train_data_dir = './data/train/indoor/'\n",
    "    val_data_dir = './data/test/SOTS/indoor/'\n",
    "elif category == 'outdoor':\n",
    "    num_epochs = 10\n",
    "    train_data_dir = './data/train/outdoor/'\n",
    "    val_data_dir = './data/test/SOTS/outdoor/'\n",
    "elif category == 'reside':\n",
    "    print('Using RESIDE dataset')\n",
    "    num_epochs = 200\n",
    "    # train_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n",
    "    # train_data_dir = '/kaggle/input/reside-processed/kaggle/working/cropped_train'\n",
    "    train_data_dir = '/kaggle/input/reside128r/cropped_t'\n",
    "    val_data_dir = '/kaggle/input/reside128r/cropped_t'\n",
    "    test_data_dir = '/kaggle/input/reside128r/cropped_v'\n",
    "    if execution_env == 'local':\n",
    "        print('Using local RESIDE dataset')\n",
    "        # train_data_dir = './dataset/reside_processed/kaggle/working/cropped_train'\n",
    "        train_data_dir = './dataset/reside_processed/kaggle/working/cropped_t'\n",
    "        val_data_dir = './dataset/reside_processed/kaggle/working/cropped_v'\n",
    "        # train_data_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/reside_processed/kaggle/working/cropped_train/'\n",
    "elif category == 'nh':\n",
    "    num_epochs = 50\n",
    "    train_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "    val_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "else:\n",
    "    raise Exception('Wrong image category. Set it to indoor or outdoor for RESIDE dataset.')\n",
    "\n",
    "# --- Adjust paths based on execution environment ---\n",
    "# if execution_env == 'kaggle':\n",
    "    # train_data_dir = '/kaggle/input/reside-dataset/' + train_data_dir.strip('./')\n",
    "    # val_data_dir = '/kaggle/input/reside-dataset/' + val_data_dir.strip('./')\n",
    "    # train_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T'\n",
    "    # val_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V' \n",
    "    # train_data_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "    # val_data_dir = '/kaggle/input/o-haze/O-HAZY/GT' \n",
    "print('\\nFinal dataset paths:')\n",
    "print(f'Training directory: {train_data_dir}')\n",
    "print(f'Validation directory: {val_data_dir}')\n",
    "print(f'Number of epochs: {num_epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:22.967040Z",
     "iopub.status.busy": "2025-04-21T16:01:22.966747Z",
     "iopub.status.idle": "2025-04-21T16:01:22.972795Z",
     "shell.execute_reply": "2025-04-21T16:01:22.971698Z",
     "shell.execute_reply.started": "2025-04-21T16:01:22.967012Z"
    },
    "papermill": {
     "duration": 0.017958,
     "end_time": "2025-04-12T13:09:35.817103",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.799145",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# hazeeffected_images_dir_train = f\"{train_data_dir}/IN\"\n",
    "hazeeffected_images_dir_train = f\"{train_data_dir}/hazy\"\n",
    "hazefree_images_dir_train = f\"{train_data_dir}/GT\"\n",
    "\n",
    "# hazeeffected_images_dir_valid = f\"{val_data_dir}/IN\"\n",
    "hazeeffected_images_dir_valid = f\"{val_data_dir}/hazy\"\n",
    "hazefree_images_dir_valid = f\"{val_data_dir}/GT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.019692Z",
     "iopub.status.busy": "2025-04-21T16:01:23.019456Z",
     "iopub.status.idle": "2025-04-21T16:01:23.039010Z",
     "shell.execute_reply": "2025-04-21T16:01:23.037920Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.019674Z"
    },
    "papermill": {
     "duration": 0.016445,
     "end_time": "2025-04-12T13:09:35.923418",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.906973",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # hazeeffected_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "# # hazefree_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "# # hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "# # hazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/GT'\n",
    "\n",
    "# hazeeffected_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN'\n",
    "# hazefree_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT'\n",
    "# hazeeffected_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN'\n",
    "# hazefree_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/GT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_enabled = True\n",
    "    # sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n",
    "    # sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X4'\n",
    "if sr_enabled:\n",
    "    sr_hr_dir = '/kaggle/input/flickr-p/working/filtered_HR'\n",
    "    sr_lr_dir = '/kaggle/input/flickr-p/working/filtered_LR_2'\n",
    "    # sr_hr_dir = '/kaggle/input/sr-flickr/kaggle/working/filtered_HR'\n",
    "    # sr_lr_dir = '/kaggle/input/sr-flickr/kaggle/working/filtered_LR_2'\n",
    "    # sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n",
    "    # sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X4'\n",
    "    if execution_env == 'local':\n",
    "        # sr_hr_dir = './dataset/Flickr2K/Flickr2K_HR'\n",
    "        # sr_lr_dir = './dataset/Flickr2K/Flickr2K_LR_unknown/X4'\n",
    "        sr_hr_dir = './dataset/SR_flickr/kaggle/working/filtered_HR'\n",
    "        sr_lr_dir = './dataset/SR_flickr/kaggle/working/filtered_LR_2'\n",
    "        # sr_lr_dir = './dataset/SR_flickr/kaggle/working/filtered_LR/X4'\n",
    "        \n",
    "        # sr_hr_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/SR_flickr/kaggle/working/filtered_HR'\n",
    "        # sr_lr_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/SR_flickr/kaggle/working/filtered_LR/X4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Define paths\n",
    "# input_dir = 'dataset/SR_flickr/kaggle/working/filtered_HR'\n",
    "# output_dir = 'dataset/SR_flickr/kaggle/working/filtered_LR_2'\n",
    "\n",
    "# # Create output directory if it doesn't exist\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Function to resize images to half their resolution\n",
    "# def resize_image(input_path, output_path, target_size=(128, 128)):\n",
    "#     img = Image.open(input_path)\n",
    "#     img_resized = img.resize(target_size, Image.LANCZOS)\n",
    "#     img_resized.save(output_path)\n",
    "\n",
    "# # Get list of images in input directory\n",
    "# try:\n",
    "#     images = [f for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "# except FileNotFoundError as e:\n",
    "#     images = []\n",
    "#     error_message = str(e)\n",
    "\n",
    "# # Resize each image and save to output directory using tqdm\n",
    "# if images:\n",
    "#     for image_name in tqdm(images, desc=\"Resizing images\"):\n",
    "#         input_path = os.path.join(input_dir, image_name)\n",
    "#         output_path = os.path.join(output_dir, image_name)\n",
    "#         resize_image(input_path, output_path, target_size=(128, 128))\n",
    "#     result = \"Resizing completed\"\n",
    "# else:\n",
    "#     result = f\"Error: {error_message}\"\n",
    "\n",
    "# result = \"Resizing completed\" if images else f\"Error: {error_message}\"\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.040582Z",
     "iopub.status.busy": "2025-04-21T16:01:23.040208Z",
     "iopub.status.idle": "2025-04-21T16:01:23.062332Z",
     "shell.execute_reply": "2025-04-21T16:01:23.061417Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.040550Z"
    },
    "papermill": {
     "duration": 0.019086,
     "end_time": "2025-04-12T13:09:35.953847",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.934761",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_log(epoch, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, category):\n",
    "    log_dir = \"./training_log\"\n",
    "    os.makedirs(log_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    log_path = os.path.join(log_dir, f\"{category}_log.txt\")\n",
    "\n",
    "    print('({0:.0f}s) Epoch [{1}/{2}], Train_PSNR:{3:.2f}, Val_PSNR:{4:.2f}, Val_SSIM:{5:.4f}'\n",
    "          .format(one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim))\n",
    "\n",
    "    # --- Write the training log --- #\n",
    "    with open(log_path, 'a') as f:\n",
    "        print('Date: {0}, Time_Cost: {1:.0f}s, Epoch: [{2}/{3}], Train_PSNR: {4:.2f}, Val_PSNR: {5:.2f}, Val_SSIM: {6:.4f}'\n",
    "              .format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "                      one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.063670Z",
     "iopub.status.busy": "2025-04-21T16:01:23.063338Z",
     "iopub.status.idle": "2025-04-21T16:01:23.081053Z",
     "shell.execute_reply": "2025-04-21T16:01:23.080012Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.063633Z"
    },
    "papermill": {
     "duration": 0.01846,
     "end_time": "2025-04-12T13:09:35.983598",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.965138",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def adjust_learning_rate(optimizer, epoch, category, lr_decay=0.90):\n",
    "#     \"\"\"\n",
    "#     Adjusts the learning rate based on the epoch and dataset category.\n",
    "\n",
    "#     :param optimizer: The optimizer (e.g., Adam, SGD).\n",
    "#     :param epoch: Current epoch number.\n",
    "#     :param category: Dataset category ('indoor', 'outdoor', or 'NH').\n",
    "#     :param lr_decay: Multiplicative factor for learning rate decay.\n",
    "#     \"\"\"\n",
    "#     # Define learning rate decay steps based on category\n",
    "#     step_dict = {'indoor': 18, 'outdoor': 3, 'NH': 20}\n",
    "#     step = step_dict.get(category, 3)  # Default step size if category is unknown\n",
    "\n",
    "#     # Decay learning rate at the specified step\n",
    "#     if epoch > 0 and epoch % step == 0:\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] *= lr_decay\n",
    "#             print(f\"Epoch {epoch}: Learning rate adjusted to {param_group['lr']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011722,
     "end_time": "2025-04-12T13:09:36.006955",
     "exception": false,
     "start_time": "2025-04-12T13:09:35.995233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.082506Z",
     "iopub.status.busy": "2025-04-21T16:01:23.082047Z",
     "iopub.status.idle": "2025-04-21T16:01:23.105640Z",
     "shell.execute_reply": "2025-04-21T16:01:23.104649Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.082473Z"
    },
    "papermill": {
     "duration": 0.020068,
     "end_time": "2025-04-12T13:09:36.038381",
     "exception": false,
     "start_time": "2025-04-12T13:09:36.018313",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Perceptual Feature Loss Network --- #\n",
    "class PerceptualLossNet(nn.Module):\n",
    "    def __init__(self, vgg_model, use_style_loss=False, style_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = vgg_model\n",
    "        self.feature_layers = {'3': \"low_level\", '8': \"mid_level\", '15': \"high_level\"}\n",
    "        self.use_style_loss = use_style_loss\n",
    "        self.style_weight = style_weight\n",
    "\n",
    "        # Freeze VGG\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def get_feature_maps(self, x):\n",
    "        feature_maps = []\n",
    "        for layer_id, layer in self.feature_extractor.named_children():\n",
    "            x = layer(x)\n",
    "            if layer_id in self.feature_layers:\n",
    "                feature_maps.append(x)\n",
    "        return feature_maps\n",
    "\n",
    "    def compute_gram(self, feature):\n",
    "        b, c, h, w = feature.size()\n",
    "        feature = feature.view(b, c, -1)\n",
    "        gram = torch.bmm(feature, feature.transpose(1, 2)) / (c * h * w)\n",
    "        return gram\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        pred_feats = self.get_feature_maps(predicted)\n",
    "        target_feats = self.get_feature_maps(target)\n",
    "\n",
    "        # Perceptual loss (normalized MSE)\n",
    "        perceptual_loss = torch.stack([\n",
    "            F.mse_loss(F.normalize(p, dim=1), F.normalize(t, dim=1))\n",
    "            for p, t in zip(pred_feats, target_feats)\n",
    "        ]).mean()\n",
    "\n",
    "        if self.use_style_loss:\n",
    "            style_loss = torch.stack([\n",
    "                F.mse_loss(self.compute_gram(p), self.compute_gram(t))\n",
    "                for p, t in zip(pred_feats, target_feats)\n",
    "            ]).mean()\n",
    "            return perceptual_loss + self.style_weight * style_loss\n",
    "\n",
    "        return perceptual_loss\n",
    "\n",
    "\n",
    "class SSFM(nn.Module):\n",
    "    def __init__(self, loss_type='l1'):\n",
    "        super(SSFM, self).__init__()\n",
    "        assert loss_type in ['l1', 'l2'], \"loss_type must be 'l1' or 'l2'\"\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def forward(self, student_feats, teacher_feats):\n",
    "        \"\"\"\n",
    "        student_feats: List of feature maps from student RDBs [rdb1, rdb2, rdb3, rdb4]\n",
    "        teacher_feats: List of corresponding feature maps from teacher\n",
    "        \"\"\"\n",
    "        assert len(student_feats) == len(teacher_feats), \"Feature lists must match\"\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for s_feat, t_feat in zip(student_feats, teacher_feats):\n",
    "            # Match resolution\n",
    "            if s_feat.shape != t_feat.shape:\n",
    "                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            if self.loss_type == 'l1':\n",
    "                loss = F.l1_loss(s_feat, t_feat)\n",
    "            else:\n",
    "                loss = F.mse_loss(s_feat, t_feat)\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss / len(student_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:29.600137Z",
     "iopub.status.busy": "2025-04-21T16:01:29.599357Z",
     "iopub.status.idle": "2025-04-21T16:01:29.780968Z",
     "shell.execute_reply": "2025-04-21T16:01:29.779763Z",
     "shell.execute_reply.started": "2025-04-21T16:01:29.599860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeatureAffinityModule(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(FeatureAffinityModule, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def forward(self, student_features, teacher_features):\n",
    "        # Debug: Print input shapes\n",
    "        # print(\"Input student_features shape:\", student_features.shape)\n",
    "        # print(\"Input teacher_features shape:\", teacher_features.shape)\n",
    "\n",
    "        # # Normalize features\n",
    "        # print(\"\\nBefore normalization:\")\n",
    "        # print(\"Student features:\", student_features.shape)  # Example for the first feature of the first batch\n",
    "        # print(\"Teacher features:\", teacher_features.shape)  # Example for the first feature of the first batch\n",
    "\n",
    "        student_norm = F.normalize(student_features.view(student_features.size(0), self.channels, -1), dim=2)\n",
    "        # print(\"Student normalized features:\", student_norm.shape)  # Checking first normalized value\n",
    "\n",
    "        # print(\"\\nother:\")\n",
    "        # print(\"student_features.size(0)\",student_features.size(0))\n",
    "        # print(\"teacher_features.size(0)\",teacher_features.size(0))\n",
    "        # print(\"self.channels\",self.channels)\n",
    "        # print(\"student_features.view(student_features.size(0), self.channels, -1)\",student_features.view(student_features.size(0), self.channels, -1).shape)\n",
    "        # print(\"teacher_features.view(teacher_features.size(0), self.channels, -1)\",teacher_features.view(teacher_features.size(0), self.channels, -1).shape)\n",
    "        \n",
    "        teacher_norm = F.normalize(teacher_features.view(teacher_features.size(0), self.channels, -1), dim=2)\n",
    "        # print(\"Teacher normalized features:\", teacher_norm[0, 0, :2])  # Checking first normalized value\n",
    "\n",
    "        # Compute affinity matrices\n",
    "        student_affinity = torch.bmm(student_norm, student_norm.transpose(1, 2))\n",
    "        teacher_affinity = torch.bmm(teacher_norm, teacher_norm.transpose(1, 2))\n",
    "\n",
    "        # Debug: Print affinity matrix shapes\n",
    "        # print(\"\\nStudent affinity matrix shape:\", student_affinity.shape)\n",
    "        # print(\"Teacher affinity matrix shape:\", teacher_affinity.shape)\n",
    "\n",
    "        # Compute KL divergence\n",
    "        loss = F.kl_div(F.log_softmax(student_affinity, dim=-1),\n",
    "                        F.softmax(teacher_affinity, dim=-1),\n",
    "                        reduction='batchmean')\n",
    "\n",
    "        # Debug: Print computed loss\n",
    "        # print(\"\\nComputed loss:\", loss.item())\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haze Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class HazeDataset(Dataset):\n",
    "    def __init__(self, hazeeffected_images_dir, hazefree_images_dir, split=\"train\", split_ratio=(0.8, 0.1, 0.1), random_seed=42):\n",
    "        \"\"\"\n",
    "        Dataset class for handling hazy and corresponding ground-truth images (already aligned and cropped).\n",
    "\n",
    "        Args:\n",
    "            hazeeffected_images_dir (str): Directory for hazy images.\n",
    "            hazefree_images_dir (str): Directory for ground-truth images.\n",
    "            split (str): \"train\", \"valid\", or \"test\" (determines data split).\n",
    "            split_ratio (tuple): A tuple of three floats representing the train, validation, and test splits.\n",
    "                                 (default 80% train, 10% validation, 10% test).\n",
    "            random_seed (int): Random seed for reproducibility in shuffling data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if sum(split_ratio) != 1.0:\n",
    "            raise ValueError(\"The sum of split_ratio must be 1.0\")\n",
    "\n",
    "        self.train_ratio, self.valid_ratio, self.test_ratio = split_ratio\n",
    "\n",
    "        valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        # print(\"Haze affected images dir: \", hazeeffected_images_dir)\n",
    "        hazy_data = [\n",
    "            f for f in glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_extensions)\n",
    "        ]\n",
    "\n",
    "        if not hazy_data:\n",
    "            raise ValueError(f\"No valid images found in {hazeeffected_images_dir}\")\n",
    "\n",
    "        hazy_data.sort()\n",
    "\n",
    "        # Shuffle the dataset for randomization\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(hazy_data)\n",
    "\n",
    "        # Split dataset based on provided ratios\n",
    "        total_images = len(hazy_data)\n",
    "        train_size = int(total_images * self.train_ratio)\n",
    "        valid_size = int(total_images * self.valid_ratio)\n",
    "        test_size = total_images - train_size - valid_size\n",
    "\n",
    "        if split == \"train\":\n",
    "            hazy_data = hazy_data[:train_size]\n",
    "        elif split == \"valid\":\n",
    "            hazy_data = hazy_data[train_size:train_size+valid_size]\n",
    "        elif split == \"test\":\n",
    "            hazy_data = hazy_data[train_size+valid_size:]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split value: {split}. Choose from ['train', 'valid', 'test'].\")\n",
    "\n",
    "        self.haze_names = []\n",
    "        self.gt_names = []\n",
    "\n",
    "        for h_image in hazy_data:\n",
    "            filename = os.path.basename(h_image)\n",
    "            haze_path = os.path.join(hazeeffected_images_dir, filename)\n",
    "            gt_path = os.path.join(hazefree_images_dir, filename)\n",
    "\n",
    "            if not os.path.exists(gt_path):\n",
    "                print(f\"Warning: Ground-truth missing for {filename}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            self.haze_names.append(haze_path)\n",
    "            self.gt_names.append(gt_path)\n",
    "\n",
    "        if not self.haze_names:\n",
    "            raise ValueError(\"No matching ground-truth images found.\")\n",
    "\n",
    "        # Define transforms\n",
    "        self.transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        self.transform_gt = Compose([ToTensor()])\n",
    "\n",
    "    def get_images(self, index):\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        try:\n",
    "            haze_img = Image.open(haze_name).convert('RGB')\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Invalid image format: {haze_name} or {gt_name}\")\n",
    "\n",
    "        haze = self.transform_haze(haze_img)\n",
    "        gt = self.transform_gt(gt_img)\n",
    "\n",
    "        if haze.shape[0] != 3 or gt.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {haze_name}\")\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_images(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haze DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training:\n",
    "train_dataset = HazeDataset(hazeeffected_images_dir=hazeeffected_images_dir_train, \n",
    "                             hazefree_images_dir=hazefree_images_dir_train, \n",
    "                             split=\"train\", \n",
    "                             split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "# For validation:\n",
    "val_dataset = HazeDataset(hazeeffected_images_dir=hazeeffected_images_dir_train, \n",
    "                             hazefree_images_dir=hazefree_images_dir_train, \n",
    "                             split=\"valid\", \n",
    "                             split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "# For testing:\n",
    "test_dataset = HazeDataset(hazeeffected_images_dir=hazeeffected_images_dir_train, \n",
    "                            hazefree_images_dir=hazefree_images_dir_train, \n",
    "                            split=\"test\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:29.782444Z",
     "iopub.status.busy": "2025-04-21T16:01:29.782054Z",
     "iopub.status.idle": "2025-04-21T16:01:55.153827Z",
     "shell.execute_reply": "2025-04-21T16:01:55.152853Z",
     "shell.execute_reply.started": "2025-04-21T16:01:29.782414Z"
    },
    "papermill": {
     "duration": 23.313227,
     "end_time": "2025-04-12T13:10:04.746196",
     "exception": false,
     "start_time": "2025-04-12T13:09:41.432969",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4800, Validation samples: 600, Test samples: 600\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:55.155297Z",
     "iopub.status.busy": "2025-04-21T16:01:55.154949Z",
     "iopub.status.idle": "2025-04-21T16:01:55.160764Z",
     "shell.execute_reply": "2025-04-21T16:01:55.159843Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.155268Z"
    },
    "papermill": {
     "duration": 0.018274,
     "end_time": "2025-04-12T13:10:04.777834",
     "exception": false,
     "start_time": "2025-04-12T13:10:04.759560",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(glob.glob( \"/kaggle/working/cropped_train/hazy/*\")), len(glob.glob(\"/kaggle/working/cropped_train/GT/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:11.972197Z",
     "iopub.status.busy": "2025-04-21T16:09:11.971341Z",
     "iopub.status.idle": "2025-04-21T16:09:11.984636Z",
     "shell.execute_reply": "2025-04-21T16:09:11.983552Z",
     "shell.execute_reply.started": "2025-04-21T16:09:11.972167Z"
    },
    "papermill": {
     "duration": 0.022717,
     "end_time": "2025-04-12T13:10:04.813435",
     "exception": false,
     "start_time": "2025-04-12T13:10:04.790718",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, scale='x4', split='train', split_ratio=(0.8, 0.1, 0.1), random_seed=42):\n",
    "        \"\"\"\n",
    "        Super-Resolution dataset that matches LR and HR image pairs based on naming pattern.\n",
    "        Assumes images are already aligned and correctly scaled. No cropping is applied.\n",
    "\n",
    "        Args:\n",
    "            lr_dir (str): Directory containing low-resolution images (e.g., x2, x3, x4).\n",
    "            hr_dir (str): Directory containing high-resolution images.\n",
    "            scale (str): Scale suffix in LR filenames (e.g., 'x2', 'x3', 'x4').\n",
    "            split (str): 'train', 'val', or 'test' (determines data split).\n",
    "            split_ratio (tuple): A tuple of three floats representing the train, validation, and test splits.\n",
    "                                 (default 80% train, 10% validation, 10% test).\n",
    "            random_seed (int): Random seed for reproducibility in shuffling data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if sum(split_ratio) != 1.0:\n",
    "            raise ValueError(\"The sum of split_ratio must be 1.0\")\n",
    "\n",
    "        self.train_ratio, self.valid_ratio, self.test_ratio = split_ratio\n",
    "\n",
    "        valid_ext = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        lr_images = sorted([\n",
    "            f for f in glob.glob(os.path.join(lr_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_ext)\n",
    "        ])\n",
    "\n",
    "        lr_hr_pairs = []\n",
    "        for lr_path in lr_images:\n",
    "            lr_name = os.path.basename(lr_path)\n",
    "            hr_name = lr_name.replace(scale, '')  # assumes naming like image_x4.png -> image.png\n",
    "            hr_path = os.path.join(hr_dir, hr_name)\n",
    "\n",
    "            if not os.path.exists(hr_path):\n",
    "                print(f\"Warning: Ground-truth missing for {lr_name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            lr_hr_pairs.append((lr_path, hr_path))\n",
    "\n",
    "        if not lr_hr_pairs:\n",
    "            raise ValueError(\"No matching LR-HR image pairs found.\")\n",
    "\n",
    "        # Shuffle the dataset for randomization\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(lr_hr_pairs)\n",
    "\n",
    "        # Split dataset based on provided ratios\n",
    "        total_pairs = len(lr_hr_pairs)\n",
    "        train_size = int(total_pairs * self.train_ratio)\n",
    "        valid_size = int(total_pairs * self.valid_ratio)\n",
    "        test_size = total_pairs - train_size - valid_size\n",
    "\n",
    "        # Splitting the data based on the chosen split\n",
    "        if split == 'train':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[:train_size]\n",
    "        elif split == 'val':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[train_size:train_size+valid_size]\n",
    "        elif split == 'test':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[train_size+valid_size:]\n",
    "        else:\n",
    "            raise ValueError(\"split must be either 'train', 'val', or 'test'\")\n",
    "\n",
    "        # Define common transform\n",
    "        self.transform = Compose([ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_hr_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_path, hr_path = self.lr_hr_pairs[idx]\n",
    "\n",
    "        try:\n",
    "            lr_img = Image.open(lr_path).convert('RGB')\n",
    "            hr_img = Image.open(hr_path).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Unidentified image at {lr_path} or {hr_path}\")\n",
    "\n",
    "        lr_tensor = self.transform(lr_img)\n",
    "        hr_tensor = self.transform(hr_img)\n",
    "\n",
    "        if lr_tensor.shape[0] != 3 or hr_tensor.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {lr_path}\")\n",
    "\n",
    "        return lr_tensor, hr_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SR DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:16.873360Z",
     "iopub.status.busy": "2025-04-21T16:09:16.873060Z",
     "iopub.status.idle": "2025-04-21T16:09:16.879312Z",
     "shell.execute_reply": "2025-04-21T16:09:16.878403Z",
     "shell.execute_reply.started": "2025-04-21T16:09:16.873340Z"
    },
    "papermill": {
     "duration": 0.018734,
     "end_time": "2025-04-12T13:10:04.844884",
     "exception": false,
     "start_time": "2025-04-12T13:10:04.826150",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def custom_collate_fn(batch):\n",
    "#     min_height = min([x[0].shape[1] for x in batch])\n",
    "#     min_width = min([x[0].shape[2] for x in batch])\n",
    "#     resized_batch = [(TF.resize(x[0], [min_height, min_width]), TF.resize(x[1], [min_height, min_width])) for x in batch]\n",
    "#     return torch.utils.data.dataloader.default_collate(resized_batch)\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Assumes each item in batch is a tuple: (input_tensor, gt_tensor)\n",
    "    # Input is already 64x64, ground truth is 256x256\n",
    "    resized_batch = []\n",
    "    for input_tensor, gt_tensor in batch:\n",
    "        # Keep input as-is, downsample ground truth to 64x64\n",
    "        resized_gt = TF.resize(gt_tensor, [64, 64], interpolation=TF.InterpolationMode.NEAREST)\n",
    "        resized_batch.append((input_tensor, resized_gt))\n",
    "    return torch.utils.data.dataloader.default_collate(resized_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:24.350816Z",
     "iopub.status.busy": "2025-04-21T16:09:24.350518Z",
     "iopub.status.idle": "2025-04-21T16:09:36.552704Z",
     "shell.execute_reply": "2025-04-21T16:09:36.551702Z",
     "shell.execute_reply.started": "2025-04-21T16:09:24.350794Z"
    },
    "papermill": {
     "duration": 6.842293,
     "end_time": "2025-04-12T13:10:11.789404",
     "exception": false,
     "start_time": "2025-04-12T13:10:04.947111",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- SR Dataset Setup --- #\n",
    "\n",
    "if sr_enabled:\n",
    "\n",
    "    # Dataset for Super-Resolution (SR) task\n",
    "    # For training:\n",
    "    sr_train_dataset = SRDataset(lr_dir=sr_lr_dir, \n",
    "                            hr_dir=sr_hr_dir,\n",
    "                            scale=\"x4\", \n",
    "                            split=\"train\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "    # For validation:\n",
    "    sr_val_dataset = SRDataset(lr_dir=sr_lr_dir, \n",
    "                            hr_dir=sr_hr_dir, \n",
    "                            scale=\"x4\", \n",
    "                            split=\"val\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "    # For testing:\n",
    "    sr_test_dataset = SRDataset(lr_dir=sr_lr_dir, \n",
    "                            hr_dir=sr_hr_dir, \n",
    "                            scale=\"x4\", \n",
    "                            split=\"test\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n",
    "    \n",
    "    # DataLoader for training with drop_last=True to avoid smaller batches\n",
    "    sr_train_loader = DataLoader(\n",
    "        sr_train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        # collate_fn=custom_collate_fn,\n",
    "        drop_last=True  # Ensure the final batch is dropped if not full\n",
    "    )\n",
    "    \n",
    "    # DataLoader for validation with drop_last=True for consistency (optional)\n",
    "    sr_val_loader = DataLoader(\n",
    "        sr_val_dataset,\n",
    "        batch_size=val_batch_size,\n",
    "        shuffle=False,  # Don't shuffle the validation set\n",
    "        # collate_fn=custom_collate_fn,\n",
    "        drop_last=True  # Optional: consider it for consistency across train/val\n",
    "    )\n",
    "\n",
    "    # DataLoader for testing (No shuffling and no drop_last)\n",
    "    sr_test_loader = DataLoader(\n",
    "        sr_test_dataset,\n",
    "        batch_size=val_batch_size,  # Define your test batch size\n",
    "        shuffle=False,  # No shuffling needed for testing\n",
    "        # collate_fn=custom_collate_fn,  # Use custom collate function if needed\n",
    "        drop_last=False  # We typically do not drop the last batch for testing\n",
    "    )\n",
    "\n",
    "    # Create an iterator for the training set\n",
    "    sr_iter = iter(sr_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR Train samples: 2120, SR Validation samples: 265, SR Test samples: 265\n"
     ]
    }
   ],
   "source": [
    "print(f\"SR Train samples: {len(sr_train_dataset)}, SR Validation samples: {len(sr_val_dataset)}, SR Test samples: {len(sr_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:36.554275Z",
     "iopub.status.busy": "2025-04-21T16:09:36.554010Z",
     "iopub.status.idle": "2025-04-21T16:09:36.665975Z",
     "shell.execute_reply": "2025-04-21T16:09:36.665053Z",
     "shell.execute_reply.started": "2025-04-21T16:09:36.554253Z"
    },
    "papermill": {
     "duration": 0.272846,
     "end_time": "2025-04-12T13:10:12.082208",
     "exception": false,
     "start_time": "2025-04-12T13:10:11.809362",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128]) torch.Size([2, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for i,o in train_data_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128]) torch.Size([2, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for i,o in val_data_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:10:00.985315Z",
     "iopub.status.busy": "2025-04-21T16:10:00.985016Z",
     "iopub.status.idle": "2025-04-21T16:10:01.651391Z",
     "shell.execute_reply": "2025-04-21T16:10:01.650352Z",
     "shell.execute_reply.started": "2025-04-21T16:10:00.985293Z"
    },
    "papermill": {
     "duration": 2.453133,
     "end_time": "2025-04-12T13:10:14.580358",
     "exception": false,
     "start_time": "2025-04-12T13:10:12.127225",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128]) torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i,o in sr_train_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:09:51.026594Z",
     "iopub.status.busy": "2025-04-21T16:09:51.026121Z",
     "iopub.status.idle": "2025-04-21T16:09:51.473913Z",
     "shell.execute_reply": "2025-04-21T16:09:51.472989Z",
     "shell.execute_reply.started": "2025-04-21T16:09:51.026568Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128]) torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i,o in sr_val_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2120, 265, 265)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sr_train_dataset), len(sr_val_dataset), len(sr_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 1: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 2: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 3: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 4: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# enumerate\n",
    "for i, (lr, hr) in enumerate(sr_train_loader):\n",
    "    print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "    if i == 4:  # Just to limit the output\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T16:01:23.106898Z",
     "iopub.status.busy": "2025-04-21T16:01:23.106596Z",
     "iopub.status.idle": "2025-04-21T16:01:29.590895Z",
     "shell.execute_reply": "2025-04-21T16:01:29.589811Z",
     "shell.execute_reply.started": "2025-04-21T16:01:23.106873Z"
    },
    "papermill": {
     "duration": 5.336581,
     "end_time": "2025-04-12T13:09:41.386705",
     "exception": false,
     "start_time": "2025-04-12T13:09:36.050124",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No pretrained weights found at /kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\n",
      "📊 Total Trainable Parameters: 5,210,051\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "\n",
    "# --- Initialize Model --- #\n",
    "net = DeepGuidedNet().to(device)\n",
    "\n",
    "# --- Enable Multi-GPU (if available) --- #\n",
    "if len(device_ids) > 1:\n",
    "    net = nn.DataParallel(net, device_ids=device_ids)\n",
    "\n",
    "# --- Optimizer --- #\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Load Pretrained VGG16 for Perceptual Loss --- #\n",
    "vgg_features = vgg16(pretrained=True).features[:16].to(device)\n",
    "for param in vgg_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "loss_network = PerceptualLossNet(vgg_features)\n",
    "loss_network.eval()\n",
    "\n",
    "# --- Load Model Weights (if available) --- #\n",
    "checkpoint_path = \"/kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\" \n",
    "\n",
    "try:\n",
    "    net.load_state_dict(torch.load(checkpoint_path, weights_only=False, map_location=torch.device('cpu')))\n",
    "    print(f\"✅ Model weights loaded from {checkpoint_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️ No pretrained weights found at {checkpoint_path}\")\n",
    "\n",
    "# --- Compute Total Trainable Parameters --- #\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"📊 Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Teacher Network --- #\n",
    "sr_net = DeepGuidedNetwork(radius=1).to(device)\n",
    "pretrained_path = r'C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run5_200_teacher_rettrain\\teacher_net_sr_final_199.pth'\n",
    "pretrained = torch.load(pretrained_path, map_location=device, weights_only=False)\n",
    "\n",
    "# Remove tail weights that do not match the new ×2 architecture\n",
    "# for key in list(pretrained.keys()):\n",
    "#     if key.startswith('tail.2') or key.startswith('tail.3'):\n",
    "#         del pretrained[key]\n",
    "\n",
    "# Load partial weights into the new model\n",
    "sr_net.load_state_dict(pretrained)\n",
    "# teacher_net.eval()\n",
    "# for param in teacher_net.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "\n",
    "# Apply Xavier initialization to the new tail.2 Conv2d (output 12 channels for PixelShuffle(2))\n",
    "# with torch.no_grad():\n",
    "#     nn.init.xavier_uniform_(teacher_net.tail[2].weight)\n",
    "#     nn.init.zeros_(teacher_net.tail[2].bias)\n",
    "# teacher_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if execution_env == 'kaggle':\n",
    "    # Load the pretrained model from Kaggle\n",
    "    pretrained_net_path = r\"/kaggle/input/reside-dehaze-student/pytorch/default/1/net_haze_iter_5.pth\"\n",
    "else:\n",
    "    # Load the pretrained model from local path\n",
    "    pretrained_net_path = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run9_guided_train\\net_final_9.pth\"\n",
    "pretrained_net = torch.load(pretrained_net_path, map_location=device, weights_only=False)\n",
    "net.load_state_dict(pretrained_net, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Affinity Module --- #\n",
    "fam = FeatureAffinityModule(channels=16).to(device)\n",
    "ssfm_loss = SSFM(loss_type='l1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "sr_net = sr_net.to(device)\n",
    "fam = fam.to(device)\n",
    "loss_network = loss_network.to(device)\n",
    "ssfm_loss = ssfm_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hybrid Residual Dense Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridResidualDenseBlock(\n",
      "  (layers): ModuleList(\n",
      "    (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(80, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "    (2): Conv2d(96, 16, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
      "    (3): Conv2d(112, 16, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
      "  )\n",
      "  (fusion): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (channel_attention): ChannelAttentionLayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (attention): Sequential(\n",
      "      (0): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (pixel_attention): PixelAttentionLayer(\n",
      "    (attention): Sequential(\n",
      "      (0): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (gate): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print Hybrid Residual Dense Network\n",
    "print(net.rdb1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Initial Validation --- #\n",
    "# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)\n",
    "# print(f\"[Dehazing Init Val] PSNR: {old_val_psnr:.2f}, SSIM: {old_val_ssim:.4f}\")\n",
    "# if sr_enabled:\n",
    "#     sr_val_psnr, sr_val_ssim = validation_sr(sr_net, sr_val_loader, device)\n",
    "#     print(f\"[SR Init Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "# # --- Training Loop --- #\n",
    "# best_psnr = old_val_psnr\n",
    "# best_psnr_sr = sr_val_psnr\n",
    "# train_psnr_prev = 0\n",
    "# distillation_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# import os\n",
    "\n",
    "# # Create a directory for TensorBoard logs\n",
    "# log_dir = \"runs/net_haze_1\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# # TensorBoard writer\n",
    "# writer = SummaryWriter(log_dir)\n",
    "\n",
    "# # Initialize global variables\n",
    "# global_iter = 0\n",
    "# total_start_time = time.time()\n",
    "\n",
    "# # Optimizer for net only\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# # LR scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# # --- Training Loop --- #\n",
    "# best_psnr = old_val_psnr\n",
    "# train_psnr_prev = 0\n",
    "\n",
    "# # till 15 epochs: co-distilation, followed by 85 epochs of pure training in:  https://www.kaggle.com/code/curiousmohammed/reside-dehaze-student-only | For permission: contact @curiousmohammed user in kaggle\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_start_time = time.time()\n",
    "#     psnr_list = []\n",
    "\n",
    "#     net.train()\n",
    "\n",
    "#     # Training loop with tqdm progress bar\n",
    "#     train_loader_tqdm = tqdm(train_data_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "#     for batch_id, (haze, gt) in enumerate(train_loader_tqdm):\n",
    "#         haze, gt = haze.to(device), gt.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass - student (net)\n",
    "#         dehaze, base, s = net(haze)\n",
    "\n",
    "#         # Student losses\n",
    "#         base_loss = F.smooth_l1_loss(base, gt)\n",
    "#         smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "#         perceptual_loss = loss_network(dehaze, gt)\n",
    "#         total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         psnr_list.extend(to_psnr(dehaze, gt))\n",
    "#         train_loader_tqdm.set_postfix(loss=total_loss.item(), iter=global_iter)\n",
    "#         global_iter += 1\n",
    "\n",
    "#     train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "#     # Save models every 5 epochs\n",
    "#     if epoch % 5 == 0:\n",
    "#         iter_model_path = f\"net_haze_iter_{epoch}.pth\"\n",
    "#         torch.save(net.state_dict(), iter_model_path)\n",
    "#         print(f\"Model - net saved in epoch {epoch}.\")\n",
    "\n",
    "#     # === Validation ===\n",
    "#     net.eval()\n",
    "#     val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "#     print(f\"[Dehazing Val] PSNR: {val_psnr:.2f}, SSIM: {val_ssim:.4f}\")\n",
    "#     writer.add_scalar(\"Metrics/Val_PSNR_Dehaze\", val_psnr, epoch)\n",
    "#     writer.add_scalar(\"Metrics/Val_SSIM_Dehaze\", val_ssim, epoch)\n",
    "\n",
    "#     epoch_duration = time.time() - epoch_start_time\n",
    "#     model_path = f\"net_haze_{version}.pth\"\n",
    "#     print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "#     # Learning rate scheduler based on validation PSNR\n",
    "#     scheduler.step(val_psnr)\n",
    "\n",
    "#     # Best model save\n",
    "#     if val_psnr >= best_psnr:\n",
    "#         best_model_path = f\"net_haze_best_{version}.pth\"\n",
    "#         torch.save(net.state_dict(), best_model_path)\n",
    "#         best_psnr = val_psnr\n",
    "\n",
    "#     train_psnr_prev = train_psnr\n",
    "\n",
    "# # Final save\n",
    "# final_path = f\"net_final_{epoch}.pth\"\n",
    "# torch.save(net.state_dict(), final_path)\n",
    "# print(f\"Model - net saved in epoch {epoch}.\")\n",
    "\n",
    "# total_time = time.time() - total_start_time\n",
    "# print(f\"🏁 Training completed in {total_time/60:.2f} minutes.\")\n",
    "\n",
    "# # Close the TensorBoard writer\n",
    "# writer.close()\n",
    "# print(\"TensorBoard logs saved in:\", log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_sr(sr_net , sr_val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics: Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.212317767362876, 0.6813360537588596)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationB(net, val_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics: Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.250782151589775, 0.6791164876520633)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationB(net, test_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics: Valid Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\abd/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:40<00:00, 6.09MB/s] \n",
      "c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24.571516756615207,\n",
       " 0.8633154951532682,\n",
       " 0.0810584848606959,\n",
       " np.float32(5.774615))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDeepguided(net, val_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics: Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\abd\\d\\ai\\dehaze\\.venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24.6658296801319,\n",
       " 0.8672080141802628,\n",
       " 0.0804763538755166,\n",
       " np.float32(5.615585))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDeepguided(net, test_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.205467Z",
     "iopub.status.idle": "2025-04-21T16:01:55.205773Z",
     "shell.execute_reply": "2025-04-21T16:01:55.205660Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.205648Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-04-12T13:10:14.625824",
     "status": "running"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# global_iter = 0\n",
    "# total_start_time = time.time()\n",
    "\n",
    "# # Joint optimizer for net and teacher_net\n",
    "# optimizer = torch.optim.Adam(list(net.parameters()) + list(sr_net.parameters()), lr=learning_rate)\n",
    "\n",
    "# # Initialize ReduceLROnPlateau scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_start_time = time.time()\n",
    "#     psnr_list = []\n",
    "\n",
    "#     net.train()\n",
    "#     if sr_enabled:\n",
    "#         sr_net.train()\n",
    "\n",
    "#     train_loader_tqdm = tqdm(train_data_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "#     for batch_id, (haze, gt) in enumerate(train_loader_tqdm):\n",
    "#         haze, gt = haze.to(device), gt.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass - student\n",
    "#         dehaze, base, s = net(haze)\n",
    "#         s1, s2, s3, s4 = s\n",
    "\n",
    "#         # Student losses\n",
    "#         base_loss = F.smooth_l1_loss(base, gt)\n",
    "#         smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "#         perceptual_loss = loss_network(dehaze, gt)\n",
    "#         total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "\n",
    "#         # Teacher SR logic\n",
    "#         if sr_enabled:\n",
    "#             try:\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             except StopIteration:\n",
    "#                 sr_iter = iter(sr_train_loader)\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n",
    "\n",
    "#             # Teacher forward\n",
    "#             sr_out, _, t = sr_net(sr_lr)\n",
    "\n",
    "#             sr_loss = F.l1_loss(sr_out, sr_hr)\n",
    "#             total_loss += sr_loss\n",
    "\n",
    "#             s_loss = ssfm_loss(s, t)\n",
    "#             total_loss += s_loss\n",
    "\n",
    "#             distillation_loss = fam(dehaze, sr_out)\n",
    "#             total_loss += distillation_loss\n",
    "\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         psnr_list.extend(to_psnr(dehaze, gt))\n",
    "#         train_loader_tqdm.set_postfix(loss=total_loss.item(), iter=global_iter)\n",
    "#         global_iter += 1\n",
    "\n",
    "#     train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "#     # Save model every 5 epochs\n",
    "#     if epoch % 5 == 0:\n",
    "#         iter_model_path = f\"net_haze_iter_{epoch}.pth\"\n",
    "#         torch.save(net.state_dict(), iter_model_path)\n",
    "#         print(f\"Model - net saved in epoch {epoch}.\")\n",
    "#         if sr_enabled:\n",
    "#             iter_model_path = f\"teacher_net_haze_iter_{epoch}.pth\"\n",
    "#             torch.save(sr_net.state_dict(), iter_model_path)\n",
    "#             print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "#     # Validation\n",
    "#     net.eval()\n",
    "#     val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "#     if sr_enabled:\n",
    "#         sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#         print(f\"[SR Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "#     epoch_duration = time.time() - epoch_start_time\n",
    "#     model_path = f\"net_haze_{version}.pth\"\n",
    "#     print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "#     elapsed = time.time() - total_start_time\n",
    "#     epochs_left = num_epochs - (epoch + 1)\n",
    "#     avg_epoch_time = elapsed / (epoch + 1)\n",
    "#     eta = avg_epoch_time * epochs_left\n",
    "#     print(f\"✅ Epoch [{epoch+1}/{num_epochs}] completed in {epoch_duration:.2f}s — ETA for {epochs_left} more: ~{eta/60:.2f} min\")\n",
    "\n",
    "#     # Adjust the learning rate based on validation PSNR\n",
    "#     scheduler.step(val_psnr)\n",
    "\n",
    "#     # Best model save\n",
    "#     if val_psnr >= best_psnr:\n",
    "#         best_model_path = f\"net_haze_best_{version}.pth\"\n",
    "#         torch.save(net.state_dict(), best_model_path)\n",
    "#         best_psnr = val_psnr\n",
    "\n",
    "#     train_psnr_prev = train_psnr\n",
    "\n",
    "# # Final save\n",
    "# final_path = f\"net_final_{epoch}.pth\"\n",
    "# torch.save(net.state_dict(), final_path)\n",
    "# print(f\"Model - net saved in epoch {epoch}.\")\n",
    "# if sr_enabled:\n",
    "#     final_path = f\"teacher_net_final_{epoch}.pth\"\n",
    "#     torch.save(sr_net.state_dict(), final_path)\n",
    "#     print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "# total_time = time.time() - total_start_time\n",
    "# print(f\"🏁 Training completed in {total_time/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Final Validation --- #\n",
    "# net.eval()\n",
    "# val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "# print(f\"[Dehazing Final Val] PSNR: {val_psnr:.2f}, SSIM: {val_ssim:.4f}\")\n",
    "# if sr_enabled:\n",
    "#     sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#     print(f\"[SR Final Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.206947Z",
     "iopub.status.idle": "2025-04-21T16:01:55.207230Z",
     "shell.execute_reply": "2025-04-21T16:01:55.207122Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.207107Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Initialize model\n",
    "# # model_path = r\"C:\\Users\\abd\\d\\ai\\dehaze\\teacher_net_sr_final_24.pth\"\n",
    "# model_path = r\"C:\\Users\\abd\\d\\ai\\dehaze\\teacher_net_sr_iter_100.pth\"\n",
    "# # model = DehazingNet().to(device)\n",
    "# teacher_net.load_state_dict(torch.load(model_path, map_location=device, weights_only=False), strict=False)\n",
    "# # net.load_state_dict(torch.load(model_path, map_location=device))\n",
    "# # net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize using test sr_test_loader\n",
    "# # model_path1 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\teacher_net_sr_final_199.pth\"\n",
    "# # model_path2 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run2\\teacher_net_sr_best_0.pth\"\n",
    "# # Load models\n",
    "# # sr_net.load_state_dict(torch.load(model_path1, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net1 = sr_net\n",
    "\n",
    "# # sr_net.load_state_dict(torch.load(model_path2, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net2 = net\n",
    "\n",
    "# # Iterate through test loader\n",
    "# for i, (lr, hr) in enumerate(sr_test_loader):\n",
    "#     print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "#     lr, hr = lr.to(device), hr.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         sr_out1, _, _ = teacher_net1(lr)\n",
    "#         sr_out2, _, _ = teacher_net2(lr)\n",
    "#         print(f\"SR Output shape: {sr_out1.shape}\")\n",
    "#         print(f\"SR Output shape: {sr_out2.shape}\")\n",
    "#         # Downsample to match the size of hr\n",
    "#         sr_out1 = F.interpolate(sr_out1, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "#         # sr_out2 = F.interpolate(sr_out2, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "#         # Plot using matplotlib\n",
    "#         fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "#         axes[0].imshow(lr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[0].set_title(\"Input (Low-Res)\")\n",
    "#         axes[0].axis(\"off\")\n",
    "        \n",
    "#         axes[1].imshow(hr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[1].set_title(\"Ground Truth\")\n",
    "#         axes[1].axis(\"off\")\n",
    "\n",
    "#         axes[2].imshow(sr_out1[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[2].set_title(\"Model Path 1 Output\")\n",
    "#         axes[2].axis(\"off\")\n",
    "\n",
    "#         axes[3].imshow(sr_out2[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[3].set_title(\"Model Path 2 Output\")\n",
    "#         axes[3].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     if i == 4:  # Just to limit the output\n",
    "#         break\n",
    "# # do same for dehaze\n",
    "# # Iterate through test loader\n",
    "# for i, (haze, gt) in enumerate(test_data_loader):\n",
    "#     print(f\"Batch {i}: Haze shape: {haze.shape}, GT shape: {gt.shape}\")\n",
    "#     haze, gt = haze.to(device), gt.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         dehaze, _, _ = net(haze)\n",
    "#         print(f\"Dehaze Output shape: {dehaze.shape}\")\n",
    "        \n",
    "#         # Plot using matplotlib\n",
    "#         fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "#         axes[0].imshow(haze[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[0].set_title(\"Input (Hazy)\")\n",
    "#         axes[0].axis(\"off\")\n",
    "        \n",
    "#         axes[1].imshow(gt[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[1].set_title(\"Ground Truth\")\n",
    "#         axes[1].axis(\"off\")\n",
    "\n",
    "#         axes[2].imshow(dehaze[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[2].set_title(\"Dehazed Output\")\n",
    "#         axes[2].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     if i == 4:  # Just to limit the output\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize using test sr_test_loader\n",
    "# model_path1 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run2\\teacher_net_sr_iter_100.pth\"\n",
    "# model_path2 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run2\\teacher_net_sr_best_0.pth\"\n",
    "# # Load models\n",
    "# sr_net.load_state_dict(torch.load(model_path1, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net1 = sr_net\n",
    "\n",
    "# sr_net.load_state_dict(torch.load(model_path2, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net2 = sr_net\n",
    "\n",
    "# # Iterate through test loader\n",
    "# for i, (lr, hr) in enumerate(sr_test_loader):\n",
    "#     print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "#     lr, hr = lr.to(device), hr.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         sr_out1, _, _ = teacher_net1(lr)\n",
    "#         sr_out2, _, _ = teacher_net2(lr)\n",
    "#         print(f\"SR Output shape: {sr_out1.shape}\")\n",
    "#         print(f\"SR Output shape: {sr_out2.shape}\")\n",
    "#         # Downsample to match the size of hr\n",
    "#         sr_out1 = F.interpolate(sr_out1, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "#         # sr_out2 = F.interpolate(sr_out2, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "#         # Plot using matplotlib\n",
    "#         fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "#         axes[0].imshow(lr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[0].set_title(\"Input (Low-Res)\")\n",
    "#         axes[0].axis(\"off\")\n",
    "        \n",
    "#         axes[1].imshow(hr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[1].set_title(\"Ground Truth\")\n",
    "#         axes[1].axis(\"off\")\n",
    "\n",
    "#         axes[2].imshow(sr_out1[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[2].set_title(\"Model Path 1 Output\")\n",
    "#         axes[2].axis(\"off\")\n",
    "\n",
    "#         axes[3].imshow(sr_out2[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[3].set_title(\"Model Path 2 Output\")\n",
    "#         axes[3].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     if i == 4:  # Just to limit the output\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize using test sr_test_loader\n",
    "# model_path1 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run2\\teacher_net_sr_iter_100.pth\"\n",
    "# model_path2 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\teacher_net_sr_iter_5.pth\"\n",
    "# # Load models\n",
    "# sr_net.load_state_dict(torch.load(model_path1, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net1 = sr_net\n",
    "# sr_net.load_state_dict(torch.load(model_path2, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net2 = sr_net\n",
    "\n",
    "# # Iterate through test loader\n",
    "# for i, (lr, hr) in enumerate(sr_test_loader):\n",
    "#     print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "#     lr, hr = lr.to(device), hr.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         sr_out1, _, _ = teacher_net1(lr)\n",
    "#         sr_out2, _, _ = teacher_net2(lr)\n",
    "        \n",
    "#         # Plot using matplotlib\n",
    "#         fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "#         axes[0].imshow(lr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[0].set_title(\"Input (Low-Res)\")\n",
    "#         axes[0].axis(\"off\")\n",
    "        \n",
    "#         axes[1].imshow(hr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[1].set_title(\"Ground Truth\")\n",
    "#         axes[1].axis(\"off\")\n",
    "\n",
    "#         axes[2].imshow(sr_out1[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[2].set_title(\"Model Path 1 Output\")\n",
    "#         axes[2].axis(\"off\")\n",
    "\n",
    "#         axes[3].imshow(sr_out2[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[3].set_title(\"Model Path 2 Output\")\n",
    "#         axes[3].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     if i == 4:  # Just to limit the output\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.208619Z",
     "iopub.status.idle": "2025-04-21T16:01:55.209247Z",
     "shell.execute_reply": "2025-04-21T16:01:55.209064Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.209046Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # LOAD TEST DATA\n",
    "# # -----------------------------\n",
    "# # check is exec env is local or kaggle\n",
    "# if execution_env == 'kaggle':\n",
    "#     test_hazy_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/hazy\"\n",
    "#     test_gt_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/GT\"\n",
    "# else:\n",
    "#     test_hazy_dir = \"./dehaze/dataset/RESIDE-6K/test/hazy\"\n",
    "#     test_gt_dir = \"./dehaze/dataset/RESIDE-6K/test/GT\"\n",
    "#     # test_hazy_dir = \"/Volumes/S/dev/project/code/Aphase/dehaze/dataset/RESIDE-6K/test/hazy\"\n",
    "#     # test_gt_dir = \"/Volumes/S/dev/project/code/Aphase/dehaze/dataset/RESIDE-6K/test/GT\"\n",
    "\n",
    "# # test_hazy_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN\"\n",
    "# # test_gt_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT\"\n",
    "\n",
    "# hazy_images = sorted(glob.glob(os.path.join(test_hazy_dir, \"*.*\")))\n",
    "# gt_images = sorted(glob.glob(os.path.join(test_gt_dir, \"*.*\")))\n",
    "\n",
    "# transform = Compose([\n",
    "#     ToTensor(),\n",
    "#     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# to_pil = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.210801Z",
     "iopub.status.idle": "2025-04-21T16:01:55.211425Z",
     "shell.execute_reply": "2025-04-21T16:01:55.211229Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.211212Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [11, 12, 13,14]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.212728Z",
     "iopub.status.idle": "2025-04-21T16:01:55.213012Z",
     "shell.execute_reply": "2025-04-21T16:01:55.212903Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.212889Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [70, 75, 90, 100]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T16:01:55.214998Z",
     "iopub.status.idle": "2025-04-21T16:01:55.215653Z",
     "shell.execute_reply": "2025-04-21T16:01:55.215495Z",
     "shell.execute_reply.started": "2025-04-21T16:01:55.215481Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [1, 3, 5]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 937211,
     "sourceId": 1587463,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2813430,
     "sourceId": 4853613,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6456606,
     "sourceId": 10417877,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6464114,
     "sourceId": 10443410,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 222875724,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 268224,
     "modelInstanceId": 246650,
     "sourceId": 287852,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 288973,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 297672,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 299643,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-12T13:09:18.306161",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
