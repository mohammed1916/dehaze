{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcdeef17",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:29.758098Z",
     "iopub.status.busy": "2025-03-23T10:32:29.757877Z",
     "iopub.status.idle": "2025-03-23T10:32:40.671475Z",
     "shell.execute_reply": "2025-03-23T10:32:40.670783Z"
    },
    "papermill": {
     "duration": 10.926132,
     "end_time": "2025-03-23T10:32:40.673091",
     "exception": false,
     "start_time": "2025-03-23T10:32:29.746959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "from timm.layers import to_2tuple, trunc_normal_\n",
    "import os\n",
    "import torchvision.utils as utils\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from random import randrange\n",
    "import time\n",
    "from math import log10\n",
    "from skimage import measure\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.nn.init import trunc_normal_\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b182bc",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.008946,
     "end_time": "2025-03-23T10:32:40.691971",
     "exception": false,
     "start_time": "2025-03-23T10:32:40.683025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RevisedLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b45e9e",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:40.711034Z",
     "iopub.status.busy": "2025-03-23T10:32:40.710654Z",
     "iopub.status.idle": "2025-03-23T10:32:40.717036Z",
     "shell.execute_reply": "2025-03-23T10:32:40.716420Z"
    },
    "papermill": {
     "duration": 0.017259,
     "end_time": "2025-03-23T10:32:40.718294",
     "exception": false,
     "start_time": "2025-03-23T10:32:40.701035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RevisedLayerNorm(nn.Module):\n",
    "    \"\"\"Revised LayerNorm\"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n",
    "        super(RevisedLayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.detach_gradient = detach_gradient\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n",
    "        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n",
    "\n",
    "        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "\n",
    "        trunc_normal_(self.scale_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.scale_mlp.bias, 1)\n",
    "\n",
    "        trunc_normal_(self.shift_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.shift_mlp.bias, 0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n",
    "        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n",
    "\n",
    "        normalized_tensor = (input_tensor - mean_value) / std_value\n",
    "\n",
    "        if self.detach_gradient:\n",
    "            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n",
    "        else:\n",
    "            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n",
    "\n",
    "        output = normalized_tensor * self.scale + self.shift\n",
    "        return output, rescale, rebias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd42e102",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:40.737208Z",
     "iopub.status.busy": "2025-03-23T10:32:40.737006Z",
     "iopub.status.idle": "2025-03-23T10:32:40.744464Z",
     "shell.execute_reply": "2025-03-23T10:32:40.743701Z"
    },
    "papermill": {
     "duration": 0.018119,
     "end_time": "2025-03-23T10:32:40.745636",
     "exception": false,
     "start_time": "2025-03-23T10:32:40.727517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n",
    "        super().__init__()\n",
    "        output_channels = output_channels or input_channels\n",
    "        hidden_channels = hidden_channels or input_channels\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            gain = (8 * self.depth) ** (-1 / 4)\n",
    "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "            trunc_normal_(layer.weight, std=std)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "\n",
    "def partition_into_windows(tensor, window_size):\n",
    "    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n",
    "    batch_size, height, width, channels = tensor.shape\n",
    "    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n",
    "\n",
    "    tensor = tensor.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n",
    "    )\n",
    "    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, height, width):\n",
    "    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n",
    "    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n",
    "    tensor = windows.view(\n",
    "        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7383803",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.008948,
     "end_time": "2025-03-23T10:32:40.763537",
     "exception": false,
     "start_time": "2025-03-23T10:32:40.754589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27701941",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:40.782520Z",
     "iopub.status.busy": "2025-03-23T10:32:40.782267Z",
     "iopub.status.idle": "2025-03-23T10:32:40.912798Z",
     "shell.execute_reply": "2025-03-23T10:32:40.912053Z"
    },
    "papermill": {
     "duration": 0.14134,
     "end_time": "2025-03-23T10:32:40.914095",
     "exception": false,
     "start_time": "2025-03-23T10:32:40.772755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 16, 16]),\n",
       " torch.Size([32, 16, 64]),\n",
       " torch.Size([2, 16, 16, 64]),\n",
       " True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the MultiLayerPerceptron with sample parameters\n",
    "depth = 4\n",
    "input_channels = 64\n",
    "hidden_channels = 128\n",
    "output_channels = 64\n",
    "\n",
    "mlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n",
    "\n",
    "# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\n",
    "input_tensor = torch.randn(2, 64, 16, 16)\n",
    "output_tensor = mlp(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "mlp_output_shape = output_tensor.shape\n",
    "\n",
    "# Test window partition and merging\n",
    "batch_size, height, width, channels = 2, 16, 16, 64\n",
    "window_size = 4\n",
    "\n",
    "# Create a random tensor for window functions (B, H, W, C) format\n",
    "input_window_tensor = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Apply partitioning and merging\n",
    "windows = partition_into_windows(input_window_tensor, window_size)\n",
    "reconstructed_tensor = merge_windows(windows, window_size, height, width)\n",
    "\n",
    "# Check shapes\n",
    "windows_shape = windows.shape\n",
    "reconstructed_shape = reconstructed_tensor.shape\n",
    "\n",
    "# Validate if the reconstruction matches the original input shape\n",
    "is_shape_correct = reconstructed_shape == input_window_tensor.shape\n",
    "\n",
    "# Output results\n",
    "mlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6579dc",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:40.933271Z",
     "iopub.status.busy": "2025-03-23T10:32:40.933033Z",
     "iopub.status.idle": "2025-03-23T10:32:40.939104Z",
     "shell.execute_reply": "2025-03-23T10:32:40.938530Z"
    },
    "papermill": {
     "duration": 0.01681,
     "end_time": "2025-03-23T10:32:40.940281",
     "exception": false,
     "start_time": "2025-03-23T10:32:40.923471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, window_size, num_heads):\n",
    "        \"\"\"Self-attention mechanism within local windows.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # (height, width)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n",
    "\n",
    "        # Compute and store relative positional encodings\n",
    "        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n",
    "        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n",
    "\n",
    "        # Learnable transformation of relative position embeddings\n",
    "        self.relative_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_heads, bias=True)\n",
    "        )\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n",
    "        batch_size, num_tokens, _ = qkv.shape\n",
    "\n",
    "        # Reshape qkv into separate query, key, and value tensors\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n",
    "\n",
    "        # Scale query for stable attention computation\n",
    "        q = q * self.scaling_factor\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Compute relative position bias\n",
    "        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n",
    "        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n",
    "        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n",
    "\n",
    "        # Apply softmax and compute weighted values\n",
    "        attention_weights = self.attention_softmax(attention_scores)\n",
    "        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a8cc58",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:40.960331Z",
     "iopub.status.busy": "2025-03-23T10:32:40.960123Z",
     "iopub.status.idle": "2025-03-23T10:32:40.964286Z",
     "shell.execute_reply": "2025-03-23T10:32:40.963508Z"
    },
    "papermill": {
     "duration": 0.015076,
     "end_time": "2025-03-23T10:32:40.965415",
     "exception": false,
     "start_time": "2025-03-23T10:32:40.950339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_log_relative_positions(window_size):\n",
    "    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n",
    "    coord_range = torch.arange(window_size)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n",
    "\n",
    "    # Compute relative positions\n",
    "    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n",
    "\n",
    "    # Format and apply log transformation\n",
    "    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n",
    "    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "    return log_relative_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c2f433",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:40.984496Z",
     "iopub.status.busy": "2025-03-23T10:32:40.984252Z",
     "iopub.status.idle": "2025-03-23T10:32:40.997336Z",
     "shell.execute_reply": "2025-03-23T10:32:40.996694Z"
    },
    "papermill": {
     "duration": 0.023873,
     "end_time": "2025-03-23T10:32:40.998516",
     "exception": false,
     "start_time": "2025-03-23T10:32:40.974643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n",
    "        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.network_depth = network_depth\n",
    "        self.enable_attention = enable_attention\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "        # Define convolutional processing based on mode\n",
    "        if self.conv_mode == 'Conv':\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "            )\n",
    "\n",
    "        if self.conv_mode == 'DWConv':\n",
    "            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n",
    "            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight_shape = module.weight.shape\n",
    "\n",
    "            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "            else:\n",
    "                gain = (8 * self.network_depth) ** (-1/4)\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def pad_for_window_processing(self, x, shift=False):\n",
    "        \"\"\"Pads the input tensor to fit window processing requirements.\"\"\"\n",
    "        _, _, height, width = x.size()\n",
    "        pad_h = (self.window_size - height % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - width % self.window_size) % self.window_size\n",
    "\n",
    "        if shift:\n",
    "            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n",
    "                          self.shift_size, (self.window_size - self.shift_size + pad_h) % self.window_size), mode='reflect')\n",
    "        else:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            v_proj = self.value_projection(x)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            qk_proj = self.query_key_projection(x)\n",
    "            qkv = torch.cat([qk_proj, v_proj], dim=1)\n",
    "\n",
    "            # Apply padding for shifted window processing\n",
    "            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n",
    "            padded_height, padded_width = padded_qkv.shape[2:]\n",
    "\n",
    "            # Partition into windows\n",
    "            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n",
    "            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n",
    "\n",
    "            # Apply window-based attention\n",
    "            attn_windows = self.window_attention(qkv_windows)\n",
    "\n",
    "            # Merge back to original spatial dimensions\n",
    "            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n",
    "\n",
    "            # Reverse the cyclic shift\n",
    "            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), self.shift_size:(self.shift_size + width), :]\n",
    "            attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.conv_mode in ['Conv', 'DWConv']:\n",
    "                conv_output = self.conv_layer(v_proj)\n",
    "                output = self.output_projection(conv_output + attn_output)\n",
    "            else:\n",
    "                output = self.output_projection(attn_output)\n",
    "\n",
    "        else:\n",
    "            if self.conv_mode == 'Conv':\n",
    "                output = self.conv_layer(x)  # No attention, using convolution only\n",
    "            elif self.conv_mode == 'DWConv':\n",
    "                output = self.output_projection(self.conv_layer(v_proj))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f2919a",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.017552Z",
     "iopub.status.busy": "2025-03-23T10:32:41.017312Z",
     "iopub.status.idle": "2025-03-23T10:32:41.026400Z",
     "shell.execute_reply": "2025-03-23T10:32:41.025629Z"
    },
    "papermill": {
     "duration": 0.019898,
     "end_time": "2025-03-23T10:32:41.027723",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.007825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n",
    "                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n",
    "        \"\"\"\n",
    "        A transformer block that includes attention (optional) and MLP layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enable_attention = enable_attention\n",
    "        self.enable_mlp_norm = enable_mlp_norm\n",
    "\n",
    "        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n",
    "        self.attention_layer = AdaptiveAttention(\n",
    "            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n",
    "            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n",
    "        )\n",
    "\n",
    "        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n",
    "        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.enable_attention:\n",
    "            x, rescale, rebias = self.pre_norm(x)\n",
    "        x = self.attention_layer(x)\n",
    "        if self.enable_attention:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        residual = x\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x, rescale, rebias = self.post_norm(x)\n",
    "        x = self.mlp_layer(x)\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, window_size=8,\n",
    "                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n",
    "        \"\"\"\n",
    "        A stage of transformer blocks with configurable attention placement.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        attention_layers = int(attention_ratio * num_layers)\n",
    "\n",
    "        if attention_placement == 'last':\n",
    "            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'first':\n",
    "            enable_attentions = [i < attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'middle':\n",
    "            enable_attentions = [\n",
    "                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(\n",
    "                network_depth=network_depth,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                enable_attention=enable_attentions[i],\n",
    "                conv_mode=conv_mode\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer stage.\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ec0c19",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.046873Z",
     "iopub.status.busy": "2025-03-23T10:32:41.046672Z",
     "iopub.status.idle": "2025-03-23T10:32:41.052290Z",
     "shell.execute_reply": "2025-03-23T10:32:41.051741Z"
    },
    "papermill": {
     "duration": 0.016669,
     "end_time": "2025-03-23T10:32:41.053467",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.036798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch embedding module that projects input images into token embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to generate patch embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class PatchReconstruction(nn.Module):\n",
    "    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch reconstruction module that converts token embeddings back to image patches.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2, padding_mode='reflect'\n",
    "            ),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to reconstruct image from embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9894b455",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.072845Z",
     "iopub.status.busy": "2025-03-23T10:32:41.072635Z",
     "iopub.status.idle": "2025-03-23T10:32:41.078360Z",
     "shell.execute_reply": "2025-03-23T10:32:41.077598Z"
    },
    "papermill": {
     "duration": 0.016776,
     "end_time": "2025-03-23T10:32:41.079558",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.062782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelectiveKernelFusion(nn.Module):\n",
    "    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n",
    "        \"\"\"\n",
    "        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n",
    "\n",
    "        Args:\n",
    "            channels (int): Number of input channels.\n",
    "            num_branches (int): Number of feature branches to fuse.\n",
    "            reduction_ratio (int): Reduction ratio for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_branches = num_branches\n",
    "        reduced_channels = max(int(channels / reduction_ratio), 4)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Forward pass for selective kernel fusion.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (list of tensors): A list of feature maps to be fused.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adaptively fused feature map.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = feature_maps[0].shape\n",
    "        \n",
    "        # Concatenate feature maps along a new dimension (num_branches)\n",
    "        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        aggregated_features = torch.sum(stacked_features, dim=1)\n",
    "        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n",
    "        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n",
    "\n",
    "        # Weighted sum of input feature maps\n",
    "        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b094f3",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.099138Z",
     "iopub.status.busy": "2025-03-23T10:32:41.098938Z",
     "iopub.status.idle": "2025-03-23T10:32:41.111925Z",
     "shell.execute_reply": "2025-03-23T10:32:41.111287Z"
    },
    "papermill": {
     "duration": 0.024316,
     "end_time": "2025-03-23T10:32:41.113175",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.088859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DehazingTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "                 layer_depths=[16, 16, 16, 8, 8],\n",
    "                 num_heads=[2, 4, 6, 1, 1],\n",
    "                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding settings\n",
    "        self.patch_size = 4\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # Backbone layers\n",
    "        self.encoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[0],\n",
    "            num_layers=layer_depths[0],\n",
    "            num_heads=num_heads[0],\n",
    "            mlp_ratio=mlp_ratios[0],\n",
    "            norm_layer=norm_layers[0],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[0],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[0]\n",
    "        )\n",
    "        \n",
    "        self.downsample1 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "        \n",
    "        self.encoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[1],\n",
    "            num_layers=layer_depths[1],\n",
    "            num_heads=num_heads[1],\n",
    "            mlp_ratio=mlp_ratios[1],\n",
    "            norm_layer=norm_layers[1],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[1],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[1]\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "        \n",
    "        self.encoder_stage3 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[2],\n",
    "            num_layers=layer_depths[2],\n",
    "            num_heads=num_heads[2],\n",
    "            mlp_ratio=mlp_ratios[2],\n",
    "            norm_layer=norm_layers[2],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[2],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[2]\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n",
    "        \n",
    "        self.decoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[3],\n",
    "            num_layers=layer_depths[3],\n",
    "            num_heads=num_heads[3],\n",
    "            mlp_ratio=mlp_ratios[3],\n",
    "            norm_layer=norm_layers[3],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[3],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[3]\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n",
    "        \n",
    "        self.decoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[4],\n",
    "            num_layers=layer_depths[4],\n",
    "            num_heads=num_heads[4],\n",
    "            mlp_ratio=mlp_ratios[4],\n",
    "            norm_layer=norm_layers[4],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[4],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[4]\n",
    "        )\n",
    "\n",
    "        # Final patch reconstruction\n",
    "        self.patch_reconstruction = PatchReconstruction(\n",
    "            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def adjust_image_size(self, x):\n",
    "        # Ensures the input image size is compatible with the patch size\n",
    "        _, _, height, width = x.size()\n",
    "        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n",
    "        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder_stage1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.downsample1(x)\n",
    "        x = self.encoder_stage2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.downsample2(x)\n",
    "        x = self.encoder_stage3(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n",
    "        x = self.decoder_stage1(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n",
    "        x = self.decoder_stage2(x)\n",
    "        x = self.patch_reconstruction(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_height, original_width = x.shape[2:]\n",
    "        x = self.adjust_image_size(x)\n",
    "\n",
    "        features = self.extract_features(x)\n",
    "        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n",
    "\n",
    "        # Dehazing formula: I = J * t + A * (1 - t)\n",
    "        x = transmission_map * x - atmospheric_light + x\n",
    "        x = x[:, :, :original_height, :original_width]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f38556b",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.132174Z",
     "iopub.status.busy": "2025-03-23T10:32:41.131955Z",
     "iopub.status.idle": "2025-03-23T10:32:41.135466Z",
     "shell.execute_reply": "2025-03-23T10:32:41.134889Z"
    },
    "papermill": {
     "duration": 0.014535,
     "end_time": "2025-03-23T10:32:41.136677",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.122142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dehazing_transformer():\n",
    "    return DehazingTransformer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "        layer_depths=[12, 12, 12, 6, 6],\n",
    "        num_heads=[2, 4, 6, 1, 1],\n",
    "        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b724edae",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.155568Z",
     "iopub.status.busy": "2025-03-23T10:32:41.155340Z",
     "iopub.status.idle": "2025-03-23T10:32:41.161968Z",
     "shell.execute_reply": "2025-03-23T10:32:41.161353Z"
    },
    "papermill": {
     "duration": 0.017263,
     "end_time": "2025-03-23T10:32:41.163138",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.145875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvolutionalGuidedFilter(nn.Module):\n",
    "    def __init__(self, radius=1, norm_layer=nn.BatchNorm2d, conv_kernel_size: int = 1):\n",
    "        super(ConvolutionalGuidedFilter, self).__init__()\n",
    "\n",
    "        self.box_filter = nn.Conv2d(\n",
    "            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n",
    "        )\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                6,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                3,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.box_filter.weight.data[...] = 1.0\n",
    "\n",
    "    def forward(self, x_low_res, y_low_res, x_high_res):\n",
    "        _, _, h_lr, w_lr = x_low_res.size()\n",
    "        _, _, h_hr, w_hr = x_high_res.size()\n",
    "\n",
    "        N = self.box_filter(x_low_res.data.new().resize_((1, 3, h_lr, w_lr)).fill_(1.0))\n",
    "        ## mean_x\n",
    "        mean_x = self.box_filter(x_low_res) / N\n",
    "        ## mean_y\n",
    "        mean_y = self.box_filter(y_low_res) / N\n",
    "        ## cov_xy\n",
    "        cov_xy = self.box_filter(x_low_res * y_low_res) / N - mean_x * mean_y\n",
    "        ## var_x\n",
    "        var_x = self.box_filter(x_low_res * x_low_res) / N - mean_x * mean_x\n",
    "\n",
    "        ## A\n",
    "        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n",
    "        ## b\n",
    "        b = mean_y - A * mean_x\n",
    "\n",
    "        ## mean_A; mean_b\n",
    "        mean_A = F.interpolate(A, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "        mean_b = F.interpolate(b, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return mean_A * x_high_res + mean_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ed1faca",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.182321Z",
     "iopub.status.busy": "2025-03-23T10:32:41.182130Z",
     "iopub.status.idle": "2025-03-23T10:32:41.187426Z",
     "shell.execute_reply": "2025-03-23T10:32:41.186836Z"
    },
    "papermill": {
     "duration": 0.016163,
     "end_time": "2025-03-23T10:32:41.188542",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.172379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(PixelAttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, 1, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_map = self.attention(x)\n",
    "        return x * attention_map\n",
    "\n",
    "class ChannelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelAttentionLayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, channels, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = self.avg_pool(x)\n",
    "        attention_map = self.attention(pooled)\n",
    "        return x * attention_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb49f5f2",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.207512Z",
     "iopub.status.busy": "2025-03-23T10:32:41.207299Z",
     "iopub.status.idle": "2025-03-23T10:32:41.213784Z",
     "shell.execute_reply": "2025-03-23T10:32:41.213151Z"
    },
    "papermill": {
     "duration": 0.017404,
     "end_time": "2025-03-23T10:32:41.214973",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.197569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SuperResolutionDilationBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_dense_layers, growth_rate):\n",
    "        super(SuperResolutionDilationBlock, self).__init__()\n",
    "\n",
    "        self.split_channels = in_channels // 4\n",
    "        kernel_size = 3\n",
    "\n",
    "        # Dilated convolutions with increasing dilation rates\n",
    "        self.conv1 = nn.Conv2d(self.split_channels, self.split_channels, kernel_size=kernel_size, padding=1, dilation=1)\n",
    "        self.conv2 = nn.Conv2d(self.split_channels * 2, self.split_channels, kernel_size=kernel_size, padding=2, dilation=2)\n",
    "        self.conv3 = nn.Conv2d(self.split_channels * 3, self.split_channels, kernel_size=kernel_size, padding=4, dilation=4)\n",
    "        self.conv4 = nn.Conv2d(self.split_channels * 4, self.split_channels, kernel_size=kernel_size, padding=8, dilation=8)\n",
    "\n",
    "        # Attention mechanisms\n",
    "        self.channel_attention = ChannelAttentionLayer(in_channels)\n",
    "        self.pixel_attention = PixelAttentionLayer(in_channels)\n",
    "\n",
    "        # Final 1x1 convolution for feature fusion\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into 4 equal parts along channel dimension\n",
    "        split_features = torch.split(x, self.split_channels, dim=1)\n",
    "\n",
    "        x0 = F.relu(self.conv1(split_features[0]))\n",
    "        tmp = torch.cat((split_features[1], x0), dim=1)\n",
    "        x1 = F.relu(self.conv2(tmp))\n",
    "\n",
    "        tmp = torch.cat((split_features[2], x0, x1), dim=1)\n",
    "        x2 = F.relu(self.conv3(tmp))\n",
    "\n",
    "        tmp = torch.cat((split_features[3], x0, x1, x2), dim=1)\n",
    "        x3 = F.relu(self.conv4(tmp))\n",
    "\n",
    "        # Concatenate all outputs\n",
    "        merged_features = torch.cat((x0, x1, x2, x3), dim=1)\n",
    "\n",
    "        # Apply 1x1 convolution for feature refinement\n",
    "        out = self.conv_1x1(merged_features)\n",
    "\n",
    "        # Apply attention mechanisms\n",
    "        out = self.channel_attention(out)\n",
    "        out = self.pixel_attention(out)\n",
    "\n",
    "        # Residual connection\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16ca0504",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.233728Z",
     "iopub.status.busy": "2025-03-23T10:32:41.233532Z",
     "iopub.status.idle": "2025-03-23T10:32:41.237512Z",
     "shell.execute_reply": "2025-03-23T10:32:41.236873Z"
    },
    "papermill": {
     "duration": 0.014625,
     "end_time": "2025-03-23T10:32:41.238625",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.224000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNormalization(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(AdaptiveInstanceNormalization, self).__init__()\n",
    "\n",
    "        # Learnable scaling factors\n",
    "        self.scale_x = nn.Parameter(torch.tensor(1.0))  # Identity scaling\n",
    "        self.scale_norm = nn.Parameter(torch.tensor(0.0))  # Initially no effect\n",
    "\n",
    "        # Instance normalization layer with affine transformation enabled\n",
    "        self.instance_norm = nn.InstanceNorm2d(num_channels, momentum=0.999, eps=0.001, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized_x = self.instance_norm(x)\n",
    "        return self.scale_x * x + self.scale_norm * normalized_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5e2c654",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.257586Z",
     "iopub.status.busy": "2025-03-23T10:32:41.257339Z",
     "iopub.status.idle": "2025-03-23T10:32:41.263617Z",
     "shell.execute_reply": "2025-03-23T10:32:41.262969Z"
    },
    "papermill": {
     "duration": 0.017098,
     "end_time": "2025-03-23T10:32:41.264802",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.247704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepGuidedNetwork(nn.Module):\n",
    "    def __init__(self, radius=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Adaptive Normalization for Guided Filtering\n",
    "        norm = AdaptiveInstanceNormalization\n",
    "        kernel_size = 3\n",
    "        depth_rate = 16\n",
    "        in_channels = 3\n",
    "        num_dense_layer = 4\n",
    "        growth_rate = 16\n",
    "\n",
    "        # Initial convolution layers\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, in_channels, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "        # Residual Dense Blocks (RDBs)\n",
    "        self.rdb1 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb2 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb3 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "        self.rdb4 = SuperResolutionDilationBlock(depth_rate, num_dense_layer, growth_rate)\n",
    "\n",
    "        # Guided Filter & Dehazing Transformer\n",
    "        self.guided_filter = ConvolutionalGuidedFilter(radius, norm_layer=norm)\n",
    "        self.dehaze_network = build_dehazing_transformer()\n",
    "\n",
    "        # Downsampling & Upsampling Layers\n",
    "        self.downsample = nn.Upsample(scale_factor=0.5, mode=\"bilinear\", align_corners=True)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "    def forward(self, x_hr):\n",
    "        # Low-resolution processing\n",
    "        x_lr = self.downsample(x_hr)\n",
    "\n",
    "        # Detail extraction through Residual Dense Blocks\n",
    "        y_features = self.conv_in(x_lr)\n",
    "        y_features = self.rdb1(y_features)\n",
    "        y_features = self.rdb2(y_features)\n",
    "        y_features = self.rdb3(y_features)\n",
    "        y_features = self.rdb4(y_features)\n",
    "        y_detail = self.conv_out(y_features)\n",
    "\n",
    "        # Base image estimation using DehazeFormer\n",
    "        y_base = self.dehaze_network(x_lr)\n",
    "\n",
    "        # Combining base and details\n",
    "        y_lr = y_base + y_detail\n",
    "        y_base_hr = self.upsample(y_base)\n",
    "\n",
    "        # Final guided filtering refinement\n",
    "        refined_output = self.guided_filter(x_lr, y_lr, x_hr)\n",
    "        \n",
    "        return refined_output, y_base_hr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e87aeb9b",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.283797Z",
     "iopub.status.busy": "2025-03-23T10:32:41.283584Z",
     "iopub.status.idle": "2025-03-23T10:32:41.286746Z",
     "shell.execute_reply": "2025-03-23T10:32:41.286157Z"
    },
    "papermill": {
     "duration": 0.014073,
     "end_time": "2025-03-23T10:32:41.287959",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.273886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_crop_size(crop_size_str):\n",
    "    try:\n",
    "        return [int(x.strip()) for x in crop_size_str.split(',')]\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid crop size format: '{crop_size_str}'. Expected comma-separated integers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4620158d",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.307035Z",
     "iopub.status.busy": "2025-03-23T10:32:41.306836Z",
     "iopub.status.idle": "2025-03-23T10:32:41.315632Z",
     "shell.execute_reply": "2025-03-23T10:32:41.315054Z"
    },
    "papermill": {
     "duration": 0.019682,
     "end_time": "2025-03-23T10:32:41.316861",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.297179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from random import randrange\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self, crop_size, hazeeffected_images_dir, hazefree_images_dir):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Ensure valid file extensions --- #\n",
    "        valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        hazy_data = [\n",
    "            f for f in glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_extensions)\n",
    "        ]\n",
    "\n",
    "        if not hazy_data:\n",
    "            raise ValueError(f\"No valid images found in {hazeeffected_images_dir}\")\n",
    "\n",
    "        self.hazeeffected_images_dir = hazeeffected_images_dir\n",
    "        self.hazefree_images_dir = hazefree_images_dir\n",
    "\n",
    "        self.haze_names = []\n",
    "        self.gt_names = []\n",
    "        \n",
    "        for h_image in hazy_data:\n",
    "            filename = os.path.basename(h_image)\n",
    "            haze_path = os.path.join(self.hazeeffected_images_dir, filename)\n",
    "            gt_path = os.path.join(self.hazefree_images_dir, filename)\n",
    "\n",
    "            if not os.path.exists(gt_path):\n",
    "                print(f\"Warning: Ground-truth missing for {filename}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            self.haze_names.append(haze_path)\n",
    "            self.gt_names.append(gt_path)\n",
    "\n",
    "        if not self.haze_names:\n",
    "            raise ValueError(\"No matching ground-truth images found.\")\n",
    "\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def get_images(self, index):\n",
    "        crop_width, crop_height = self.crop_size\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        try:\n",
    "            haze_img = Image.open(haze_name).convert('RGB')\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Invalid image format: {haze_name} or {gt_name}\")\n",
    "\n",
    "        width, height = haze_img.size\n",
    "\n",
    "        # --- Handle small images --- #\n",
    "        if width < crop_width or height < crop_height:\n",
    "            raise ValueError(f\"Image too small for cropping: {haze_name}\")\n",
    "\n",
    "        # --- Random crop --- #\n",
    "        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n",
    "        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "\n",
    "        # --- Transform to tensor --- #\n",
    "        transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        transform_gt = Compose([ToTensor()])\n",
    "        haze = transform_haze(haze_crop_img)\n",
    "        gt = transform_gt(gt_crop_img)\n",
    "\n",
    "        # --- Check channels --- #\n",
    "        if haze.shape[0] != 3 or gt.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {haze_name}\")\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_images(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d263110",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.335936Z",
     "iopub.status.busy": "2025-03-23T10:32:41.335737Z",
     "iopub.status.idle": "2025-03-23T10:32:41.345059Z",
     "shell.execute_reply": "2025-03-23T10:32:41.344464Z"
    },
    "papermill": {
     "duration": 0.020168,
     "end_time": "2025-03-23T10:32:41.346235",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.326067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from random import randrange, shuffle\n",
    "\n",
    "class HazeDataset(Dataset):\n",
    "    def __init__(self, crop_size, hazeeffected_images_dir, hazefree_images_dir, split=\"train\", split_ratio=0.8):\n",
    "        \"\"\"\n",
    "        Dataset class for handling both training and validation dynamically.\n",
    "        \n",
    "        Args:\n",
    "            crop_size (tuple): (width, height) of the random crop.\n",
    "            hazeeffected_images_dir (str): Directory for hazy images.\n",
    "            hazefree_images_dir (str): Directory for ground-truth images.\n",
    "            split (str): \"train\" or \"valid\" (determines data split).\n",
    "            split_ratio (float): Percentage of images to use for training (default 80% train, 20% validation).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Ensure valid file extensions --- #\n",
    "        valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        hazy_data = [\n",
    "            f for f in glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_extensions)\n",
    "        ]\n",
    "\n",
    "        if not hazy_data:\n",
    "            raise ValueError(f\"No valid images found in {hazeeffected_images_dir}\")\n",
    "\n",
    "        # # --- Sort and shuffle to ensure random split --- #\n",
    "        hazy_data.sort()\n",
    "        # shuffle(hazy_data)  \n",
    "\n",
    "        # --- Split into train and validation --- #\n",
    "        split_idx = int(len(hazy_data) * split_ratio)\n",
    "        if split == \"train\":\n",
    "            hazy_data = hazy_data[:split_idx]\n",
    "        else:  # \"valid\"\n",
    "            hazy_data = hazy_data[split_idx:]\n",
    "\n",
    "        self.haze_names = []\n",
    "        self.gt_names = []\n",
    "        \n",
    "        for h_image in hazy_data:\n",
    "            filename = os.path.basename(h_image)\n",
    "            haze_path = os.path.join(hazeeffected_images_dir, filename)\n",
    "            gt_path = os.path.join(hazefree_images_dir, filename)\n",
    "\n",
    "            if not os.path.exists(gt_path):\n",
    "                print(f\"Warning: Ground-truth missing for {filename}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            self.haze_names.append(haze_path)\n",
    "            self.gt_names.append(gt_path)\n",
    "\n",
    "        if not self.haze_names:\n",
    "            raise ValueError(\"No matching ground-truth images found.\")\n",
    "\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def get_images(self, index):\n",
    "        crop_width, crop_height = self.crop_size\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        try:\n",
    "            haze_img = Image.open(haze_name).convert('RGB')\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Invalid image format: {haze_name} or {gt_name}\")\n",
    "\n",
    "        width, height = haze_img.size\n",
    "\n",
    "        # --- Handle small images --- #\n",
    "        if width < crop_width or height < crop_height:\n",
    "            raise ValueError(f\"Image too small for cropping: {haze_name}\")\n",
    "\n",
    "        # --- Random crop --- #\n",
    "        x, y = randrange(0, width - crop_width + 1), randrange(0, height - crop_height + 1)\n",
    "        haze_crop_img = haze_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "        gt_crop_img = gt_img.crop((x, y, x + crop_width, y + crop_height))\n",
    "\n",
    "        # --- Transform to tensor --- #\n",
    "        transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        transform_gt = Compose([ToTensor()])\n",
    "        haze = transform_haze(haze_crop_img)\n",
    "        gt = transform_gt(gt_crop_img)\n",
    "\n",
    "        # --- Check channels --- #\n",
    "        if haze.shape[0] != 3 or gt.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {haze_name}\")\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_images(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565023bb",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.008895,
     "end_time": "2025-03-23T10:32:41.364609",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.355714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1183870",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.383301Z",
     "iopub.status.busy": "2025-03-23T10:32:41.383102Z",
     "iopub.status.idle": "2025-03-23T10:32:41.386801Z",
     "shell.execute_reply": "2025-03-23T10:32:41.386184Z"
    },
    "papermill": {
     "duration": 0.014384,
     "end_time": "2025-03-23T10:32:41.387983",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.373599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_psnr(dehaze, gt):\n",
    "    \"\"\"\n",
    "    Compute PSNR (Peak Signal-to-Noise Ratio) between dehazed and ground truth images.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: PSNR values for each image in the batch.\n",
    "    \"\"\"\n",
    "    mse = F.mse_loss(dehaze, gt, reduction='none').mean(dim=[1, 2, 3])  # Compute MSE per image\n",
    "    intensity_max = 1.0\n",
    "\n",
    "    # Compute PSNR safely, avoiding division by zero and extreme values\n",
    "    psnr_list = [10.0 * log10(intensity_max / max(mse_val.item(), 1e-6)) for mse_val in mse]\n",
    "\n",
    "    return psnr_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a53da2dc",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:41.406967Z",
     "iopub.status.busy": "2025-03-23T10:32:41.406767Z",
     "iopub.status.idle": "2025-03-23T10:32:43.645736Z",
     "shell.execute_reply": "2025-03-23T10:32:43.644795Z"
    },
    "papermill": {
     "duration": 2.250194,
     "end_time": "2025-03-23T10:32:43.647309",
     "exception": false,
     "start_time": "2025-03-23T10:32:41.397115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "# Define SSIM metric\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0, reduction='none')\n",
    "\n",
    "def to_ssim(dehaze: torch.Tensor, gt: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute SSIM directly on the GPU using torchmetrics.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: SSIM values for each image in the batch.\n",
    "    \"\"\"\n",
    "    ssim_values = ssim_metric(dehaze, gt)  # Shape: [B]\n",
    "    # print(\"1\",ssim_values)\n",
    "    # print(\"2\",[ssim_values])\n",
    "    ssim_values = ssim_values.tolist() \n",
    "    # print(type(ssim_values))\n",
    "    if isinstance(ssim_values, float):  # Correct way to check for a float\n",
    "        return [ssim_values]  # Convert single float to a list\n",
    "    return ssim_values  # Otherwise, return as is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56460ca4",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:43.667334Z",
     "iopub.status.busy": "2025-03-23T10:32:43.666970Z",
     "iopub.status.idle": "2025-03-23T10:32:43.805711Z",
     "shell.execute_reply": "2025-03-23T10:32:43.804663Z"
    },
    "papermill": {
     "duration": 0.150007,
     "end_time": "2025-03-23T10:32:43.807214",
     "exception": false,
     "start_time": "2025-03-23T10:32:43.657207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0067753116600215435]\n"
     ]
    }
   ],
   "source": [
    "# Test with a dummy tensor\n",
    "dehaze = torch.rand(1, 3, 360, 360)  # Random batch of images\n",
    "gt = torch.rand(1, 3, 360, 360)  # Random ground truth images\n",
    "\n",
    "ssim_scores = to_ssim(dehaze, gt)\n",
    "print(ssim_scores)  # Should print a list of 6 SSIM values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4c082a4",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:43.827580Z",
     "iopub.status.busy": "2025-03-23T10:32:43.827292Z",
     "iopub.status.idle": "2025-03-23T10:32:43.832629Z",
     "shell.execute_reply": "2025-03-23T10:32:43.831780Z"
    },
    "papermill": {
     "duration": 0.016701,
     "end_time": "2025-03-23T10:32:43.834007",
     "exception": false,
     "start_time": "2025-03-23T10:32:43.817306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validationB(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: Your deep learning model\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: GPU/CPU device\n",
    "    :param category: dataset type (indoor/outdoor)\n",
    "    :param save_tag: whether to save images\n",
    "    :return: average PSNR & SSIM values\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    \n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            haze, gt = val_data\n",
    "            haze, gt = haze.to(device), gt.to(device)\n",
    "            dehaze, _ = net(haze)\n",
    "\n",
    "        # --- Compute PSNR & SSIM --- #\n",
    "        batch_psnr = to_psnr(dehaze, gt)  # This returns a list\n",
    "        # print(batch_psnr)\n",
    "        batch_ssim = to_ssim(dehaze, gt)  # This returns a list\n",
    "        # print(batch_ssim)\n",
    "\n",
    "        psnr_list.extend(batch_psnr)  # Flatten the list\n",
    "        ssim_list.extend(batch_ssim)  # Flatten the list\n",
    "\n",
    "    # --- Ensure lists are not empty to avoid division by zero --- #\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ec823",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.008821,
     "end_time": "2025-03-23T10:32:43.851897",
     "exception": false,
     "start_time": "2025-03-23T10:32:43.843076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb30e446",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:43.917671Z",
     "iopub.status.busy": "2025-03-23T10:32:43.917430Z",
     "iopub.status.idle": "2025-03-23T10:32:43.925653Z",
     "shell.execute_reply": "2025-03-23T10:32:43.924845Z"
    },
    "papermill": {
     "duration": 0.019446,
     "end_time": "2025-03-23T10:32:43.926820",
     "exception": false,
     "start_time": "2025-03-23T10:32:43.907374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c201648397b4493cbf48336a44bb293e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Execution Env:', options=('local', 'kaggle'), value='local')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execution_env_widget = widgets.Dropdown(options=['local', 'kaggle'], value='local', description='Execution Env:')\n",
    "display(execution_env_widget)\n",
    "\n",
    "if os.path.exists('/kaggle'):\n",
    "    execution_env_widget.value = 'kaggle' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8748dac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:43.945989Z",
     "iopub.status.busy": "2025-03-23T10:32:43.945786Z",
     "iopub.status.idle": "2025-03-23T10:32:43.982728Z",
     "shell.execute_reply": "2025-03-23T10:32:43.981982Z"
    },
    "papermill": {
     "duration": 0.048076,
     "end_time": "2025-03-23T10:32:43.984037",
     "exception": false,
     "start_time": "2025-03-23T10:32:43.935961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51668ffb38d74d32a2936bcbca1a3e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Learning Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37614b7ee5de4d8e9628ec203dd7f10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='128,128', description='Crop Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff309398841442f8e57d6e11428ee21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=6, description='Train Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bad20d09feb44269f057d9a8c61605c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=0, description='Version:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac5e63c2cb94eafaec67327fcd845bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='Growth Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629773cfd6bd4a3fbdd821946baedda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.04, description='Lambda Loss:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d667b6e85f4f20924a59c5abf71581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Val Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bc23f6b8cd4741ae37a0ed67800c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', index=2, options=('indoor', 'outdoor', 'reside', 'nh'), value='reside')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters set:\n",
      "learning_rate: 0.0001\n",
      "crop_size: [128, 128]\n",
      "train_batch_size: 6\n",
      "version: 0\n",
      "growth_rate: 16\n",
      "lambda_loss: 0.04\n",
      "val_batch_size: 2\n",
      "category: reside\n",
      "execution_env: kaggle\n",
      "\n",
      "Final dataset paths:\n",
      "Training directory: /kaggle/input/reside6k/RESIDE-6K/train\n",
      "Validation directory: /kaggle/input/reside6k/RESIDE-6K/train\n",
      "Number of epochs: 85\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Create widgets for each hyper-parameter ---\n",
    "learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n",
    "crop_size_widget = widgets.Text(value='128,128', description='Crop Size:')\n",
    "train_batch_size_widget = widgets.IntText(value=6, description='Train Batch Size:')\n",
    "version_widget = widgets.IntText(value=0, description='Version:')\n",
    "growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n",
    "lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n",
    "val_batch_size_widget = widgets.IntText(value=2, description='Val Batch Size:')\n",
    "category_widget = widgets.Dropdown(options=['indoor', 'outdoor', 'reside', 'nh'], value='reside', description='Category:')\n",
    "\n",
    "# --- Display the widgets ---\n",
    "display(\n",
    "    learning_rate_widget, crop_size_widget, train_batch_size_widget, version_widget,\n",
    "    growth_rate_widget, lambda_loss_widget, \n",
    "    val_batch_size_widget, category_widget\n",
    ")\n",
    "\n",
    "# --- Function to parse crop size ---\n",
    "def parse_crop_size(crop_size_str):\n",
    "    return [int(x) for x in crop_size_str.split(',')]\n",
    "\n",
    "# --- Assign the widget values to variables ---\n",
    "learning_rate = learning_rate_widget.value\n",
    "crop_size = parse_crop_size(crop_size_widget.value)\n",
    "train_batch_size = train_batch_size_widget.value\n",
    "version = version_widget.value\n",
    "growth_rate = growth_rate_widget.value\n",
    "lambda_loss = lambda_loss_widget.value\n",
    "val_batch_size = val_batch_size_widget.value\n",
    "category = category_widget.value\n",
    "\n",
    "execution_env = execution_env_widget.value  # Local or Kaggle\n",
    "\n",
    "\n",
    "print('\\nHyper-parameters set:')\n",
    "print(f'learning_rate: {learning_rate}')\n",
    "print(f'crop_size: {crop_size}')\n",
    "print(f'train_batch_size: {train_batch_size}')\n",
    "print(f'version: {version}')\n",
    "print(f'growth_rate: {growth_rate}')\n",
    "print(f'lambda_loss: {lambda_loss}')\n",
    "print(f'val_batch_size: {val_batch_size}')\n",
    "print(f'category: {category}')\n",
    "print(f'execution_env: {execution_env}')\n",
    "\n",
    "# --- Set category-specific hyper-parameters ---\n",
    "if category == 'indoor':\n",
    "    num_epochs = 1500\n",
    "    train_data_dir = './data/train/indoor/'\n",
    "    val_data_dir = './data/test/SOTS/indoor/'\n",
    "elif category == 'outdoor':\n",
    "    num_epochs = 10\n",
    "    train_data_dir = './data/train/outdoor/'\n",
    "    val_data_dir = './data/test/SOTS/outdoor/'\n",
    "elif category == 'reside':\n",
    "    num_epochs = 85\n",
    "    train_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n",
    "    val_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n",
    "    test_data_dir = '/kaggle/input/reside6k/RESIDE-6K/test'\n",
    "elif category == 'nh':\n",
    "    num_epochs = 50\n",
    "    train_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "    val_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "else:\n",
    "    raise Exception('Wrong image category. Set it to indoor or outdoor for RESIDE dataset.')\n",
    "\n",
    "# --- Adjust paths based on execution environment ---\n",
    "# if execution_env == 'kaggle':\n",
    "    # train_data_dir = '/kaggle/input/reside-dataset/' + train_data_dir.strip('./')\n",
    "    # val_data_dir = '/kaggle/input/reside-dataset/' + val_data_dir.strip('./')\n",
    "    # train_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T'\n",
    "    # val_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V' \n",
    "    # train_data_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "    # val_data_dir = '/kaggle/input/o-haze/O-HAZY/GT' \n",
    "print('\\nFinal dataset paths:')\n",
    "print(f'Training directory: {train_data_dir}')\n",
    "print(f'Validation directory: {val_data_dir}')\n",
    "print(f'Number of epochs: {num_epochs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f61304f",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:44.005396Z",
     "iopub.status.busy": "2025-03-23T10:32:44.005173Z",
     "iopub.status.idle": "2025-03-23T10:32:44.008458Z",
     "shell.execute_reply": "2025-03-23T10:32:44.007665Z"
    },
    "papermill": {
     "duration": 0.015356,
     "end_time": "2025-03-23T10:32:44.009818",
     "exception": false,
     "start_time": "2025-03-23T10:32:43.994462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hazeeffected_images_dir_train = f\"{train_data_dir}/IN\"\n",
    "hazeeffected_images_dir_train = f\"{train_data_dir}/hazy\"\n",
    "hazefree_images_dir_train = f\"{train_data_dir}/GT\"\n",
    "\n",
    "# hazeeffected_images_dir_valid = f\"{val_data_dir}/IN\"\n",
    "hazeeffected_images_dir_valid = f\"{val_data_dir}/hazy\"\n",
    "hazefree_images_dir_valid = f\"{val_data_dir}/GT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd3e449a",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:44.118477Z",
     "iopub.status.busy": "2025-03-23T10:32:44.118185Z",
     "iopub.status.idle": "2025-03-23T10:32:44.122771Z",
     "shell.execute_reply": "2025-03-23T10:32:44.121940Z"
    },
    "papermill": {
     "duration": 0.052997,
     "end_time": "2025-03-23T10:32:44.124016",
     "exception": false,
     "start_time": "2025-03-23T10:32:44.071019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_log(epoch, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, category):\n",
    "    log_dir = \"./training_log\"\n",
    "    os.makedirs(log_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    log_path = os.path.join(log_dir, f\"{category}_log.txt\")\n",
    "\n",
    "    print('({0:.0f}s) Epoch [{1}/{2}], Train_PSNR:{3:.2f}, Val_PSNR:{4:.2f}, Val_SSIM:{5:.4f}'\n",
    "          .format(one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim))\n",
    "\n",
    "    # --- Write the training log --- #\n",
    "    with open(log_path, 'a') as f:\n",
    "        print('Date: {0}, Time_Cost: {1:.0f}s, Epoch: [{2}/{3}], Train_PSNR: {4:.2f}, Val_PSNR: {5:.2f}, Val_SSIM: {6:.4f}'\n",
    "              .format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "                      one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c93b3b2",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:44.144700Z",
     "iopub.status.busy": "2025-03-23T10:32:44.144463Z",
     "iopub.status.idle": "2025-03-23T10:32:44.148603Z",
     "shell.execute_reply": "2025-03-23T10:32:44.147788Z"
    },
    "papermill": {
     "duration": 0.015743,
     "end_time": "2025-03-23T10:32:44.149821",
     "exception": false,
     "start_time": "2025-03-23T10:32:44.134078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, category, lr_decay=0.90):\n",
    "    \"\"\"\n",
    "    Adjusts the learning rate based on the epoch and dataset category.\n",
    "\n",
    "    :param optimizer: The optimizer (e.g., Adam, SGD).\n",
    "    :param epoch: Current epoch number.\n",
    "    :param category: Dataset category ('indoor', 'outdoor', or 'NH').\n",
    "    :param lr_decay: Multiplicative factor for learning rate decay.\n",
    "    \"\"\"\n",
    "    # Define learning rate decay steps based on category\n",
    "    step_dict = {'indoor': 18, 'outdoor': 3, 'NH': 20}\n",
    "    step = step_dict.get(category, 3)  # Default step size if category is unknown\n",
    "\n",
    "    # Decay learning rate at the specified step\n",
    "    if epoch > 0 and epoch % step == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= lr_decay\n",
    "            print(f\"Epoch {epoch}: Learning rate adjusted to {param_group['lr']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ad3d8",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.010084,
     "end_time": "2025-03-23T10:32:44.170116",
     "exception": false,
     "start_time": "2025-03-23T10:32:44.160032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b991f033",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:44.191098Z",
     "iopub.status.busy": "2025-03-23T10:32:44.190865Z",
     "iopub.status.idle": "2025-03-23T10:32:44.195804Z",
     "shell.execute_reply": "2025-03-23T10:32:44.195015Z"
    },
    "papermill": {
     "duration": 0.016621,
     "end_time": "2025-03-23T10:32:44.196973",
     "exception": false,
     "start_time": "2025-03-23T10:32:44.180352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Perceptual Feature Loss Network --- #\n",
    "class PerceptualLossNet(nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = vgg_model\n",
    "        self.feature_layers = {'3': \"low_level\", '8': \"mid_level\", '15': \"high_level\"}\n",
    "\n",
    "    def get_feature_maps(self, x):\n",
    "        feature_maps = []\n",
    "        for layer_id, layer in self.feature_extractor.named_children():\n",
    "            x = layer(x)\n",
    "            if layer_id in self.feature_layers:\n",
    "                feature_maps.append(x)\n",
    "        return feature_maps\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        pred_features = self.get_feature_maps(predicted)\n",
    "        target_features = self.get_feature_maps(target)\n",
    "        \n",
    "        # Compute perceptual loss as mean squared error across feature maps\n",
    "        loss = torch.stack([F.mse_loss(p, t) for p, t in zip(pred_features, target_features)]).mean()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36a7ac55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:44.217770Z",
     "iopub.status.busy": "2025-03-23T10:32:44.217538Z",
     "iopub.status.idle": "2025-03-23T10:32:49.516857Z",
     "shell.execute_reply": "2025-03-23T10:32:49.515828Z"
    },
    "papermill": {
     "duration": 5.311204,
     "end_time": "2025-03-23T10:32:49.518256",
     "exception": false,
     "start_time": "2025-03-23T10:32:44.207052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:02<00:00, 201MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model weights loaded from /kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\n",
      "📊 Total Trainable Parameters: 4,645,694\n"
     ]
    }
   ],
   "source": [
    "# --- Imports --- #\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "# --- Device Setup --- #\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_ids = list(range(torch.cuda.device_count()))\n",
    "\n",
    "# --- Initialize Model --- #\n",
    "net = DeepGuidedNetwork().to(device)\n",
    "\n",
    "# --- Enable Multi-GPU (if available) --- #\n",
    "if len(device_ids) > 1:\n",
    "    net = nn.DataParallel(net, device_ids=device_ids)\n",
    "\n",
    "# --- Optimizer --- #\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Load Pretrained VGG16 for Perceptual Loss --- #\n",
    "vgg_features = vgg16(pretrained=True).features[:16].to(device)\n",
    "for param in vgg_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "loss_network = PerceptualLossNet(vgg_features)\n",
    "loss_network.eval()\n",
    "\n",
    "# --- Load Model Weights (if available) --- #\n",
    "model_name = 'formernew'\n",
    "# checkpoint_path = f\"{model_name}_{category}_haze_best_{version}\"\n",
    "checkpoint_path = \"/kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\" \n",
    "\n",
    "try:\n",
    "    net.load_state_dict(torch.load(checkpoint_path, weights_only=False, map_location=torch.device('cpu')))\n",
    "    print(f\"✅ Model weights loaded from {checkpoint_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️ No pretrained weights found at {checkpoint_path}\")\n",
    "\n",
    "# --- Compute Total Trainable Parameters --- #\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"📊 Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4db7b82b",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:32:49.542748Z",
     "iopub.status.busy": "2025-03-23T10:32:49.542467Z",
     "iopub.status.idle": "2025-03-23T10:33:00.859676Z",
     "shell.execute_reply": "2025-03-23T10:33:00.858623Z"
    },
    "papermill": {
     "duration": 11.330701,
     "end_time": "2025-03-23T10:33:00.861119",
     "exception": false,
     "start_time": "2025-03-23T10:32:49.530418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4800, Validation samples: 1200\n"
     ]
    }
   ],
   "source": [
    "# Create train and validation datasets\n",
    "train_dataset = HazeDataset(crop_size=crop_size, \n",
    "                            hazeeffected_images_dir=hazeeffected_images_dir_train,\n",
    "                            hazefree_images_dir=hazefree_images_dir_train,\n",
    "                            split=\"train\")\n",
    "\n",
    "val_dataset = HazeDataset(crop_size=crop_size, \n",
    "                          hazeeffected_images_dir=hazeeffected_images_dir_train,\n",
    "                          hazefree_images_dir=hazefree_images_dir_train,\n",
    "                          split=\"valid\")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6aae82b",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:33:00.886509Z",
     "iopub.status.busy": "2025-03-23T10:33:00.886214Z",
     "iopub.status.idle": "2025-03-23T10:33:00.890058Z",
     "shell.execute_reply": "2025-03-23T10:33:00.889192Z"
    },
    "papermill": {
     "duration": 0.017474,
     "end_time": "2025-03-23T10:33:00.891198",
     "exception": false,
     "start_time": "2025-03-23T10:33:00.873724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9649e085",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-23T10:33:00.970826Z",
     "iopub.status.busy": "2025-03-23T10:33:00.970613Z",
     "iopub.status.idle": "2025-03-23T10:33:01.094791Z",
     "shell.execute_reply": "2025-03-23T10:33:01.093994Z"
    },
    "papermill": {
     "duration": 0.137719,
     "end_time": "2025-03-23T10:33:01.096071",
     "exception": false,
     "start_time": "2025-03-23T10:33:00.958352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 128, 128]) torch.Size([6, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for i,o in train_data_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6bc8067",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T10:33:01.120982Z",
     "iopub.status.busy": "2025-03-23T10:33:01.120762Z",
     "iopub.status.idle": "2025-03-23T21:17:18.264753Z",
     "shell.execute_reply": "2025-03-23T21:17:18.263977Z"
    },
    "papermill": {
     "duration": 38657.157881,
     "end_time": "2025-03-23T21:17:18.266256",
     "exception": false,
     "start_time": "2025-03-23T10:33:01.108375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Validation -> PSNR: 28.94, SSIM: 0.9239\n",
      "Epoch [0/85], Iteration [0]\n",
      "Epoch [0/85], Iteration [85]\n",
      "Epoch [0/85], Iteration [170]\n",
      "Epoch [0/85], Iteration [255]\n",
      "Epoch [0/85], Iteration [340]\n",
      "Epoch [0/85], Iteration [425]\n",
      "Epoch [0/85], Iteration [510]\n",
      "Epoch [0/85], Iteration [595]\n",
      "Epoch [0/85], Iteration [680]\n",
      "Epoch [0/85], Iteration [765]\n",
      "Model saved in epoch 0.\n",
      "(235s) Epoch [1/85], Train_PSNR:27.21, Val_PSNR:26.13, Val_SSIM:0.8960\n",
      "Epoch [1/85], Iteration [0]\n",
      "Epoch [1/85], Iteration [85]\n",
      "Epoch [1/85], Iteration [170]\n",
      "Epoch [1/85], Iteration [255]\n",
      "Epoch [1/85], Iteration [340]\n",
      "Epoch [1/85], Iteration [425]\n",
      "Epoch [1/85], Iteration [510]\n",
      "Epoch [1/85], Iteration [595]\n",
      "Epoch [1/85], Iteration [680]\n",
      "Epoch [1/85], Iteration [765]\n",
      "(200s) Epoch [2/85], Train_PSNR:27.27, Val_PSNR:24.98, Val_SSIM:0.8807\n",
      "Epoch [2/85], Iteration [0]\n",
      "Epoch [2/85], Iteration [85]\n",
      "Epoch [2/85], Iteration [170]\n",
      "Epoch [2/85], Iteration [255]\n",
      "Epoch [2/85], Iteration [340]\n",
      "Epoch [2/85], Iteration [425]\n",
      "Epoch [2/85], Iteration [510]\n",
      "Epoch [2/85], Iteration [595]\n",
      "Epoch [2/85], Iteration [680]\n",
      "Epoch [2/85], Iteration [765]\n",
      "(206s) Epoch [3/85], Train_PSNR:26.79, Val_PSNR:27.25, Val_SSIM:0.8921\n",
      "Epoch 3: Learning rate adjusted to 0.000090\n",
      "Epoch [3/85], Iteration [0]\n",
      "Epoch [3/85], Iteration [85]\n",
      "Epoch [3/85], Iteration [170]\n",
      "Epoch [3/85], Iteration [255]\n",
      "Epoch [3/85], Iteration [340]\n",
      "Epoch [3/85], Iteration [425]\n",
      "Epoch [3/85], Iteration [510]\n",
      "Epoch [3/85], Iteration [595]\n",
      "Epoch [3/85], Iteration [680]\n",
      "Epoch [3/85], Iteration [765]\n",
      "(210s) Epoch [4/85], Train_PSNR:27.33, Val_PSNR:25.82, Val_SSIM:0.8853\n",
      "Epoch [4/85], Iteration [0]\n",
      "Epoch [4/85], Iteration [85]\n",
      "Epoch [4/85], Iteration [170]\n",
      "Epoch [4/85], Iteration [255]\n",
      "Epoch [4/85], Iteration [340]\n",
      "Epoch [4/85], Iteration [425]\n",
      "Epoch [4/85], Iteration [510]\n",
      "Epoch [4/85], Iteration [595]\n",
      "Epoch [4/85], Iteration [680]\n",
      "Epoch [4/85], Iteration [765]\n",
      "(218s) Epoch [5/85], Train_PSNR:27.54, Val_PSNR:26.78, Val_SSIM:0.9036\n",
      "Epoch [5/85], Iteration [0]\n",
      "Epoch [5/85], Iteration [85]\n",
      "Epoch [5/85], Iteration [170]\n",
      "Epoch [5/85], Iteration [255]\n",
      "Epoch [5/85], Iteration [340]\n",
      "Epoch [5/85], Iteration [425]\n",
      "Epoch [5/85], Iteration [510]\n",
      "Epoch [5/85], Iteration [595]\n",
      "Epoch [5/85], Iteration [680]\n",
      "Epoch [5/85], Iteration [765]\n",
      "Model saved in epoch 5.\n",
      "(225s) Epoch [6/85], Train_PSNR:27.64, Val_PSNR:27.01, Val_SSIM:0.9073\n",
      "Epoch 6: Learning rate adjusted to 0.000081\n",
      "Epoch [6/85], Iteration [0]\n",
      "Epoch [6/85], Iteration [85]\n",
      "Epoch [6/85], Iteration [170]\n",
      "Epoch [6/85], Iteration [255]\n",
      "Epoch [6/85], Iteration [340]\n",
      "Epoch [6/85], Iteration [425]\n",
      "Epoch [6/85], Iteration [510]\n",
      "Epoch [6/85], Iteration [595]\n",
      "Epoch [6/85], Iteration [680]\n",
      "Epoch [6/85], Iteration [765]\n",
      "(229s) Epoch [7/85], Train_PSNR:27.47, Val_PSNR:27.82, Val_SSIM:0.9162\n",
      "Epoch [7/85], Iteration [0]\n",
      "Epoch [7/85], Iteration [85]\n",
      "Epoch [7/85], Iteration [170]\n",
      "Epoch [7/85], Iteration [255]\n",
      "Epoch [7/85], Iteration [340]\n",
      "Epoch [7/85], Iteration [425]\n",
      "Epoch [7/85], Iteration [510]\n",
      "Epoch [7/85], Iteration [595]\n",
      "Epoch [7/85], Iteration [680]\n",
      "Epoch [7/85], Iteration [765]\n",
      "(235s) Epoch [8/85], Train_PSNR:27.44, Val_PSNR:25.74, Val_SSIM:0.8853\n",
      "Epoch [8/85], Iteration [0]\n",
      "Epoch [8/85], Iteration [85]\n",
      "Epoch [8/85], Iteration [170]\n",
      "Epoch [8/85], Iteration [255]\n",
      "Epoch [8/85], Iteration [340]\n",
      "Epoch [8/85], Iteration [425]\n",
      "Epoch [8/85], Iteration [510]\n",
      "Epoch [8/85], Iteration [595]\n",
      "Epoch [8/85], Iteration [680]\n",
      "Epoch [8/85], Iteration [765]\n",
      "(242s) Epoch [9/85], Train_PSNR:27.72, Val_PSNR:27.74, Val_SSIM:0.9140\n",
      "Epoch 9: Learning rate adjusted to 0.000073\n",
      "Epoch [9/85], Iteration [0]\n",
      "Epoch [9/85], Iteration [85]\n",
      "Epoch [9/85], Iteration [170]\n",
      "Epoch [9/85], Iteration [255]\n",
      "Epoch [9/85], Iteration [340]\n",
      "Epoch [9/85], Iteration [425]\n",
      "Epoch [9/85], Iteration [510]\n",
      "Epoch [9/85], Iteration [595]\n",
      "Epoch [9/85], Iteration [680]\n",
      "Epoch [9/85], Iteration [765]\n",
      "(247s) Epoch [10/85], Train_PSNR:27.73, Val_PSNR:27.45, Val_SSIM:0.9063\n",
      "Epoch [10/85], Iteration [0]\n",
      "Epoch [10/85], Iteration [85]\n",
      "Epoch [10/85], Iteration [170]\n",
      "Epoch [10/85], Iteration [255]\n",
      "Epoch [10/85], Iteration [340]\n",
      "Epoch [10/85], Iteration [425]\n",
      "Epoch [10/85], Iteration [510]\n",
      "Epoch [10/85], Iteration [595]\n",
      "Epoch [10/85], Iteration [680]\n",
      "Epoch [10/85], Iteration [765]\n",
      "Model saved in epoch 10.\n",
      "(253s) Epoch [11/85], Train_PSNR:27.91, Val_PSNR:27.71, Val_SSIM:0.9137\n",
      "Epoch [11/85], Iteration [0]\n",
      "Epoch [11/85], Iteration [85]\n",
      "Epoch [11/85], Iteration [170]\n",
      "Epoch [11/85], Iteration [255]\n",
      "Epoch [11/85], Iteration [340]\n",
      "Epoch [11/85], Iteration [425]\n",
      "Epoch [11/85], Iteration [510]\n",
      "Epoch [11/85], Iteration [595]\n",
      "Epoch [11/85], Iteration [680]\n",
      "Epoch [11/85], Iteration [765]\n",
      "(258s) Epoch [12/85], Train_PSNR:27.92, Val_PSNR:27.69, Val_SSIM:0.9126\n",
      "Epoch 12: Learning rate adjusted to 0.000066\n",
      "Epoch [12/85], Iteration [0]\n",
      "Epoch [12/85], Iteration [85]\n",
      "Epoch [12/85], Iteration [170]\n",
      "Epoch [12/85], Iteration [255]\n",
      "Epoch [12/85], Iteration [340]\n",
      "Epoch [12/85], Iteration [425]\n",
      "Epoch [12/85], Iteration [510]\n",
      "Epoch [12/85], Iteration [595]\n",
      "Epoch [12/85], Iteration [680]\n",
      "Epoch [12/85], Iteration [765]\n",
      "(268s) Epoch [13/85], Train_PSNR:28.00, Val_PSNR:27.99, Val_SSIM:0.9160\n",
      "Epoch [13/85], Iteration [0]\n",
      "Epoch [13/85], Iteration [85]\n",
      "Epoch [13/85], Iteration [170]\n",
      "Epoch [13/85], Iteration [255]\n",
      "Epoch [13/85], Iteration [340]\n",
      "Epoch [13/85], Iteration [425]\n",
      "Epoch [13/85], Iteration [510]\n",
      "Epoch [13/85], Iteration [595]\n",
      "Epoch [13/85], Iteration [680]\n",
      "Epoch [13/85], Iteration [765]\n",
      "(271s) Epoch [14/85], Train_PSNR:27.96, Val_PSNR:27.01, Val_SSIM:0.9099\n",
      "Epoch [14/85], Iteration [0]\n",
      "Epoch [14/85], Iteration [85]\n",
      "Epoch [14/85], Iteration [170]\n",
      "Epoch [14/85], Iteration [255]\n",
      "Epoch [14/85], Iteration [340]\n",
      "Epoch [14/85], Iteration [425]\n",
      "Epoch [14/85], Iteration [510]\n",
      "Epoch [14/85], Iteration [595]\n",
      "Epoch [14/85], Iteration [680]\n",
      "Epoch [14/85], Iteration [765]\n",
      "(280s) Epoch [15/85], Train_PSNR:28.17, Val_PSNR:27.82, Val_SSIM:0.9175\n",
      "Epoch 15: Learning rate adjusted to 0.000059\n",
      "Epoch [15/85], Iteration [0]\n",
      "Epoch [15/85], Iteration [85]\n",
      "Epoch [15/85], Iteration [170]\n",
      "Epoch [15/85], Iteration [255]\n",
      "Epoch [15/85], Iteration [340]\n",
      "Epoch [15/85], Iteration [425]\n",
      "Epoch [15/85], Iteration [510]\n",
      "Epoch [15/85], Iteration [595]\n",
      "Epoch [15/85], Iteration [680]\n",
      "Epoch [15/85], Iteration [765]\n",
      "Model saved in epoch 15.\n",
      "(285s) Epoch [16/85], Train_PSNR:28.22, Val_PSNR:27.78, Val_SSIM:0.9017\n",
      "Epoch [16/85], Iteration [0]\n",
      "Epoch [16/85], Iteration [85]\n",
      "Epoch [16/85], Iteration [170]\n",
      "Epoch [16/85], Iteration [255]\n",
      "Epoch [16/85], Iteration [340]\n",
      "Epoch [16/85], Iteration [425]\n",
      "Epoch [16/85], Iteration [510]\n",
      "Epoch [16/85], Iteration [595]\n",
      "Epoch [16/85], Iteration [680]\n",
      "Epoch [16/85], Iteration [765]\n",
      "(291s) Epoch [17/85], Train_PSNR:28.15, Val_PSNR:27.68, Val_SSIM:0.9113\n",
      "Epoch [17/85], Iteration [0]\n",
      "Epoch [17/85], Iteration [85]\n",
      "Epoch [17/85], Iteration [170]\n",
      "Epoch [17/85], Iteration [255]\n",
      "Epoch [17/85], Iteration [340]\n",
      "Epoch [17/85], Iteration [425]\n",
      "Epoch [17/85], Iteration [510]\n",
      "Epoch [17/85], Iteration [595]\n",
      "Epoch [17/85], Iteration [680]\n",
      "Epoch [17/85], Iteration [765]\n",
      "(296s) Epoch [18/85], Train_PSNR:28.01, Val_PSNR:28.11, Val_SSIM:0.9178\n",
      "Epoch 18: Learning rate adjusted to 0.000053\n",
      "Epoch [18/85], Iteration [0]\n",
      "Epoch [18/85], Iteration [85]\n",
      "Epoch [18/85], Iteration [170]\n",
      "Epoch [18/85], Iteration [255]\n",
      "Epoch [18/85], Iteration [340]\n",
      "Epoch [18/85], Iteration [425]\n",
      "Epoch [18/85], Iteration [510]\n",
      "Epoch [18/85], Iteration [595]\n",
      "Epoch [18/85], Iteration [680]\n",
      "Epoch [18/85], Iteration [765]\n",
      "(302s) Epoch [19/85], Train_PSNR:28.46, Val_PSNR:27.53, Val_SSIM:0.9052\n",
      "Epoch [19/85], Iteration [0]\n",
      "Epoch [19/85], Iteration [85]\n",
      "Epoch [19/85], Iteration [170]\n",
      "Epoch [19/85], Iteration [255]\n",
      "Epoch [19/85], Iteration [340]\n",
      "Epoch [19/85], Iteration [425]\n",
      "Epoch [19/85], Iteration [510]\n",
      "Epoch [19/85], Iteration [595]\n",
      "Epoch [19/85], Iteration [680]\n",
      "Epoch [19/85], Iteration [765]\n",
      "(307s) Epoch [20/85], Train_PSNR:28.51, Val_PSNR:28.05, Val_SSIM:0.9188\n",
      "Epoch [20/85], Iteration [0]\n",
      "Epoch [20/85], Iteration [85]\n",
      "Epoch [20/85], Iteration [170]\n",
      "Epoch [20/85], Iteration [255]\n",
      "Epoch [20/85], Iteration [340]\n",
      "Epoch [20/85], Iteration [425]\n",
      "Epoch [20/85], Iteration [510]\n",
      "Epoch [20/85], Iteration [595]\n",
      "Epoch [20/85], Iteration [680]\n",
      "Epoch [20/85], Iteration [765]\n",
      "Model saved in epoch 20.\n",
      "(311s) Epoch [21/85], Train_PSNR:28.29, Val_PSNR:27.79, Val_SSIM:0.9155\n",
      "Epoch 21: Learning rate adjusted to 0.000048\n",
      "Epoch [21/85], Iteration [0]\n",
      "Epoch [21/85], Iteration [85]\n",
      "Epoch [21/85], Iteration [170]\n",
      "Epoch [21/85], Iteration [255]\n",
      "Epoch [21/85], Iteration [340]\n",
      "Epoch [21/85], Iteration [425]\n",
      "Epoch [21/85], Iteration [510]\n",
      "Epoch [21/85], Iteration [595]\n",
      "Epoch [21/85], Iteration [680]\n",
      "Epoch [21/85], Iteration [765]\n",
      "(321s) Epoch [22/85], Train_PSNR:28.50, Val_PSNR:28.18, Val_SSIM:0.9188\n",
      "Epoch [22/85], Iteration [0]\n",
      "Epoch [22/85], Iteration [85]\n",
      "Epoch [22/85], Iteration [170]\n",
      "Epoch [22/85], Iteration [255]\n",
      "Epoch [22/85], Iteration [340]\n",
      "Epoch [22/85], Iteration [425]\n",
      "Epoch [22/85], Iteration [510]\n",
      "Epoch [22/85], Iteration [595]\n",
      "Epoch [22/85], Iteration [680]\n",
      "Epoch [22/85], Iteration [765]\n",
      "(325s) Epoch [23/85], Train_PSNR:28.66, Val_PSNR:27.22, Val_SSIM:0.9071\n",
      "Epoch [23/85], Iteration [0]\n",
      "Epoch [23/85], Iteration [85]\n",
      "Epoch [23/85], Iteration [170]\n",
      "Epoch [23/85], Iteration [255]\n",
      "Epoch [23/85], Iteration [340]\n",
      "Epoch [23/85], Iteration [425]\n",
      "Epoch [23/85], Iteration [510]\n",
      "Epoch [23/85], Iteration [595]\n",
      "Epoch [23/85], Iteration [680]\n",
      "Epoch [23/85], Iteration [765]\n",
      "(329s) Epoch [24/85], Train_PSNR:28.52, Val_PSNR:28.23, Val_SSIM:0.9206\n",
      "Epoch 24: Learning rate adjusted to 0.000043\n",
      "Epoch [24/85], Iteration [0]\n",
      "Epoch [24/85], Iteration [85]\n",
      "Epoch [24/85], Iteration [170]\n",
      "Epoch [24/85], Iteration [255]\n",
      "Epoch [24/85], Iteration [340]\n",
      "Epoch [24/85], Iteration [425]\n",
      "Epoch [24/85], Iteration [510]\n",
      "Epoch [24/85], Iteration [595]\n",
      "Epoch [24/85], Iteration [680]\n",
      "Epoch [24/85], Iteration [765]\n",
      "(335s) Epoch [25/85], Train_PSNR:28.73, Val_PSNR:28.42, Val_SSIM:0.9195\n",
      "Epoch [25/85], Iteration [0]\n",
      "Epoch [25/85], Iteration [85]\n",
      "Epoch [25/85], Iteration [170]\n",
      "Epoch [25/85], Iteration [255]\n",
      "Epoch [25/85], Iteration [340]\n",
      "Epoch [25/85], Iteration [425]\n",
      "Epoch [25/85], Iteration [510]\n",
      "Epoch [25/85], Iteration [595]\n",
      "Epoch [25/85], Iteration [680]\n",
      "Epoch [25/85], Iteration [765]\n",
      "Model saved in epoch 25.\n",
      "(346s) Epoch [26/85], Train_PSNR:28.74, Val_PSNR:27.89, Val_SSIM:0.9181\n",
      "Epoch [26/85], Iteration [0]\n",
      "Epoch [26/85], Iteration [85]\n",
      "Epoch [26/85], Iteration [170]\n",
      "Epoch [26/85], Iteration [255]\n",
      "Epoch [26/85], Iteration [340]\n",
      "Epoch [26/85], Iteration [425]\n",
      "Epoch [26/85], Iteration [510]\n",
      "Epoch [26/85], Iteration [595]\n",
      "Epoch [26/85], Iteration [680]\n",
      "Epoch [26/85], Iteration [765]\n",
      "(352s) Epoch [27/85], Train_PSNR:28.75, Val_PSNR:28.03, Val_SSIM:0.9205\n",
      "Epoch 27: Learning rate adjusted to 0.000039\n",
      "Epoch [27/85], Iteration [0]\n",
      "Epoch [27/85], Iteration [85]\n",
      "Epoch [27/85], Iteration [170]\n",
      "Epoch [27/85], Iteration [255]\n",
      "Epoch [27/85], Iteration [340]\n",
      "Epoch [27/85], Iteration [425]\n",
      "Epoch [27/85], Iteration [510]\n",
      "Epoch [27/85], Iteration [595]\n",
      "Epoch [27/85], Iteration [680]\n",
      "Epoch [27/85], Iteration [765]\n",
      "(357s) Epoch [28/85], Train_PSNR:28.77, Val_PSNR:26.93, Val_SSIM:0.8959\n",
      "Epoch [28/85], Iteration [0]\n",
      "Epoch [28/85], Iteration [85]\n",
      "Epoch [28/85], Iteration [170]\n",
      "Epoch [28/85], Iteration [255]\n",
      "Epoch [28/85], Iteration [340]\n",
      "Epoch [28/85], Iteration [425]\n",
      "Epoch [28/85], Iteration [510]\n",
      "Epoch [28/85], Iteration [595]\n",
      "Epoch [28/85], Iteration [680]\n",
      "Epoch [28/85], Iteration [765]\n",
      "(365s) Epoch [29/85], Train_PSNR:28.75, Val_PSNR:27.81, Val_SSIM:0.9166\n",
      "Epoch [29/85], Iteration [0]\n",
      "Epoch [29/85], Iteration [85]\n",
      "Epoch [29/85], Iteration [170]\n",
      "Epoch [29/85], Iteration [255]\n",
      "Epoch [29/85], Iteration [340]\n",
      "Epoch [29/85], Iteration [425]\n",
      "Epoch [29/85], Iteration [510]\n",
      "Epoch [29/85], Iteration [595]\n",
      "Epoch [29/85], Iteration [680]\n",
      "Epoch [29/85], Iteration [765]\n",
      "(371s) Epoch [30/85], Train_PSNR:28.93, Val_PSNR:28.02, Val_SSIM:0.9173\n",
      "Epoch 30: Learning rate adjusted to 0.000035\n",
      "Epoch [30/85], Iteration [0]\n",
      "Epoch [30/85], Iteration [85]\n",
      "Epoch [30/85], Iteration [170]\n",
      "Epoch [30/85], Iteration [255]\n",
      "Epoch [30/85], Iteration [340]\n",
      "Epoch [30/85], Iteration [425]\n",
      "Epoch [30/85], Iteration [510]\n",
      "Epoch [30/85], Iteration [595]\n",
      "Epoch [30/85], Iteration [680]\n",
      "Epoch [30/85], Iteration [765]\n",
      "Model saved in epoch 30.\n",
      "(388s) Epoch [31/85], Train_PSNR:28.85, Val_PSNR:28.38, Val_SSIM:0.9087\n",
      "Epoch [31/85], Iteration [0]\n",
      "Epoch [31/85], Iteration [85]\n",
      "Epoch [31/85], Iteration [170]\n",
      "Epoch [31/85], Iteration [255]\n",
      "Epoch [31/85], Iteration [340]\n",
      "Epoch [31/85], Iteration [425]\n",
      "Epoch [31/85], Iteration [510]\n",
      "Epoch [31/85], Iteration [595]\n",
      "Epoch [31/85], Iteration [680]\n",
      "Epoch [31/85], Iteration [765]\n",
      "(396s) Epoch [32/85], Train_PSNR:29.03, Val_PSNR:28.54, Val_SSIM:0.9238\n",
      "Epoch [32/85], Iteration [0]\n",
      "Epoch [32/85], Iteration [85]\n",
      "Epoch [32/85], Iteration [170]\n",
      "Epoch [32/85], Iteration [255]\n",
      "Epoch [32/85], Iteration [340]\n",
      "Epoch [32/85], Iteration [425]\n",
      "Epoch [32/85], Iteration [510]\n",
      "Epoch [32/85], Iteration [595]\n",
      "Epoch [32/85], Iteration [680]\n",
      "Epoch [32/85], Iteration [765]\n",
      "(400s) Epoch [33/85], Train_PSNR:28.95, Val_PSNR:27.89, Val_SSIM:0.9146\n",
      "Epoch 33: Learning rate adjusted to 0.000031\n",
      "Epoch [33/85], Iteration [0]\n",
      "Epoch [33/85], Iteration [85]\n",
      "Epoch [33/85], Iteration [170]\n",
      "Epoch [33/85], Iteration [255]\n",
      "Epoch [33/85], Iteration [340]\n",
      "Epoch [33/85], Iteration [425]\n",
      "Epoch [33/85], Iteration [510]\n",
      "Epoch [33/85], Iteration [595]\n",
      "Epoch [33/85], Iteration [680]\n",
      "Epoch [33/85], Iteration [765]\n",
      "(410s) Epoch [34/85], Train_PSNR:29.05, Val_PSNR:28.29, Val_SSIM:0.9186\n",
      "Epoch [34/85], Iteration [0]\n",
      "Epoch [34/85], Iteration [85]\n",
      "Epoch [34/85], Iteration [170]\n",
      "Epoch [34/85], Iteration [255]\n",
      "Epoch [34/85], Iteration [340]\n",
      "Epoch [34/85], Iteration [425]\n",
      "Epoch [34/85], Iteration [510]\n",
      "Epoch [34/85], Iteration [595]\n",
      "Epoch [34/85], Iteration [680]\n",
      "Epoch [34/85], Iteration [765]\n",
      "(415s) Epoch [35/85], Train_PSNR:29.26, Val_PSNR:28.11, Val_SSIM:0.9138\n",
      "Epoch [35/85], Iteration [0]\n",
      "Epoch [35/85], Iteration [85]\n",
      "Epoch [35/85], Iteration [170]\n",
      "Epoch [35/85], Iteration [255]\n",
      "Epoch [35/85], Iteration [340]\n",
      "Epoch [35/85], Iteration [425]\n",
      "Epoch [35/85], Iteration [510]\n",
      "Epoch [35/85], Iteration [595]\n",
      "Epoch [35/85], Iteration [680]\n",
      "Epoch [35/85], Iteration [765]\n",
      "Model saved in epoch 35.\n",
      "(426s) Epoch [36/85], Train_PSNR:29.14, Val_PSNR:28.81, Val_SSIM:0.9243\n",
      "Epoch 36: Learning rate adjusted to 0.000028\n",
      "Epoch [36/85], Iteration [0]\n",
      "Epoch [36/85], Iteration [85]\n",
      "Epoch [36/85], Iteration [170]\n",
      "Epoch [36/85], Iteration [255]\n",
      "Epoch [36/85], Iteration [340]\n",
      "Epoch [36/85], Iteration [425]\n",
      "Epoch [36/85], Iteration [510]\n",
      "Epoch [36/85], Iteration [595]\n",
      "Epoch [36/85], Iteration [680]\n",
      "Epoch [36/85], Iteration [765]\n",
      "(428s) Epoch [37/85], Train_PSNR:29.37, Val_PSNR:28.68, Val_SSIM:0.9223\n",
      "Epoch [37/85], Iteration [0]\n",
      "Epoch [37/85], Iteration [85]\n",
      "Epoch [37/85], Iteration [170]\n",
      "Epoch [37/85], Iteration [255]\n",
      "Epoch [37/85], Iteration [340]\n",
      "Epoch [37/85], Iteration [425]\n",
      "Epoch [37/85], Iteration [510]\n",
      "Epoch [37/85], Iteration [595]\n",
      "Epoch [37/85], Iteration [680]\n",
      "Epoch [37/85], Iteration [765]\n",
      "(433s) Epoch [38/85], Train_PSNR:29.25, Val_PSNR:28.65, Val_SSIM:0.9244\n",
      "Epoch [38/85], Iteration [0]\n",
      "Epoch [38/85], Iteration [85]\n",
      "Epoch [38/85], Iteration [170]\n",
      "Epoch [38/85], Iteration [255]\n",
      "Epoch [38/85], Iteration [340]\n",
      "Epoch [38/85], Iteration [425]\n",
      "Epoch [38/85], Iteration [510]\n",
      "Epoch [38/85], Iteration [595]\n",
      "Epoch [38/85], Iteration [680]\n",
      "Epoch [38/85], Iteration [765]\n",
      "(440s) Epoch [39/85], Train_PSNR:29.23, Val_PSNR:28.25, Val_SSIM:0.9111\n",
      "Epoch 39: Learning rate adjusted to 0.000025\n",
      "Epoch [39/85], Iteration [0]\n",
      "Epoch [39/85], Iteration [85]\n",
      "Epoch [39/85], Iteration [170]\n",
      "Epoch [39/85], Iteration [255]\n",
      "Epoch [39/85], Iteration [340]\n",
      "Epoch [39/85], Iteration [425]\n",
      "Epoch [39/85], Iteration [510]\n",
      "Epoch [39/85], Iteration [595]\n",
      "Epoch [39/85], Iteration [680]\n",
      "Epoch [39/85], Iteration [765]\n",
      "(446s) Epoch [40/85], Train_PSNR:29.37, Val_PSNR:28.87, Val_SSIM:0.9237\n",
      "Epoch [40/85], Iteration [0]\n",
      "Epoch [40/85], Iteration [85]\n",
      "Epoch [40/85], Iteration [170]\n",
      "Epoch [40/85], Iteration [255]\n",
      "Epoch [40/85], Iteration [340]\n",
      "Epoch [40/85], Iteration [425]\n",
      "Epoch [40/85], Iteration [510]\n",
      "Epoch [40/85], Iteration [595]\n",
      "Epoch [40/85], Iteration [680]\n",
      "Epoch [40/85], Iteration [765]\n",
      "Model saved in epoch 40.\n",
      "(456s) Epoch [41/85], Train_PSNR:29.35, Val_PSNR:28.66, Val_SSIM:0.9204\n",
      "Epoch [41/85], Iteration [0]\n",
      "Epoch [41/85], Iteration [85]\n",
      "Epoch [41/85], Iteration [170]\n",
      "Epoch [41/85], Iteration [255]\n",
      "Epoch [41/85], Iteration [340]\n",
      "Epoch [41/85], Iteration [425]\n",
      "Epoch [41/85], Iteration [510]\n",
      "Epoch [41/85], Iteration [595]\n",
      "Epoch [41/85], Iteration [680]\n",
      "Epoch [41/85], Iteration [765]\n",
      "(461s) Epoch [42/85], Train_PSNR:29.34, Val_PSNR:28.81, Val_SSIM:0.9236\n",
      "Epoch 42: Learning rate adjusted to 0.000023\n",
      "Epoch [42/85], Iteration [0]\n",
      "Epoch [42/85], Iteration [85]\n",
      "Epoch [42/85], Iteration [170]\n",
      "Epoch [42/85], Iteration [255]\n",
      "Epoch [42/85], Iteration [340]\n",
      "Epoch [42/85], Iteration [425]\n",
      "Epoch [42/85], Iteration [510]\n",
      "Epoch [42/85], Iteration [595]\n",
      "Epoch [42/85], Iteration [680]\n",
      "Epoch [42/85], Iteration [765]\n",
      "(466s) Epoch [43/85], Train_PSNR:29.46, Val_PSNR:28.86, Val_SSIM:0.9260\n",
      "Epoch [43/85], Iteration [0]\n",
      "Epoch [43/85], Iteration [85]\n",
      "Epoch [43/85], Iteration [170]\n",
      "Epoch [43/85], Iteration [255]\n",
      "Epoch [43/85], Iteration [340]\n",
      "Epoch [43/85], Iteration [425]\n",
      "Epoch [43/85], Iteration [510]\n",
      "Epoch [43/85], Iteration [595]\n",
      "Epoch [43/85], Iteration [680]\n",
      "Epoch [43/85], Iteration [765]\n",
      "(464s) Epoch [44/85], Train_PSNR:29.39, Val_PSNR:28.89, Val_SSIM:0.9225\n",
      "Epoch [44/85], Iteration [0]\n",
      "Epoch [44/85], Iteration [85]\n",
      "Epoch [44/85], Iteration [170]\n",
      "Epoch [44/85], Iteration [255]\n",
      "Epoch [44/85], Iteration [340]\n",
      "Epoch [44/85], Iteration [425]\n",
      "Epoch [44/85], Iteration [510]\n",
      "Epoch [44/85], Iteration [595]\n",
      "Epoch [44/85], Iteration [680]\n",
      "Epoch [44/85], Iteration [765]\n",
      "(463s) Epoch [45/85], Train_PSNR:29.46, Val_PSNR:28.78, Val_SSIM:0.9250\n",
      "Epoch 45: Learning rate adjusted to 0.000021\n",
      "Epoch [45/85], Iteration [0]\n",
      "Epoch [45/85], Iteration [85]\n",
      "Epoch [45/85], Iteration [170]\n",
      "Epoch [45/85], Iteration [255]\n",
      "Epoch [45/85], Iteration [340]\n",
      "Epoch [45/85], Iteration [425]\n",
      "Epoch [45/85], Iteration [510]\n",
      "Epoch [45/85], Iteration [595]\n",
      "Epoch [45/85], Iteration [680]\n",
      "Epoch [45/85], Iteration [765]\n",
      "Model saved in epoch 45.\n",
      "(470s) Epoch [46/85], Train_PSNR:29.52, Val_PSNR:28.58, Val_SSIM:0.9249\n",
      "Epoch [46/85], Iteration [0]\n",
      "Epoch [46/85], Iteration [85]\n",
      "Epoch [46/85], Iteration [170]\n",
      "Epoch [46/85], Iteration [255]\n",
      "Epoch [46/85], Iteration [340]\n",
      "Epoch [46/85], Iteration [425]\n",
      "Epoch [46/85], Iteration [510]\n",
      "Epoch [46/85], Iteration [595]\n",
      "Epoch [46/85], Iteration [680]\n",
      "Epoch [46/85], Iteration [765]\n",
      "(475s) Epoch [47/85], Train_PSNR:29.51, Val_PSNR:28.77, Val_SSIM:0.9229\n",
      "Epoch [47/85], Iteration [0]\n",
      "Epoch [47/85], Iteration [85]\n",
      "Epoch [47/85], Iteration [170]\n",
      "Epoch [47/85], Iteration [255]\n",
      "Epoch [47/85], Iteration [340]\n",
      "Epoch [47/85], Iteration [425]\n",
      "Epoch [47/85], Iteration [510]\n",
      "Epoch [47/85], Iteration [595]\n",
      "Epoch [47/85], Iteration [680]\n",
      "Epoch [47/85], Iteration [765]\n",
      "(483s) Epoch [48/85], Train_PSNR:29.56, Val_PSNR:29.15, Val_SSIM:0.9270\n",
      "Epoch 48: Learning rate adjusted to 0.000019\n",
      "Epoch [48/85], Iteration [0]\n",
      "Epoch [48/85], Iteration [85]\n",
      "Epoch [48/85], Iteration [170]\n",
      "Epoch [48/85], Iteration [255]\n",
      "Epoch [48/85], Iteration [340]\n",
      "Epoch [48/85], Iteration [425]\n",
      "Epoch [48/85], Iteration [510]\n",
      "Epoch [48/85], Iteration [595]\n",
      "Epoch [48/85], Iteration [680]\n",
      "Epoch [48/85], Iteration [765]\n",
      "(505s) Epoch [49/85], Train_PSNR:29.64, Val_PSNR:28.70, Val_SSIM:0.9274\n",
      "Epoch [49/85], Iteration [0]\n",
      "Epoch [49/85], Iteration [85]\n",
      "Epoch [49/85], Iteration [170]\n",
      "Epoch [49/85], Iteration [255]\n",
      "Epoch [49/85], Iteration [340]\n",
      "Epoch [49/85], Iteration [425]\n",
      "Epoch [49/85], Iteration [510]\n",
      "Epoch [49/85], Iteration [595]\n",
      "Epoch [49/85], Iteration [680]\n",
      "Epoch [49/85], Iteration [765]\n",
      "(510s) Epoch [50/85], Train_PSNR:29.59, Val_PSNR:29.08, Val_SSIM:0.9262\n",
      "Epoch [50/85], Iteration [0]\n",
      "Epoch [50/85], Iteration [85]\n",
      "Epoch [50/85], Iteration [170]\n",
      "Epoch [50/85], Iteration [255]\n",
      "Epoch [50/85], Iteration [340]\n",
      "Epoch [50/85], Iteration [425]\n",
      "Epoch [50/85], Iteration [510]\n",
      "Epoch [50/85], Iteration [595]\n",
      "Epoch [50/85], Iteration [680]\n",
      "Epoch [50/85], Iteration [765]\n",
      "Model saved in epoch 50.\n",
      "(518s) Epoch [51/85], Train_PSNR:29.62, Val_PSNR:29.03, Val_SSIM:0.9229\n",
      "Epoch 51: Learning rate adjusted to 0.000017\n",
      "Epoch [51/85], Iteration [0]\n",
      "Epoch [51/85], Iteration [85]\n",
      "Epoch [51/85], Iteration [170]\n",
      "Epoch [51/85], Iteration [255]\n",
      "Epoch [51/85], Iteration [340]\n",
      "Epoch [51/85], Iteration [425]\n",
      "Epoch [51/85], Iteration [510]\n",
      "Epoch [51/85], Iteration [595]\n",
      "Epoch [51/85], Iteration [680]\n",
      "Epoch [51/85], Iteration [765]\n",
      "(515s) Epoch [52/85], Train_PSNR:29.63, Val_PSNR:29.15, Val_SSIM:0.9271\n",
      "Epoch [52/85], Iteration [0]\n",
      "Epoch [52/85], Iteration [85]\n",
      "Epoch [52/85], Iteration [170]\n",
      "Epoch [52/85], Iteration [255]\n",
      "Epoch [52/85], Iteration [340]\n",
      "Epoch [52/85], Iteration [425]\n",
      "Epoch [52/85], Iteration [510]\n",
      "Epoch [52/85], Iteration [595]\n",
      "Epoch [52/85], Iteration [680]\n",
      "Epoch [52/85], Iteration [765]\n",
      "(523s) Epoch [53/85], Train_PSNR:29.66, Val_PSNR:28.92, Val_SSIM:0.9253\n",
      "Epoch [53/85], Iteration [0]\n",
      "Epoch [53/85], Iteration [85]\n",
      "Epoch [53/85], Iteration [170]\n",
      "Epoch [53/85], Iteration [255]\n",
      "Epoch [53/85], Iteration [340]\n",
      "Epoch [53/85], Iteration [425]\n",
      "Epoch [53/85], Iteration [510]\n",
      "Epoch [53/85], Iteration [595]\n",
      "Epoch [53/85], Iteration [680]\n",
      "Epoch [53/85], Iteration [765]\n",
      "(527s) Epoch [54/85], Train_PSNR:29.65, Val_PSNR:29.04, Val_SSIM:0.9244\n",
      "Epoch 54: Learning rate adjusted to 0.000015\n",
      "Epoch [54/85], Iteration [0]\n",
      "Epoch [54/85], Iteration [85]\n",
      "Epoch [54/85], Iteration [170]\n",
      "Epoch [54/85], Iteration [255]\n",
      "Epoch [54/85], Iteration [340]\n",
      "Epoch [54/85], Iteration [425]\n",
      "Epoch [54/85], Iteration [510]\n",
      "Epoch [54/85], Iteration [595]\n",
      "Epoch [54/85], Iteration [680]\n",
      "Epoch [54/85], Iteration [765]\n",
      "(530s) Epoch [55/85], Train_PSNR:29.83, Val_PSNR:28.97, Val_SSIM:0.9221\n",
      "Epoch [55/85], Iteration [0]\n",
      "Epoch [55/85], Iteration [85]\n",
      "Epoch [55/85], Iteration [170]\n",
      "Epoch [55/85], Iteration [255]\n",
      "Epoch [55/85], Iteration [340]\n",
      "Epoch [55/85], Iteration [425]\n",
      "Epoch [55/85], Iteration [510]\n",
      "Epoch [55/85], Iteration [595]\n",
      "Epoch [55/85], Iteration [680]\n",
      "Epoch [55/85], Iteration [765]\n",
      "Model saved in epoch 55.\n",
      "(532s) Epoch [56/85], Train_PSNR:29.75, Val_PSNR:29.06, Val_SSIM:0.9276\n",
      "Epoch [56/85], Iteration [0]\n",
      "Epoch [56/85], Iteration [85]\n",
      "Epoch [56/85], Iteration [170]\n",
      "Epoch [56/85], Iteration [255]\n",
      "Epoch [56/85], Iteration [340]\n",
      "Epoch [56/85], Iteration [425]\n",
      "Epoch [56/85], Iteration [510]\n",
      "Epoch [56/85], Iteration [595]\n",
      "Epoch [56/85], Iteration [680]\n",
      "Epoch [56/85], Iteration [765]\n",
      "(538s) Epoch [57/85], Train_PSNR:29.88, Val_PSNR:28.95, Val_SSIM:0.9265\n",
      "Epoch 57: Learning rate adjusted to 0.000014\n",
      "Epoch [57/85], Iteration [0]\n",
      "Epoch [57/85], Iteration [85]\n",
      "Epoch [57/85], Iteration [170]\n",
      "Epoch [57/85], Iteration [255]\n",
      "Epoch [57/85], Iteration [340]\n",
      "Epoch [57/85], Iteration [425]\n",
      "Epoch [57/85], Iteration [510]\n",
      "Epoch [57/85], Iteration [595]\n",
      "Epoch [57/85], Iteration [680]\n",
      "Epoch [57/85], Iteration [765]\n",
      "(543s) Epoch [58/85], Train_PSNR:29.86, Val_PSNR:28.68, Val_SSIM:0.9267\n",
      "Epoch [58/85], Iteration [0]\n",
      "Epoch [58/85], Iteration [85]\n",
      "Epoch [58/85], Iteration [170]\n",
      "Epoch [58/85], Iteration [255]\n",
      "Epoch [58/85], Iteration [340]\n",
      "Epoch [58/85], Iteration [425]\n",
      "Epoch [58/85], Iteration [510]\n",
      "Epoch [58/85], Iteration [595]\n",
      "Epoch [58/85], Iteration [680]\n",
      "Epoch [58/85], Iteration [765]\n",
      "(551s) Epoch [59/85], Train_PSNR:29.82, Val_PSNR:29.26, Val_SSIM:0.9269\n",
      "Epoch [59/85], Iteration [0]\n",
      "Epoch [59/85], Iteration [85]\n",
      "Epoch [59/85], Iteration [170]\n",
      "Epoch [59/85], Iteration [255]\n",
      "Epoch [59/85], Iteration [340]\n",
      "Epoch [59/85], Iteration [425]\n",
      "Epoch [59/85], Iteration [510]\n",
      "Epoch [59/85], Iteration [595]\n",
      "Epoch [59/85], Iteration [680]\n",
      "Epoch [59/85], Iteration [765]\n",
      "(558s) Epoch [60/85], Train_PSNR:29.77, Val_PSNR:29.05, Val_SSIM:0.9266\n",
      "Epoch 60: Learning rate adjusted to 0.000012\n",
      "Epoch [60/85], Iteration [0]\n",
      "Epoch [60/85], Iteration [85]\n",
      "Epoch [60/85], Iteration [170]\n",
      "Epoch [60/85], Iteration [255]\n",
      "Epoch [60/85], Iteration [340]\n",
      "Epoch [60/85], Iteration [425]\n",
      "Epoch [60/85], Iteration [510]\n",
      "Epoch [60/85], Iteration [595]\n",
      "Epoch [60/85], Iteration [680]\n",
      "Epoch [60/85], Iteration [765]\n",
      "Model saved in epoch 60.\n",
      "(571s) Epoch [61/85], Train_PSNR:29.87, Val_PSNR:28.95, Val_SSIM:0.9269\n",
      "Epoch [61/85], Iteration [0]\n",
      "Epoch [61/85], Iteration [85]\n",
      "Epoch [61/85], Iteration [170]\n",
      "Epoch [61/85], Iteration [255]\n",
      "Epoch [61/85], Iteration [340]\n",
      "Epoch [61/85], Iteration [425]\n",
      "Epoch [61/85], Iteration [510]\n",
      "Epoch [61/85], Iteration [595]\n",
      "Epoch [61/85], Iteration [680]\n",
      "Epoch [61/85], Iteration [765]\n",
      "(576s) Epoch [62/85], Train_PSNR:29.84, Val_PSNR:29.04, Val_SSIM:0.9255\n",
      "Epoch [62/85], Iteration [0]\n",
      "Epoch [62/85], Iteration [85]\n",
      "Epoch [62/85], Iteration [170]\n",
      "Epoch [62/85], Iteration [255]\n",
      "Epoch [62/85], Iteration [340]\n",
      "Epoch [62/85], Iteration [425]\n",
      "Epoch [62/85], Iteration [510]\n",
      "Epoch [62/85], Iteration [595]\n",
      "Epoch [62/85], Iteration [680]\n",
      "Epoch [62/85], Iteration [765]\n",
      "(585s) Epoch [63/85], Train_PSNR:29.82, Val_PSNR:29.05, Val_SSIM:0.9233\n",
      "Epoch 63: Learning rate adjusted to 0.000011\n",
      "Epoch [63/85], Iteration [0]\n",
      "Epoch [63/85], Iteration [85]\n",
      "Epoch [63/85], Iteration [170]\n",
      "Epoch [63/85], Iteration [255]\n",
      "Epoch [63/85], Iteration [340]\n",
      "Epoch [63/85], Iteration [425]\n",
      "Epoch [63/85], Iteration [510]\n",
      "Epoch [63/85], Iteration [595]\n",
      "Epoch [63/85], Iteration [680]\n",
      "Epoch [63/85], Iteration [765]\n",
      "(589s) Epoch [64/85], Train_PSNR:29.85, Val_PSNR:28.99, Val_SSIM:0.9248\n",
      "Epoch [64/85], Iteration [0]\n",
      "Epoch [64/85], Iteration [85]\n",
      "Epoch [64/85], Iteration [170]\n",
      "Epoch [64/85], Iteration [255]\n",
      "Epoch [64/85], Iteration [340]\n",
      "Epoch [64/85], Iteration [425]\n",
      "Epoch [64/85], Iteration [510]\n",
      "Epoch [64/85], Iteration [595]\n",
      "Epoch [64/85], Iteration [680]\n",
      "Epoch [64/85], Iteration [765]\n",
      "(592s) Epoch [65/85], Train_PSNR:29.94, Val_PSNR:29.13, Val_SSIM:0.9290\n",
      "Epoch [65/85], Iteration [0]\n",
      "Epoch [65/85], Iteration [85]\n",
      "Epoch [65/85], Iteration [170]\n",
      "Epoch [65/85], Iteration [255]\n",
      "Epoch [65/85], Iteration [340]\n",
      "Epoch [65/85], Iteration [425]\n",
      "Epoch [65/85], Iteration [510]\n",
      "Epoch [65/85], Iteration [595]\n",
      "Epoch [65/85], Iteration [680]\n",
      "Epoch [65/85], Iteration [765]\n",
      "Model saved in epoch 65.\n",
      "(605s) Epoch [66/85], Train_PSNR:29.97, Val_PSNR:29.16, Val_SSIM:0.9294\n",
      "Epoch 66: Learning rate adjusted to 0.000010\n",
      "Epoch [66/85], Iteration [0]\n",
      "Epoch [66/85], Iteration [85]\n",
      "Epoch [66/85], Iteration [170]\n",
      "Epoch [66/85], Iteration [255]\n",
      "Epoch [66/85], Iteration [340]\n",
      "Epoch [66/85], Iteration [425]\n",
      "Epoch [66/85], Iteration [510]\n",
      "Epoch [66/85], Iteration [595]\n",
      "Epoch [66/85], Iteration [680]\n",
      "Epoch [66/85], Iteration [765]\n",
      "(601s) Epoch [67/85], Train_PSNR:29.93, Val_PSNR:29.33, Val_SSIM:0.9279\n",
      "Epoch [67/85], Iteration [0]\n",
      "Epoch [67/85], Iteration [85]\n",
      "Epoch [67/85], Iteration [170]\n",
      "Epoch [67/85], Iteration [255]\n",
      "Epoch [67/85], Iteration [340]\n",
      "Epoch [67/85], Iteration [425]\n",
      "Epoch [67/85], Iteration [510]\n",
      "Epoch [67/85], Iteration [595]\n",
      "Epoch [67/85], Iteration [680]\n",
      "Epoch [67/85], Iteration [765]\n",
      "(606s) Epoch [68/85], Train_PSNR:29.96, Val_PSNR:29.06, Val_SSIM:0.9274\n",
      "Epoch [68/85], Iteration [0]\n",
      "Epoch [68/85], Iteration [85]\n",
      "Epoch [68/85], Iteration [170]\n",
      "Epoch [68/85], Iteration [255]\n",
      "Epoch [68/85], Iteration [340]\n",
      "Epoch [68/85], Iteration [425]\n",
      "Epoch [68/85], Iteration [510]\n",
      "Epoch [68/85], Iteration [595]\n",
      "Epoch [68/85], Iteration [680]\n",
      "Epoch [68/85], Iteration [765]\n",
      "(611s) Epoch [69/85], Train_PSNR:30.02, Val_PSNR:29.21, Val_SSIM:0.9274\n",
      "Epoch 69: Learning rate adjusted to 0.000009\n",
      "Epoch [69/85], Iteration [0]\n",
      "Epoch [69/85], Iteration [85]\n",
      "Epoch [69/85], Iteration [170]\n",
      "Epoch [69/85], Iteration [255]\n",
      "Epoch [69/85], Iteration [340]\n",
      "Epoch [69/85], Iteration [425]\n",
      "Epoch [69/85], Iteration [510]\n",
      "Epoch [69/85], Iteration [595]\n",
      "Epoch [69/85], Iteration [680]\n",
      "Epoch [69/85], Iteration [765]\n",
      "(618s) Epoch [70/85], Train_PSNR:29.97, Val_PSNR:29.14, Val_SSIM:0.9273\n",
      "Epoch [70/85], Iteration [0]\n",
      "Epoch [70/85], Iteration [85]\n",
      "Epoch [70/85], Iteration [170]\n",
      "Epoch [70/85], Iteration [255]\n",
      "Epoch [70/85], Iteration [340]\n",
      "Epoch [70/85], Iteration [425]\n",
      "Epoch [70/85], Iteration [510]\n",
      "Epoch [70/85], Iteration [595]\n",
      "Epoch [70/85], Iteration [680]\n",
      "Epoch [70/85], Iteration [765]\n",
      "Model saved in epoch 70.\n",
      "(628s) Epoch [71/85], Train_PSNR:29.94, Val_PSNR:29.22, Val_SSIM:0.9287\n",
      "Epoch [71/85], Iteration [0]\n",
      "Epoch [71/85], Iteration [85]\n",
      "Epoch [71/85], Iteration [170]\n",
      "Epoch [71/85], Iteration [255]\n",
      "Epoch [71/85], Iteration [340]\n",
      "Epoch [71/85], Iteration [425]\n",
      "Epoch [71/85], Iteration [510]\n",
      "Epoch [71/85], Iteration [595]\n",
      "Epoch [71/85], Iteration [680]\n",
      "Epoch [71/85], Iteration [765]\n",
      "(634s) Epoch [72/85], Train_PSNR:29.95, Val_PSNR:29.45, Val_SSIM:0.9305\n",
      "Epoch 72: Learning rate adjusted to 0.000008\n",
      "Epoch [72/85], Iteration [0]\n",
      "Epoch [72/85], Iteration [85]\n",
      "Epoch [72/85], Iteration [170]\n",
      "Epoch [72/85], Iteration [255]\n",
      "Epoch [72/85], Iteration [340]\n",
      "Epoch [72/85], Iteration [425]\n",
      "Epoch [72/85], Iteration [510]\n",
      "Epoch [72/85], Iteration [595]\n",
      "Epoch [72/85], Iteration [680]\n",
      "Epoch [72/85], Iteration [765]\n",
      "(638s) Epoch [73/85], Train_PSNR:30.00, Val_PSNR:29.20, Val_SSIM:0.9275\n",
      "Epoch [73/85], Iteration [0]\n",
      "Epoch [73/85], Iteration [85]\n",
      "Epoch [73/85], Iteration [170]\n",
      "Epoch [73/85], Iteration [255]\n",
      "Epoch [73/85], Iteration [340]\n",
      "Epoch [73/85], Iteration [425]\n",
      "Epoch [73/85], Iteration [510]\n",
      "Epoch [73/85], Iteration [595]\n",
      "Epoch [73/85], Iteration [680]\n",
      "Epoch [73/85], Iteration [765]\n",
      "(640s) Epoch [74/85], Train_PSNR:30.02, Val_PSNR:29.26, Val_SSIM:0.9292\n",
      "Epoch [74/85], Iteration [0]\n",
      "Epoch [74/85], Iteration [85]\n",
      "Epoch [74/85], Iteration [170]\n",
      "Epoch [74/85], Iteration [255]\n",
      "Epoch [74/85], Iteration [340]\n",
      "Epoch [74/85], Iteration [425]\n",
      "Epoch [74/85], Iteration [510]\n",
      "Epoch [74/85], Iteration [595]\n",
      "Epoch [74/85], Iteration [680]\n",
      "Epoch [74/85], Iteration [765]\n",
      "(648s) Epoch [75/85], Train_PSNR:30.09, Val_PSNR:29.38, Val_SSIM:0.9287\n",
      "Epoch 75: Learning rate adjusted to 0.000007\n",
      "Epoch [75/85], Iteration [0]\n",
      "Epoch [75/85], Iteration [85]\n",
      "Epoch [75/85], Iteration [170]\n",
      "Epoch [75/85], Iteration [255]\n",
      "Epoch [75/85], Iteration [340]\n",
      "Epoch [75/85], Iteration [425]\n",
      "Epoch [75/85], Iteration [510]\n",
      "Epoch [75/85], Iteration [595]\n",
      "Epoch [75/85], Iteration [680]\n",
      "Epoch [75/85], Iteration [765]\n",
      "Model saved in epoch 75.\n",
      "(652s) Epoch [76/85], Train_PSNR:30.09, Val_PSNR:29.38, Val_SSIM:0.9286\n",
      "Epoch [76/85], Iteration [0]\n",
      "Epoch [76/85], Iteration [85]\n",
      "Epoch [76/85], Iteration [170]\n",
      "Epoch [76/85], Iteration [255]\n",
      "Epoch [76/85], Iteration [340]\n",
      "Epoch [76/85], Iteration [425]\n",
      "Epoch [76/85], Iteration [510]\n",
      "Epoch [76/85], Iteration [595]\n",
      "Epoch [76/85], Iteration [680]\n",
      "Epoch [76/85], Iteration [765]\n",
      "(654s) Epoch [77/85], Train_PSNR:30.10, Val_PSNR:29.23, Val_SSIM:0.9281\n",
      "Epoch [77/85], Iteration [0]\n",
      "Epoch [77/85], Iteration [85]\n",
      "Epoch [77/85], Iteration [170]\n",
      "Epoch [77/85], Iteration [255]\n",
      "Epoch [77/85], Iteration [340]\n",
      "Epoch [77/85], Iteration [425]\n",
      "Epoch [77/85], Iteration [510]\n",
      "Epoch [77/85], Iteration [595]\n",
      "Epoch [77/85], Iteration [680]\n",
      "Epoch [77/85], Iteration [765]\n",
      "(658s) Epoch [78/85], Train_PSNR:30.21, Val_PSNR:29.33, Val_SSIM:0.9271\n",
      "Epoch 78: Learning rate adjusted to 0.000006\n",
      "Epoch [78/85], Iteration [0]\n",
      "Epoch [78/85], Iteration [85]\n",
      "Epoch [78/85], Iteration [170]\n",
      "Epoch [78/85], Iteration [255]\n",
      "Epoch [78/85], Iteration [340]\n",
      "Epoch [78/85], Iteration [425]\n",
      "Epoch [78/85], Iteration [510]\n",
      "Epoch [78/85], Iteration [595]\n",
      "Epoch [78/85], Iteration [680]\n",
      "Epoch [78/85], Iteration [765]\n",
      "(665s) Epoch [79/85], Train_PSNR:30.08, Val_PSNR:29.24, Val_SSIM:0.9292\n",
      "Epoch [79/85], Iteration [0]\n",
      "Epoch [79/85], Iteration [85]\n",
      "Epoch [79/85], Iteration [170]\n",
      "Epoch [79/85], Iteration [255]\n",
      "Epoch [79/85], Iteration [340]\n",
      "Epoch [79/85], Iteration [425]\n",
      "Epoch [79/85], Iteration [510]\n",
      "Epoch [79/85], Iteration [595]\n",
      "Epoch [79/85], Iteration [680]\n",
      "Epoch [79/85], Iteration [765]\n",
      "(668s) Epoch [80/85], Train_PSNR:30.18, Val_PSNR:29.20, Val_SSIM:0.9292\n",
      "Epoch [80/85], Iteration [0]\n",
      "Epoch [80/85], Iteration [85]\n",
      "Epoch [80/85], Iteration [170]\n",
      "Epoch [80/85], Iteration [255]\n",
      "Epoch [80/85], Iteration [340]\n",
      "Epoch [80/85], Iteration [425]\n",
      "Epoch [80/85], Iteration [510]\n",
      "Epoch [80/85], Iteration [595]\n",
      "Epoch [80/85], Iteration [680]\n",
      "Epoch [80/85], Iteration [765]\n",
      "Model saved in epoch 80.\n",
      "(676s) Epoch [81/85], Train_PSNR:30.10, Val_PSNR:29.20, Val_SSIM:0.9274\n",
      "Epoch 81: Learning rate adjusted to 0.000006\n",
      "Epoch [81/85], Iteration [0]\n",
      "Epoch [81/85], Iteration [85]\n",
      "Epoch [81/85], Iteration [170]\n",
      "Epoch [81/85], Iteration [255]\n",
      "Epoch [81/85], Iteration [340]\n",
      "Epoch [81/85], Iteration [425]\n",
      "Epoch [81/85], Iteration [510]\n",
      "Epoch [81/85], Iteration [595]\n",
      "Epoch [81/85], Iteration [680]\n",
      "Epoch [81/85], Iteration [765]\n",
      "(684s) Epoch [82/85], Train_PSNR:30.19, Val_PSNR:29.50, Val_SSIM:0.9280\n",
      "Epoch [82/85], Iteration [0]\n",
      "Epoch [82/85], Iteration [85]\n",
      "Epoch [82/85], Iteration [170]\n",
      "Epoch [82/85], Iteration [255]\n",
      "Epoch [82/85], Iteration [340]\n",
      "Epoch [82/85], Iteration [425]\n",
      "Epoch [82/85], Iteration [510]\n",
      "Epoch [82/85], Iteration [595]\n",
      "Epoch [82/85], Iteration [680]\n",
      "Epoch [82/85], Iteration [765]\n",
      "(686s) Epoch [83/85], Train_PSNR:30.20, Val_PSNR:29.31, Val_SSIM:0.9298\n",
      "Epoch [83/85], Iteration [0]\n",
      "Epoch [83/85], Iteration [85]\n",
      "Epoch [83/85], Iteration [170]\n",
      "Epoch [83/85], Iteration [255]\n",
      "Epoch [83/85], Iteration [340]\n",
      "Epoch [83/85], Iteration [425]\n",
      "Epoch [83/85], Iteration [510]\n",
      "Epoch [83/85], Iteration [595]\n",
      "Epoch [83/85], Iteration [680]\n",
      "Epoch [83/85], Iteration [765]\n",
      "(690s) Epoch [84/85], Train_PSNR:30.16, Val_PSNR:29.22, Val_SSIM:0.9295\n",
      "Epoch 84: Learning rate adjusted to 0.000005\n",
      "Epoch [84/85], Iteration [0]\n",
      "Epoch [84/85], Iteration [85]\n",
      "Epoch [84/85], Iteration [170]\n",
      "Epoch [84/85], Iteration [255]\n",
      "Epoch [84/85], Iteration [340]\n",
      "Epoch [84/85], Iteration [425]\n",
      "Epoch [84/85], Iteration [510]\n",
      "Epoch [84/85], Iteration [595]\n",
      "Epoch [84/85], Iteration [680]\n",
      "Epoch [84/85], Iteration [765]\n",
      "(697s) Epoch [85/85], Train_PSNR:30.17, Val_PSNR:29.60, Val_SSIM:0.9297\n"
     ]
    }
   ],
   "source": [
    "# --- Initial Validation --- #\n",
    "old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)\n",
    "print(f\"Initial Validation -> PSNR: {old_val_psnr:.2f}, SSIM: {old_val_ssim:.4f}\")\n",
    "\n",
    "# --- Training Loop --- #\n",
    "best_psnr = old_val_psnr  # Track best validation PSNR\n",
    "train_psnr_prev = 0  # Track previous training PSNR\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    psnr_list = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Adjust Learning Rate --- #\n",
    "    adjust_learning_rate(optimizer, epoch, category=category)\n",
    "\n",
    "    # --- Training --- #\n",
    "    net.train()\n",
    "    for batch_id, (haze, gt) in enumerate(train_data_loader):\n",
    "        haze, gt = haze.to(device), gt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        dehaze, base = net(haze)\n",
    "        \n",
    "        # Compute Losses\n",
    "        base_loss = F.smooth_l1_loss(base, gt)\n",
    "        smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "        perceptual_loss = loss_network(dehaze, gt)\n",
    "        total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "\n",
    "        # Backward Pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute PSNR for Monitoring\n",
    "        psnr_list.extend(to_psnr(dehaze, gt))\n",
    "\n",
    "        # --- Save Model Every 5 Iterations --- #\n",
    "\n",
    "        if batch_id % num_epochs == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Iteration [{batch_id}]\")\n",
    "    if epoch % 5 == 0:\n",
    "        iter_model_path = f\"{model_name}{category}_haze_iter_{epoch}.pth\"\n",
    "        torch.save(net.state_dict(), iter_model_path)\n",
    "        print(f\"Model saved in epoch {epoch}.\")\n",
    "\n",
    "    # --- Compute Training PSNR --- #\n",
    "    train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "    # # --- Save Model Checkpoint at End of Epoch --- #\n",
    "    model_path = f\"{model_name}{category}_haze_{version}.pth\"\n",
    "    # torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    # --- Validation --- #\n",
    "    net.eval()\n",
    "    val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "\n",
    "    # --- Log Progress --- #\n",
    "    epoch_duration = time.time() - start_time\n",
    "    print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "    # --- Adjust Learning Rate if Training PSNR Drops --- #\n",
    "    if train_psnr < train_psnr_prev:\n",
    "        adjust_learning_rate(optimizer, num_epochs, category=category)\n",
    "\n",
    "    # --- Save Best Model (Based on Validation PSNR) --- #\n",
    "    if val_psnr >= best_psnr:\n",
    "        best_model_path = f\"{model_name}{category}_haze_best_{version}.pth\"\n",
    "        torch.save(net.state_dict(), best_model_path)\n",
    "        best_psnr = val_psnr\n",
    "\n",
    "    # --- Update Previous Training PSNR --- #\n",
    "    train_psnr_prev = train_psnr\n",
    "model_path = f\"{model_name}{category}_final_{epoch}.pth\"\n",
    "torch.save(net.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 937211,
     "sourceId": 1587463,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6456606,
     "sourceId": 10417877,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6464114,
     "sourceId": 10443410,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 222875724,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 268224,
     "modelInstanceId": 246650,
     "sourceId": 287852,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 288973,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 269124,
     "modelInstanceId": 247592,
     "sourceId": 297672,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38701.81454,
   "end_time": "2025-03-23T21:17:28.996950",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-23T10:32:27.182410",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03fbf906343444db84bda052e8662497": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0a0395a9c3a34b1da5ddf389dec8b0fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0a6a497f83c645329fdf3213fb70fefd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1968072fac26468e8f099a0c105825b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1b918c626ab144abb981285e61f61fea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1ff309398841442f8e57d6e11428ee21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Train Batch Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_1968072fac26468e8f099a0c105825b1",
       "step": 1,
       "style": "IPY_MODEL_be2d11c2abcc47238e6fe9b5e752d7d5",
       "tabbable": null,
       "tooltip": null,
       "value": 6
      }
     },
     "25bc23f6b8cd4741ae37a0ed67800c78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DropdownModel",
       "_options_labels": [
        "indoor",
        "outdoor",
        "reside",
        "nh"
       ],
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "DropdownView",
       "description": "Category:",
       "description_allow_html": false,
       "disabled": false,
       "index": 2,
       "layout": "IPY_MODEL_03fbf906343444db84bda052e8662497",
       "style": "IPY_MODEL_1b918c626ab144abb981285e61f61fea",
       "tabbable": null,
       "tooltip": null
      }
     },
     "34e597f104c741b99f7f20eb7a58d7ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "37614b7ee5de4d8e9628ec203dd7f10a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "TextView",
       "continuous_update": true,
       "description": "Crop Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_0a0395a9c3a34b1da5ddf389dec8b0fb",
       "placeholder": "​",
       "style": "IPY_MODEL_0a6a497f83c645329fdf3213fb70fefd",
       "tabbable": null,
       "tooltip": null,
       "value": "128,128"
      }
     },
     "4a2d3506825c4aa0b3bc1a6e389e3783": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4a3940d12ac4434f8924b2171916eb4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "51668ffb38d74d32a2936bcbca1a3e36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "FloatTextView",
       "continuous_update": false,
       "description": "Learning Rate:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_ac901ad20b3c422dbc8c912df3a70ddf",
       "step": null,
       "style": "IPY_MODEL_4a3940d12ac4434f8924b2171916eb4a",
       "tabbable": null,
       "tooltip": null,
       "value": 0.0001
      }
     },
     "5bd1a3564f2e44d4844a8f7786615a24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5e6f6e9daaa74e3b98fc814033006037": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "629773cfd6bd4a3fbdd821946baedda9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "FloatTextView",
       "continuous_update": false,
       "description": "Lambda Loss:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_7e9048ff71e74868952d3dc341318117",
       "step": null,
       "style": "IPY_MODEL_4a2d3506825c4aa0b3bc1a6e389e3783",
       "tabbable": null,
       "tooltip": null,
       "value": 0.04
      }
     },
     "62cab16c589f460aa55c7b7f04030b57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7e9048ff71e74868952d3dc341318117": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "87df09c5363d4efa9b3901bb24fade93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9bad20d09feb44269f057d9a8c61605c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Version:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_87df09c5363d4efa9b3901bb24fade93",
       "step": 1,
       "style": "IPY_MODEL_62cab16c589f460aa55c7b7f04030b57",
       "tabbable": null,
       "tooltip": null,
       "value": 0
      }
     },
     "a6e224a7ca3e4666bfeb2665e9f72326": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a7ba556872d54c73af4238c37bf425ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ac901ad20b3c422dbc8c912df3a70ddf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be2d11c2abcc47238e6fe9b5e752d7d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c201648397b4493cbf48336a44bb293e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DropdownModel",
       "_options_labels": [
        "local",
        "kaggle"
       ],
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "DropdownView",
       "description": "Execution Env:",
       "description_allow_html": false,
       "disabled": false,
       "index": 1,
       "layout": "IPY_MODEL_a7ba556872d54c73af4238c37bf425ea",
       "style": "IPY_MODEL_5e6f6e9daaa74e3b98fc814033006037",
       "tabbable": null,
       "tooltip": null
      }
     },
     "cac5e63c2cb94eafaec67327fcd845bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Growth Rate:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_e6f22c1d92054aa3a18c8ae44262c12b",
       "step": 1,
       "style": "IPY_MODEL_5bd1a3564f2e44d4844a8f7786615a24",
       "tabbable": null,
       "tooltip": null,
       "value": 16
      }
     },
     "e6f22c1d92054aa3a18c8ae44262c12b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8d667b6e85f4f20924a59c5abf71581": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Val Batch Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_34e597f104c741b99f7f20eb7a58d7ca",
       "step": 1,
       "style": "IPY_MODEL_a6e224a7ca3e4666bfeb2665e9f72326",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
