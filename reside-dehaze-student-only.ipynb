{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e58076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:28.679732Z",
     "iopub.status.busy": "2025-05-02T08:43:28.679471Z",
     "iopub.status.idle": "2025-05-02T08:43:28.847793Z",
     "shell.execute_reply": "2025-05-02T08:43:28.846815Z"
    },
    "papermill": {
     "duration": 0.189125,
     "end_time": "2025-05-02T08:43:28.849299",
     "exception": false,
     "start_time": "2025-05-02T08:43:28.660174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  2 08:43:28 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   38C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from IPython import get_ipython\n",
    "\n",
    "if shutil.which(\"nvidia-smi\") is not None:\n",
    "    get_ipython().system(\"nvidia-smi\")\n",
    "else:\n",
    "    print(\"No NVIDIA GPU or driver detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48f99d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:28.883477Z",
     "iopub.status.busy": "2025-05-02T08:43:28.883188Z",
     "iopub.status.idle": "2025-05-02T08:43:32.899162Z",
     "shell.execute_reply": "2025-05-02T08:43:32.898323Z"
    },
    "papermill": {
     "duration": 4.034397,
     "end_time": "2025-05-02T08:43:32.900462",
     "exception": false,
     "start_time": "2025-05-02T08:43:28.866065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53667a13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:32.934367Z",
     "iopub.status.busy": "2025-05-02T08:43:32.933745Z",
     "iopub.status.idle": "2025-05-02T08:43:32.956324Z",
     "shell.execute_reply": "2025-05-02T08:43:32.955453Z"
    },
    "papermill": {
     "duration": 0.040414,
     "end_time": "2025-05-02T08:43:32.957515",
     "exception": false,
     "start_time": "2025-05-02T08:43:32.917101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Device IDs: [0]\n"
     ]
    }
   ],
   "source": [
    "# --- Device Setup --- #\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_ids = list(range(torch.cuda.device_count()))\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Device IDs: {device_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3840fc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:32.992284Z",
     "iopub.status.busy": "2025-05-02T08:43:32.991644Z",
     "iopub.status.idle": "2025-05-02T08:43:33.001917Z",
     "shell.execute_reply": "2025-05-02T08:43:33.001207Z"
    },
    "papermill": {
     "duration": 0.028421,
     "end_time": "2025-05-02T08:43:33.003053",
     "exception": false,
     "start_time": "2025-05-02T08:43:32.974632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "Current GPU: 0\n",
      "GPU Name: Tesla P100-PCIE-16GB\n",
      "GPU Memory Allocated: 0 bytes\n",
      "GPU Memory Cached: 0 bytes\n",
      "GPU Memory Allocated (Total): 0 bytes\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(device)}\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "    print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(device)} bytes\")\n",
    "    print(f\"GPU Memory Allocated (Total): {torch.cuda.memory_allocated()} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa61acf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:33.036762Z",
     "iopub.status.busy": "2025-05-02T08:43:33.036488Z",
     "iopub.status.idle": "2025-05-02T08:43:42.371399Z",
     "shell.execute_reply": "2025-05-02T08:43:42.370771Z"
    },
    "papermill": {
     "duration": 9.353111,
     "end_time": "2025-05-02T08:43:42.372710",
     "exception": false,
     "start_time": "2025-05-02T08:43:33.019599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "from timm.layers import to_2tuple, trunc_normal_\n",
    "import os\n",
    "import torchvision.utils as utils\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "from torchvision.models import vgg16\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from random import randrange\n",
    "import time\n",
    "from math import log10\n",
    "from skimage import measure\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.nn.init import trunc_normal_\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c061669",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:42.405881Z",
     "iopub.status.busy": "2025-05-02T08:43:42.405516Z",
     "iopub.status.idle": "2025-05-02T08:43:42.409175Z",
     "shell.execute_reply": "2025-05-02T08:43:42.408471Z"
    },
    "papermill": {
     "duration": 0.021116,
     "end_time": "2025-05-02T08:43:42.410241",
     "exception": false,
     "start_time": "2025-05-02T08:43:42.389125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00723e",
   "metadata": {
    "papermill": {
     "duration": 0.015498,
     "end_time": "2025-05-02T08:43:42.441453",
     "exception": false,
     "start_time": "2025-05-02T08:43:42.425955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4622817c",
   "metadata": {
    "papermill": {
     "duration": 0.016309,
     "end_time": "2025-05-02T08:43:42.529266",
     "exception": false,
     "start_time": "2025-05-02T08:43:42.512957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RevisedLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a3a1e44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:42.563150Z",
     "iopub.status.busy": "2025-05-02T08:43:42.562456Z",
     "iopub.status.idle": "2025-05-02T08:43:42.570909Z",
     "shell.execute_reply": "2025-05-02T08:43:42.570082Z"
    },
    "papermill": {
     "duration": 0.026735,
     "end_time": "2025-05-02T08:43:42.572163",
     "exception": false,
     "start_time": "2025-05-02T08:43:42.545428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RevisedLayerNorm(nn.Module):\n",
    "    \"\"\"Revised LayerNorm\"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n",
    "        super(RevisedLayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.detach_gradient = detach_gradient\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n",
    "        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n",
    "\n",
    "        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "\n",
    "        trunc_normal_(self.scale_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.scale_mlp.bias, 1)\n",
    "\n",
    "        trunc_normal_(self.shift_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.shift_mlp.bias, 0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n",
    "        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n",
    "\n",
    "        normalized_tensor = (input_tensor - mean_value) / std_value\n",
    "\n",
    "        if self.detach_gradient:\n",
    "            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n",
    "        else:\n",
    "            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n",
    "\n",
    "        output = normalized_tensor * self.scale + self.shift\n",
    "        return output, rescale, rebias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d222353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:42.605787Z",
     "iopub.status.busy": "2025-05-02T08:43:42.605252Z",
     "iopub.status.idle": "2025-05-02T08:43:42.613370Z",
     "shell.execute_reply": "2025-05-02T08:43:42.612821Z"
    },
    "papermill": {
     "duration": 0.026092,
     "end_time": "2025-05-02T08:43:42.614445",
     "exception": false,
     "start_time": "2025-05-02T08:43:42.588353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n",
    "        super().__init__()\n",
    "        output_channels = output_channels or input_channels\n",
    "        hidden_channels = hidden_channels or input_channels\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            gain = (8 * self.depth) ** (-1 / 4)\n",
    "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "            trunc_normal_(layer.weight, std=std)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "\n",
    "def partition_into_windows(tensor, window_size):\n",
    "    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n",
    "    batch_size, height, width, channels = tensor.shape\n",
    "    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n",
    "\n",
    "    tensor = tensor.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n",
    "    )\n",
    "    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, height, width):\n",
    "    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n",
    "    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n",
    "    tensor = windows.view(\n",
    "        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ff0b0",
   "metadata": {
    "papermill": {
     "duration": 0.01581,
     "end_time": "2025-05-02T08:43:42.646537",
     "exception": false,
     "start_time": "2025-05-02T08:43:42.630727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ff47cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:42.679896Z",
     "iopub.status.busy": "2025-05-02T08:43:42.679344Z",
     "iopub.status.idle": "2025-05-02T08:43:42.813717Z",
     "shell.execute_reply": "2025-05-02T08:43:42.812988Z"
    },
    "papermill": {
     "duration": 0.15254,
     "end_time": "2025-05-02T08:43:42.814996",
     "exception": false,
     "start_time": "2025-05-02T08:43:42.662456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 16, 16]),\n",
       " torch.Size([32, 16, 64]),\n",
       " torch.Size([2, 16, 16, 64]),\n",
       " True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize the MultiLayerPerceptron with sample parameters\n",
    "depth = 4\n",
    "input_channels = 64\n",
    "hidden_channels = 128\n",
    "output_channels = 64\n",
    "\n",
    "mlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n",
    "\n",
    "# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\n",
    "input_tensor = torch.randn(2, 64, 16, 16)\n",
    "output_tensor = mlp(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "mlp_output_shape = output_tensor.shape\n",
    "\n",
    "# Test window partition and merging\n",
    "batch_size, height, width, channels = 2, 16, 16, 64\n",
    "window_size = 4\n",
    "\n",
    "# Create a random tensor for window functions (B, H, W, C) format\n",
    "input_window_tensor = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Apply partitioning and merging\n",
    "windows = partition_into_windows(input_window_tensor, window_size)\n",
    "reconstructed_tensor = merge_windows(windows, window_size, height, width)\n",
    "\n",
    "# Check shapes\n",
    "windows_shape = windows.shape\n",
    "reconstructed_shape = reconstructed_tensor.shape\n",
    "\n",
    "# Validate if the reconstruction matches the original input shape\n",
    "is_shape_correct = reconstructed_shape == input_window_tensor.shape\n",
    "\n",
    "# Output results\n",
    "mlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab92a949",
   "metadata": {
    "papermill": {
     "duration": 0.016061,
     "end_time": "2025-05-02T08:43:42.848276",
     "exception": false,
     "start_time": "2025-05-02T08:43:42.832215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### LocalWindowAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f29c6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:42.882035Z",
     "iopub.status.busy": "2025-05-02T08:43:42.881504Z",
     "iopub.status.idle": "2025-05-02T08:43:42.888666Z",
     "shell.execute_reply": "2025-05-02T08:43:42.887962Z"
    },
    "papermill": {
     "duration": 0.02519,
     "end_time": "2025-05-02T08:43:42.889741",
     "exception": false,
     "start_time": "2025-05-02T08:43:42.864551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, window_size, num_heads):\n",
    "        \"\"\"Self-attention mechanism within local windows.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # (height, width)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n",
    "\n",
    "        # Compute and store relative positional encodings\n",
    "        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n",
    "        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n",
    "\n",
    "        # Learnable transformation of relative position embeddings\n",
    "        self.relative_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_heads, bias=True)\n",
    "        )\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n",
    "        batch_size, num_tokens, _ = qkv.shape\n",
    "\n",
    "        # Reshape qkv into separate query, key, and value tensors\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n",
    "\n",
    "        # Scale query for stable attention computation\n",
    "        q = q * self.scaling_factor\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Compute relative position bias\n",
    "        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n",
    "        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n",
    "        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n",
    "\n",
    "        # Apply softmax and compute weighted values\n",
    "        attention_weights = self.attention_softmax(attention_scores)\n",
    "        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96e44d",
   "metadata": {
    "papermill": {
     "duration": 0.015916,
     "end_time": "2025-05-02T08:43:42.922040",
     "exception": false,
     "start_time": "2025-05-02T08:43:42.906124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### compute_log_relative_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1d05351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:42.955814Z",
     "iopub.status.busy": "2025-05-02T08:43:42.955186Z",
     "iopub.status.idle": "2025-05-02T08:43:42.960122Z",
     "shell.execute_reply": "2025-05-02T08:43:42.959417Z"
    },
    "papermill": {
     "duration": 0.023115,
     "end_time": "2025-05-02T08:43:42.961267",
     "exception": false,
     "start_time": "2025-05-02T08:43:42.938152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_log_relative_positions(window_size):\n",
    "    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n",
    "    coord_range = torch.arange(window_size)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n",
    "\n",
    "    # Compute relative positions\n",
    "    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n",
    "\n",
    "    # Format and apply log transformation\n",
    "    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n",
    "    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "    return log_relative_positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61beb6b0",
   "metadata": {
    "papermill": {
     "duration": 0.016101,
     "end_time": "2025-05-02T08:43:42.994091",
     "exception": false,
     "start_time": "2025-05-02T08:43:42.977990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### AdaptiveAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b72cf4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.027913Z",
     "iopub.status.busy": "2025-05-02T08:43:43.027662Z",
     "iopub.status.idle": "2025-05-02T08:43:43.040722Z",
     "shell.execute_reply": "2025-05-02T08:43:43.040143Z"
    },
    "papermill": {
     "duration": 0.031473,
     "end_time": "2025-05-02T08:43:43.041799",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.010326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n",
    "        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.network_depth = network_depth\n",
    "        self.enable_attention = enable_attention\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "        # Define convolutional processing based on mode\n",
    "        if self.conv_mode == 'Conv':\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "            )\n",
    "\n",
    "        if self.conv_mode == 'DWConv':\n",
    "            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n",
    "            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight_shape = module.weight.shape\n",
    "\n",
    "            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "            else:\n",
    "                gain = (8 * self.network_depth) ** (-1/4)\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def pad_for_window_processing(self, x, shift=False):\n",
    "        \"\"\"Pads the input tensor to fit window processing requirements with left and right shifts.\"\"\"\n",
    "        _, _, height, width = x.size()\n",
    "        pad_h = (self.window_size - height % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - width % self.window_size) % self.window_size\n",
    "\n",
    "        if shift:\n",
    "            # Apply left or right shift instead of cyclic shift\n",
    "            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n",
    "                          0, (self.window_size - pad_h) % self.window_size), mode='reflect')\n",
    "        else:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            v_proj = self.value_projection(x)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            qk_proj = self.query_key_projection(x)\n",
    "            qkv = torch.cat([qk_proj, v_proj], dim=1)\n",
    "\n",
    "            # Apply padding for shifted window processing\n",
    "            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n",
    "            padded_height, padded_width = padded_qkv.shape[2:]\n",
    "\n",
    "            # Partition into windows\n",
    "            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n",
    "            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n",
    "\n",
    "            # Apply window-based attention\n",
    "            attn_windows = self.window_attention(qkv_windows)\n",
    "\n",
    "            # Merge back to original spatial dimensions\n",
    "            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n",
    "\n",
    "            # Apply left or right shift (avoid cyclic)\n",
    "            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), 0:(self.shift_size + width), :]\n",
    "            attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.conv_mode in ['Conv', 'DWConv']:\n",
    "                conv_output = self.conv_layer(v_proj)\n",
    "                print(f\"conv_output shape: {conv_output.shape}\")\n",
    "                print(f\"attn_output shape: {attn_output.shape}\")\n",
    "                # print(f\"conv_output + attn_output shape: {conv_output + attn_output.shape}\")\n",
    "                print(f\"self.output_projection: {self.output_projection}\")\n",
    "                output = self.output_projection(conv_output + attn_output)\n",
    "            else:\n",
    "                output = self.output_projection(attn_output)\n",
    "\n",
    "        else:\n",
    "            if self.conv_mode == 'Conv':\n",
    "                output = self.conv_layer(x)  # No attention, using convolution only\n",
    "            elif self.conv_mode == 'DWConv':\n",
    "                output = self.output_projection(self.conv_layer(v_proj))\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a6bda",
   "metadata": {
    "papermill": {
     "duration": 0.016461,
     "end_time": "2025-05-02T08:43:43.074797",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.058336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### VisionTransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83214370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.109285Z",
     "iopub.status.busy": "2025-05-02T08:43:43.108606Z",
     "iopub.status.idle": "2025-05-02T08:43:43.115189Z",
     "shell.execute_reply": "2025-05-02T08:43:43.114571Z"
    },
    "papermill": {
     "duration": 0.025088,
     "end_time": "2025-05-02T08:43:43.116320",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.091232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n",
    "                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n",
    "        \"\"\"\n",
    "        A transformer block that includes attention (optional) and MLP layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enable_attention = enable_attention\n",
    "        self.enable_mlp_norm = enable_mlp_norm\n",
    "\n",
    "        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n",
    "        self.attention_layer = AdaptiveAttention(\n",
    "            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n",
    "            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n",
    "        )\n",
    "\n",
    "        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n",
    "        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.enable_attention:\n",
    "            x, rescale, rebias = self.pre_norm(x)\n",
    "        x = self.attention_layer(x)\n",
    "        if self.enable_attention:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        residual = x\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x, rescale, rebias = self.post_norm(x)\n",
    "        x = self.mlp_layer(x)\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ae0eb",
   "metadata": {
    "papermill": {
     "duration": 0.016164,
     "end_time": "2025-05-02T08:43:43.149084",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.132920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PatchEmbedding and PatchReconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00effa84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.182506Z",
     "iopub.status.busy": "2025-05-02T08:43:43.182240Z",
     "iopub.status.idle": "2025-05-02T08:43:43.188649Z",
     "shell.execute_reply": "2025-05-02T08:43:43.187958Z"
    },
    "papermill": {
     "duration": 0.024573,
     "end_time": "2025-05-02T08:43:43.189767",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.165194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch embedding module that projects input images into token embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to generate patch embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class PatchReconstruction(nn.Module):\n",
    "    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch reconstruction module that converts token embeddings back to image patches.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2, padding_mode='reflect'\n",
    "            ),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to reconstruct image from embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ffe3b",
   "metadata": {
    "papermill": {
     "duration": 0.015867,
     "end_time": "2025-05-02T08:43:43.222011",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.206144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SelectiveKernelFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "993ad094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.255475Z",
     "iopub.status.busy": "2025-05-02T08:43:43.254815Z",
     "iopub.status.idle": "2025-05-02T08:43:43.261273Z",
     "shell.execute_reply": "2025-05-02T08:43:43.260715Z"
    },
    "papermill": {
     "duration": 0.02424,
     "end_time": "2025-05-02T08:43:43.262364",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.238124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelectiveKernelFusion(nn.Module):\n",
    "    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n",
    "        \"\"\"\n",
    "        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n",
    "\n",
    "        Args:\n",
    "            channels (int): Number of input channels.\n",
    "            num_branches (int): Number of feature branches to fuse.\n",
    "            reduction_ratio (int): Reduction ratio for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_branches = num_branches\n",
    "        reduced_channels = max(int(channels / reduction_ratio), 4)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Forward pass for selective kernel fusion.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (list of tensors): A list of feature maps to be fused.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adaptively fused feature map.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = feature_maps[0].shape\n",
    "        \n",
    "        # Concatenate feature maps along a new dimension (num_branches)\n",
    "        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        aggregated_features = torch.sum(stacked_features, dim=1)\n",
    "        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n",
    "        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n",
    "\n",
    "        # Weighted sum of input feature maps\n",
    "        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e62343",
   "metadata": {
    "papermill": {
     "duration": 0.015643,
     "end_time": "2025-05-02T08:43:43.295307",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.279664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TransformerStage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acae1abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.328722Z",
     "iopub.status.busy": "2025-05-02T08:43:43.328190Z",
     "iopub.status.idle": "2025-05-02T08:43:43.334450Z",
     "shell.execute_reply": "2025-05-02T08:43:43.333994Z"
    },
    "papermill": {
     "duration": 0.023554,
     "end_time": "2025-05-02T08:43:43.335421",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.311867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, window_size=8,\n",
    "                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n",
    "        \"\"\"\n",
    "        A stage of transformer blocks with configurable attention placement.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        attention_layers = int(attention_ratio * num_layers)\n",
    "\n",
    "        if attention_placement == 'last':\n",
    "            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'first':\n",
    "            enable_attentions = [i < attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'middle':\n",
    "            enable_attentions = [\n",
    "                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(\n",
    "                network_depth=network_depth,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                enable_attention=enable_attentions[i],\n",
    "                conv_mode=conv_mode\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer stage.\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb88d63",
   "metadata": {
    "papermill": {
     "duration": 0.01575,
     "end_time": "2025-05-02T08:43:43.367148",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.351398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DehazingTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3207615",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.399838Z",
     "iopub.status.busy": "2025-05-02T08:43:43.399595Z",
     "iopub.status.idle": "2025-05-02T08:43:43.413801Z",
     "shell.execute_reply": "2025-05-02T08:43:43.413299Z"
    },
    "papermill": {
     "duration": 0.031795,
     "end_time": "2025-05-02T08:43:43.414842",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.383047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DehazingTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "                 layer_depths=[16, 16, 16, 8, 8],\n",
    "                 num_heads=[2, 4, 6, 1, 1],\n",
    "                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding settings\n",
    "        self.patch_size = 4\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # Backbone layers\n",
    "        self.encoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[0],\n",
    "            num_layers=layer_depths[0],\n",
    "            num_heads=num_heads[0],\n",
    "            mlp_ratio=mlp_ratios[0],\n",
    "            norm_layer=norm_layers[0],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[0],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[0]\n",
    "        )\n",
    "        \n",
    "        self.downsample1 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "        \n",
    "        self.encoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[1],\n",
    "            num_layers=layer_depths[1],\n",
    "            num_heads=num_heads[1],\n",
    "            mlp_ratio=mlp_ratios[1],\n",
    "            norm_layer=norm_layers[1],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[1],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[1]\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "        \n",
    "        self.encoder_stage3 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[2],\n",
    "            num_layers=layer_depths[2],\n",
    "            num_heads=num_heads[2],\n",
    "            mlp_ratio=mlp_ratios[2],\n",
    "            norm_layer=norm_layers[2],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[2],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[2]\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n",
    "        \n",
    "        self.decoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[3],\n",
    "            num_layers=layer_depths[3],\n",
    "            num_heads=num_heads[3],\n",
    "            mlp_ratio=mlp_ratios[3],\n",
    "            norm_layer=norm_layers[3],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[3],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[3]\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n",
    "        \n",
    "        self.decoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[4],\n",
    "            num_layers=layer_depths[4],\n",
    "            num_heads=num_heads[4],\n",
    "            mlp_ratio=mlp_ratios[4],\n",
    "            norm_layer=norm_layers[4],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[4],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[4]\n",
    "        )\n",
    "\n",
    "        # Final patch reconstruction\n",
    "        self.patch_reconstruction = PatchReconstruction(\n",
    "            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def adjust_image_size(self, x):\n",
    "        # Ensures the input image size is compatible with the patch size\n",
    "        _, _, height, width = x.size()\n",
    "        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n",
    "        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder_stage1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.downsample1(x)\n",
    "        x = self.encoder_stage2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.downsample2(x)\n",
    "        x = self.encoder_stage3(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n",
    "        x = self.decoder_stage1(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n",
    "        x = self.decoder_stage2(x)\n",
    "        x = self.patch_reconstruction(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_height, original_width = x.shape[2:]\n",
    "        x = self.adjust_image_size(x)\n",
    "\n",
    "        features = self.extract_features(x)\n",
    "        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n",
    "\n",
    "        # Dehazing formula: I = J * t + A * (1 - t)\n",
    "        x = transmission_map * x - atmospheric_light + x\n",
    "        x = x[:, :, :original_height, :original_width]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f08d94",
   "metadata": {
    "papermill": {
     "duration": 0.015716,
     "end_time": "2025-05-02T08:43:43.446747",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.431031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### build_dehazing_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "440c4221",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.479406Z",
     "iopub.status.busy": "2025-05-02T08:43:43.479152Z",
     "iopub.status.idle": "2025-05-02T08:43:43.483072Z",
     "shell.execute_reply": "2025-05-02T08:43:43.482543Z"
    },
    "papermill": {
     "duration": 0.021459,
     "end_time": "2025-05-02T08:43:43.484131",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.462672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dehazing_transformer():\n",
    "    return DehazingTransformer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "        layer_depths=[12, 12, 12, 6, 6],\n",
    "        num_heads=[2, 4, 6, 1, 1],\n",
    "        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b146e",
   "metadata": {
    "papermill": {
     "duration": 0.015659,
     "end_time": "2025-05-02T08:43:43.515893",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.500234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RevisedLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "377322fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.548346Z",
     "iopub.status.busy": "2025-05-02T08:43:43.548099Z",
     "iopub.status.idle": "2025-05-02T08:43:43.554362Z",
     "shell.execute_reply": "2025-05-02T08:43:43.553857Z"
    },
    "papermill": {
     "duration": 0.023771,
     "end_time": "2025-05-02T08:43:43.555420",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.531649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RevisedLayerNorm(nn.Module):\n",
    "    \"\"\"Revised LayerNorm\"\"\"\n",
    "    def __init__(self, embed_dim, epsilon=1e-5, detach_gradient=False):\n",
    "        super(RevisedLayerNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.detach_gradient = detach_gradient\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones((1, embed_dim, 1, 1)))\n",
    "        self.shift = nn.Parameter(torch.zeros((1, embed_dim, 1, 1)))\n",
    "\n",
    "        self.scale_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "        self.shift_mlp = nn.Conv2d(1, embed_dim, 1)\n",
    "\n",
    "        trunc_normal_(self.scale_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.scale_mlp.bias, 1)\n",
    "\n",
    "        trunc_normal_(self.shift_mlp.weight, std=.02)\n",
    "        nn.init.constant_(self.shift_mlp.bias, 0)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        mean_value = torch.mean(input_tensor, dim=(1, 2, 3), keepdim=True)\n",
    "        std_value = torch.sqrt((input_tensor - mean_value).pow(2).mean(dim=(1, 2, 3), keepdim=True) + self.epsilon)\n",
    "\n",
    "        normalized_tensor = (input_tensor - mean_value) / std_value\n",
    "\n",
    "        if self.detach_gradient:\n",
    "            rescale, rebias = self.scale_mlp(std_value.detach()), self.shift_mlp(mean_value.detach())\n",
    "        else:\n",
    "            rescale, rebias = self.scale_mlp(std_value), self.shift_mlp(mean_value)\n",
    "\n",
    "        output = normalized_tensor * self.scale + self.shift\n",
    "        return output, rescale, rebias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cac687bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.588364Z",
     "iopub.status.busy": "2025-05-02T08:43:43.588155Z",
     "iopub.status.idle": "2025-05-02T08:43:43.595463Z",
     "shell.execute_reply": "2025-05-02T08:43:43.594963Z"
    },
    "papermill": {
     "duration": 0.024905,
     "end_time": "2025-05-02T08:43:43.596463",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.571558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, depth, input_channels, hidden_channels=None, output_channels=None):\n",
    "        super().__init__()\n",
    "        output_channels = output_channels or input_channels\n",
    "        hidden_channels = hidden_channels or input_channels\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            gain = (8 * self.depth) ** (-1 / 4)\n",
    "            fan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(layer.weight)\n",
    "            std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "            trunc_normal_(layer.weight, std=std)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "\n",
    "def partition_into_windows(tensor, window_size):\n",
    "    \"\"\"Splits the input tensor into non-overlapping windows.\"\"\"\n",
    "    batch_size, height, width, channels = tensor.shape\n",
    "    assert height % window_size == 0 and width % window_size == 0, \"Height and width must be divisible by window_size\"\n",
    "\n",
    "    tensor = tensor.view(\n",
    "        batch_size, height // window_size, window_size, width // window_size, window_size, channels\n",
    "    )\n",
    "    windows = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size**2, channels)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, height, width):\n",
    "    \"\"\"Reconstructs the original tensor from partitioned windows.\"\"\"\n",
    "    batch_size = windows.shape[0] // ((height * width) // (window_size**2))\n",
    "    tensor = windows.view(\n",
    "        batch_size, height // window_size, width // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    tensor = tensor.permute(0, 1, 3, 2, 4, 5).contiguous().view(batch_size, height, width, -1)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e9cc1",
   "metadata": {
    "papermill": {
     "duration": 0.015677,
     "end_time": "2025-05-02T08:43:43.628264",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.612587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa4a7f50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.660809Z",
     "iopub.status.busy": "2025-05-02T08:43:43.660593Z",
     "iopub.status.idle": "2025-05-02T08:43:43.673394Z",
     "shell.execute_reply": "2025-05-02T08:43:43.672827Z"
    },
    "papermill": {
     "duration": 0.030323,
     "end_time": "2025-05-02T08:43:43.674396",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.644073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 64, 16, 16]),\n",
       " torch.Size([32, 16, 64]),\n",
       " torch.Size([2, 16, 16, 64]),\n",
       " True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the MultiLayerPerceptron with sample parameters\n",
    "depth = 4\n",
    "input_channels = 64\n",
    "hidden_channels = 128\n",
    "output_channels = 64\n",
    "\n",
    "mlp = MultiLayerPerceptron(depth, input_channels, hidden_channels, output_channels)\n",
    "\n",
    "# Create a random tensor to test MLP (batch_size=2, channels=64, height=16, width=16)\n",
    "input_tensor = torch.randn(2, 64, 16, 16)\n",
    "output_tensor = mlp(input_tensor)\n",
    "\n",
    "# Check output shape\n",
    "mlp_output_shape = output_tensor.shape\n",
    "\n",
    "# Test window partition and merging\n",
    "batch_size, height, width, channels = 2, 16, 16, 64\n",
    "window_size = 4\n",
    "\n",
    "# Create a random tensor for window functions (B, H, W, C) format\n",
    "input_window_tensor = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Apply partitioning and merging\n",
    "windows = partition_into_windows(input_window_tensor, window_size)\n",
    "reconstructed_tensor = merge_windows(windows, window_size, height, width)\n",
    "\n",
    "# Check shapes\n",
    "windows_shape = windows.shape\n",
    "reconstructed_shape = reconstructed_tensor.shape\n",
    "\n",
    "# Validate if the reconstruction matches the original input shape\n",
    "is_shape_correct = reconstructed_shape == input_window_tensor.shape\n",
    "\n",
    "# Output results\n",
    "mlp_output_shape, windows_shape, reconstructed_shape, is_shape_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f36e7f27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.708076Z",
     "iopub.status.busy": "2025-05-02T08:43:43.707547Z",
     "iopub.status.idle": "2025-05-02T08:43:43.714177Z",
     "shell.execute_reply": "2025-05-02T08:43:43.713659Z"
    },
    "papermill": {
     "duration": 0.024435,
     "end_time": "2025-05-02T08:43:43.715226",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.690791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, window_size, num_heads):\n",
    "        \"\"\"Self-attention mechanism within local windows.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size  # (height, width)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scaling_factor = head_dim ** -0.5  # Scaled dot-product attention\n",
    "\n",
    "        # Compute and store relative positional encodings\n",
    "        relative_positional_encodings = compute_log_relative_positions(self.window_size)\n",
    "        self.register_buffer(\"relative_positional_encodings\", relative_positional_encodings)\n",
    "\n",
    "        # Learnable transformation of relative position embeddings\n",
    "        self.relative_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 256, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_heads, bias=True)\n",
    "        )\n",
    "\n",
    "        self.attention_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"Computes attention scores and applies self-attention within a window.\"\"\"\n",
    "        batch_size, num_tokens, _ = qkv.shape\n",
    "\n",
    "        # Reshape qkv into separate query, key, and value tensors\n",
    "        qkv = qkv.reshape(batch_size, num_tokens, 3, self.num_heads, self.embed_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Unpacking query, key, and value\n",
    "\n",
    "        # Scale query for stable attention computation\n",
    "        q = q * self.scaling_factor\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # Compute relative position bias\n",
    "        relative_bias = self.relative_mlp(self.relative_positional_encodings)\n",
    "        relative_bias = relative_bias.permute(2, 0, 1).contiguous()  # Shape: (num_heads, window_size², window_size²)\n",
    "        attention_scores = attention_scores + relative_bias.unsqueeze(0)\n",
    "\n",
    "        # Apply softmax and compute weighted values\n",
    "        attention_weights = self.attention_softmax(attention_scores)\n",
    "        output = (attention_weights @ v).transpose(1, 2).reshape(batch_size, num_tokens, self.embed_dim)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1abdd740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.748238Z",
     "iopub.status.busy": "2025-05-02T08:43:43.748040Z",
     "iopub.status.idle": "2025-05-02T08:43:43.752738Z",
     "shell.execute_reply": "2025-05-02T08:43:43.752055Z"
    },
    "papermill": {
     "duration": 0.022279,
     "end_time": "2025-05-02T08:43:43.753877",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.731598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_log_relative_positions(window_size):\n",
    "    \"\"\"Computes log-scaled relative position embeddings for a given window size.\"\"\"\n",
    "    coord_range = torch.arange(window_size)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    coord_grid = torch.stack(torch.meshgrid([coord_range, coord_range]))  # Shape: (2, window_size, window_size)\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    flattened_coords = torch.flatten(coord_grid, 1)  # Shape: (2, window_size * window_size)\n",
    "\n",
    "    # Compute relative positions\n",
    "    relative_positions = flattened_coords[:, :, None] - flattened_coords[:, None, :]  # Shape: (2, window_size^2, window_size^2)\n",
    "\n",
    "    # Format and apply log transformation\n",
    "    relative_positions = relative_positions.permute(1, 2, 0).contiguous()  # Shape: (window_size^2, window_size^2, 2)\n",
    "    log_relative_positions = torch.sign(relative_positions) * torch.log(1. + relative_positions.abs())\n",
    "\n",
    "    return log_relative_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "805db709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.789664Z",
     "iopub.status.busy": "2025-05-02T08:43:43.789438Z",
     "iopub.status.idle": "2025-05-02T08:43:43.801623Z",
     "shell.execute_reply": "2025-05-02T08:43:43.801143Z"
    },
    "papermill": {
     "duration": 0.03234,
     "end_time": "2025-05-02T08:43:43.802549",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.770209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, window_size, shift_size, enable_attention=False, conv_mode=None):\n",
    "        \"\"\"Hybrid attention-convolution module with optional window-based attention.\"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.network_depth = network_depth\n",
    "        self.enable_attention = enable_attention\n",
    "        self.conv_mode = conv_mode\n",
    "\n",
    "        # Define convolutional processing based on mode\n",
    "        if self.conv_mode == 'Conv':\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, padding_mode='reflect')\n",
    "            )\n",
    "\n",
    "        if self.conv_mode == 'DWConv':\n",
    "            self.conv_layer = nn.Conv2d(embed_dim, embed_dim, kernel_size=5, padding=2, groups=embed_dim, padding_mode='reflect')\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            self.value_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "            self.output_projection = nn.Conv2d(embed_dim, embed_dim, 1)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            self.query_key_projection = nn.Conv2d(embed_dim, embed_dim * 2, 1)\n",
    "            self.window_attention = LocalWindowAttention(embed_dim, window_size, num_heads)\n",
    "\n",
    "        self.apply(self._initialize_weights)\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        \"\"\"Custom weight initialization.\"\"\"\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight_shape = module.weight.shape\n",
    "\n",
    "            if weight_shape[0] == self.embed_dim * 2:  # Query-Key projection\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "            else:\n",
    "                gain = (8 * self.network_depth) ** (-1/4)\n",
    "                fan_in, fan_out = _calculate_fan_in_and_fan_out(module.weight)\n",
    "                std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n",
    "                trunc_normal_(module.weight, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def pad_for_window_processing(self, x, shift=False):\n",
    "        \"\"\"Pads the input tensor to fit window processing requirements.\"\"\"\n",
    "        _, _, height, width = x.size()\n",
    "        pad_h = (self.window_size - height % self.window_size) % self.window_size\n",
    "        pad_w = (self.window_size - width % self.window_size) % self.window_size\n",
    "\n",
    "        if shift:\n",
    "            x = F.pad(x, (self.shift_size, (self.window_size - self.shift_size + pad_w) % self.window_size,\n",
    "                          self.shift_size, (self.window_size - self.shift_size + pad_h) % self.window_size), mode='reflect')\n",
    "        else:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes the output with optional attention and convolution.\"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.conv_mode == 'DWConv' or self.enable_attention:\n",
    "            v_proj = self.value_projection(x)\n",
    "\n",
    "        if self.enable_attention:\n",
    "            qk_proj = self.query_key_projection(x)\n",
    "            qkv = torch.cat([qk_proj, v_proj], dim=1)\n",
    "\n",
    "            # Apply padding for shifted window processing\n",
    "            padded_qkv = self.pad_for_window_processing(qkv, self.shift_size > 0)\n",
    "            padded_height, padded_width = padded_qkv.shape[2:]\n",
    "\n",
    "            # Partition into windows\n",
    "            padded_qkv = padded_qkv.permute(0, 2, 3, 1)\n",
    "            qkv_windows = partition_into_windows(padded_qkv, self.window_size)  # (num_windows * batch, window_size², channels)\n",
    "\n",
    "            # Apply window-based attention\n",
    "            attn_windows = self.window_attention(qkv_windows)\n",
    "\n",
    "            # Merge back to original spatial dimensions\n",
    "            merged_output = merge_windows(attn_windows, self.window_size, padded_height, padded_width)\n",
    "\n",
    "            # Reverse the cyclic shift\n",
    "            attn_output = merged_output[:, self.shift_size:(self.shift_size + height), self.shift_size:(self.shift_size + width), :]\n",
    "            attn_output = attn_output.permute(0, 3, 1, 2)\n",
    "\n",
    "            if self.conv_mode in ['Conv', 'DWConv']:\n",
    "                conv_output = self.conv_layer(v_proj)\n",
    "                output = self.output_projection(conv_output + attn_output)\n",
    "            else:\n",
    "                output = self.output_projection(attn_output)\n",
    "\n",
    "        else:\n",
    "            if self.conv_mode == 'Conv':\n",
    "                output = self.conv_layer(x)  # No attention, using convolution only\n",
    "            elif self.conv_mode == 'DWConv':\n",
    "                output = self.output_projection(self.conv_layer(v_proj))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70b6771a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.835596Z",
     "iopub.status.busy": "2025-05-02T08:43:43.835400Z",
     "iopub.status.idle": "2025-05-02T08:43:43.844698Z",
     "shell.execute_reply": "2025-05-02T08:43:43.843973Z"
    },
    "papermill": {
     "duration": 0.026935,
     "end_time": "2025-05-02T08:43:43.845809",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.818874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, enable_mlp_norm=False,\n",
    "                 window_size=8, shift_size=0, enable_attention=True, conv_mode=None):\n",
    "        \"\"\"\n",
    "        A transformer block that includes attention (optional) and MLP layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enable_attention = enable_attention\n",
    "        self.enable_mlp_norm = enable_mlp_norm\n",
    "\n",
    "        self.pre_norm = norm_layer(embed_dim) if enable_attention else nn.Identity()\n",
    "        self.attention_layer = AdaptiveAttention(\n",
    "            network_depth, embed_dim, num_heads=num_heads, window_size=window_size,\n",
    "            shift_size=shift_size, enable_attention=enable_attention, conv_mode=conv_mode\n",
    "        )\n",
    "\n",
    "        self.post_norm = norm_layer(embed_dim) if enable_attention and enable_mlp_norm else nn.Identity()\n",
    "        self.mlp_layer = MultiLayerPerceptron(network_depth, embed_dim, hidden_channels=int(embed_dim * mlp_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if self.enable_attention:\n",
    "            x, rescale, rebias = self.pre_norm(x)\n",
    "        x = self.attention_layer(x)\n",
    "        if self.enable_attention:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        residual = x\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x, rescale, rebias = self.post_norm(x)\n",
    "        x = self.mlp_layer(x)\n",
    "        if self.enable_attention and self.enable_mlp_norm:\n",
    "            x = x * rescale + rebias\n",
    "        x = residual + x  # Residual connection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, network_depth, embed_dim, num_layers, num_heads, mlp_ratio=4.0,\n",
    "                 norm_layer=nn.LayerNorm, window_size=8,\n",
    "                 attention_ratio=0.0, attention_placement='last', conv_mode=None):\n",
    "        \"\"\"\n",
    "        A stage of transformer blocks with configurable attention placement.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        attention_layers = int(attention_ratio * num_layers)\n",
    "\n",
    "        if attention_placement == 'last':\n",
    "            enable_attentions = [i >= num_layers - attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'first':\n",
    "            enable_attentions = [i < attention_layers for i in range(num_layers)]\n",
    "        elif attention_placement == 'middle':\n",
    "            enable_attentions = [\n",
    "                (i >= (num_layers - attention_layers) // 2) and (i < (num_layers + attention_layers) // 2)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        # Build transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VisionTransformerBlock(\n",
    "                network_depth=network_depth,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                enable_attention=enable_attentions[i],\n",
    "                conv_mode=conv_mode\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer stage.\n",
    "        \"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e871b5f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.879365Z",
     "iopub.status.busy": "2025-05-02T08:43:43.878717Z",
     "iopub.status.idle": "2025-05-02T08:43:43.884767Z",
     "shell.execute_reply": "2025-05-02T08:43:43.884128Z"
    },
    "papermill": {
     "duration": 0.023638,
     "end_time": "2025-05-02T08:43:43.885774",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.862136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, input_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch embedding module that projects input images into token embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            input_channels, embedding_dim, kernel_size=kernel_size, stride=patch_size,\n",
    "            padding=(kernel_size - patch_size + 1) // 2, padding_mode='reflect'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to generate patch embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n",
    "\n",
    "\n",
    "class PatchReconstruction(nn.Module):\n",
    "    def __init__(self, patch_size=4, output_channels=3, embedding_dim=96, kernel_size=None):\n",
    "        \"\"\"\n",
    "        Patch reconstruction module that converts token embeddings back to image patches.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if kernel_size is None:\n",
    "            kernel_size = 1\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embedding_dim, output_channels * patch_size ** 2, kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2, padding_mode='reflect'\n",
    "            ),\n",
    "            nn.PixelShuffle(patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to reconstruct image from embeddings.\n",
    "        \"\"\"\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b202c53a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.919119Z",
     "iopub.status.busy": "2025-05-02T08:43:43.918906Z",
     "iopub.status.idle": "2025-05-02T08:43:43.924533Z",
     "shell.execute_reply": "2025-05-02T08:43:43.923906Z"
    },
    "papermill": {
     "duration": 0.023376,
     "end_time": "2025-05-02T08:43:43.925512",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.902136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelectiveKernelFusion(nn.Module):\n",
    "    def __init__(self, channels, num_branches=2, reduction_ratio=8):\n",
    "        \"\"\"\n",
    "        Selective Kernel Fusion (SKFusion) module for adaptive feature selection.\n",
    "\n",
    "        Args:\n",
    "            channels (int): Number of input channels.\n",
    "            num_branches (int): Number of feature branches to fuse.\n",
    "            reduction_ratio (int): Reduction ratio for the attention mechanism.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_branches = num_branches\n",
    "        reduced_channels = max(int(channels / reduction_ratio), 4)\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(reduced_channels, channels * num_branches, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Forward pass for selective kernel fusion.\n",
    "\n",
    "        Args:\n",
    "            feature_maps (list of tensors): A list of feature maps to be fused.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adaptively fused feature map.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = feature_maps[0].shape\n",
    "        \n",
    "        # Concatenate feature maps along a new dimension (num_branches)\n",
    "        stacked_features = torch.cat(feature_maps, dim=1).view(batch_size, self.num_branches, channels, height, width)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        aggregated_features = torch.sum(stacked_features, dim=1)\n",
    "        attention_weights = self.channel_attention(self.global_avg_pool(aggregated_features))\n",
    "        attention_weights = self.softmax(attention_weights.view(batch_size, self.num_branches, channels, 1, 1))\n",
    "\n",
    "        # Weighted sum of input feature maps\n",
    "        fused_output = torch.sum(stacked_features * attention_weights, dim=1)\n",
    "        return fused_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5580a946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:43.959178Z",
     "iopub.status.busy": "2025-05-02T08:43:43.958953Z",
     "iopub.status.idle": "2025-05-02T08:43:43.972450Z",
     "shell.execute_reply": "2025-05-02T08:43:43.971902Z"
    },
    "papermill": {
     "duration": 0.031545,
     "end_time": "2025-05-02T08:43:43.973438",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.941893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DehazingTransformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=4, window_size=8,\n",
    "                 embed_dims=[24, 48, 96, 48, 24],\n",
    "                 mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "                 layer_depths=[16, 16, 16, 8, 8],\n",
    "                 num_heads=[2, 4, 6, 1, 1],\n",
    "                 attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "                 conv_types=['DWConv', 'DWConv', 'DWConv', 'DWConv', 'DWConv'],\n",
    "                 norm_layers=[RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm, RevisedLayerNorm]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding settings\n",
    "        self.patch_size = 4\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Initial patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            patch_size=1, input_channels=input_channels, embedding_dim=embed_dims[0], kernel_size=3)\n",
    "\n",
    "        # Backbone layers\n",
    "        self.encoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[0],\n",
    "            num_layers=layer_depths[0],\n",
    "            num_heads=num_heads[0],\n",
    "            mlp_ratio=mlp_ratios[0],\n",
    "            norm_layer=norm_layers[0],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[0],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[0]\n",
    "        )\n",
    "        \n",
    "        self.downsample1 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[0], embedding_dim=embed_dims[1]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection1 = nn.Conv2d(embed_dims[0], embed_dims[0], 1)\n",
    "        \n",
    "        self.encoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[1],\n",
    "            num_layers=layer_depths[1],\n",
    "            num_heads=num_heads[1],\n",
    "            mlp_ratio=mlp_ratios[1],\n",
    "            norm_layer=norm_layers[1],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[1],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[1]\n",
    "        )\n",
    "        \n",
    "        self.downsample2 = PatchEmbedding(\n",
    "            patch_size=2, input_channels=embed_dims[1], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = nn.Conv2d(embed_dims[1], embed_dims[1], 1)\n",
    "        \n",
    "        self.encoder_stage3 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[2],\n",
    "            num_layers=layer_depths[2],\n",
    "            num_heads=num_heads[2],\n",
    "            mlp_ratio=mlp_ratios[2],\n",
    "            norm_layer=norm_layers[2],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[2],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[2]\n",
    "        )\n",
    "        \n",
    "        self.upsample1 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[3], embedding_dim=embed_dims[2]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[1] == embed_dims[3]\n",
    "        self.fusion_layer1 = SelectiveKernelFusion(embed_dims[3])\n",
    "        \n",
    "        self.decoder_stage1 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[3],\n",
    "            num_layers=layer_depths[3],\n",
    "            num_heads=num_heads[3],\n",
    "            mlp_ratio=mlp_ratios[3],\n",
    "            norm_layer=norm_layers[3],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[3],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[3]\n",
    "        )\n",
    "        \n",
    "        self.upsample2 = PatchReconstruction(\n",
    "            patch_size=2, output_channels=embed_dims[4], embedding_dim=embed_dims[3]\n",
    "        )\n",
    "        \n",
    "        assert embed_dims[0] == embed_dims[4]\n",
    "        self.fusion_layer2 = SelectiveKernelFusion(embed_dims[4])\n",
    "        \n",
    "        self.decoder_stage2 = TransformerStage(\n",
    "            network_depth=sum(layer_depths),\n",
    "            embed_dim=embed_dims[4],\n",
    "            num_layers=layer_depths[4],\n",
    "            num_heads=num_heads[4],\n",
    "            mlp_ratio=mlp_ratios[4],\n",
    "            norm_layer=norm_layers[4],\n",
    "            window_size=window_size,\n",
    "            attention_ratio=attention_ratios[4],\n",
    "            attention_placement='last',\n",
    "            conv_mode=conv_types[4]\n",
    "        )\n",
    "\n",
    "        # Final patch reconstruction\n",
    "        self.patch_reconstruction = PatchReconstruction(\n",
    "            patch_size=1, output_channels=output_channels, embedding_dim=embed_dims[4], kernel_size=3)\n",
    "\n",
    "    def adjust_image_size(self, x):\n",
    "        # Ensures the input image size is compatible with the patch size\n",
    "        _, _, height, width = x.size()\n",
    "        pad_height = (self.patch_size - height % self.patch_size) % self.patch_size\n",
    "        pad_width = (self.patch_size - width % self.patch_size) % self.patch_size\n",
    "        x = F.pad(x, (0, pad_width, 0, pad_height), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder_stage1(x)\n",
    "        skip1 = x\n",
    "\n",
    "        x = self.downsample1(x)\n",
    "        x = self.encoder_stage2(x)\n",
    "        skip2 = x\n",
    "\n",
    "        x = self.downsample2(x)\n",
    "        x = self.encoder_stage3(x)\n",
    "        x = self.upsample1(x)\n",
    "\n",
    "        x = self.fusion_layer1([x, self.skip_connection2(skip2)]) + x\n",
    "        x = self.decoder_stage1(x)\n",
    "        x = self.upsample2(x)\n",
    "\n",
    "        x = self.fusion_layer2([x, self.skip_connection1(skip1)]) + x\n",
    "        x = self.decoder_stage2(x)\n",
    "        x = self.patch_reconstruction(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_height, original_width = x.shape[2:]\n",
    "        x = self.adjust_image_size(x)\n",
    "\n",
    "        features = self.extract_features(x)\n",
    "        transmission_map, atmospheric_light = torch.split(features, (1, 3), dim=1)\n",
    "\n",
    "        # Dehazing formula: I = J * t + A * (1 - t)\n",
    "        x = transmission_map * x - atmospheric_light + x\n",
    "        x = x[:, :, :original_height, :original_width]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "259724de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:44.008326Z",
     "iopub.status.busy": "2025-05-02T08:43:44.007772Z",
     "iopub.status.idle": "2025-05-02T08:43:44.012080Z",
     "shell.execute_reply": "2025-05-02T08:43:44.011398Z"
    },
    "papermill": {
     "duration": 0.022864,
     "end_time": "2025-05-02T08:43:44.013172",
     "exception": false,
     "start_time": "2025-05-02T08:43:43.990308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dehazing_transformer():\n",
    "    return DehazingTransformer(\n",
    "        embed_dims=[24, 48, 96, 48, 24],\n",
    "        mlp_ratios=[2., 4., 4., 2., 2.],\n",
    "        layer_depths=[12, 12, 12, 6, 6],\n",
    "        num_heads=[2, 4, 6, 1, 1],\n",
    "        attention_ratios=[1/4, 1/2, 3/4, 0, 0],\n",
    "        conv_types=['Conv', 'Conv', 'Conv', 'Conv', 'Conv']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a4ecbf",
   "metadata": {
    "papermill": {
     "duration": 0.015884,
     "end_time": "2025-05-02T08:43:44.045772",
     "exception": false,
     "start_time": "2025-05-02T08:43:44.029888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ConvolutionalGuidedFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f2422b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:44.080036Z",
     "iopub.status.busy": "2025-05-02T08:43:44.079524Z",
     "iopub.status.idle": "2025-05-02T08:43:44.087070Z",
     "shell.execute_reply": "2025-05-02T08:43:44.086360Z"
    },
    "papermill": {
     "duration": 0.02565,
     "end_time": "2025-05-02T08:43:44.088148",
     "exception": false,
     "start_time": "2025-05-02T08:43:44.062498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvolutionalGuidedFilter(nn.Module):\n",
    "    def __init__(self, radius=1, norm_layer=nn.BatchNorm2d, conv_kernel_size: int = 1):\n",
    "        super(ConvolutionalGuidedFilter, self).__init__()\n",
    "\n",
    "        self.box_filter = nn.Conv2d(\n",
    "            3, 3, kernel_size=3, padding=radius, dilation=radius, bias=False, groups=3\n",
    "        )\n",
    "        self.conv_a = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                6,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                32,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "            norm_layer(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                32,\n",
    "                3,\n",
    "                kernel_size=conv_kernel_size,\n",
    "                padding=conv_kernel_size // 2,\n",
    "                bias=False,\n",
    "            ),\n",
    "        )\n",
    "        self.box_filter.weight.data[...] = 1.0\n",
    "\n",
    "    def forward(self, x_low_res, y_low_res, x_high_res):\n",
    "        _, _, h_lr, w_lr = x_low_res.size()\n",
    "        _, _, h_hr, w_hr = x_high_res.size()\n",
    "\n",
    "        N = self.box_filter(x_low_res.data.new().resize_((1, 3, h_lr, w_lr)).fill_(1.0))\n",
    "        ## mean_x\n",
    "        mean_x = self.box_filter(x_low_res) / N\n",
    "        ## mean_y\n",
    "        mean_y = self.box_filter(y_low_res) / N\n",
    "        ## cov_xy\n",
    "        cov_xy = self.box_filter(x_low_res * y_low_res) / N - mean_x * mean_y\n",
    "        ## var_x\n",
    "        var_x = self.box_filter(x_low_res * x_low_res) / N - mean_x * mean_x\n",
    "\n",
    "        ## A\n",
    "        A = self.conv_a(torch.cat([cov_xy, var_x], dim=1))\n",
    "        ## b\n",
    "        b = mean_y - A * mean_x\n",
    "\n",
    "        ## mean_A; mean_b\n",
    "        mean_A = F.interpolate(A, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "        mean_b = F.interpolate(b, (h_hr, w_hr), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return mean_A * x_high_res + mean_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e39b6",
   "metadata": {
    "papermill": {
     "duration": 0.016138,
     "end_time": "2025-05-02T08:43:44.121167",
     "exception": false,
     "start_time": "2025-05-02T08:43:44.105029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PixelAttentionLayer and ChannelAttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77f65906",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:44.155052Z",
     "iopub.status.busy": "2025-05-02T08:43:44.154575Z",
     "iopub.status.idle": "2025-05-02T08:43:44.160750Z",
     "shell.execute_reply": "2025-05-02T08:43:44.159994Z"
    },
    "papermill": {
     "duration": 0.024154,
     "end_time": "2025-05-02T08:43:44.161900",
     "exception": false,
     "start_time": "2025-05-02T08:43:44.137746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(PixelAttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, 1, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_map = self.attention(x)\n",
    "        return x * attention_map\n",
    "\n",
    "class ChannelAttentionLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelAttentionLayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.attention = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channels // 8, channels, kernel_size=1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = self.avg_pool(x)\n",
    "        attention_map = self.attention(pooled)\n",
    "        return x * attention_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363e8c3",
   "metadata": {
    "papermill": {
     "duration": 0.016668,
     "end_time": "2025-05-02T08:43:44.194843",
     "exception": false,
     "start_time": "2025-05-02T08:43:44.178175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## HybridResidualDenseBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a9f08e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:44.229053Z",
     "iopub.status.busy": "2025-05-02T08:43:44.228761Z",
     "iopub.status.idle": "2025-05-02T08:43:44.235515Z",
     "shell.execute_reply": "2025-05-02T08:43:44.234918Z"
    },
    "papermill": {
     "duration": 0.025327,
     "end_time": "2025-05-02T08:43:44.236525",
     "exception": false,
     "start_time": "2025-05-02T08:43:44.211198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HybridResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_dense_layers=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.growth_rate = growth_rate\n",
    "        self.in_channels = in_channels\n",
    "        total_channels = in_channels\n",
    "\n",
    "        for i in range(num_dense_layers):\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(total_channels, growth_rate, kernel_size=3, padding=2**i,dilation=2**i)\n",
    "            )\n",
    "            total_channels += growth_rate\n",
    "\n",
    "        self.fusion = nn.Conv2d(total_channels, in_channels, kernel_size=1)\n",
    "        self.channel_attention = ChannelAttentionLayer(in_channels)\n",
    "        self.pixel_attention = PixelAttentionLayer(in_channels)\n",
    "\n",
    "        # Gated residual fusion\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"self.layers\", self.layers)\n",
    "        # print(\"HybridResidualDenseBlock: x.shape\", x.shape)\n",
    "        # x: (B, C, H, W)\n",
    "        features = [x]\n",
    "        # x: (B, C, H, W) -> (B, C, H, W) + (B, growth_rate, H, W) * num_dense_layers\n",
    "        # features: [(B, C, H, W), (B, growth_rate, H, W), ...] \n",
    "        for conv in self.layers:\n",
    "            # Apply convolution and ReLU activation\n",
    "            out = F.relu(conv(torch.cat(features, dim=1)))\n",
    "            # print(\"self.layers -> out.shape: \", out.shape)\n",
    "            features.append(out)\n",
    "        \n",
    "        # print(\"self.layers -> features: \", features)\n",
    "\n",
    "        dense_out = torch.cat(features, dim=1)\n",
    "        # print(\"self.layers -> dense_out.shape: \", dense_out.shape)\n",
    "        fused = self.fusion(dense_out)\n",
    "        # print(\"self.layers -> fused.shape: \", fused.shape)\n",
    "        # Apply channel attention and pixel attention\n",
    "        # fused: (B, C, H, W) -> (B, C, H, W) + (B, C, H, W) * 2\n",
    "        ca = self.channel_attention(fused)\n",
    "        # print(\"self.layers -> ca.shape: \", ca.shape)\n",
    "        pa = self.pixel_attention(ca)\n",
    "        # print(\"self.layers -> pa.shape: \", pa.shape)\n",
    "        # Gated residual fusion\n",
    "        # x: (B, C, H, W) + (B, C, H, W) * 2\n",
    "        gate_input = torch.cat([x, pa], dim=1)\n",
    "        # print(\"self.layers -> gate_input.shape: \", gate_input.shape)\n",
    "        gated_fusion = self.gate(gate_input)\n",
    "        # print(\"self.layers -> gated_fusion.shape: \", gated_fusion.shape)\n",
    "        # Apply gated fusion\n",
    "        return x * (1 - gated_fusion) + pa * gated_fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a622e4",
   "metadata": {
    "papermill": {
     "duration": 0.016726,
     "end_time": "2025-05-02T08:43:44.269856",
     "exception": false,
     "start_time": "2025-05-02T08:43:44.253130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## AdaptiveInstanceNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0fb00d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:44.303261Z",
     "iopub.status.busy": "2025-05-02T08:43:44.302993Z",
     "iopub.status.idle": "2025-05-02T08:43:44.307699Z",
     "shell.execute_reply": "2025-05-02T08:43:44.307034Z"
    },
    "papermill": {
     "duration": 0.0228,
     "end_time": "2025-05-02T08:43:44.308852",
     "exception": false,
     "start_time": "2025-05-02T08:43:44.286052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNormalization(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(AdaptiveInstanceNormalization, self).__init__()\n",
    "\n",
    "        # Learnable scaling factors\n",
    "        self.scale_x = nn.Parameter(torch.tensor(1.0))  # Identity scaling\n",
    "        self.scale_norm = nn.Parameter(torch.tensor(0.0))  # Initially no effect\n",
    "\n",
    "        # Instance normalization layer with affine transformation enabled\n",
    "        self.instance_norm = nn.InstanceNorm2d(num_channels, momentum=0.999, eps=0.001, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized_x = self.instance_norm(x)\n",
    "        return self.scale_x * x + self.scale_norm * normalized_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be09cce",
   "metadata": {
    "papermill": {
     "duration": 0.015926,
     "end_time": "2025-05-02T08:43:44.341295",
     "exception": false,
     "start_time": "2025-05-02T08:43:44.325369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DEEPGUIDEDNETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d92c5abd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:44.375561Z",
     "iopub.status.busy": "2025-05-02T08:43:44.375117Z",
     "iopub.status.idle": "2025-05-02T08:43:44.382090Z",
     "shell.execute_reply": "2025-05-02T08:43:44.381360Z"
    },
    "papermill": {
     "duration": 0.02548,
     "end_time": "2025-05-02T08:43:44.383250",
     "exception": false,
     "start_time": "2025-05-02T08:43:44.357770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepGuidedNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=3, radius=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        norm = AdaptiveInstanceNormalization  # define this separately\n",
    "        kernel_size = 3\n",
    "        depth_rate = 64\n",
    "        growth_rate = 16\n",
    "        num_dense_layer = 4\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "        self.rdb1 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb2 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb3 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb4 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(depth_rate, 256, 3, padding=1),       # 64 → 256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 48, 3, padding=1), # 256 → 48 for 3 × 4²\n",
    "            nn.PixelShuffle(4)                              # 48 → 3, upsample ×4\n",
    "        )\n",
    "\n",
    "    def forward(self, x, sr= False):\n",
    "        # print(f\"Shape of x: {x.shape}\")\n",
    "        x_in = self.conv_in(x)\n",
    "        # print(f\"Shape of x_in: {x_in.shape}\")\n",
    "        feat1 = self.rdb1(x_in)\n",
    "        # print(f\"Shape of feat1: {feat1.shape}\")\n",
    "        feat2 = self.rdb2(feat1)\n",
    "        # print(f\"Shape of feat2: {feat2.shape}\")\n",
    "        feat3 = self.rdb3(feat2)\n",
    "        # print(f\"Shape of feat3: {feat3.shape}\")\n",
    "        feat4 = self.rdb4(feat3)\n",
    "        # print(f\"Shape of feat4: {feat4.shape}\")\n",
    "        out_feat = self.conv_out(feat4)\n",
    "        # print(\"CONV Tail final\", self.tail)\n",
    "\n",
    "        sr = self.tail(out_feat)  # shape: (B, 3, H×4, W×4)\n",
    "        return sr, sr, [feat1, feat2, feat3, feat4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92e3ff80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:44.417244Z",
     "iopub.status.busy": "2025-05-02T08:43:44.416806Z",
     "iopub.status.idle": "2025-05-02T08:43:44.424237Z",
     "shell.execute_reply": "2025-05-02T08:43:44.423561Z"
    },
    "papermill": {
     "duration": 0.02534,
     "end_time": "2025-05-02T08:43:44.425284",
     "exception": false,
     "start_time": "2025-05-02T08:43:44.399944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepGuidedNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, radius=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        norm = AdaptiveInstanceNormalization  # define this separately\n",
    "        kernel_size = 3\n",
    "        depth_rate = 64\n",
    "        growth_rate = 16\n",
    "        num_dense_layer = 4\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "        self.conv_out = nn.Conv2d(depth_rate, depth_rate, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "        self.rdb1 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb2 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb3 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "        self.rdb4 = HybridResidualDenseBlock(depth_rate, growth_rate, num_dense_layer)\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(depth_rate, 256, 3, padding=1),       # 64 → 256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 48, 3, padding=1), # 256 → 48 for 3 × 4²\n",
    "            nn.PixelShuffle(4)                              # 48 → 3, upsample ×4\n",
    "        )\n",
    "        self.downsample = nn.Upsample(scale_factor=0.25, mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "        # Guided Filter & Dehazing Transformer\n",
    "        self.guided_filter = ConvolutionalGuidedFilter(radius, norm_layer=norm)\n",
    "        self.dehaze_network = build_dehazing_transformer()\n",
    "\n",
    "    def forward(self, x, sr= False):\n",
    "        # print(f\"Shape of x: {x.shape}\")\n",
    "        x_lr = self.downsample(x)\n",
    "        x_in = self.conv_in(x_lr)\n",
    "        # print(f\"Shape of x_in: {x_in.shape}\")\n",
    "        feat1 = self.rdb1(x_in)\n",
    "        # print(f\"Shape of feat1: {feat1.shape}\")\n",
    "        feat2 = self.rdb2(feat1)\n",
    "        # print(f\"Shape of feat2: {feat2.shape}\")\n",
    "        feat3 = self.rdb3(feat2)\n",
    "        # print(f\"Shape of feat3: {feat3.shape}\")\n",
    "        feat4 = self.rdb4(feat3)\n",
    "        # print(f\"Shape of feat4: {feat4.shape}\")\n",
    "        out_feat = self.conv_out(feat4)\n",
    "        # print(\"CONV Tail final\", self.tail)\n",
    "        \n",
    "        y_base = self.dehaze_network(x_lr)\n",
    "        # print(\"y_base\", y_base.shape)\n",
    "        # print(\"x_in\", x_in.shape)\n",
    "        # print(\"x\", x.shape)\n",
    "        refined_output = self.guided_filter(x_lr, y_base, x)\n",
    "        # print(\"refined_output\", refined_output.shape)\n",
    "        # print(\"out_feat\", out_feat.shape)\n",
    "\n",
    "        sr = self.tail(out_feat)  # shape: (B, 3, H×4, W×4)\n",
    "        # print(\"sr\", sr.shape)\n",
    "        y_out  = refined_output + sr\n",
    "        # print(\"y_out\", y_out.shape)\n",
    "        return sr, refined_output, [feat1, feat2, feat3, feat4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1b6220b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:44.458468Z",
     "iopub.status.busy": "2025-05-02T08:43:44.458297Z",
     "iopub.status.idle": "2025-05-02T08:43:45.245970Z",
     "shell.execute_reply": "2025-05-02T08:43:45.245372Z"
    },
    "papermill": {
     "duration": 0.805673,
     "end_time": "2025-05-02T08:43:45.247265",
     "exception": false,
     "start_time": "2025-05-02T08:43:44.441592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test dehazing transformer\n",
    "d_transformer = build_dehazing_transformer()\n",
    "x = torch.randn(1, 3, 64, 64)  # Example input tensor\n",
    "t_out = d_transformer(x)\n",
    "t_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d72969e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:45.286042Z",
     "iopub.status.busy": "2025-05-02T08:43:45.285745Z",
     "iopub.status.idle": "2025-05-02T08:43:46.006792Z",
     "shell.execute_reply": "2025-05-02T08:43:46.005851Z"
    },
    "papermill": {
     "duration": 0.741119,
     "end_time": "2025-05-02T08:43:46.008170",
     "exception": false,
     "start_time": "2025-05-02T08:43:45.267051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sr shape: torch.Size([1, 3, 128, 128])\n",
      "refined_output shape: torch.Size([1, 3, 128, 128])\n",
      "feat1 shape: torch.Size([1, 64, 32, 32])\n",
      "feat2 shape: torch.Size([1, 64, 32, 32])\n",
      "feat3 shape: torch.Size([1, 64, 32, 32])\n",
      "feat4 shape: torch.Size([1, 64, 32, 32])\n",
      "t_out shape: torch.Size([1, 3, 64, 64])\n",
      "guided_filter output shape: torch.Size([1, 3, 128, 128])\n",
      "PixelAttentionLayer output shape: torch.Size([1, 64, 32, 32])\n",
      "ChannelAttentionLayer output shape: torch.Size([1, 64, 32, 32])\n",
      "HybridResidualDenseBlock output shape: torch.Size([1, 64, 32, 32])\n",
      "AdaptiveInstanceNormalization output shape: torch.Size([1, 64, 32, 32])\n",
      "PatchEmbedding output shape: torch.Size([1, 96, 32, 32])\n",
      "PatchReconstruction output shape: torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "test_net = DeepGuidedNet()\n",
    "x = torch.randn(1, 3, 128, 128)  # Example input tensor\n",
    "y_t = test_net(x)\n",
    "sr, refined_output, [feat1, feat2, feat3, feat4] = y_t\n",
    "print(\"sr shape:\", sr.shape)  # Output shape after the network\n",
    "print(\"refined_output shape:\", refined_output.shape)  # Output shape after the network\n",
    "print(\"feat1 shape:\", feat1.shape)  # Output shape after the network\n",
    "print(\"feat2 shape:\", feat2.shape)  # Output shape after the network\n",
    "print(\"feat3 shape:\", feat3.shape)  # Output shape after the network\n",
    "print(\"feat4 shape:\", feat4.shape)  # Output shape after the network\n",
    "# test the dehazing transformer\n",
    "x = torch.randn(1, 3, 64, 64)  # Example input tensor\n",
    "t_out = d_transformer(x)\n",
    "print(\"t_out shape:\", t_out.shape)  # Output shape after the network\n",
    "# test the convolutional guided filter\n",
    "x_low_res = torch.randn(1, 3, 32, 32)  # Example low-resolution input tensor\n",
    "y_low_res = torch.randn(1, 3, 32, 32)  # Example low-resolution input tensor\n",
    "x_high_res = torch.randn(1, 3, 128, 128)  # Example high-resolution input tensor\n",
    "guided_filter = ConvolutionalGuidedFilter(radius=1)\n",
    "output = guided_filter(x_low_res, y_low_res, x_high_res)\n",
    "print(\"guided_filter output shape:\", output.shape)  # Output shape after the network\n",
    "# test the pixel attention layer\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "pixel_attention_layer = PixelAttentionLayer(channels=64)\n",
    "output = pixel_attention_layer(x)\n",
    "print(\"PixelAttentionLayer output shape:\", output.shape)  # Output shape after the network\n",
    "# test the channel attention layer\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "channel_attention_layer = ChannelAttentionLayer(channels=64)\n",
    "output = channel_attention_layer(x)\n",
    "print(\"ChannelAttentionLayer output shape:\", output.shape)  # Output shape after the network\n",
    "# test the hybrid residual dense block\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "hybrid_residual_dense_block = HybridResidualDenseBlock(in_channels=64, growth_rate=16, num_dense_layers=4)\n",
    "output = hybrid_residual_dense_block(x)\n",
    "print(\"HybridResidualDenseBlock output shape:\", output.shape)  # Output shape after the network\n",
    "# test the adaptive instance normalization\n",
    "x = torch.randn(1, 64, 32, 32)  # Example input tensor  # (batch_size, channels, height, width)\n",
    "adaptive_instance_norm = AdaptiveInstanceNormalization(num_channels=64)\n",
    "output = adaptive_instance_norm(x)\n",
    "print(\"AdaptiveInstanceNormalization output shape:\", output.shape)  # Output shape after the network\n",
    "# test the patch embedding\n",
    "x = torch.randn(1, 3, 128, 128)  # Example input tensor\n",
    "# (batch_size, channels, height, width)\n",
    "patch_embedding = PatchEmbedding(patch_size=4, input_channels=3, embedding_dim=96, kernel_size=3)\n",
    "output = patch_embedding(x)\n",
    "print(\"PatchEmbedding output shape:\", output.shape)  # Output shape after the network\n",
    "# test the patch reconstruction\n",
    "x = torch.randn(1, 96, 32, 32)  # Example input tensor\n",
    "# (batch_size, channels, height, width)\n",
    "patch_reconstruction = PatchReconstruction(patch_size=4, output_channels=3, embedding_dim=96, kernel_size=3)\n",
    "output = patch_reconstruction(x)\n",
    "print(\"PatchReconstruction output shape:\", output.shape)  # Output shape after the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d6903",
   "metadata": {
    "papermill": {
     "duration": 0.016324,
     "end_time": "2025-05-02T08:43:46.042002",
     "exception": false,
     "start_time": "2025-05-02T08:43:46.025678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With pixle shuffle 2 and conv out 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd57c50e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:46.075896Z",
     "iopub.status.busy": "2025-05-02T08:43:46.075669Z",
     "iopub.status.idle": "2025-05-02T08:43:46.384317Z",
     "shell.execute_reply": "2025-05-02T08:43:46.383573Z"
    },
    "papermill": {
     "duration": 0.327039,
     "end_time": "2025-05-02T08:43:46.385551",
     "exception": false,
     "start_time": "2025-05-02T08:43:46.058512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3, 512, 512])\n",
      "Feature shapes: [torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128])]\n"
     ]
    }
   ],
   "source": [
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, 3, 128, 128)  # Batch size of 1, 3 channels, 256x256 image\n",
    "\n",
    "# Initialize the DeepGuidedNetwork\n",
    "model = DeepGuidedNetwork(radius=1)\n",
    "   \n",
    "# Forward pass through the network\n",
    "output_tensor, dummy_out, features = model(input_tensor)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output_tensor.shape)\n",
    "print(\"Feature shapes:\", [f.shape for f in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d260b4a",
   "metadata": {
    "papermill": {
     "duration": 0.01691,
     "end_time": "2025-05-02T08:43:46.420065",
     "exception": false,
     "start_time": "2025-05-02T08:43:46.403155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With pixle shuffle 2 and conv out 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8f21a35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:46.454828Z",
     "iopub.status.busy": "2025-05-02T08:43:46.454572Z",
     "iopub.status.idle": "2025-05-02T08:43:46.742744Z",
     "shell.execute_reply": "2025-05-02T08:43:46.741783Z"
    },
    "papermill": {
     "duration": 0.307103,
     "end_time": "2025-05-02T08:43:46.743971",
     "exception": false,
     "start_time": "2025-05-02T08:43:46.436868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3, 512, 512])\n",
      "Feature shapes: [torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128]), torch.Size([1, 64, 128, 128])]\n"
     ]
    }
   ],
   "source": [
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, 3, 128, 128)  # Batch size of 1, 3 channels, 256x256 image\n",
    "\n",
    "# Initialize the DeepGuidedNetwork\n",
    "model = DeepGuidedNetwork(radius=1)\n",
    "\n",
    "# Forward pass through the network\n",
    "output_tensor, dummy_out, features = model(input_tensor)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Output shape:\", output_tensor.shape)\n",
    "print(\"Feature shapes:\", [f.shape for f in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e470c2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:46.780050Z",
     "iopub.status.busy": "2025-05-02T08:43:46.779478Z",
     "iopub.status.idle": "2025-05-02T08:43:46.782751Z",
     "shell.execute_reply": "2025-05-02T08:43:46.782066Z"
    },
    "papermill": {
     "duration": 0.022135,
     "end_time": "2025-05-02T08:43:46.783910",
     "exception": false,
     "start_time": "2025-05-02T08:43:46.761775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test to_psnr\n",
    "# to_psnr(input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1136207",
   "metadata": {
    "papermill": {
     "duration": 0.017296,
     "end_time": "2025-05-02T08:43:46.818757",
     "exception": false,
     "start_time": "2025-05-02T08:43:46.801461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a780d536",
   "metadata": {
    "papermill": {
     "duration": 0.017222,
     "end_time": "2025-05-02T08:43:46.853367",
     "exception": false,
     "start_time": "2025-05-02T08:43:46.836145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5d0b07e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:46.889144Z",
     "iopub.status.busy": "2025-05-02T08:43:46.888840Z",
     "iopub.status.idle": "2025-05-02T08:43:46.893406Z",
     "shell.execute_reply": "2025-05-02T08:43:46.892848Z"
    },
    "papermill": {
     "duration": 0.023591,
     "end_time": "2025-05-02T08:43:46.894464",
     "exception": false,
     "start_time": "2025-05-02T08:43:46.870873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_psnr(dehaze, gt):\n",
    "    \"\"\"\n",
    "    Compute PSNR (Peak Signal-to-Noise Ratio) between dehazed and ground truth images.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: PSNR values for each image in the batch.\n",
    "    \"\"\"\n",
    "    # print(\"Shapes: \", dehaze.shape, gt.shape)\n",
    "    mse = F.mse_loss(dehaze, gt, reduction='none').mean(dim=[1, 2, 3])  # Compute MSE per image\n",
    "    intensity_max = 1.0\n",
    "\n",
    "    # Compute PSNR safely, avoiding division by zero and extreme values\n",
    "    psnr_list = [10.0 * log10(intensity_max / max(mse_val.item(), 1e-6)) for mse_val in mse]\n",
    "\n",
    "    return psnr_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71a723",
   "metadata": {
    "papermill": {
     "duration": 0.01694,
     "end_time": "2025-05-02T08:43:46.929078",
     "exception": false,
     "start_time": "2025-05-02T08:43:46.912138",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d771364",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:46.965397Z",
     "iopub.status.busy": "2025-05-02T08:43:46.964809Z",
     "iopub.status.idle": "2025-05-02T08:43:50.199507Z",
     "shell.execute_reply": "2025-05-02T08:43:50.198894Z"
    },
    "papermill": {
     "duration": 3.254275,
     "end_time": "2025-05-02T08:43:50.200863",
     "exception": false,
     "start_time": "2025-05-02T08:43:46.946588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "# Define SSIM metric\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0, reduction='none')\n",
    "\n",
    "def to_ssim(dehaze: torch.Tensor, gt: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute SSIM directly on the GPU using torchmetrics.\n",
    "\n",
    "    Args:\n",
    "        dehaze (torch.Tensor): Dehazed image tensor (B, C, H, W)\n",
    "        gt (torch.Tensor): Ground truth image tensor (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        List[float]: SSIM values for each image in the batch.\n",
    "    \"\"\"\n",
    "    ssim_values = ssim_metric(dehaze, gt)  # Shape: [B]\n",
    "    # print(\"1\",ssim_values)\n",
    "    # print(\"2\",[ssim_values])\n",
    "    ssim_values = ssim_values.tolist() \n",
    "    # print(type(ssim_values))\n",
    "    if isinstance(ssim_values, float):  # Correct way to check for a float\n",
    "        return [ssim_values]  # Convert single float to a list\n",
    "    return ssim_values  # Otherwise, return as is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a02f585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:50.241535Z",
     "iopub.status.busy": "2025-05-02T08:43:50.241076Z",
     "iopub.status.idle": "2025-05-02T08:43:50.291371Z",
     "shell.execute_reply": "2025-05-02T08:43:50.290455Z"
    },
    "papermill": {
     "duration": 0.070206,
     "end_time": "2025-05-02T08:43:50.292671",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.222465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0009607386309653521]\n"
     ]
    }
   ],
   "source": [
    "# Test with a dummy tensor\n",
    "dehaze = torch.rand(1, 3, 360, 360)  # Random batch of images\n",
    "gt = torch.rand(1, 3, 360, 360)  # Random ground truth images\n",
    "\n",
    "ssim_scores = to_ssim(dehaze, gt)\n",
    "print(ssim_scores)  # Should print a list of 6 SSIM values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08989318",
   "metadata": {
    "papermill": {
     "duration": 0.020777,
     "end_time": "2025-05-02T08:43:50.335510",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.314733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Validation Haze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8a913e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:50.393724Z",
     "iopub.status.busy": "2025-05-02T08:43:50.393129Z",
     "iopub.status.idle": "2025-05-02T08:43:50.398659Z",
     "shell.execute_reply": "2025-05-02T08:43:50.398078Z"
    },
    "papermill": {
     "duration": 0.035189,
     "end_time": "2025-05-02T08:43:50.399905",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.364716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validationB(net, val_data_loader, device, category, save_tag=False):\n",
    "    \"\"\"\n",
    "    :param net: Your deep learning model\n",
    "    :param val_data_loader: validation loader\n",
    "    :param device: GPU/CPU device\n",
    "    :param category: dataset type (indoor/outdoor)\n",
    "    :param save_tag: whether to save images\n",
    "    :return: average PSNR & SSIM values\n",
    "    \"\"\"\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    \n",
    "    for batch_id, val_data in enumerate(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            haze, gt = val_data\n",
    "            haze, gt = haze.to(device), gt.to(device)\n",
    "            _, dehaze, _ = net(haze)\n",
    "\n",
    "        # --- Compute PSNR & SSIM --- #\n",
    "        batch_psnr = to_psnr(dehaze, gt)  # This returns a list\n",
    "        # print(batch_psnr)\n",
    "        batch_ssim = to_ssim(dehaze, gt)  # This returns a list\n",
    "        # print(batch_ssim)\n",
    "\n",
    "        psnr_list.extend(batch_psnr)  # Flatten the list\n",
    "        ssim_list.extend(batch_ssim)  # Flatten the list\n",
    "\n",
    "    # --- Ensure lists are not empty to avoid division by zero --- #\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69c34f",
   "metadata": {
    "papermill": {
     "duration": 0.020347,
     "end_time": "2025-05-02T08:43:50.440902",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.420555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Validation SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5aa7e63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:50.480545Z",
     "iopub.status.busy": "2025-05-02T08:43:50.480046Z",
     "iopub.status.idle": "2025-05-02T08:43:50.485302Z",
     "shell.execute_reply": "2025-05-02T08:43:50.484588Z"
    },
    "papermill": {
     "duration": 0.025273,
     "end_time": "2025-05-02T08:43:50.486485",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.461212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validation_sr(net, sr_val_loader, device):\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    for lr, hr in sr_val_loader:\n",
    "        with torch.no_grad():\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr_out, _, _ = net(lr, sr = False)\n",
    "            hr = F.interpolate(hr, size=sr_out.shape[2:], mode='bilinear', align_corners=False)\n",
    "        # print(\"Shapes 1: \", sr_out.shape, hr.shape)\n",
    "        psnr_list.extend(to_psnr(sr_out, hr))\n",
    "        ssim_list.extend(to_ssim(sr_out, hr))\n",
    "\n",
    "    avr_psnr = sum(psnr_list) / len(psnr_list) if psnr_list else 0.0\n",
    "    avr_ssim = sum(ssim_list) / len(ssim_list) if ssim_list else 0.0\n",
    "    return avr_psnr, avr_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df3f88",
   "metadata": {
    "papermill": {
     "duration": 0.016889,
     "end_time": "2025-05-02T08:43:50.521318",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.504429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21e25ff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:50.557215Z",
     "iopub.status.busy": "2025-05-02T08:43:50.556949Z",
     "iopub.status.idle": "2025-05-02T08:43:50.560484Z",
     "shell.execute_reply": "2025-05-02T08:43:50.559746Z"
    },
    "papermill": {
     "duration": 0.022768,
     "end_time": "2025-05-02T08:43:50.561554",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.538786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aab7446b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:50.598208Z",
     "iopub.status.busy": "2025-05-02T08:43:50.597907Z",
     "iopub.status.idle": "2025-05-02T08:43:50.601312Z",
     "shell.execute_reply": "2025-05-02T08:43:50.600602Z"
    },
    "papermill": {
     "duration": 0.022993,
     "end_time": "2025-05-02T08:43:50.602462",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.579469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# psnr, ssim = validationB(net, val_data_loader, device, category)\n",
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # psnr, ssim = validationB(model, val_loader, device, \"indoor\", save_tag=True)\n",
    "# print(f\"Validation PSNR: {psnr:.2f}, SSIM: {ssim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a579cd",
   "metadata": {
    "papermill": {
     "duration": 0.017934,
     "end_time": "2025-05-02T08:43:50.637961",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.620027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20979d86",
   "metadata": {
    "papermill": {
     "duration": 0.016904,
     "end_time": "2025-05-02T08:43:50.672106",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.655202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Exec Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d95e4d78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:50.707949Z",
     "iopub.status.busy": "2025-05-02T08:43:50.707400Z",
     "iopub.status.idle": "2025-05-02T08:43:50.715637Z",
     "shell.execute_reply": "2025-05-02T08:43:50.714957Z"
    },
    "papermill": {
     "duration": 0.027402,
     "end_time": "2025-05-02T08:43:50.716797",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.689395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ca0eca7a5b4c57b06dd242c7d9d70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Execution Env:', index=1, options=('local', 'kaggle'), value='kaggle')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execution_env_widget = widgets.Dropdown(options=['local', 'kaggle'], value='kaggle', description='Execution Env:')\n",
    "display(execution_env_widget)\n",
    "\n",
    "# check if not in windows and not in kaggle\n",
    "if os.path.exists('/kaggle') and os.path.exists('/mnt'):\n",
    "    execution_env_widget.value = 'kaggle' \n",
    "else:\n",
    "    execution_env_widget.value = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b8cc3b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:50.753543Z",
     "iopub.status.busy": "2025-05-02T08:43:50.753284Z",
     "iopub.status.idle": "2025-05-02T08:43:50.784119Z",
     "shell.execute_reply": "2025-05-02T08:43:50.783354Z"
    },
    "papermill": {
     "duration": 0.049946,
     "end_time": "2025-05-02T08:43:50.785205",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.735259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d7a1da20674d48b23b22feb7df641e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Learning Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed58cd5d5944422b512da63b432bedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='128,128', description='Crop Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c863d9efb9db46c49df8d9615dd70485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Train Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec0e1b4b73f4525bc84535ebf99b2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=0, description='Version:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2a0b0961c246a899a926e2e8b82c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='Growth Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5caaf4a5844e6091ba791198feb642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.04, description='Lambda Loss:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ca3bf7448c4da3bfc60e35cbae8ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Val Batch Size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1251f9cd8fb34356a9385a580b1b6a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', index=2, options=('indoor', 'outdoor', 'reside', 'nh'), value='reside')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters set:\n",
      "learning_rate: 0.0001\n",
      "crop_size: [128, 128]\n",
      "train_batch_size: 2\n",
      "version: 0\n",
      "growth_rate: 16\n",
      "lambda_loss: 0.04\n",
      "val_batch_size: 2\n",
      "category: reside\n",
      "execution_env: kaggle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Create widgets for each hyper-parameter ---\n",
    "learning_rate_widget = widgets.FloatText(value=1e-4, description='Learning Rate:')\n",
    "crop_size_widget = widgets.Text(value='128,128', description='Crop Size:')\n",
    "train_batch_size_widget = widgets.IntText(value=2, description='Train Batch Size:')\n",
    "version_widget = widgets.IntText(value=0, description='Version:')\n",
    "growth_rate_widget = widgets.IntText(value=16, description='Growth Rate:')\n",
    "lambda_loss_widget = widgets.FloatText(value=0.04, description='Lambda Loss:')\n",
    "val_batch_size_widget = widgets.IntText(value=2, description='Val Batch Size:')\n",
    "category_widget = widgets.Dropdown(options=['indoor', 'outdoor', 'reside', 'nh'], value='reside', description='Category:')\n",
    "\n",
    "# --- Display the widgets ---\n",
    "display(\n",
    "    learning_rate_widget, crop_size_widget, train_batch_size_widget, version_widget,\n",
    "    growth_rate_widget, lambda_loss_widget, \n",
    "    val_batch_size_widget, category_widget\n",
    ")\n",
    "\n",
    "# --- Function to parse crop size ---\n",
    "def parse_crop_size(crop_size_str):\n",
    "    return [int(x) for x in crop_size_str.split(',')]\n",
    "\n",
    "# --- Assign the widget values to variables ---\n",
    "learning_rate = learning_rate_widget.value\n",
    "crop_size = parse_crop_size(crop_size_widget.value)\n",
    "train_batch_size = train_batch_size_widget.value\n",
    "version = version_widget.value\n",
    "growth_rate = growth_rate_widget.value\n",
    "lambda_loss = lambda_loss_widget.value\n",
    "val_batch_size = val_batch_size_widget.value\n",
    "category = category_widget.value\n",
    "\n",
    "execution_env = execution_env_widget.value  # Local or Kaggle\n",
    "\n",
    "\n",
    "print('\\nHyper-parameters set:')\n",
    "print(f'learning_rate: {learning_rate}')\n",
    "print(f'crop_size: {crop_size}')\n",
    "print(f'train_batch_size: {train_batch_size}')\n",
    "print(f'version: {version}')\n",
    "print(f'growth_rate: {growth_rate}')\n",
    "print(f'lambda_loss: {lambda_loss}')\n",
    "print(f'val_batch_size: {val_batch_size}')\n",
    "print(f'category: {category}')\n",
    "print(f'execution_env: {execution_env}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cca31e",
   "metadata": {
    "papermill": {
     "duration": 0.017457,
     "end_time": "2025-05-02T08:43:50.821321",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.803864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Paths Dehaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd25dce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:50.858463Z",
     "iopub.status.busy": "2025-05-02T08:43:50.858183Z",
     "iopub.status.idle": "2025-05-02T08:43:50.864820Z",
     "shell.execute_reply": "2025-05-02T08:43:50.863962Z"
    },
    "papermill": {
     "duration": 0.026885,
     "end_time": "2025-05-02T08:43:50.866159",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.839274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RESIDE dataset\n",
      "\n",
      "Final dataset paths:\n",
      "Training directory: /kaggle/input/reside128r/cropped_t\n",
      "Validation directory: /kaggle/input/reside128r/cropped_t\n",
      "Number of epochs: 10\n"
     ]
    }
   ],
   "source": [
    "# --- Set category-specific hyper-parameters ---\n",
    "if category == 'indoor':\n",
    "    num_epochs = 1500\n",
    "    train_data_dir = './data/train/indoor/'\n",
    "    val_data_dir = './data/test/SOTS/indoor/'\n",
    "elif category == 'outdoor':\n",
    "    num_epochs = 10\n",
    "    train_data_dir = './data/train/outdoor/'\n",
    "    val_data_dir = './data/test/SOTS/outdoor/'\n",
    "elif category == 'reside':\n",
    "    print('Using RESIDE dataset')\n",
    "    num_epochs = 10\n",
    "    # train_data_dir = '/kaggle/input/reside6k/RESIDE-6K/train'\n",
    "    # train_data_dir = '/kaggle/input/reside-processed/kaggle/working/cropped_train'\n",
    "    train_data_dir = '/kaggle/input/reside128r/cropped_t'\n",
    "    val_data_dir = '/kaggle/input/reside128r/cropped_t'\n",
    "    test_data_dir = '/kaggle/input/reside128r/cropped_v'\n",
    "    if execution_env == 'local':\n",
    "        print('Using local RESIDE dataset')\n",
    "        # train_data_dir = './dataset/reside_processed/kaggle/working/cropped_train'\n",
    "        train_data_dir = './dataset/reside_processed/kaggle/working/cropped_t'\n",
    "        val_data_dir = './dataset/reside_processed/kaggle/working/cropped_v'\n",
    "        # train_data_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/reside_processed/kaggle/working/cropped_train/'\n",
    "elif category == 'nh':\n",
    "    num_epochs = 50\n",
    "    train_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "    val_data_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "else:\n",
    "    raise Exception('Wrong image category. Set it to indoor or outdoor for RESIDE dataset.')\n",
    "\n",
    "# --- Adjust paths based on execution environment ---\n",
    "# if execution_env == 'kaggle':\n",
    "    # train_data_dir = '/kaggle/input/reside-dataset/' + train_data_dir.strip('./')\n",
    "    # val_data_dir = '/kaggle/input/reside-dataset/' + val_data_dir.strip('./')\n",
    "    # train_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T'\n",
    "    # val_data_dir = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V' \n",
    "    # train_data_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "    # val_data_dir = '/kaggle/input/o-haze/O-HAZY/GT' \n",
    "print('\\nFinal dataset paths:')\n",
    "print(f'Training directory: {train_data_dir}')\n",
    "print(f'Validation directory: {val_data_dir}')\n",
    "print(f'Number of epochs: {num_epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fa997db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:50.905640Z",
     "iopub.status.busy": "2025-05-02T08:43:50.905005Z",
     "iopub.status.idle": "2025-05-02T08:43:50.908659Z",
     "shell.execute_reply": "2025-05-02T08:43:50.907966Z"
    },
    "papermill": {
     "duration": 0.024059,
     "end_time": "2025-05-02T08:43:50.909676",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.885617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e56efb47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:50.948116Z",
     "iopub.status.busy": "2025-05-02T08:43:50.947828Z",
     "iopub.status.idle": "2025-05-02T08:43:50.951762Z",
     "shell.execute_reply": "2025-05-02T08:43:50.951182Z"
    },
    "papermill": {
     "duration": 0.025278,
     "end_time": "2025-05-02T08:43:50.952845",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.927567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hazeeffected_images_dir_train = f\"{train_data_dir}/IN\"\n",
    "hazeeffected_images_dir_train = f\"{train_data_dir}/hazy\"\n",
    "hazefree_images_dir_train = f\"{train_data_dir}/GT\"\n",
    "\n",
    "# hazeeffected_images_dir_valid = f\"{val_data_dir}/IN\"\n",
    "hazeeffected_images_dir_valid = f\"{val_data_dir}/hazy\"\n",
    "hazefree_images_dir_valid = f\"{val_data_dir}/GT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6c6562d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:50.991585Z",
     "iopub.status.busy": "2025-05-02T08:43:50.991365Z",
     "iopub.status.idle": "2025-05-02T08:43:50.994499Z",
     "shell.execute_reply": "2025-05-02T08:43:50.993965Z"
    },
    "papermill": {
     "duration": 0.022536,
     "end_time": "2025-05-02T08:43:50.995545",
     "exception": false,
     "start_time": "2025-05-02T08:43:50.973009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # hazeeffected_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/hazy'\n",
    "# # hazefree_images_dir = '/Volumes/S/dev/project/code/Aphase/Dehaze_2/data/NH-Haze_Dense-Haze_datasets/NH-HAZE-T/train/GT'\n",
    "# # hazeeffected_images_dir = '/kaggle/input/o-haze/O-HAZY/hazy'\n",
    "# # hazefree_images_dir = '/kaggle/input/o-haze/O-HAZY/GT'\n",
    "\n",
    "# hazeeffected_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN'\n",
    "# hazefree_images_dir_train = '/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT'\n",
    "# hazeeffected_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/IN'\n",
    "# hazefree_images_dir_valid = '/kaggle/input/nh-dense-haze/NH-HAZE-V/NH-HAZE-V/GT'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252fc99",
   "metadata": {
    "papermill": {
     "duration": 0.017517,
     "end_time": "2025-05-02T08:43:51.030662",
     "exception": false,
     "start_time": "2025-05-02T08:43:51.013145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Paths SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "959b531d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:51.067140Z",
     "iopub.status.busy": "2025-05-02T08:43:51.066541Z",
     "iopub.status.idle": "2025-05-02T08:43:51.070603Z",
     "shell.execute_reply": "2025-05-02T08:43:51.069905Z"
    },
    "papermill": {
     "duration": 0.023451,
     "end_time": "2025-05-02T08:43:51.071682",
     "exception": false,
     "start_time": "2025-05-02T08:43:51.048231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sr_enabled = True\n",
    "    # sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n",
    "    # sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X4'\n",
    "if sr_enabled:\n",
    "    sr_hr_dir = '/kaggle/input/flickr-p/working/filtered_HR'\n",
    "    sr_lr_dir = '/kaggle/input/flickr-p/working/filtered_LR_2'\n",
    "    # sr_hr_dir = '/kaggle/input/sr-flickr/kaggle/working/filtered_HR'\n",
    "    # sr_lr_dir = '/kaggle/input/sr-flickr/kaggle/working/filtered_LR_2'\n",
    "    # sr_hr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_HR'\n",
    "    # sr_lr_dir = '/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X4'\n",
    "    if execution_env == 'local':\n",
    "        # sr_hr_dir = './dataset/Flickr2K/Flickr2K_HR'\n",
    "        # sr_lr_dir = './dataset/Flickr2K/Flickr2K_LR_unknown/X4'\n",
    "        sr_hr_dir = './dataset/SR_flickr/kaggle/working/filtered_HR'\n",
    "        sr_lr_dir = './dataset/SR_flickr/kaggle/working/filtered_LR_2'\n",
    "        # sr_lr_dir = './dataset/SR_flickr/kaggle/working/filtered_LR/X4'\n",
    "        \n",
    "        # sr_hr_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/SR_flickr/kaggle/working/filtered_HR'\n",
    "        # sr_lr_dir = '/Volumes/S/dev/project/code/Aphase/dehaze/dataset/SR_flickr/kaggle/working/filtered_LR/X4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5becf6cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:51.108255Z",
     "iopub.status.busy": "2025-05-02T08:43:51.107727Z",
     "iopub.status.idle": "2025-05-02T08:43:51.111328Z",
     "shell.execute_reply": "2025-05-02T08:43:51.110779Z"
    },
    "papermill": {
     "duration": 0.022772,
     "end_time": "2025-05-02T08:43:51.112318",
     "exception": false,
     "start_time": "2025-05-02T08:43:51.089546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Define paths\n",
    "# input_dir = 'dataset/SR_flickr/kaggle/working/filtered_HR'\n",
    "# output_dir = 'dataset/SR_flickr/kaggle/working/filtered_LR_2'\n",
    "\n",
    "# # Create output directory if it doesn't exist\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Function to resize images to half their resolution\n",
    "# def resize_image(input_path, output_path, target_size=(128, 128)):\n",
    "#     img = Image.open(input_path)\n",
    "#     img_resized = img.resize(target_size, Image.LANCZOS)\n",
    "#     img_resized.save(output_path)\n",
    "\n",
    "# # Get list of images in input directory\n",
    "# try:\n",
    "#     images = [f for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "# except FileNotFoundError as e:\n",
    "#     images = []\n",
    "#     error_message = str(e)\n",
    "\n",
    "# # Resize each image and save to output directory using tqdm\n",
    "# if images:\n",
    "#     for image_name in tqdm(images, desc=\"Resizing images\"):\n",
    "#         input_path = os.path.join(input_dir, image_name)\n",
    "#         output_path = os.path.join(output_dir, image_name)\n",
    "#         resize_image(input_path, output_path, target_size=(128, 128))\n",
    "#     result = \"Resizing completed\"\n",
    "# else:\n",
    "#     result = f\"Error: {error_message}\"\n",
    "\n",
    "# result = \"Resizing completed\" if images else f\"Error: {error_message}\"\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91f1f481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:51.149041Z",
     "iopub.status.busy": "2025-05-02T08:43:51.148763Z",
     "iopub.status.idle": "2025-05-02T08:43:51.153355Z",
     "shell.execute_reply": "2025-05-02T08:43:51.152812Z"
    },
    "papermill": {
     "duration": 0.024198,
     "end_time": "2025-05-02T08:43:51.154428",
     "exception": false,
     "start_time": "2025-05-02T08:43:51.130230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_log(epoch, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, category):\n",
    "    log_dir = \"./training_log\"\n",
    "    os.makedirs(log_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    log_path = os.path.join(log_dir, f\"{category}_log.txt\")\n",
    "\n",
    "    print('({0:.0f}s) Epoch [{1}/{2}], Train_PSNR:{3:.2f}, Val_PSNR:{4:.2f}, Val_SSIM:{5:.4f}'\n",
    "          .format(one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim))\n",
    "\n",
    "    # --- Write the training log --- #\n",
    "    with open(log_path, 'a') as f:\n",
    "        print('Date: {0}, Time_Cost: {1:.0f}s, Epoch: [{2}/{3}], Train_PSNR: {4:.2f}, Val_PSNR: {5:.2f}, Val_SSIM: {6:.4f}'\n",
    "              .format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
    "                      one_epoch_time, epoch, num_epochs, train_psnr, val_psnr, val_ssim), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b1161cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:51.191802Z",
     "iopub.status.busy": "2025-05-02T08:43:51.191537Z",
     "iopub.status.idle": "2025-05-02T08:43:51.194951Z",
     "shell.execute_reply": "2025-05-02T08:43:51.194397Z"
    },
    "papermill": {
     "duration": 0.023144,
     "end_time": "2025-05-02T08:43:51.196023",
     "exception": false,
     "start_time": "2025-05-02T08:43:51.172879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def adjust_learning_rate(optimizer, epoch, category, lr_decay=0.90):\n",
    "#     \"\"\"\n",
    "#     Adjusts the learning rate based on the epoch and dataset category.\n",
    "\n",
    "#     :param optimizer: The optimizer (e.g., Adam, SGD).\n",
    "#     :param epoch: Current epoch number.\n",
    "#     :param category: Dataset category ('indoor', 'outdoor', or 'NH').\n",
    "#     :param lr_decay: Multiplicative factor for learning rate decay.\n",
    "#     \"\"\"\n",
    "#     # Define learning rate decay steps based on category\n",
    "#     step_dict = {'indoor': 18, 'outdoor': 3, 'NH': 20}\n",
    "#     step = step_dict.get(category, 3)  # Default step size if category is unknown\n",
    "\n",
    "#     # Decay learning rate at the specified step\n",
    "#     if epoch > 0 and epoch % step == 0:\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] *= lr_decay\n",
    "#             print(f\"Epoch {epoch}: Learning rate adjusted to {param_group['lr']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ce7fc",
   "metadata": {
    "papermill": {
     "duration": 0.017798,
     "end_time": "2025-05-02T08:43:51.232135",
     "exception": false,
     "start_time": "2025-05-02T08:43:51.214337",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e55016a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:51.269347Z",
     "iopub.status.busy": "2025-05-02T08:43:51.269073Z",
     "iopub.status.idle": "2025-05-02T08:43:51.280318Z",
     "shell.execute_reply": "2025-05-02T08:43:51.279744Z"
    },
    "papermill": {
     "duration": 0.031306,
     "end_time": "2025-05-02T08:43:51.281415",
     "exception": false,
     "start_time": "2025-05-02T08:43:51.250109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Perceptual Feature Loss Network --- #\n",
    "class PerceptualLossNet(nn.Module):\n",
    "    def __init__(self, vgg_model, use_style_loss=False, style_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = vgg_model\n",
    "        self.feature_layers = {'3': \"low_level\", '8': \"mid_level\", '15': \"high_level\"}\n",
    "        self.use_style_loss = use_style_loss\n",
    "        self.style_weight = style_weight\n",
    "\n",
    "        # Freeze VGG\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def get_feature_maps(self, x):\n",
    "        feature_maps = []\n",
    "        for layer_id, layer in self.feature_extractor.named_children():\n",
    "            x = layer(x)\n",
    "            if layer_id in self.feature_layers:\n",
    "                feature_maps.append(x)\n",
    "        return feature_maps\n",
    "\n",
    "    def compute_gram(self, feature):\n",
    "        b, c, h, w = feature.size()\n",
    "        feature = feature.view(b, c, -1)\n",
    "        gram = torch.bmm(feature, feature.transpose(1, 2)) / (c * h * w)\n",
    "        return gram\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        pred_feats = self.get_feature_maps(predicted)\n",
    "        target_feats = self.get_feature_maps(target)\n",
    "\n",
    "        # Perceptual loss (normalized MSE)\n",
    "        perceptual_loss = torch.stack([\n",
    "            F.mse_loss(F.normalize(p, dim=1), F.normalize(t, dim=1))\n",
    "            for p, t in zip(pred_feats, target_feats)\n",
    "        ]).mean()\n",
    "\n",
    "        if self.use_style_loss:\n",
    "            style_loss = torch.stack([\n",
    "                F.mse_loss(self.compute_gram(p), self.compute_gram(t))\n",
    "                for p, t in zip(pred_feats, target_feats)\n",
    "            ]).mean()\n",
    "            return perceptual_loss + self.style_weight * style_loss\n",
    "\n",
    "        return perceptual_loss\n",
    "\n",
    "\n",
    "class SSFM(nn.Module):\n",
    "    def __init__(self, loss_type='l1'):\n",
    "        super(SSFM, self).__init__()\n",
    "        assert loss_type in ['l1', 'l2'], \"loss_type must be 'l1' or 'l2'\"\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def forward(self, student_feats, teacher_feats):\n",
    "        \"\"\"\n",
    "        student_feats: List of feature maps from student RDBs [rdb1, rdb2, rdb3, rdb4]\n",
    "        teacher_feats: List of corresponding feature maps from teacher\n",
    "        \"\"\"\n",
    "        assert len(student_feats) == len(teacher_feats), \"Feature lists must match\"\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for s_feat, t_feat in zip(student_feats, teacher_feats):\n",
    "            # Match resolution\n",
    "            if s_feat.shape != t_feat.shape:\n",
    "                t_feat = F.interpolate(t_feat, size=s_feat.shape[2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            if self.loss_type == 'l1':\n",
    "                loss = F.l1_loss(s_feat, t_feat)\n",
    "            else:\n",
    "                loss = F.mse_loss(s_feat, t_feat)\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss / len(student_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e764152",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:51.320337Z",
     "iopub.status.busy": "2025-05-02T08:43:51.320100Z",
     "iopub.status.idle": "2025-05-02T08:43:55.661421Z",
     "shell.execute_reply": "2025-05-02T08:43:55.660592Z"
    },
    "papermill": {
     "duration": 4.361578,
     "end_time": "2025-05-02T08:43:55.662600",
     "exception": false,
     "start_time": "2025-05-02T08:43:51.301022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:02<00:00, 220MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No pretrained weights found at /kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\n",
      "📊 Total Trainable Parameters: 5,210,051\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "\n",
    "\n",
    "# --- Initialize Model --- #\n",
    "net = DeepGuidedNet().to(device)\n",
    "\n",
    "# --- Enable Multi-GPU (if available) --- #\n",
    "if len(device_ids) > 1:\n",
    "    net = nn.DataParallel(net, device_ids=device_ids)\n",
    "\n",
    "# --- Optimizer --- #\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Load Pretrained VGG16 for Perceptual Loss --- #\n",
    "vgg_features = vgg16(pretrained=True).features[:16].to(device)\n",
    "for param in vgg_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "loss_network = PerceptualLossNet(vgg_features)\n",
    "loss_network.eval()\n",
    "\n",
    "# --- Load Model Weights (if available) --- #\n",
    "checkpoint_path = \"/kaggle/input/reside-dehaze/pytorch/default/2/formernewreside_haze_iter_85.pth\" \n",
    "\n",
    "try:\n",
    "    net.load_state_dict(torch.load(checkpoint_path, weights_only=False, map_location=torch.device('cpu')))\n",
    "    print(f\"✅ Model weights loaded from {checkpoint_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️ No pretrained weights found at {checkpoint_path}\")\n",
    "\n",
    "# --- Compute Total Trainable Parameters --- #\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"📊 Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26fbf2b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:55.702381Z",
     "iopub.status.busy": "2025-05-02T08:43:55.702155Z",
     "iopub.status.idle": "2025-05-02T08:43:55.708207Z",
     "shell.execute_reply": "2025-05-02T08:43:55.707488Z"
    },
    "papermill": {
     "duration": 0.026671,
     "end_time": "2025-05-02T08:43:55.709265",
     "exception": false,
     "start_time": "2025-05-02T08:43:55.682594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeatureAffinityModule(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(FeatureAffinityModule, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "    def forward(self, student_features, teacher_features):\n",
    "        # Debug: Print input shapes\n",
    "        # print(\"Input student_features shape:\", student_features.shape)\n",
    "        # print(\"Input teacher_features shape:\", teacher_features.shape)\n",
    "\n",
    "        # # Normalize features\n",
    "        # print(\"\\nBefore normalization:\")\n",
    "        # print(\"Student features:\", student_features.shape)  # Example for the first feature of the first batch\n",
    "        # print(\"Teacher features:\", teacher_features.shape)  # Example for the first feature of the first batch\n",
    "\n",
    "        student_norm = F.normalize(student_features.view(student_features.size(0), self.channels, -1), dim=2)\n",
    "        # print(\"Student normalized features:\", student_norm.shape)  # Checking first normalized value\n",
    "\n",
    "        # print(\"\\nother:\")\n",
    "        # print(\"student_features.size(0)\",student_features.size(0))\n",
    "        # print(\"teacher_features.size(0)\",teacher_features.size(0))\n",
    "        # print(\"self.channels\",self.channels)\n",
    "        # print(\"student_features.view(student_features.size(0), self.channels, -1)\",student_features.view(student_features.size(0), self.channels, -1).shape)\n",
    "        # print(\"teacher_features.view(teacher_features.size(0), self.channels, -1)\",teacher_features.view(teacher_features.size(0), self.channels, -1).shape)\n",
    "        \n",
    "        teacher_norm = F.normalize(teacher_features.view(teacher_features.size(0), self.channels, -1), dim=2)\n",
    "        # print(\"Teacher normalized features:\", teacher_norm[0, 0, :2])  # Checking first normalized value\n",
    "\n",
    "        # Compute affinity matrices\n",
    "        student_affinity = torch.bmm(student_norm, student_norm.transpose(1, 2))\n",
    "        teacher_affinity = torch.bmm(teacher_norm, teacher_norm.transpose(1, 2))\n",
    "\n",
    "        # Debug: Print affinity matrix shapes\n",
    "        # print(\"\\nStudent affinity matrix shape:\", student_affinity.shape)\n",
    "        # print(\"Teacher affinity matrix shape:\", teacher_affinity.shape)\n",
    "\n",
    "        # Compute KL divergence\n",
    "        loss = F.kl_div(F.log_softmax(student_affinity, dim=-1),\n",
    "                        F.softmax(teacher_affinity, dim=-1),\n",
    "                        reduction='batchmean')\n",
    "\n",
    "        # Debug: Print computed loss\n",
    "        # print(\"\\nComputed loss:\", loss.item())\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2bfaf31f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:55.748199Z",
     "iopub.status.busy": "2025-05-02T08:43:55.748011Z",
     "iopub.status.idle": "2025-05-02T08:43:55.750770Z",
     "shell.execute_reply": "2025-05-02T08:43:55.750293Z"
    },
    "papermill": {
     "duration": 0.023394,
     "end_time": "2025-05-02T08:43:55.751779",
     "exception": false,
     "start_time": "2025-05-02T08:43:55.728385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5604045",
   "metadata": {
    "papermill": {
     "duration": 0.018913,
     "end_time": "2025-05-02T08:43:55.789871",
     "exception": false,
     "start_time": "2025-05-02T08:43:55.770958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72bc21b",
   "metadata": {
    "papermill": {
     "duration": 0.018603,
     "end_time": "2025-05-02T08:43:55.827584",
     "exception": false,
     "start_time": "2025-05-02T08:43:55.808981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Haze Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dff49edf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:55.866698Z",
     "iopub.status.busy": "2025-05-02T08:43:55.866203Z",
     "iopub.status.idle": "2025-05-02T08:43:55.876798Z",
     "shell.execute_reply": "2025-05-02T08:43:55.876096Z"
    },
    "papermill": {
     "duration": 0.031658,
     "end_time": "2025-05-02T08:43:55.878090",
     "exception": false,
     "start_time": "2025-05-02T08:43:55.846432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class HazeDataset(Dataset):\n",
    "    def __init__(self, hazeeffected_images_dir, hazefree_images_dir, split=\"train\", split_ratio=(0.8, 0.1, 0.1), random_seed=42):\n",
    "        \"\"\"\n",
    "        Dataset class for handling hazy and corresponding ground-truth images (already aligned and cropped).\n",
    "\n",
    "        Args:\n",
    "            hazeeffected_images_dir (str): Directory for hazy images.\n",
    "            hazefree_images_dir (str): Directory for ground-truth images.\n",
    "            split (str): \"train\", \"valid\", or \"test\" (determines data split).\n",
    "            split_ratio (tuple): A tuple of three floats representing the train, validation, and test splits.\n",
    "                                 (default 80% train, 10% validation, 10% test).\n",
    "            random_seed (int): Random seed for reproducibility in shuffling data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if sum(split_ratio) != 1.0:\n",
    "            raise ValueError(\"The sum of split_ratio must be 1.0\")\n",
    "\n",
    "        self.train_ratio, self.valid_ratio, self.test_ratio = split_ratio\n",
    "\n",
    "        valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        # print(\"Haze affected images dir: \", hazeeffected_images_dir)\n",
    "        hazy_data = [\n",
    "            f for f in glob.glob(os.path.join(hazeeffected_images_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_extensions)\n",
    "        ]\n",
    "\n",
    "        if not hazy_data:\n",
    "            raise ValueError(f\"No valid images found in {hazeeffected_images_dir}\")\n",
    "\n",
    "        hazy_data.sort()\n",
    "\n",
    "        # Shuffle the dataset for randomization\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(hazy_data)\n",
    "\n",
    "        # Split dataset based on provided ratios\n",
    "        total_images = len(hazy_data)\n",
    "        train_size = int(total_images * self.train_ratio)\n",
    "        valid_size = int(total_images * self.valid_ratio)\n",
    "        test_size = total_images - train_size - valid_size\n",
    "\n",
    "        if split == \"train\":\n",
    "            hazy_data = hazy_data[:train_size]\n",
    "        elif split == \"valid\":\n",
    "            hazy_data = hazy_data[train_size:train_size+valid_size]\n",
    "        elif split == \"test\":\n",
    "            hazy_data = hazy_data[train_size+valid_size:]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split value: {split}. Choose from ['train', 'valid', 'test'].\")\n",
    "\n",
    "        self.haze_names = []\n",
    "        self.gt_names = []\n",
    "\n",
    "        for h_image in hazy_data:\n",
    "            filename = os.path.basename(h_image)\n",
    "            haze_path = os.path.join(hazeeffected_images_dir, filename)\n",
    "            gt_path = os.path.join(hazefree_images_dir, filename)\n",
    "\n",
    "            if not os.path.exists(gt_path):\n",
    "                print(f\"Warning: Ground-truth missing for {filename}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            self.haze_names.append(haze_path)\n",
    "            self.gt_names.append(gt_path)\n",
    "\n",
    "        if not self.haze_names:\n",
    "            raise ValueError(\"No matching ground-truth images found.\")\n",
    "\n",
    "        # Define transforms\n",
    "        self.transform_haze = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        self.transform_gt = Compose([ToTensor()])\n",
    "\n",
    "    def get_images(self, index):\n",
    "        haze_name = self.haze_names[index]\n",
    "        gt_name = self.gt_names[index]\n",
    "\n",
    "        try:\n",
    "            haze_img = Image.open(haze_name).convert('RGB')\n",
    "            gt_img = Image.open(gt_name).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Invalid image format: {haze_name} or {gt_name}\")\n",
    "\n",
    "        haze = self.transform_haze(haze_img)\n",
    "        gt = self.transform_gt(gt_img)\n",
    "\n",
    "        if haze.shape[0] != 3 or gt.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {haze_name}\")\n",
    "\n",
    "        return haze, gt\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_images(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d63f7",
   "metadata": {
    "papermill": {
     "duration": 0.018775,
     "end_time": "2025-05-02T08:43:55.916122",
     "exception": false,
     "start_time": "2025-05-02T08:43:55.897347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Haze DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "959b6ed0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:43:55.955289Z",
     "iopub.status.busy": "2025-05-02T08:43:55.954607Z",
     "iopub.status.idle": "2025-05-02T08:44:04.254570Z",
     "shell.execute_reply": "2025-05-02T08:44:04.253973Z"
    },
    "papermill": {
     "duration": 8.320939,
     "end_time": "2025-05-02T08:44:04.255877",
     "exception": false,
     "start_time": "2025-05-02T08:43:55.934938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For training:\n",
    "train_dataset = HazeDataset(hazeeffected_images_dir=hazeeffected_images_dir_train, \n",
    "                             hazefree_images_dir=hazefree_images_dir_train, \n",
    "                             split=\"train\", \n",
    "                             split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "# For validation:\n",
    "val_dataset = HazeDataset(hazeeffected_images_dir=hazeeffected_images_dir_train, \n",
    "                             hazefree_images_dir=hazefree_images_dir_train, \n",
    "                             split=\"valid\", \n",
    "                             split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "# For testing:\n",
    "test_dataset = HazeDataset(hazeeffected_images_dir=hazeeffected_images_dir_train, \n",
    "                            hazefree_images_dir=hazefree_images_dir_train, \n",
    "                            split=\"test\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6592a8f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:04.297748Z",
     "iopub.status.busy": "2025-05-02T08:44:04.297303Z",
     "iopub.status.idle": "2025-05-02T08:44:04.301142Z",
     "shell.execute_reply": "2025-05-02T08:44:04.300544Z"
    },
    "papermill": {
     "duration": 0.025205,
     "end_time": "2025-05-02T08:44:04.302233",
     "exception": false,
     "start_time": "2025-05-02T08:44:04.277028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4800, Validation samples: 600, Test samples: 600\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "80fd3942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:04.343212Z",
     "iopub.status.busy": "2025-05-02T08:44:04.342557Z",
     "iopub.status.idle": "2025-05-02T08:44:04.346579Z",
     "shell.execute_reply": "2025-05-02T08:44:04.345900Z"
    },
    "papermill": {
     "duration": 0.026397,
     "end_time": "2025-05-02T08:44:04.347662",
     "exception": false,
     "start_time": "2025-05-02T08:44:04.321265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=val_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b815c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:04.386911Z",
     "iopub.status.busy": "2025-05-02T08:44:04.386356Z",
     "iopub.status.idle": "2025-05-02T08:44:04.389600Z",
     "shell.execute_reply": "2025-05-02T08:44:04.388964Z"
    },
    "papermill": {
     "duration": 0.024049,
     "end_time": "2025-05-02T08:44:04.390647",
     "exception": false,
     "start_time": "2025-05-02T08:44:04.366598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(glob.glob( \"/kaggle/working/cropped_train/hazy/*\")), len(glob.glob(\"/kaggle/working/cropped_train/GT/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46237266",
   "metadata": {
    "papermill": {
     "duration": 0.01894,
     "end_time": "2025-05-02T08:44:04.429021",
     "exception": false,
     "start_time": "2025-05-02T08:44:04.410081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f324372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:04.468492Z",
     "iopub.status.busy": "2025-05-02T08:44:04.468218Z",
     "iopub.status.idle": "2025-05-02T08:44:04.477804Z",
     "shell.execute_reply": "2025-05-02T08:44:04.477134Z"
    },
    "papermill": {
     "duration": 0.03075,
     "end_time": "2025-05-02T08:44:04.478865",
     "exception": false,
     "start_time": "2025-05-02T08:44:04.448115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, scale='x4', split='train', split_ratio=(0.8, 0.1, 0.1), random_seed=42):\n",
    "        \"\"\"\n",
    "        Super-Resolution dataset that matches LR and HR image pairs based on naming pattern.\n",
    "        Assumes images are already aligned and correctly scaled. No cropping is applied.\n",
    "\n",
    "        Args:\n",
    "            lr_dir (str): Directory containing low-resolution images (e.g., x2, x3, x4).\n",
    "            hr_dir (str): Directory containing high-resolution images.\n",
    "            scale (str): Scale suffix in LR filenames (e.g., 'x2', 'x3', 'x4').\n",
    "            split (str): 'train', 'val', or 'test' (determines data split).\n",
    "            split_ratio (tuple): A tuple of three floats representing the train, validation, and test splits.\n",
    "                                 (default 80% train, 10% validation, 10% test).\n",
    "            random_seed (int): Random seed for reproducibility in shuffling data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if sum(split_ratio) != 1.0:\n",
    "            raise ValueError(\"The sum of split_ratio must be 1.0\")\n",
    "\n",
    "        self.train_ratio, self.valid_ratio, self.test_ratio = split_ratio\n",
    "\n",
    "        valid_ext = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\")\n",
    "        lr_images = sorted([\n",
    "            f for f in glob.glob(os.path.join(lr_dir, \"*.*\"))\n",
    "            if f.lower().endswith(valid_ext)\n",
    "        ])\n",
    "\n",
    "        lr_hr_pairs = []\n",
    "        for lr_path in lr_images:\n",
    "            lr_name = os.path.basename(lr_path)\n",
    "            hr_name = lr_name.replace(scale, '')  # assumes naming like image_x4.png -> image.png\n",
    "            hr_path = os.path.join(hr_dir, hr_name)\n",
    "\n",
    "            if not os.path.exists(hr_path):\n",
    "                print(f\"Warning: Ground-truth missing for {lr_name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            lr_hr_pairs.append((lr_path, hr_path))\n",
    "\n",
    "        if not lr_hr_pairs:\n",
    "            raise ValueError(\"No matching LR-HR image pairs found.\")\n",
    "\n",
    "        # Shuffle the dataset for randomization\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(lr_hr_pairs)\n",
    "\n",
    "        # Split dataset based on provided ratios\n",
    "        total_pairs = len(lr_hr_pairs)\n",
    "        train_size = int(total_pairs * self.train_ratio)\n",
    "        valid_size = int(total_pairs * self.valid_ratio)\n",
    "        test_size = total_pairs - train_size - valid_size\n",
    "\n",
    "        # Splitting the data based on the chosen split\n",
    "        if split == 'train':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[:train_size]\n",
    "        elif split == 'val':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[train_size:train_size+valid_size]\n",
    "        elif split == 'test':\n",
    "            self.lr_hr_pairs = lr_hr_pairs[train_size+valid_size:]\n",
    "        else:\n",
    "            raise ValueError(\"split must be either 'train', 'val', or 'test'\")\n",
    "\n",
    "        # Define common transform\n",
    "        self.transform = Compose([ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_hr_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_path, hr_path = self.lr_hr_pairs[idx]\n",
    "\n",
    "        try:\n",
    "            lr_img = Image.open(lr_path).convert('RGB')\n",
    "            hr_img = Image.open(hr_path).convert('RGB')\n",
    "        except UnidentifiedImageError:\n",
    "            raise ValueError(f\"Unidentified image at {lr_path} or {hr_path}\")\n",
    "\n",
    "        lr_tensor = self.transform(lr_img)\n",
    "        hr_tensor = self.transform(hr_img)\n",
    "\n",
    "        if lr_tensor.shape[0] != 3 or hr_tensor.shape[0] != 3:\n",
    "            raise ValueError(f\"Invalid image channels: {lr_path}\")\n",
    "\n",
    "        return lr_tensor, hr_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be1ed0",
   "metadata": {
    "papermill": {
     "duration": 0.018956,
     "end_time": "2025-05-02T08:44:04.517173",
     "exception": false,
     "start_time": "2025-05-02T08:44:04.498217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### SR DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0525366e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:04.556851Z",
     "iopub.status.busy": "2025-05-02T08:44:04.556117Z",
     "iopub.status.idle": "2025-05-02T08:44:04.560510Z",
     "shell.execute_reply": "2025-05-02T08:44:04.560023Z"
    },
    "papermill": {
     "duration": 0.025188,
     "end_time": "2025-05-02T08:44:04.561514",
     "exception": false,
     "start_time": "2025-05-02T08:44:04.536326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def custom_collate_fn(batch):\n",
    "#     min_height = min([x[0].shape[1] for x in batch])\n",
    "#     min_width = min([x[0].shape[2] for x in batch])\n",
    "#     resized_batch = [(TF.resize(x[0], [min_height, min_width]), TF.resize(x[1], [min_height, min_width])) for x in batch]\n",
    "#     return torch.utils.data.dataloader.default_collate(resized_batch)\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Assumes each item in batch is a tuple: (input_tensor, gt_tensor)\n",
    "    # Input is already 64x64, ground truth is 256x256\n",
    "    resized_batch = []\n",
    "    for input_tensor, gt_tensor in batch:\n",
    "        # Keep input as-is, downsample ground truth to 64x64\n",
    "        resized_gt = TF.resize(gt_tensor, [64, 64], interpolation=TF.InterpolationMode.NEAREST)\n",
    "        resized_batch.append((input_tensor, resized_gt))\n",
    "    return torch.utils.data.dataloader.default_collate(resized_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3af46d80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:04.601584Z",
     "iopub.status.busy": "2025-05-02T08:44:04.600898Z",
     "iopub.status.idle": "2025-05-02T08:44:08.502275Z",
     "shell.execute_reply": "2025-05-02T08:44:08.501458Z"
    },
    "papermill": {
     "duration": 3.922796,
     "end_time": "2025-05-02T08:44:08.503672",
     "exception": false,
     "start_time": "2025-05-02T08:44:04.580876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- SR Dataset Setup --- #\n",
    "\n",
    "if sr_enabled:\n",
    "\n",
    "    # Dataset for Super-Resolution (SR) task\n",
    "    # For training:\n",
    "    sr_train_dataset = SRDataset(lr_dir=sr_lr_dir, \n",
    "                            hr_dir=sr_hr_dir,\n",
    "                            scale=\"x4\", \n",
    "                            split=\"train\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "    # For validation:\n",
    "    sr_val_dataset = SRDataset(lr_dir=sr_lr_dir, \n",
    "                            hr_dir=sr_hr_dir, \n",
    "                            scale=\"x4\", \n",
    "                            split=\"val\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n",
    "\n",
    "    # For testing:\n",
    "    sr_test_dataset = SRDataset(lr_dir=sr_lr_dir, \n",
    "                            hr_dir=sr_hr_dir, \n",
    "                            scale=\"x4\", \n",
    "                            split=\"test\", \n",
    "                            split_ratio=(0.8, 0.1, 0.1))\n",
    "    \n",
    "    # DataLoader for training with drop_last=True to avoid smaller batches\n",
    "    sr_train_loader = DataLoader(\n",
    "        sr_train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        # collate_fn=custom_collate_fn,\n",
    "        drop_last=True  # Ensure the final batch is dropped if not full\n",
    "    )\n",
    "    \n",
    "    # DataLoader for validation with drop_last=True for consistency (optional)\n",
    "    sr_val_loader = DataLoader(\n",
    "        sr_val_dataset,\n",
    "        batch_size=val_batch_size,\n",
    "        shuffle=False,  # Don't shuffle the validation set\n",
    "        # collate_fn=custom_collate_fn,\n",
    "        drop_last=True  # Optional: consider it for consistency across train/val\n",
    "    )\n",
    "\n",
    "    # DataLoader for testing (No shuffling and no drop_last)\n",
    "    sr_test_loader = DataLoader(\n",
    "        sr_test_dataset,\n",
    "        batch_size=val_batch_size,  # Define your test batch size\n",
    "        shuffle=False,  # No shuffling needed for testing\n",
    "        # collate_fn=custom_collate_fn,  # Use custom collate function if needed\n",
    "        drop_last=False  # We typically do not drop the last batch for testing\n",
    "    )\n",
    "\n",
    "    # Create an iterator for the training set\n",
    "    sr_iter = iter(sr_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ffe10ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:08.544750Z",
     "iopub.status.busy": "2025-05-02T08:44:08.544483Z",
     "iopub.status.idle": "2025-05-02T08:44:08.548617Z",
     "shell.execute_reply": "2025-05-02T08:44:08.547859Z"
    },
    "papermill": {
     "duration": 0.025568,
     "end_time": "2025-05-02T08:44:08.549599",
     "exception": false,
     "start_time": "2025-05-02T08:44:08.524031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR Train samples: 2120, SR Validation samples: 265, SR Test samples: 265\n"
     ]
    }
   ],
   "source": [
    "print(f\"SR Train samples: {len(sr_train_dataset)}, SR Validation samples: {len(sr_val_dataset)}, SR Test samples: {len(sr_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ebe5b360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:08.590072Z",
     "iopub.status.busy": "2025-05-02T08:44:08.589532Z",
     "iopub.status.idle": "2025-05-02T08:44:08.649439Z",
     "shell.execute_reply": "2025-05-02T08:44:08.648778Z"
    },
    "papermill": {
     "duration": 0.081234,
     "end_time": "2025-05-02T08:44:08.650591",
     "exception": false,
     "start_time": "2025-05-02T08:44:08.569357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128]) torch.Size([2, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for i,o in train_data_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "043279d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:08.691204Z",
     "iopub.status.busy": "2025-05-02T08:44:08.690959Z",
     "iopub.status.idle": "2025-05-02T08:44:08.717323Z",
     "shell.execute_reply": "2025-05-02T08:44:08.716495Z"
    },
    "papermill": {
     "duration": 0.047813,
     "end_time": "2025-05-02T08:44:08.718456",
     "exception": false,
     "start_time": "2025-05-02T08:44:08.670643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128]) torch.Size([2, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for i,o in val_data_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e104d9b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:08.761580Z",
     "iopub.status.busy": "2025-05-02T08:44:08.761351Z",
     "iopub.status.idle": "2025-05-02T08:44:08.789778Z",
     "shell.execute_reply": "2025-05-02T08:44:08.788986Z"
    },
    "papermill": {
     "duration": 0.049982,
     "end_time": "2025-05-02T08:44:08.790970",
     "exception": false,
     "start_time": "2025-05-02T08:44:08.740988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128]) torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i,o in sr_train_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8108c863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:08.831738Z",
     "iopub.status.busy": "2025-05-02T08:44:08.831315Z",
     "iopub.status.idle": "2025-05-02T08:44:08.860039Z",
     "shell.execute_reply": "2025-05-02T08:44:08.859312Z"
    },
    "papermill": {
     "duration": 0.049732,
     "end_time": "2025-05-02T08:44:08.861234",
     "exception": false,
     "start_time": "2025-05-02T08:44:08.811502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 128, 128]) torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i,o in sr_val_loader:\n",
    "    print(i.shape, o.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4a888e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:08.902097Z",
     "iopub.status.busy": "2025-05-02T08:44:08.901612Z",
     "iopub.status.idle": "2025-05-02T08:44:08.906296Z",
     "shell.execute_reply": "2025-05-02T08:44:08.905718Z"
    },
    "papermill": {
     "duration": 0.026069,
     "end_time": "2025-05-02T08:44:08.907334",
     "exception": false,
     "start_time": "2025-05-02T08:44:08.881265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2120, 265, 265)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sr_train_dataset), len(sr_val_dataset), len(sr_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "32ea0e25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:08.948124Z",
     "iopub.status.busy": "2025-05-02T08:44:08.947491Z",
     "iopub.status.idle": "2025-05-02T08:44:09.070202Z",
     "shell.execute_reply": "2025-05-02T08:44:09.069503Z"
    },
    "papermill": {
     "duration": 0.144279,
     "end_time": "2025-05-02T08:44:09.071351",
     "exception": false,
     "start_time": "2025-05-02T08:44:08.927072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 1: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 2: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 3: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n",
      "Batch 4: LR shape: torch.Size([2, 3, 128, 128]), HR shape: torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# enumerate\n",
    "for i, (lr, hr) in enumerate(sr_train_loader):\n",
    "    print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "    if i == 4:  # Just to limit the output\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b148610",
   "metadata": {
    "papermill": {
     "duration": 0.020042,
     "end_time": "2025-05-02T08:44:09.111936",
     "exception": false,
     "start_time": "2025-05-02T08:44:09.091894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Init Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b064371d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:09.153252Z",
     "iopub.status.busy": "2025-05-02T08:44:09.152634Z",
     "iopub.status.idle": "2025-05-02T08:44:09.169087Z",
     "shell.execute_reply": "2025-05-02T08:44:09.168503Z"
    },
    "papermill": {
     "duration": 0.038665,
     "end_time": "2025-05-02T08:44:09.170407",
     "exception": false,
     "start_time": "2025-05-02T08:44:09.131742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Teacher Network --- #\n",
    "sr_net = DeepGuidedNetwork(radius=1).to(device)\n",
    "# pretrained_path = r'C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run5_200_teacher_rettrain\\teacher_net_sr_final_199.pth'\n",
    "# pretrained = torch.load(pretrained_path, map_location=device, weights_only=False)\n",
    "# sr_net.load_state_dict(pretrained)\n",
    "\n",
    "# Remove tail weights that do not match the new ×2 architecture\n",
    "# for key in list(pretrained.keys()):\n",
    "#     if key.startswith('tail.2') or key.startswith('tail.3'):\n",
    "#         del pretrained[key]\n",
    "\n",
    "# Load partial weights into the new model\n",
    "# teacher_net.eval()\n",
    "# for param in teacher_net.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "\n",
    "# Apply Xavier initialization to the new tail.2 Conv2d (output 12 channels for PixelShuffle(2))\n",
    "# with torch.no_grad():\n",
    "#     nn.init.xavier_uniform_(teacher_net.tail[2].weight)\n",
    "#     nn.init.zeros_(teacher_net.tail[2].bias)\n",
    "# teacher_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7bfb70c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:09.212793Z",
     "iopub.status.busy": "2025-05-02T08:44:09.212125Z",
     "iopub.status.idle": "2025-05-02T08:44:09.571463Z",
     "shell.execute_reply": "2025-05-02T08:44:09.570829Z"
    },
    "papermill": {
     "duration": 0.381462,
     "end_time": "2025-05-02T08:44:09.572596",
     "exception": false,
     "start_time": "2025-05-02T08:44:09.191134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_net_path = r\"/kaggle/input/reside-dehaze-from-student/pytorch/default/1/net_haze_iter_85.pth\"\n",
    "pretrained_net = torch.load(pretrained_net_path, map_location=device, weights_only=False)\n",
    "net.load_state_dict(pretrained_net, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "184b085b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:09.628655Z",
     "iopub.status.busy": "2025-05-02T08:44:09.628414Z",
     "iopub.status.idle": "2025-05-02T08:44:35.008859Z",
     "shell.execute_reply": "2025-05-02T08:44:35.008003Z"
    },
    "papermill": {
     "duration": 25.409871,
     "end_time": "2025-05-02T08:44:35.010198",
     "exception": false,
     "start_time": "2025-05-02T08:44:09.600327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Init Val] PSNR: 24.51, SSIM: 0.8613\n",
      "[SR Init Val] PSNR: 6.57, SSIM: -0.0153\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Feature Affinity Module --- #\n",
    "fam = FeatureAffinityModule(channels=16).to(device)\n",
    "ssfm_loss = SSFM(loss_type='l1') \n",
    "\n",
    "\n",
    "# --- Initial Validation --- #\n",
    "old_val_psnr, old_val_ssim = validationB(net, val_data_loader, device, category)\n",
    "print(f\"[Dehazing Init Val] PSNR: {old_val_psnr:.2f}, SSIM: {old_val_ssim:.4f}\")\n",
    "if sr_enabled:\n",
    "    sr_val_psnr, sr_val_ssim = validation_sr(sr_net, sr_val_loader, device)\n",
    "    print(f\"[SR Init Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "# --- Training Loop --- #\n",
    "best_psnr = old_val_psnr\n",
    "best_psnr_sr = sr_val_psnr\n",
    "train_psnr_prev = 0\n",
    "distillation_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "edefa145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:35.052379Z",
     "iopub.status.busy": "2025-05-02T08:44:35.051602Z",
     "iopub.status.idle": "2025-05-02T08:44:35.063030Z",
     "shell.execute_reply": "2025-05-02T08:44:35.062523Z"
    },
    "papermill": {
     "duration": 0.03321,
     "end_time": "2025-05-02T08:44:35.064173",
     "exception": false,
     "start_time": "2025-05-02T08:44:35.030963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "sr_net = sr_net.to(device)\n",
    "fam = fam.to(device)\n",
    "loss_network = loss_network.to(device)\n",
    "ssfm_loss = ssfm_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b728015",
   "metadata": {
    "papermill": {
     "duration": 0.019778,
     "end_time": "2025-05-02T08:44:35.104199",
     "exception": false,
     "start_time": "2025-05-02T08:44:35.084421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3ba1ee57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T08:44:35.145674Z",
     "iopub.status.busy": "2025-05-02T08:44:35.145151Z",
     "iopub.status.idle": "2025-05-02T09:49:39.216126Z",
     "shell.execute_reply": "2025-05-02T09:49:39.215216Z"
    },
    "papermill": {
     "duration": 3906.298249,
     "end_time": "2025-05-02T09:49:41.422747",
     "exception": false,
     "start_time": "2025-05-02T08:44:35.124498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 08:44:37.067181: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746175477.256172      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746175477.316324      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - net saved in epoch 0.\n",
      "[Dehazing Val] PSNR: 24.32, SSIM: 0.8584\n",
      "(396s) Epoch [1/10], Train_PSNR:21.84, Val_PSNR:24.32, Val_SSIM:0.8584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 24.32, SSIM: 0.8593\n",
      "(377s) Epoch [2/10], Train_PSNR:21.84, Val_PSNR:24.32, Val_SSIM:0.8593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 24.18, SSIM: 0.8569\n",
      "(383s) Epoch [3/10], Train_PSNR:21.87, Val_PSNR:24.18, Val_SSIM:0.8569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 24.19, SSIM: 0.8575\n",
      "(385s) Epoch [4/10], Train_PSNR:21.92, Val_PSNR:24.19, Val_SSIM:0.8575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 24.23, SSIM: 0.8586\n",
      "(387s) Epoch [5/10], Train_PSNR:21.97, Val_PSNR:24.23, Val_SSIM:0.8586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - net saved in epoch 5.\n",
      "[Dehazing Val] PSNR: 24.48, SSIM: 0.8614\n",
      "(387s) Epoch [6/10], Train_PSNR:22.35, Val_PSNR:24.48, Val_SSIM:0.8614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 24.44, SSIM: 0.8611\n",
      "(387s) Epoch [7/10], Train_PSNR:22.40, Val_PSNR:24.44, Val_SSIM:0.8611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 24.59, SSIM: 0.8619\n",
      "(390s) Epoch [8/10], Train_PSNR:22.45, Val_PSNR:24.59, Val_SSIM:0.8619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 24.49, SSIM: 0.8622\n",
      "(393s) Epoch [9/10], Train_PSNR:22.48, Val_PSNR:24.49, Val_SSIM:0.8622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dehazing Val] PSNR: 24.57, SSIM: 0.8633\n",
      "(404s) Epoch [10/10], Train_PSNR:22.51, Val_PSNR:24.57, Val_SSIM:0.8633\n",
      "Model - net saved in epoch 9.\n",
      "🏁 Training completed in 64.84 minutes.\n",
      "TensorBoard logs saved in: runs/net_haze_1\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# Create a directory for TensorBoard logs\n",
    "log_dir = \"runs/net_haze_1\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Initialize global variables\n",
    "global_iter = 0\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Optimizer for net only\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# LR scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# --- Training Loop --- #\n",
    "best_psnr = old_val_psnr\n",
    "train_psnr_prev = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    psnr_list = []\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # Training loop with tqdm progress bar\n",
    "    train_loader_tqdm = tqdm(train_data_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "    for batch_id, (haze, gt) in enumerate(train_loader_tqdm):\n",
    "        haze, gt = haze.to(device), gt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass - student (net)\n",
    "        dehaze, base, s = net(haze)\n",
    "\n",
    "        # Student losses\n",
    "        base_loss = F.smooth_l1_loss(base, gt)\n",
    "        smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "        perceptual_loss = loss_network(base, gt)\n",
    "        total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        psnr_list.extend(to_psnr(dehaze, gt))\n",
    "        train_loader_tqdm.set_postfix(loss=total_loss.item(), iter=global_iter)\n",
    "        global_iter += 1\n",
    "\n",
    "    train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "    # Save models every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        iter_model_path = f\"net_haze_iter_{epoch}.pth\"\n",
    "        torch.save(net.state_dict(), iter_model_path)\n",
    "        print(f\"Model - net saved in epoch {epoch}.\")\n",
    "\n",
    "    # === Validation ===\n",
    "    net.eval()\n",
    "    val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "    print(f\"[Dehazing Val] PSNR: {val_psnr:.2f}, SSIM: {val_ssim:.4f}\")\n",
    "    writer.add_scalar(\"Metrics/Val_PSNR_Dehaze\", val_psnr, epoch)\n",
    "    writer.add_scalar(\"Metrics/Val_SSIM_Dehaze\", val_ssim, epoch)\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    model_path = f\"net_haze_{version}.pth\"\n",
    "    print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "    # Learning rate scheduler based on validation PSNR\n",
    "    scheduler.step(val_psnr)\n",
    "\n",
    "    # Best model save\n",
    "    if val_psnr >= best_psnr:\n",
    "        best_model_path = f\"net_haze_best_{version}.pth\"\n",
    "        torch.save(net.state_dict(), best_model_path)\n",
    "        best_psnr = val_psnr\n",
    "\n",
    "    train_psnr_prev = train_psnr\n",
    "\n",
    "# Final save\n",
    "final_path = f\"net_final_{epoch}.pth\"\n",
    "torch.save(net.state_dict(), final_path)\n",
    "print(f\"Model - net saved in epoch {epoch}.\")\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"🏁 Training completed in {total_time/60:.2f} minutes.\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "print(\"TensorBoard logs saved in:\", log_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebfd096",
   "metadata": {},
   "source": [
    "Model available at: https://www.kaggle.com/code/curiousmohammed/reside-dehaze-student-only/output?scriptVersionId=237359791 | contact @curiousmohammed in kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84771b51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:49:45.561283Z",
     "iopub.status.busy": "2025-05-02T09:49:45.560692Z",
     "iopub.status.idle": "2025-05-02T09:49:45.566762Z",
     "shell.execute_reply": "2025-05-02T09:49:45.566149Z"
    },
    "papermill": {
     "duration": 2.07294,
     "end_time": "2025-05-02T09:49:45.568593",
     "exception": false,
     "start_time": "2025-05-02T09:49:43.495653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Create a directory for TensorBoard logs\n",
    "# log_dir = \"runs/co_distillation_logs\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# # TensorBoard writer\n",
    "# writer = SummaryWriter(log_dir)\n",
    "\n",
    "# # Initialize global variables\n",
    "# global_iter = 0\n",
    "# total_start_time = time.time()\n",
    "\n",
    "# # Optimizer for net and teacher_net (joint training)\n",
    "# optimizer = torch.optim.Adam(list(net.parameters()) + list(sr_net.parameters()), lr=learning_rate)\n",
    "\n",
    "# # LR scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# # Apply Xavier initialization to the tail of the teacher_net (for PixelShuffle)\n",
    "# with torch.no_grad():\n",
    "#     nn.init.xavier_uniform_(sr_net.tail[2].weight)\n",
    "#     nn.init.zeros_(sr_net.tail[2].bias)\n",
    "\n",
    "# # --- Feature Affinity Module and SSFM loss --- #\n",
    "# fam = FeatureAffinityModule(channels=16).to(device)\n",
    "# ssfm_loss = SSFM(loss_type='l1')\n",
    "\n",
    "# # --- Training Loop --- #\n",
    "# best_psnr = old_val_psnr\n",
    "# best_psnr_sr = sr_val_psnr\n",
    "# train_psnr_prev = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_start_time = time.time()\n",
    "#     psnr_list = []\n",
    "\n",
    "#     net.train()\n",
    "#     sr_net.train()\n",
    "\n",
    "#     # Training loop with tqdm progress bar\n",
    "#     train_loader_tqdm = tqdm(train_data_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "#     for batch_id, (haze, gt) in enumerate(train_loader_tqdm):\n",
    "#         haze, gt = haze.to(device), gt.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass - student (net)\n",
    "#         dehaze, base, s = net(haze)\n",
    "#         s1, s2, s3, s4 = s\n",
    "\n",
    "#         # Student losses\n",
    "#         base_loss = F.smooth_l1_loss(base, gt)\n",
    "#         smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "#         perceptual_loss = loss_network(dehaze, gt)\n",
    "#         total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "\n",
    "#         # Teacher SR logic (if SR is enabled)\n",
    "#         if sr_enabled:\n",
    "#             try:\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             except StopIteration:\n",
    "#                 sr_iter = iter(sr_train_loader)\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n",
    "\n",
    "#             # Teacher forward (SR)\n",
    "#             sr_out, _, t = sr_net(sr_lr)\n",
    "#             sr_hr = F.interpolate(sr_hr, size=sr_out.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "#             sr_loss = F.l1_loss(sr_out, sr_hr)\n",
    "#             total_loss += sr_loss\n",
    "\n",
    "#             # SSFM loss (for feature matching between student and teacher)\n",
    "#             s_loss = ssfm_loss(s, t)\n",
    "#             total_loss += s_loss\n",
    "\n",
    "#             # Distillation loss\n",
    "#             distillation_loss = fam(dehaze, sr_out)\n",
    "#             total_loss += distillation_loss\n",
    "\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         psnr_list.extend(to_psnr(dehaze, gt))\n",
    "#         train_loader_tqdm.set_postfix(loss=total_loss.item(), iter=global_iter)\n",
    "#         global_iter += 1\n",
    "\n",
    "#     train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "#     # Save models every 5 epochs\n",
    "#     if epoch % 5 == 0:\n",
    "#         iter_model_path = f\"net_haze_iter_{epoch}.pth\"\n",
    "#         torch.save(net.state_dict(), iter_model_path)\n",
    "#         print(f\"Model - net saved in epoch {epoch}.\")\n",
    "#         if sr_enabled:\n",
    "#             iter_model_path = f\"teacher_net_haze_iter_{epoch}.pth\"\n",
    "#             torch.save(sr_net.state_dict(), iter_model_path)\n",
    "#             print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "#     # === Validation ===\n",
    "#     net.eval()\n",
    "#     val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "#     print(f\"[Dehazing Val] PSNR: {val_psnr:.2f}, SSIM: {val_ssim:.4f}\")\n",
    "#     writer.add_scalar(\"Metrics/Val_PSNR_Dehaze\", val_psnr, epoch)\n",
    "#     writer.add_scalar(\"Metrics/Val_SSIM_Dehaze\", val_ssim, epoch)\n",
    "\n",
    "#     # Optional SR validation (if enabled)\n",
    "#     if sr_enabled:\n",
    "#         sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#         print(f\"[SR Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "#         writer.add_scalar(\"Metrics/Val_PSNR_SR\", sr_val_psnr, epoch)\n",
    "#         writer.add_scalar(\"Metrics/Val_SSIM_SR\", sr_val_ssim, epoch)\n",
    "\n",
    "#     epoch_duration = time.time() - epoch_start_time\n",
    "#     model_path = f\"net_haze_{version}.pth\"\n",
    "#     print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "#     # Learning rate scheduler based on validation PSNR\n",
    "#     scheduler.step(val_psnr)\n",
    "\n",
    "#     # Best model save\n",
    "#     if val_psnr >= best_psnr:\n",
    "#         best_model_path = f\"net_haze_best_{version}.pth\"\n",
    "#         torch.save(net.state_dict(), best_model_path)\n",
    "#         best_psnr = val_psnr\n",
    "\n",
    "#     # SR best model save (if enabled)\n",
    "#     if sr_enabled and sr_val_psnr >= best_psnr_sr:\n",
    "#         best_sr_model_path = f\"teacher_net_haze_best_sr_{version}.pth\"\n",
    "#         torch.save(sr_net.state_dict(), best_sr_model_path)\n",
    "#         best_psnr_sr = sr_val_psnr\n",
    "\n",
    "#     train_psnr_prev = train_psnr\n",
    "\n",
    "# # Final save\n",
    "# final_path = f\"net_final_{epoch}.pth\"\n",
    "# torch.save(net.state_dict(), final_path)\n",
    "# print(f\"Model - net saved in epoch {epoch}.\")\n",
    "# if sr_enabled:\n",
    "#     final_path = f\"teacher_net_final_{epoch}.pth\"\n",
    "#     torch.save(sr_net.state_dict(), final_path)\n",
    "#     print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "# total_time = time.time() - total_start_time\n",
    "# print(f\"🏁 Training completed in {total_time/60:.2f} minutes.\")\n",
    "\n",
    "# # Close the TensorBoard writer\n",
    "# writer.close()\n",
    "# print(\"TensorBoard logs saved in:\", log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf747a5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:49:49.901271Z",
     "iopub.status.busy": "2025-05-02T09:49:49.900987Z",
     "iopub.status.idle": "2025-05-02T09:49:49.906642Z",
     "shell.execute_reply": "2025-05-02T09:49:49.906104Z"
    },
    "papermill": {
     "duration": 2.1782,
     "end_time": "2025-05-02T09:49:49.907692",
     "exception": false,
     "start_time": "2025-05-02T09:49:47.729492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Create a directory for TensorBoard logs\n",
    "# log_dir = \"runs/co_distillation_logs\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# # TensorBoard writer\n",
    "# writer = SummaryWriter(log_dir)\n",
    "\n",
    "# # Initialize global variables\n",
    "# global_iter = 0\n",
    "# total_start_time = time.time()\n",
    "\n",
    "# # Optimizer for net and teacher_net (joint training)\n",
    "# optimizer = torch.optim.Adam(list(net.parameters()) + list(sr_net.parameters()), lr=learning_rate)\n",
    "\n",
    "# # LR scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# # Apply Xavier initialization to the tail of the teacher_net (for PixelShuffle)\n",
    "# with torch.no_grad():\n",
    "#     nn.init.xavier_uniform_(sr_net.tail[2].weight)\n",
    "#     nn.init.zeros_(sr_net.tail[2].bias)\n",
    "\n",
    "# # --- Feature Affinity Module and SSFM loss --- #\n",
    "# fam = FeatureAffinityModule(channels=16).to(device)\n",
    "# ssfm_loss = SSFM(loss_type='l1')\n",
    "\n",
    "# # --- Training Loop --- #\n",
    "# best_psnr = old_val_psnr\n",
    "# best_psnr_sr = sr_val_psnr\n",
    "# train_psnr_prev = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_start_time = time.time()\n",
    "#     psnr_list = []\n",
    "\n",
    "#     net.train()\n",
    "#     sr_net.train()\n",
    "\n",
    "#     # Training loop with tqdm progress bar\n",
    "#     train_loader_tqdm = tqdm(train_data_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "#     for batch_id, (haze, gt) in enumerate(train_loader_tqdm):\n",
    "#         haze, gt = haze.to(device), gt.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass - student (net)\n",
    "#         dehaze, base, s = net(haze)\n",
    "#         s1, s2, s3, s4 = s\n",
    "\n",
    "#         # Student losses\n",
    "#         base_loss = F.smooth_l1_loss(base, gt)\n",
    "#         smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "#         perceptual_loss = loss_network(dehaze, gt)\n",
    "#         total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "\n",
    "#         # Teacher SR logic (if SR is enabled)\n",
    "#         if sr_enabled:\n",
    "#             try:\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             except StopIteration:\n",
    "#                 sr_iter = iter(sr_train_loader)\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n",
    "\n",
    "#             # Teacher forward (SR)\n",
    "#             sr_out, _, t = sr_net(sr_lr)\n",
    "#             sr_hr = F.interpolate(sr_hr, size=sr_out.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "#             sr_loss = F.l1_loss(sr_out, sr_hr)\n",
    "#             total_loss += sr_loss\n",
    "\n",
    "#             # SSFM loss (for feature matching between student and teacher)\n",
    "#             s_loss = ssfm_loss(s, t)\n",
    "#             total_loss += s_loss\n",
    "\n",
    "#             # Distillation loss\n",
    "#             distillation_loss = fam(dehaze, sr_out)\n",
    "#             total_loss += distillation_loss\n",
    "\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         psnr_list.extend(to_psnr(dehaze, gt))\n",
    "#         train_loader_tqdm.set_postfix(loss=total_loss.item(), iter=global_iter)\n",
    "#         global_iter += 1\n",
    "\n",
    "#     train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "#     # Save models every 5 epochs\n",
    "#     if epoch % 5 == 0:\n",
    "#         iter_model_path = f\"net_haze_iter_{epoch}.pth\"\n",
    "#         torch.save(net.state_dict(), iter_model_path)\n",
    "#         print(f\"Model - net saved in epoch {epoch}.\")\n",
    "#         if sr_enabled:\n",
    "#             iter_model_path = f\"teacher_net_haze_iter_{epoch}.pth\"\n",
    "#             torch.save(sr_net.state_dict(), iter_model_path)\n",
    "#             print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "#     # === Validation ===\n",
    "#     net.eval()\n",
    "#     val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "#     print(f\"[Dehazing Val] PSNR: {val_psnr:.2f}, SSIM: {val_ssim:.4f}\")\n",
    "#     writer.add_scalar(\"Metrics/Val_PSNR_Dehaze\", val_psnr, epoch)\n",
    "#     writer.add_scalar(\"Metrics/Val_SSIM_Dehaze\", val_ssim, epoch)\n",
    "\n",
    "#     # Optional SR validation (if enabled)\n",
    "#     if sr_enabled:\n",
    "#         sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#         print(f\"[SR Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "#         writer.add_scalar(\"Metrics/Val_PSNR_SR\", sr_val_psnr, epoch)\n",
    "#         writer.add_scalar(\"Metrics/Val_SSIM_SR\", sr_val_ssim, epoch)\n",
    "\n",
    "#     epoch_duration = time.time() - epoch_start_time\n",
    "#     model_path = f\"net_haze_{version}.pth\"\n",
    "#     print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "#     # Learning rate scheduler based on validation PSNR\n",
    "#     scheduler.step(val_psnr)\n",
    "\n",
    "#     # Best model save\n",
    "#     if val_psnr >= best_psnr:\n",
    "#         best_model_path = f\"net_haze_best_{version}.pth\"\n",
    "#         torch.save(net.state_dict(), best_model_path)\n",
    "#         best_psnr = val_psnr\n",
    "\n",
    "#     # SR best model save (if enabled)\n",
    "#     if sr_enabled and sr_val_psnr >= best_psnr_sr:\n",
    "#         best_sr_model_path = f\"teacher_net_haze_best_sr_{version}.pth\"\n",
    "#         torch.save(sr_net.state_dict(), best_sr_model_path)\n",
    "#         best_psnr_sr = sr_val_psnr\n",
    "\n",
    "#     train_psnr_prev = train_psnr\n",
    "\n",
    "# # Final save\n",
    "# final_path = f\"net_final_{epoch}.pth\"\n",
    "# torch.save(net.state_dict(), final_path)\n",
    "# print(f\"Model - net saved in epoch {epoch}.\")\n",
    "# if sr_enabled:\n",
    "#     final_path = f\"teacher_net_final_{epoch}.pth\"\n",
    "#     torch.save(sr_net.state_dict(), final_path)\n",
    "#     print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "# total_time = time.time() - total_start_time\n",
    "# print(f\"🏁 Training completed in {total_time/60:.2f} minutes.\")\n",
    "\n",
    "# # Close the TensorBoard writer\n",
    "# writer.close()\n",
    "# print(\"TensorBoard logs saved in:\", log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "90ae05e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:49:54.039593Z",
     "iopub.status.busy": "2025-05-02T09:49:54.039330Z",
     "iopub.status.idle": "2025-05-02T09:49:54.042357Z",
     "shell.execute_reply": "2025-05-02T09:49:54.041825Z"
    },
    "papermill": {
     "duration": 2.048721,
     "end_time": "2025-05-02T09:49:54.043443",
     "exception": false,
     "start_time": "2025-05-02T09:49:51.994722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# validation_sr(sr_net , sr_val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "302795c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:49:58.243139Z",
     "iopub.status.busy": "2025-05-02T09:49:58.242577Z",
     "iopub.status.idle": "2025-05-02T09:50:29.343519Z",
     "shell.execute_reply": "2025-05-02T09:50:29.342711Z"
    },
    "papermill": {
     "duration": 35.324964,
     "end_time": "2025-05-02T09:50:31.420331",
     "exception": false,
     "start_time": "2025-05-02T09:49:56.095367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.57146820747286, 0.8633150042841832)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationB(net, val_data_loader, device, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f371cb70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:50:35.721970Z",
     "iopub.status.busy": "2025-05-02T09:50:35.721652Z",
     "iopub.status.idle": "2025-05-02T09:50:35.726249Z",
     "shell.execute_reply": "2025-05-02T09:50:35.725680Z"
    },
    "papermill": {
     "duration": 2.169662,
     "end_time": "2025-05-02T09:50:35.727341",
     "exception": false,
     "start_time": "2025-05-02T09:50:33.557679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# global_iter = 0\n",
    "# total_start_time = time.time()\n",
    "\n",
    "# # Joint optimizer for net and teacher_net\n",
    "# optimizer = torch.optim.Adam(list(net.parameters()) + list(sr_net.parameters()), lr=learning_rate)\n",
    "\n",
    "# # Initialize ReduceLROnPlateau scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_start_time = time.time()\n",
    "#     psnr_list = []\n",
    "\n",
    "#     net.train()\n",
    "#     if sr_enabled:\n",
    "#         sr_net.train()\n",
    "\n",
    "#     train_loader_tqdm = tqdm(train_data_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "#     for batch_id, (haze, gt) in enumerate(train_loader_tqdm):\n",
    "#         haze, gt = haze.to(device), gt.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass - student\n",
    "#         dehaze, base, s = net(haze)\n",
    "#         s1, s2, s3, s4 = s\n",
    "\n",
    "#         # Student losses\n",
    "#         base_loss = F.smooth_l1_loss(base, gt)\n",
    "#         smooth_loss = F.smooth_l1_loss(dehaze, gt)\n",
    "#         perceptual_loss = loss_network(dehaze, gt)\n",
    "#         total_loss = smooth_loss + lambda_loss * perceptual_loss + base_loss\n",
    "\n",
    "#         # Teacher SR logic\n",
    "#         if sr_enabled:\n",
    "#             try:\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             except StopIteration:\n",
    "#                 sr_iter = iter(sr_train_loader)\n",
    "#                 sr_lr, sr_hr = next(sr_iter)\n",
    "#             sr_lr, sr_hr = sr_lr.to(device), sr_hr.to(device)\n",
    "\n",
    "#             # Teacher forward\n",
    "#             sr_out, _, t = sr_net(sr_lr)\n",
    "\n",
    "#             sr_loss = F.l1_loss(sr_out, sr_hr)\n",
    "#             total_loss += sr_loss\n",
    "\n",
    "#             s_loss = ssfm_loss(s, t)\n",
    "#             total_loss += s_loss\n",
    "\n",
    "#             distillation_loss = fam(dehaze, sr_out)\n",
    "#             total_loss += distillation_loss\n",
    "\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         psnr_list.extend(to_psnr(dehaze, gt))\n",
    "#         train_loader_tqdm.set_postfix(loss=total_loss.item(), iter=global_iter)\n",
    "#         global_iter += 1\n",
    "\n",
    "#     train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "#     # Save model every 5 epochs\n",
    "#     if epoch % 5 == 0:\n",
    "#         iter_model_path = f\"net_haze_iter_{epoch}.pth\"\n",
    "#         torch.save(net.state_dict(), iter_model_path)\n",
    "#         print(f\"Model - net saved in epoch {epoch}.\")\n",
    "#         if sr_enabled:\n",
    "#             iter_model_path = f\"teacher_net_haze_iter_{epoch}.pth\"\n",
    "#             torch.save(sr_net.state_dict(), iter_model_path)\n",
    "#             print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "#     # Validation\n",
    "#     net.eval()\n",
    "#     val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "#     if sr_enabled:\n",
    "#         sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#         print(f\"[SR Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")\n",
    "\n",
    "#     epoch_duration = time.time() - epoch_start_time\n",
    "#     model_path = f\"net_haze_{version}.pth\"\n",
    "#     print_log(epoch + 1, num_epochs, epoch_duration, train_psnr, val_psnr, val_ssim, model_path)\n",
    "\n",
    "#     elapsed = time.time() - total_start_time\n",
    "#     epochs_left = num_epochs - (epoch + 1)\n",
    "#     avg_epoch_time = elapsed / (epoch + 1)\n",
    "#     eta = avg_epoch_time * epochs_left\n",
    "#     print(f\"✅ Epoch [{epoch+1}/{num_epochs}] completed in {epoch_duration:.2f}s — ETA for {epochs_left} more: ~{eta/60:.2f} min\")\n",
    "\n",
    "#     # Adjust the learning rate based on validation PSNR\n",
    "#     scheduler.step(val_psnr)\n",
    "\n",
    "#     # Best model save\n",
    "#     if val_psnr >= best_psnr:\n",
    "#         best_model_path = f\"net_haze_best_{version}.pth\"\n",
    "#         torch.save(net.state_dict(), best_model_path)\n",
    "#         best_psnr = val_psnr\n",
    "\n",
    "#     train_psnr_prev = train_psnr\n",
    "\n",
    "# # Final save\n",
    "# final_path = f\"net_final_{epoch}.pth\"\n",
    "# torch.save(net.state_dict(), final_path)\n",
    "# print(f\"Model - net saved in epoch {epoch}.\")\n",
    "# if sr_enabled:\n",
    "#     final_path = f\"teacher_net_final_{epoch}.pth\"\n",
    "#     torch.save(sr_net.state_dict(), final_path)\n",
    "#     print(f\"Model - teacher_net saved in epoch {epoch}.\")\n",
    "\n",
    "# total_time = time.time() - total_start_time\n",
    "# print(f\"🏁 Training completed in {total_time/60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f7ec82d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:50:39.891912Z",
     "iopub.status.busy": "2025-05-02T09:50:39.891193Z",
     "iopub.status.idle": "2025-05-02T09:50:39.895123Z",
     "shell.execute_reply": "2025-05-02T09:50:39.894487Z"
    },
    "papermill": {
     "duration": 2.116547,
     "end_time": "2025-05-02T09:50:39.896332",
     "exception": false,
     "start_time": "2025-05-02T09:50:37.779785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # --- Final Validation --- #\n",
    "# net.eval()\n",
    "# val_psnr, val_ssim = validationB(net, val_data_loader, device, category)\n",
    "# print(f\"[Dehazing Final Val] PSNR: {val_psnr:.2f}, SSIM: {val_ssim:.4f}\")\n",
    "# if sr_enabled:\n",
    "#     sr_val_psnr, sr_val_ssim = validation_sr(net, sr_val_loader, device)\n",
    "#     print(f\"[SR Final Val] PSNR: {sr_val_psnr:.2f}, SSIM: {sr_val_ssim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "64ab32d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:50:44.137124Z",
     "iopub.status.busy": "2025-05-02T09:50:44.136529Z",
     "iopub.status.idle": "2025-05-02T09:50:44.139904Z",
     "shell.execute_reply": "2025-05-02T09:50:44.139353Z"
    },
    "papermill": {
     "duration": 2.151156,
     "end_time": "2025-05-02T09:50:44.140984",
     "exception": false,
     "start_time": "2025-05-02T09:50:41.989828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initialize model\n",
    "# # model_path = r\"C:\\Users\\abd\\d\\ai\\dehaze\\teacher_net_sr_final_24.pth\"\n",
    "# model_path = r\"C:\\Users\\abd\\d\\ai\\dehaze\\teacher_net_sr_iter_100.pth\"\n",
    "# # model = DehazingNet().to(device)\n",
    "# teacher_net.load_state_dict(torch.load(model_path, map_location=device, weights_only=False), strict=False)\n",
    "# # net.load_state_dict(torch.load(model_path, map_location=device))\n",
    "# # net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "da19c2b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:50:48.242545Z",
     "iopub.status.busy": "2025-05-02T09:50:48.241870Z",
     "iopub.status.idle": "2025-05-02T09:50:48.246263Z",
     "shell.execute_reply": "2025-05-02T09:50:48.245682Z"
    },
    "papermill": {
     "duration": 2.055372,
     "end_time": "2025-05-02T09:50:48.247347",
     "exception": false,
     "start_time": "2025-05-02T09:50:46.191975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Visualize using test sr_test_loader\n",
    "# # model_path1 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\teacher_net_sr_final_199.pth\"\n",
    "# # model_path2 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run2\\teacher_net_sr_best_0.pth\"\n",
    "# # Load models\n",
    "# # sr_net.load_state_dict(torch.load(model_path1, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net1 = sr_net\n",
    "\n",
    "# # sr_net.load_state_dict(torch.load(model_path2, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net2 = net\n",
    "\n",
    "# # Iterate through test loader\n",
    "# for i, (lr, hr) in enumerate(sr_test_loader):\n",
    "#     print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "#     lr, hr = lr.to(device), hr.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         sr_out1, _, _ = teacher_net1(lr)\n",
    "#         sr_out2, _, _ = teacher_net2(lr)\n",
    "#         print(f\"SR Output shape: {sr_out1.shape}\")\n",
    "#         print(f\"SR Output shape: {sr_out2.shape}\")\n",
    "#         # Downsample to match the size of hr\n",
    "#         sr_out1 = F.interpolate(sr_out1, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "#         # sr_out2 = F.interpolate(sr_out2, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "#         # Plot using matplotlib\n",
    "#         fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "#         axes[0].imshow(lr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[0].set_title(\"Input (Low-Res)\")\n",
    "#         axes[0].axis(\"off\")\n",
    "        \n",
    "#         axes[1].imshow(hr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[1].set_title(\"Ground Truth\")\n",
    "#         axes[1].axis(\"off\")\n",
    "\n",
    "#         axes[2].imshow(sr_out1[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[2].set_title(\"Model Path 1 Output\")\n",
    "#         axes[2].axis(\"off\")\n",
    "\n",
    "#         axes[3].imshow(sr_out2[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[3].set_title(\"Model Path 2 Output\")\n",
    "#         axes[3].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     if i == 4:  # Just to limit the output\n",
    "#         break\n",
    "# # do same for dehaze\n",
    "# # Iterate through test loader\n",
    "# for i, (haze, gt) in enumerate(test_data_loader):\n",
    "#     print(f\"Batch {i}: Haze shape: {haze.shape}, GT shape: {gt.shape}\")\n",
    "#     haze, gt = haze.to(device), gt.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         dehaze, _, _ = net(haze)\n",
    "#         print(f\"Dehaze Output shape: {dehaze.shape}\")\n",
    "        \n",
    "#         # Plot using matplotlib\n",
    "#         fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "#         axes[0].imshow(haze[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[0].set_title(\"Input (Hazy)\")\n",
    "#         axes[0].axis(\"off\")\n",
    "        \n",
    "#         axes[1].imshow(gt[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[1].set_title(\"Ground Truth\")\n",
    "#         axes[1].axis(\"off\")\n",
    "\n",
    "#         axes[2].imshow(dehaze[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[2].set_title(\"Dehazed Output\")\n",
    "#         axes[2].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     if i == 4:  # Just to limit the output\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b8810fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:50:52.476393Z",
     "iopub.status.busy": "2025-05-02T09:50:52.476141Z",
     "iopub.status.idle": "2025-05-02T09:50:52.480240Z",
     "shell.execute_reply": "2025-05-02T09:50:52.479528Z"
    },
    "papermill": {
     "duration": 2.064794,
     "end_time": "2025-05-02T09:50:52.481328",
     "exception": false,
     "start_time": "2025-05-02T09:50:50.416534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Visualize using test sr_test_loader\n",
    "# model_path1 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run2\\teacher_net_sr_iter_100.pth\"\n",
    "# model_path2 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run2\\teacher_net_sr_best_0.pth\"\n",
    "# # Load models\n",
    "# sr_net.load_state_dict(torch.load(model_path1, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net1 = sr_net\n",
    "\n",
    "# sr_net.load_state_dict(torch.load(model_path2, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net2 = sr_net\n",
    "\n",
    "# # Iterate through test loader\n",
    "# for i, (lr, hr) in enumerate(sr_test_loader):\n",
    "#     print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "#     lr, hr = lr.to(device), hr.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         sr_out1, _, _ = teacher_net1(lr)\n",
    "#         sr_out2, _, _ = teacher_net2(lr)\n",
    "#         print(f\"SR Output shape: {sr_out1.shape}\")\n",
    "#         print(f\"SR Output shape: {sr_out2.shape}\")\n",
    "#         # Downsample to match the size of hr\n",
    "#         sr_out1 = F.interpolate(sr_out1, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "#         # sr_out2 = F.interpolate(sr_out2, size=hr.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "#         # Plot using matplotlib\n",
    "#         fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "#         axes[0].imshow(lr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[0].set_title(\"Input (Low-Res)\")\n",
    "#         axes[0].axis(\"off\")\n",
    "        \n",
    "#         axes[1].imshow(hr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[1].set_title(\"Ground Truth\")\n",
    "#         axes[1].axis(\"off\")\n",
    "\n",
    "#         axes[2].imshow(sr_out1[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[2].set_title(\"Model Path 1 Output\")\n",
    "#         axes[2].axis(\"off\")\n",
    "\n",
    "#         axes[3].imshow(sr_out2[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[3].set_title(\"Model Path 2 Output\")\n",
    "#         axes[3].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     if i == 4:  # Just to limit the output\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "61ac1bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:50:56.688425Z",
     "iopub.status.busy": "2025-05-02T09:50:56.688149Z",
     "iopub.status.idle": "2025-05-02T09:50:56.692217Z",
     "shell.execute_reply": "2025-05-02T09:50:56.691483Z"
    },
    "papermill": {
     "duration": 2.091717,
     "end_time": "2025-05-02T09:50:56.693459",
     "exception": false,
     "start_time": "2025-05-02T09:50:54.601742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Visualize using test sr_test_loader\n",
    "# model_path1 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\models_1\\run2\\teacher_net_sr_iter_100.pth\"\n",
    "# model_path2 = r\"C:\\Users\\abd\\d\\ai\\dehaze\\teacher_net_sr_iter_5.pth\"\n",
    "# # Load models\n",
    "# sr_net.load_state_dict(torch.load(model_path1, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net1 = sr_net\n",
    "# sr_net.load_state_dict(torch.load(model_path2, map_location=device, weights_only=False), strict=False)\n",
    "# teacher_net2 = sr_net\n",
    "\n",
    "# # Iterate through test loader\n",
    "# for i, (lr, hr) in enumerate(sr_test_loader):\n",
    "#     print(f\"Batch {i}: LR shape: {lr.shape}, HR shape: {hr.shape}\")\n",
    "#     lr, hr = lr.to(device), hr.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         sr_out1, _, _ = teacher_net1(lr)\n",
    "#         sr_out2, _, _ = teacher_net2(lr)\n",
    "        \n",
    "#         # Plot using matplotlib\n",
    "#         fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "#         axes[0].imshow(lr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[0].set_title(\"Input (Low-Res)\")\n",
    "#         axes[0].axis(\"off\")\n",
    "        \n",
    "#         axes[1].imshow(hr[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[1].set_title(\"Ground Truth\")\n",
    "#         axes[1].axis(\"off\")\n",
    "\n",
    "#         axes[2].imshow(sr_out1[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[2].set_title(\"Model Path 1 Output\")\n",
    "#         axes[2].axis(\"off\")\n",
    "\n",
    "#         axes[3].imshow(sr_out2[0].cpu().permute(1, 2, 0).numpy())\n",
    "#         axes[3].set_title(\"Model Path 2 Output\")\n",
    "#         axes[3].axis(\"off\")\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "#     if i == 4:  # Just to limit the output\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bf6607c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:51:00.905445Z",
     "iopub.status.busy": "2025-05-02T09:51:00.905175Z",
     "iopub.status.idle": "2025-05-02T09:51:00.908869Z",
     "shell.execute_reply": "2025-05-02T09:51:00.908195Z"
    },
    "papermill": {
     "duration": 2.160617,
     "end_time": "2025-05-02T09:51:00.910098",
     "exception": false,
     "start_time": "2025-05-02T09:50:58.749481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # LOAD TEST DATA\n",
    "# # -----------------------------\n",
    "# # check is exec env is local or kaggle\n",
    "# if execution_env == 'kaggle':\n",
    "#     test_hazy_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/hazy\"\n",
    "#     test_gt_dir = \"/kaggle/input/reside6k/RESIDE-6K/test/GT\"\n",
    "# else:\n",
    "#     test_hazy_dir = \"./dehaze/dataset/RESIDE-6K/test/hazy\"\n",
    "#     test_gt_dir = \"./dehaze/dataset/RESIDE-6K/test/GT\"\n",
    "#     # test_hazy_dir = \"/Volumes/S/dev/project/code/Aphase/dehaze/dataset/RESIDE-6K/test/hazy\"\n",
    "#     # test_gt_dir = \"/Volumes/S/dev/project/code/Aphase/dehaze/dataset/RESIDE-6K/test/GT\"\n",
    "\n",
    "# # test_hazy_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/IN\"\n",
    "# # test_gt_dir = \"/kaggle/input/nh-dense-haze/NH-HAZE-T/NH-HAZE-T/GT\"\n",
    "\n",
    "# hazy_images = sorted(glob.glob(os.path.join(test_hazy_dir, \"*.*\")))\n",
    "# gt_images = sorted(glob.glob(os.path.join(test_gt_dir, \"*.*\")))\n",
    "\n",
    "# transform = Compose([\n",
    "#     ToTensor(),\n",
    "#     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# to_pil = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f75cfcb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:51:05.119555Z",
     "iopub.status.busy": "2025-05-02T09:51:05.118827Z",
     "iopub.status.idle": "2025-05-02T09:51:05.122938Z",
     "shell.execute_reply": "2025-05-02T09:51:05.122252Z"
    },
    "papermill": {
     "duration": 2.063169,
     "end_time": "2025-05-02T09:51:05.124208",
     "exception": false,
     "start_time": "2025-05-02T09:51:03.061039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [11, 12, 13,14]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth \")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "31603c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:51:09.234372Z",
     "iopub.status.busy": "2025-05-02T09:51:09.234100Z",
     "iopub.status.idle": "2025-05-02T09:51:09.238061Z",
     "shell.execute_reply": "2025-05-02T09:51:09.237358Z"
    },
    "papermill": {
     "duration": 2.067111,
     "end_time": "2025-05-02T09:51:09.239309",
     "exception": false,
     "start_time": "2025-05-02T09:51:07.172198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [70, 75, 90, 100]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c22bcfe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T09:51:13.570197Z",
     "iopub.status.busy": "2025-05-02T09:51:13.569639Z",
     "iopub.status.idle": "2025-05-02T09:51:13.573504Z",
     "shell.execute_reply": "2025-05-02T09:51:13.572801Z"
    },
    "papermill": {
     "duration": 2.066221,
     "end_time": "2025-05-02T09:51:13.574719",
     "exception": false,
     "start_time": "2025-05-02T09:51:11.508498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # INFERENCE & VISUALIZATION FOR SPECIFIC IMAGES\n",
    "# # -----------------------------\n",
    "# image_indices = [1, 3, 5]  # Indices of images to visualize\n",
    "\n",
    "# plt.figure(figsize=(10, len(image_indices) * 5))\n",
    "\n",
    "# for idx, i in enumerate(image_indices):\n",
    "#     hazy_img = Image.open(hazy_images[i+1])\n",
    "#     gt_img = Image.open(gt_images[i+1])\n",
    "\n",
    "#     # Transform for model input\n",
    "#     input_tensor = transform(hazy_img).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Inference\n",
    "#     with torch.no_grad():\n",
    "#         res = net(input_tensor)\n",
    "#         print(res[0].shape)\n",
    "#         output_tensor = res[0].cpu().squeeze(0)\n",
    "\n",
    "#     # Convert back to image\n",
    "#     output_img = to_pil(output_tensor)\n",
    "\n",
    "#     # Display results\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 1)\n",
    "#     plt.imshow(hazy_img)\n",
    "#     plt.title(f\"Hazy Input {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 2)\n",
    "#     plt.imshow(output_img)\n",
    "#     plt.title(f\"Dehazed Output {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.subplot(len(image_indices), 3, 3 * idx + 3)\n",
    "#     plt.imshow(gt_img)\n",
    "#     plt.title(f\"Ground Truth {i}\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7302936,
     "sourceId": 11638847,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7303093,
     "sourceId": 11639061,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 325527,
     "modelInstanceId": 305084,
     "sourceId": 368260,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 326116,
     "modelInstanceId": 305661,
     "sourceId": 369089,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4074.920524,
   "end_time": "2025-05-02T09:51:19.334360",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-02T08:43:24.413836",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0c23dd75cf2c4e7f97bb92d9944366e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1251f9cd8fb34356a9385a580b1b6a3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DropdownModel",
       "_options_labels": [
        "indoor",
        "outdoor",
        "reside",
        "nh"
       ],
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "DropdownView",
       "description": "Category:",
       "description_allow_html": false,
       "disabled": false,
       "index": 2,
       "layout": "IPY_MODEL_55555aad1d48429f872c14f2a5816c9f",
       "style": "IPY_MODEL_4d0e8e7743b6439898fd608174f4fb3a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "1ed58cd5d5944422b512da63b432bedb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "TextView",
       "continuous_update": true,
       "description": "Crop Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_e8c6189f74954199b5f5902c9293d913",
       "placeholder": "​",
       "style": "IPY_MODEL_d7ac1681acdd4e3bbfd15a5893806925",
       "tabbable": null,
       "tooltip": null,
       "value": "128,128"
      }
     },
     "209d839a7136459b82d574c1e08a6f6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "21ca3bf7448c4da3bfc60e35cbae8ffe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Val Batch Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_8e96b6cb5d6d4091b126fe3f36c563ac",
       "step": 1,
       "style": "IPY_MODEL_8d904275ef864927b8d2890359e1a3b6",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "351355333b1d4471804957545da9430e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "44f4f741abcc4ad9b12d58c8ac4bc81e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4d0e8e7743b6439898fd608174f4fb3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "55555aad1d48429f872c14f2a5816c9f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a5caaf4a5844e6091ba791198feb642": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "FloatTextView",
       "continuous_update": false,
       "description": "Lambda Loss:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_44f4f741abcc4ad9b12d58c8ac4bc81e",
       "step": null,
       "style": "IPY_MODEL_0c23dd75cf2c4e7f97bb92d9944366e0",
       "tabbable": null,
       "tooltip": null,
       "value": 0.04
      }
     },
     "6eb5240921324163a7d5d8003d248347": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "73ca0eca7a5b4c57b06dd242c7d9d70c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DropdownModel",
       "_options_labels": [
        "local",
        "kaggle"
       ],
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "DropdownView",
       "description": "Execution Env:",
       "description_allow_html": false,
       "disabled": false,
       "index": 1,
       "layout": "IPY_MODEL_ad29f1f0d52542eb9152c009b68806ad",
       "style": "IPY_MODEL_6eb5240921324163a7d5d8003d248347",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7e67d4f8dd7742c8b04398e2ed8fdb82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "86c2570ca678400bbbd7f58390953c9c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d904275ef864927b8d2890359e1a3b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8e96b6cb5d6d4091b126fe3f36c563ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8ec0e1b4b73f4525bc84535ebf99b2fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Version:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_cc6e6a735ff747619e1098124565efb2",
       "step": 1,
       "style": "IPY_MODEL_351355333b1d4471804957545da9430e",
       "tabbable": null,
       "tooltip": null,
       "value": 0
      }
     },
     "ab57bdaf1a3f47c4b11d34289de24a32": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad29f1f0d52542eb9152c009b68806ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4d7a1da20674d48b23b22feb7df641e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "FloatTextView",
       "continuous_update": false,
       "description": "Learning Rate:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_ab57bdaf1a3f47c4b11d34289de24a32",
       "step": null,
       "style": "IPY_MODEL_f58f361bf97e4417b88e7959cd05f441",
       "tabbable": null,
       "tooltip": null,
       "value": 0.0001
      }
     },
     "bf2a0b0961c246a899a926e2e8b82c1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Growth Rate:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_86c2570ca678400bbbd7f58390953c9c",
       "step": 1,
       "style": "IPY_MODEL_7e67d4f8dd7742c8b04398e2ed8fdb82",
       "tabbable": null,
       "tooltip": null,
       "value": 16
      }
     },
     "c5b0d6c7851d41e2b351adcdb89ce540": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c863d9efb9db46c49df8d9615dd70485": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "IntTextModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "IntTextModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "IntTextView",
       "continuous_update": false,
       "description": "Train Batch Size:",
       "description_allow_html": false,
       "disabled": false,
       "layout": "IPY_MODEL_209d839a7136459b82d574c1e08a6f6a",
       "step": 1,
       "style": "IPY_MODEL_c5b0d6c7851d41e2b351adcdb89ce540",
       "tabbable": null,
       "tooltip": null,
       "value": 2
      }
     },
     "cc6e6a735ff747619e1098124565efb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d7ac1681acdd4e3bbfd15a5893806925": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "TextStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e8c6189f74954199b5f5902c9293d913": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f58f361bf97e4417b88e7959cd05f441": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
